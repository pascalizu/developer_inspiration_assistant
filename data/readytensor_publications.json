[
  {
    "id": "adaptive-autonomous-ai-co-browser-xaHHBdNBf7sr",
    "username": null,
    "license": null,
    "title": null,
    "publication_description": "Failed after 3 retries",
    "awards": [],
    "elements": {}
  },
  {
    "id": "adaptivefuzz-llm-based-adaptive-fuzzing-for-vulnerability-discovery-1ea3egPQ1Ow7",
    "username": "Vigneshwar Sundararajan",
    "license": "MIT License",
    "title": "AdaptiveFuzz: LLM-Based Adaptive Fuzzing for Vulnerability Discovery",
    "publication_description": "Back to publications\nOct 20, 2025\n●\n2 reads\n●\nMIT License\nAdaptiveFuzz: LLM-Based Adaptive Fuzzing for Vulnerability Discovery\nAAIDC\nAAIDC2025\nAgenticAI\nCybersecurity\nFastMCP\nLanggraph\nMCP\nVigneshwar Sundararajan\nLike\nBookmark\nShare\nIntroduction\n\nThis project, AdaptiveFuzz, presents an LLM-powered multi-agent framework designed to automate the reconnaissance phase of penetration testing. The system emulates the adaptive, step-by-step reasoning of a human red-team operator by moving beyond static scripts. It leverages a team of five specialized agents orchestrated by LangGraph to intelligently plan tasks, execute them using custom tools via the Model Context Protocol (MCP), analyze findings, and propose new strategies. A critical Human-in-the-Loop (HITL) component ensures a human operator retains final approval over the \"adaptive\" workflow, directing the system's next steps.\n\nThis project fulfills the Module 2 requirements for the Agentic AI Developer Certification by demonstrating multi-agent collaboration, advanced tool integration, and orchestration.\n\nArchitecting Multi-Agent Systems\n\nThe Challenge: Static vs. Adaptive Reconnaissance\n\nTraditional penetration testing often relies on static scripts or manual, predefined patterns. The true skill of a human red-team operator, however, is their adaptive, step-by-step reasoning. They run a tool, analyze the results, form a new hypothesis, and then choose their next tool and target based on those findings.\n\nThe goal of AdaptiveFuzz is to emulate this human-centric, adaptive workflow, moving beyond static scripts to create an intelligent assistant for security professionals.\n\nThe Solution: An Orchestrated Agent Team\n\nAdaptiveFuzz functions as a team of specialized AI agents. It intelligently plans enumeration tasks, executes them by calling real security tools, analyzes the results, and proposes new strategies. Crucially, it keeps a human operator in the loop for final approval before proceeding.\n\nThis entire system is built on two key technologies:\n\nLangGraph: The entire multi-agent workflow is orchestrated using LangGraph. Each agent and step in the reconnaissance process is a node in a stateful graph, making the system modular, debuggable, and capable of complex, cyclical reasoning.\n\nModel Context Protocol (MCP): MCP serves as the communication bridge that allows the LangGraph agents to interact with the actual security tools. The project uses FastMCP to serve Python-based tools, which the agents can then call to scan targets and gather live information.\n\nArchitecture: The Agentic Workflow\n\nThe system's logic is defined in code/graph.py and operates as a team of five specialized agents, plus one human control point.\n\n---\nconfig:\n  flowchart:\n    curve: CARDINAL\n---\ngraph TD;\n\t__start__([<p>__start__</p>]):::first\n\tconversational_handler(conversational_handler)\n\trecon_executor(recon_executor)\n\tweb_analyzer(web_analyzer)\n\tresult_interpreter(result_interpreter)\n\tstrategy_advisor(strategy_advisor)\n\thuman_in_loop(human_in_loop)\n\t__end__([<p>__end__</p>]):::last\n\t__start__ --> conversational_handler;\n\tconversational_handler -. &nbsp;review&nbsp; .-> human_in_loop;\n\tconversational_handler -. &nbsp;proceed&nbsp; .-> recon_executor;\n\thuman_in_loop -. &nbsp;stop&nbsp; .-> __end__;\n\thuman_in_loop -. &nbsp;continue&nbsp; .-> recon_executor;\n\trecon_executor --> web_analyzer;\n\tresult_interpreter --> strategy_advisor;\n\tstrategy_advisor --> human_in_loop;\n\tweb_analyzer --> result_interpreter;\n\tclassDef default fill:,line-height:1.2\n\tclassDef first fill-opacity:0\n\tclassDef last fill:\n\nConversational Handler: The entry point. This agent receives the high-level goal from the user (e.g., \"Scan this target IP for web vulnerabilities\") and decomposes it into a list of specific, executable tasks.\nRecon Executor: This agent takes the task list and executes each one by calling the appropriate MCP tools (like port_scanner or banner_grabber) to gather raw data from the target.\nWeb Analyzer: The raw tool output (e.g., \"Port 80 open, banner: Apache/2.4.18\") is passed to this agent. It uses the web_search tool to gather external context, such as identifying service versions or searching for known CVEs.\nResult Interpreter: This agent synthesizes all the information—both the raw scan results and the web search context—into a clean, human-readable list of concise security findings.\nStrategy Advisor: Based on the unified findings, this agent acts like a senior penetration tester. It reviews the situation and proposes exactly three prioritized, strategic next steps to continue the enumeration.\nThe \"Adaptive\" Loop: Human-in-the-Loop (HITL)\n\nThe process does not auto-run. The three strategies proposed by the Strategy Advisor are presented to the human operator.\n\nThis Human-in-the-Loop node is the critical control point. The system pauses its execution (using langgraph.types.interrupt) and waits for the user to choose which strategy to pursue (1, 2, or 3) or to type 'stop' to end the test.\n\nIf the user chooses a strategy, the graph routes back to the Recon Executor to run the new tasks. This iterative cycle of Execute -> Analyze -> Strategize -> Get Human Approval is what makes the system truly \"adaptive\".\n\nCustom Tool Integration\n\nThe system's capabilities are extended beyond the LLM's knowledge by integrating three custom tools via FastMCP:\n\nport_scanner: A custom tool built with Python's socket library to scan a target IP for a list of open ports.\nbanner_grabber: Connects to open ports to grab service banners, helping to identify running software.\nweb_search: A tool that performs a Google web search using aiohttp to gather external intelligence on findings.\nConclusion\n\nAdaptiveFuzz demonstrates a practical, powerful implementation of a multi-agent system. By combining the stateful orchestration of LangGraph, the real-world tool-use of MCP, and the critical reasoning of a human-in-the-loop, it creates an adaptive framework that effectively assists with the complex, dynamic process of cybersecurity reconnaissance.\n\nYou can explore the project here: \nAdaptiveFuzz on GitHub\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nIntroduction\n\nThe Challenge: Static vs. Adaptive Reconnaissance\n\nThe Solution: An Orchestrated Agent Team\n\nArchitecture: The Agentic Workflow\n\nThe \"Adaptive\" Loop: Human-in-the-Loop (HITL)\n\nCustom Tool Integration\n\nConclusion\n\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "advanced-image-captioning-transformer-with-reinforcement-learning-optimization-MPM1xO3YGPuo",
    "username": "dDaniel Plotkin",
    "license": null,
    "title": "Advanced Image Captioning Transformer with Reinforcement Learning Optimization",
    "publication_description": "Back to publications\nDec 22, 2024\n●\n233 reads\n●\nCreative Commons Attribution (CC BY)\nWinner of\nMost Promising Innovation\nat the\nComputer Vision Projects Expo 2024\nAdvanced Image Captioning Transformer with Reinforcement Learning Optimization\nbeam search\ncomputer vision\nimage captioning\nmulti-modal learning\nnlp\npolicy gradients\nreinforcement learning\nself-critical sequence training\ntransformers\nD\nDaniel Plotkin\nLike\nBookmark\nShare\n1. Abstract\n\nGenerating high-quality captions for images is a critical challenge in AI, requiring advancements at the intersection of computer vision and natural language processing. This study introduces a novel image captioning pipeline that integrates the CPTR (Full Transformer Network) architecture with Self-critical Sequence Training (SCST) for optimization. By using a two-step training process, we demonstrate substantial improvements in caption quality, as measured by the METEOR score. This work highlights the potential of combining state-of-the-art transformers with reinforcement learning techniques to address complex computer vision tasks.\n\n2. Introduction\n\nImage captioning, the task of generating textual descriptions for visual content, bridges the gap between vision and language understanding. While traditional models have achieved notable success in optimizing differentiable objectives, they often fall short when it comes to non-differentiable evaluation metrics such as METEOR and BLEU. These metrics, which better reflect human evaluation criteria, remain challenging to optimize directly with standard training approaches. Addressing this limitation is essential for improving alignment between model-generated captions and human judgment.\n\nThis study presents a two-step training pipeline that leverages the strengths of:\n\nCPTR, a transformer-based architecture with a Vision Transformer (ViT) encoder and transformer decoder.\nSCST, a reinforcement learning technique that directly optimizes non-differentiable metrics.\n\nWe hypothesize that reinforcement learning can significantly enhance alignment between model-generated captions and human evaluation metrics. The contributions of this work are as follows:\n\nImplementation of a CPTR-based baseline model for image captioning.\nOptimization of caption quality using SCST with METEOR as the reward signal.\nEvaluation on the Flickr8K dataset, showing noticeable improvements in METEOR score.\n3. Related Work\n3.1 Transformer Architectures in Image Captioning\n\nThe transformer model, initially proposed by Vaswani et al. (2017), has become a cornerstone of NLP and computer vision tasks. CPTR [1], a fully transformer-based image captioning model, eliminates the need for convolutional backbones by utilizing a Vision Transformer encoder.\n\n3.2 Reinforcement Learning for Captioning\n\nSCST [2], introduced by Rennie et al., revolutionized image captioning by enabling direct optimization of evaluation metrics. Unlike supervised learning, SCST trains models to generate captions that maximize rewards such as CIDEr, BLEU, or METEOR.\n\n4. Methodology\n4.1 CPTR Architecture\n\nOur baseline model employs CPTR, which integrates:\n\nA Vision Transformer (ViT) encoder that processes images into sequences of visual tokens.\nA Transformer decoder that generates captions by attending to these visual tokens.\n\n4.2 SCST Optimization\n\nIn the second training phase, we apply SCST to optimize captions using the METEOR score as a reward signal. The SCST loss function is defined as:\n\nwhere:\n\n: model parameters,\n: sampled sequence,\n: reward (METEOR score),\n: baseline reward (e.g., greedy-decoded sequence).\n\nSCST encourages the model to generate captions with higher rewards than the baseline.\n\n5. Loss Function\n5.1 Baseline Loss: Zero-masked Categorical Cross Entropy\n\nThe baseline training uses:\n\nwhere  is the conditional probability of the target token  given previous tokens and image features.\n\n5.2 SCST Loss\n\nSCST optimizes METEOR directly, as defined in Section 4.2.\n\n6. Dataset\n\nWe evaluate our pipeline on the \nFlickr8K dataset\n, which contains:\n\nTraining: 23,890 image-caption pairs.\nValidation: 5,975 image-caption pairs.\nTesting: 7,470 image-caption pairs.\n7. Experimental Setup\n7.1 Platform and Tools\nHardware: Colab L4 GPU.\nPretrained Components:\nTokenizer: distilbert-base-uncased\nVision Encoder: google/vit-base-patch16-384.\n7.2 Training Details\nBaseline Training\nWarmup-cosine decaying learning rate schedule.\nEarly stopping based on validation loss.\nEpochs: 15, stopped at epoch 10.\n\n\nFigure: Baseline Training and Validation Loss by Epoch\n\n\nFigure: Baseline Training and Validation Accuracy by Epoch\n\nSCST Optimization\nTrain for 8 epochs.\nInitial learning rate: . We then decay the learning rate by 0.5 for the remaining epochs.\nBeam search (beam size: 3) for caption generation.\n8. Results\n8.1 Baseline Performance\n\nWe use a batched single-reference meteor score to evaluate our baseline and post-SCST captions on our test set.\n\nMetric\tScore\nSingle METEOR\t0.276\n8.2 Post-SCST Optimization\nMetric\tBefore SCST\tAfter SCST\nSingle METEOR\t0.276\t0.301\n8.3 Final Evaluation Metrics (Beam Search)\n\nWe use beam search to decode our captions for final evaluation and generation, with a beam size of 3. A simple normalized function is used as the score function for beam search. It is defined as:\n\nMETEOR\tBLEU 1\tBLEU 2\tBLEU 3\tBLEU 4\n0.4659\t0.5528\t0.4006\t0.2714\t0.1743\n9. Discussion\n9.1 Strengths\nAlignment with Human Evaluation: Optimizing METEOR ensures captions better reflect human judgment.\nArchitecture Flexibility: CPTR allows seamless integration of transformer advancements.\n9.2 Limitations\nHardware Constraints: Limited by computational power and memory on machine, limiting the number of training epochs and decoder layers.\nDecoder Training: A pre-trained decoder with a deeper architecture could improve results and training time.\nLarger Dataset: Training on Flickr30k or MSCOCO would improve training size and generalization.\n9.3 Future Work\nTrain on larger datasets with high-performance hardware.\nIncorporate pre-trained decoders to boost performance.\nGain access to stronger computational hardware to allow longer SCST training.\n10. Conclusion\n\nThis study demonstrates the synergy between transformers and reinforcement learning in image captioning. By aligning model outputs with human evaluation metrics, the proposed pipeline achieves significant improvements in caption quality, providing a foundation for further research.\n\n11. References\n\n[1] Wei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu, and Jing Liu. \"CPTR: Full Transformer Network for Image Captioning.\" CoRR, vol. abs/2101.10804, 2021. Available: \nhttps://arxiv.org/abs/2101.10804\n.\n\n[2] Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. \"Self-critical Sequence Training for Image Captioning.\" CoRR, vol. abs/1612.00563, 2016. Available: \nhttp://arxiv.org/abs/1612.00563\n.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nAbstract\nIntroduction\nRelated Work\n\n3.1 Transformer Architectures in Image Captioning\n\n3.2 Reinforcement Learning for Captioning\n\nMethodology\n\n4.1 CPTR Architecture\n\n4.2 SCST Optimization\n\nLoss Function\n\n5.1 Baseline Loss: Zero-masked Categorical Cross Entropy\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nYou might be interested\nTransformVision: AI-Powered Caption Generation\nDec 10, 202449 reads\n#AttentionMechanisam#FastAPI+3\nImage Captioning with Multiple Decoder Architectures\nO\nDec 23, 202449 reads\nAIComputer Vision+5\nPLEDGE: Paragraph LEvel image Description GEneration\nS\nDec 24, 202412 reads\nCross-modalityEnd-to-end Model+2\nImage Captioning Model using CNN and LSTM\nZ\nDec 28, 202419 reads\nCNNDeep Learning+3",
    "awards": [
      "most promising innovation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "advancing-autonomous-driving-deep-learning-innovations-for-enhanced-perception-and-decisionmaking-EFEFMQSUNvvm",
    "username": "sPol Navarro Solà",
    "license": null,
    "title": "Advancing Autonomous Driving: Deep Learning Innovations for Enhanced Perception and Decision-Making",
    "publication_description": "Back to publications\nDec 27, 2024\n●\n121 reads\n●\nApache 2.0\nWinner of\nBest Overall Project\nat the\nComputer Vision Projects Expo 2024\nAdvancing Autonomous Driving: Deep Learning Innovations for Enhanced Perception and Decision-Making\nAI\nComputer Vision\nDeep Learning\nML\nMultiModal\nVLM\nS\nPol Navarro Solà\nLike\nBookmark\nShare\nAdvancing Autonomous Driving: Deep Learning Innovations for Enhanced Perception and Decision-Making\n\nFigure 0. Real Time detection\n\nAbstract\n\nAutonomous driving systems rely heavily on robust object detection and perception to ensure safety and efficiency. Traditional methods often struggle to address the challenges posed by dynamic environments and varying conditions. This publication introduces a comprehensive approach leveraging state-of-the-art deep learning techniques to enhance perception and decision-making capabilities in autonomous systems.\n\nKey contributions include the application of YOLOv8 for 2D object detection and VoxelNeXt for 3D object detection, alongside the integration of simulation tools like the CARLA simulator for real-time evaluation. These innovations address core challenges, such as object recognition in complex scenarios, by utilizing tailored datasets like Mapillary Traffic Sign and ONCE. Results demonstrate significant advancements in detection precision and real-world applicability.\n\nThis work sets a foundation for future enhancements in Advanced Driver Assistance Systems (ADAS) and autonomous vehicles, emphasizing scalability, real-time performance, and integration of multimodal data.\n\n1. Introduction\n\nThe field of autonomous driving has witnessed remarkable advancements through the integration of deep learning. However, challenges like real-time object detection, dynamic environments, and scalability persist. This research aims to address these issues through innovative approaches in 2D and 3D object detection and visualization.\n\nKey innovations of this study include:\n\nDevelopment and training of YOLOv8 on the Mapillary Traffic Sign dataset for robust 2D object detection.\nImplementation of VoxelNeXt within the OpenPCDet framework, leveraging the ONCE dataset for accurate 3D object recognition.\nCreation of real-time simulation visualizations using the CARLA simulator to evaluate model performance dynamically.\nVisualization of deep learning concepts through Manim animations to enhance understanding and engagement.\nGeneration of comprehensive plots and metrics for detailed performance analysis.\n\nThese contributions demonstrate the potential of advanced deep learning architectures to improve perception in autonomous driving, paving the way for safer and more efficient systems.\n\n2. Related Works\n\nObject detection and perception in autonomous driving is a critical area of research, focusing on automating decision-making processes and enhancing safety. Traditional methods initially relied on rule-based image processing techniques, but the rise of machine learning and deep learning has significantly advanced the field. This section explores prior research efforts, comparing traditional methods, state-of-the-art architectures, and their applications in 2D and 3D detection tasks.\n\nTraditional Image Processing Techniques\n\nEarly approaches to object detection relied on classical image processing techniques, such as edge detection, thresholding, and feature extraction. These methods were computationally lightweight but often struggled with environmental variability, such as occlusions and lighting conditions. Techniques like histogram equalization and contour-based region detection offered incremental improvements but lacked generalization across diverse scenarios.\n\nAdvances in Deep Learning for Object Detection\n\nThe introduction of deep learning revolutionized object detection, enabling robust performance in complex environments. Several architectures have been pivotal:\n\nYOLO (You Only Look Once):\nRedmon et al. (2016) introduced YOLO, a unified model capable of real-time detection.\nSuccessive iterations, including YOLOv8, have focused on balancing speed and accuracy, achieving state-of-the-art performance in 2D detection tasks.\nVoxel-based 3D Detection:\nVoxelNet and its successors, like VoxelNeXt, leverage voxelized representations of LiDAR data for accurate spatial recognition.\nIntegration within frameworks like OpenPCDet has facilitated scalable and high-performing 3D object detection.\nHybrid Approaches:\nMethods combining 2D and 3D data streams, such as PointPillars, optimize perception pipelines by fusing RGB and LiDAR inputs.\nEmerging multi-sensor fusion techniques demonstrate improved detection under challenging conditions.\nEvaluation Metrics and Comparative Analysis\n\nStandard metrics, including Precision, Recall, IoU, and F1 Score, are employed to evaluate detection models. Comparative analyses reveal:\n\nYOLO-based models excel in real-time applications, achieving up to 90% precision on datasets like Mapillary Traffic Sign.\nVoxelNeXt achieves superior performance in 3D tasks, with IoU thresholds exceeding 0.7 in benchmark tests using the ONCE dataset.\nChallenges and Future Directions\n\nWhile deep learning offers substantial improvements, challenges such as scalability, real-time processing, and multimodal integration remain. Research continues to explore lightweight architectures and efficient training methodologies to address these limitations.\n\n3. Methodology\n3.1 Mapillary Dataset Description\n\nThe dataset utilized in this study consists of 52,000 fully annotated images captured from various driving scenarios. Each 2D image is paired with bounding box annotations specifically for traffic signs.\n\nThe dataset includes over 300 traffic sign classes, each with bounding box annotations, making it highly suitable for traffic sign detection tasks. It has a global geographic reach, with images and traffic sign classes covering six continents, ensuring a broad representation of real-world driving conditions. Additionally, the dataset features a variety of weather conditions, seasons, times of day, as well as diverse camera types and viewpoints, offering comprehensive coverage of various environmental and situational factors.\n\nDataset Summary:\n\nCharacteristic\tDetail\nTotal Images\t100,000\nFully Annotated Images\t52,453\nPartially Annotated Images\t47,547\nResolution\t1080p+ (High-resolution images)\nTotal Classes (Traffic Signs)\t401\nTotal Bounding Boxes\t257,543\n\nFigure 1. Mapillary Dataset Class Labelling\n\nExample Annotations: This visual representation highlights the meticulous annotation process that characterizes the Mapillary Traffic Sign Dataset (MTSD), which is critical for training accurate and reliable object detection models. The dataset’s extensive and varied data make it an indispensable resource for advancing traffic sign detection capabilities, ultimately contributing to the development of safer and more reliable autonomous driving systems.\n\n3.2 Mapillary Data Preprocessing\n\nInitially, the Mapillary Dataset posed a challenge due to its non-standard format, making it incompatible with pre-trained models. To address this issue, a script was created to convert the dataset into the YOLOv8 format, commonly used in object detection tasks. This conversion allowed the dataset to be properly integrated into the model training process, ensuring compatibility with YOLOv8.\n\nClass Imbalance and Data Augmentation\n\nUpon inspecting the dataset, a significant class imbalance was identified, particularly the dominance of the “other-sign” class, which accounted for more than half of the total 200,000 distinct annotations. This imbalance resulted in a model with high recall (correctly identifying many signs) but low precision (misclassifying many signs as “other-sign” due to its overrepresentation).\n\nFigure 2. Mapillary Dataset Histogram\n\nTo address this, the following techniques were applied:\n\nData Augmentation:\n\nMosaic and Mixup were employed to increase the diversity of samples in the Convolutional Neural Network (CNN) training process. These augmentation techniques generate new training examples by combining different images or regions of images.\nMosaic combines multiple images into one, allowing the model to learn from more varied contexts.\nMixup blends two images and their labels, providing a more diverse set of training examples, helping the model generalize better.\nThese techniques were expected to help reduce class imbalance, especially for the \"other-sign\" class.\n\nRegion Cropping and Focused Augmentation:\n\nTo further improve balance, an image cropping algorithm was designed to crop the regions defined by the bounding boxes for each traffic sign class. This method allowed the focus to be on individual objects (traffic signs) rather than the entire image, resulting in more balanced and meaningful training samples.\nHowever, cropping the images often led to the cropped objects occupying the entire frame, which could prevent the model from learning to detect objects in more complex, real-world scenes. This presented a new challenge: ensuring the model still learned to detect objects within diverse scenes.\n\nAspect Ratio Preservation:\n\nAspect ratio preservation is crucial for traffic signs, as their shapes are key to identification. To ensure the model could maintain the correct proportions of the objects, the images were scaled while retaining their original aspect ratios.\nAny extra space created by scaling was filled with a black background, ensuring that the traffic signs remained centered and properly proportioned. This also allowed the model to learn both object classification (identifying what the object is) and localization (determining where the object is) while preserving the integrity of the sign shapes.\nThe labels were adjusted to exclude the black background, ensuring that only the relevant portions of the image were considered during training.\n\nData Normalization:\n\nNormalization of the pixel values may have been applied to ensure that the image data was standardized, with pixel values typically scaled to a range such as [0, 1]. This step ensures that all images contribute equally to the model training, helping the model converge faster and improving overall performance.\n\nFinal Adjustments and Training Preparation\n\nAfter applying data augmentation, cropping, and aspect ratio normalization, the dataset was stratified and rebalanced to ensure the model had a more even distribution of traffic sign classes. This helped mitigate the class imbalance and provided a better basis for training.\n\nThe final dataset, with its properly scaled images, rebalanced classes, and focused augmentation, was then ready for training. By addressing the initial dataset issues and ensuring that the model could learn both to classify and localize objects, the overall performance of the model in recognizing and detecting traffic signs in real-world environments significantly improved.\n\n3.3 ONCE Dataset Description\n\nThe ONCE (One Million Scenes) Dataset was selected for this research due to its comprehensive collection of autonomous driving scenarios, aimed at training and evaluating 3D perception models. This dataset offers rich, multi-modal data, including point clouds from LiDAR, camera images, and radar signals, making it a highly valuable resource for advancing autonomous vehicle technology.\n\nWith a focus on 3D scene understanding, the ONCE dataset provides high-quality data from real-world driving scenarios, supporting the development of models that accurately perceive and react to complex road environments. The dataset’s detailed annotations are critical for 3D object detection, segmentation, and tracking, which are essential for ensuring the safety and reliability of autonomous driving systems.\n\nDataset Summary:\n\nCharacteristic\tDetail\nTotal Scenes\t1,000,000\nAnnotations\t3D Bounding Boxes\nSensors\tLiDAR, Camera, Radar\nObject Categories\tCars, Pedestrians, Cyclists, Trucks, etc.\nEnvironmental Diversity\tUrban, Highway, Rural, Various Weather, Day/Night\n\nFigure 3. Example Scene from ONCE Dataset\n\nExample Annotations: The ONCE dataset includes 3D bounding boxes that highlight the locations of various objects like cars, pedestrians, and cyclists. These precise annotations enable the training of models capable of understanding spatial relationships and dynamics in complex environments, which are vital for the development of autonomous systems.\n\n3.4 ONCE Data Preprocessing\n\nThe ONCE Dataset, with its rich multi-modal data, required preprocessing steps to integrate its diverse data types and format them for effective model training. The following preprocessing steps were applied:\n\n1. Data Fusion:\n\nThe ONCE dataset includes multiple sensor modalities (LiDAR, camera, radar), which were fused to create a unified representation of each scene. The fusion process ensures that the model can leverage complementary information from all sensors to enhance object detection accuracy.\n\n2. Voxelization:\n\nTo leverage the 3D data provided by LiDAR, voxelization was applied to convert the raw point cloud data into a structured voxel grid. This transformation allows the model to process 3D spatial information more efficiently and enables accurate object detection in 3D space.\n\n3. Bounding Box Transformation:\n\nThe 3D bounding boxes were used to annotate the objects in the dataset. These boxes were adjusted to maintain the integrity of object shapes while ensuring compatibility with the voxelized representation. This step was essential to ensure that the model could learn both the object classification and the precise 3D localization of each object.\n\n4. Data Augmentation:\n\nRandom Rotation and Flipping: These augmentation techniques were applied to increase the variety of training samples and help the model generalize better to different viewpoints and orientations.\nNoise Addition: LiDAR point clouds were augmented by introducing controlled noise to simulate different environmental conditions, helping the model become more robust to real-world challenges.\nScaling and Translation: To ensure that the model could handle objects at various distances, the dataset underwent scaling and translation operations to simulate objects at different sizes and locations within the 3D space.\n\n5. Data Normalization:\n\nThe pixel values for camera images and the LiDAR data were normalized to a consistent range to ensure uniformity across the dataset. This step helps the model converge faster during training and improves overall performance.\n\nFinal Adjustments and Training Preparation:\n\nAfter preprocessing, the dataset was split into training, validation, and test sets to evaluate the model’s performance in different stages.\nThe data was carefully balanced, particularly ensuring that less-represented classes had adequate representation through augmentation techniques. This step is crucial for addressing any potential class imbalance that could affect model accuracy.\n3.5 Model Architectures\nYOLOv8 for 2D Detection\n\nYOLOv8 is selected for real-time 2D object detection tasks primarily due to its lightweight architecture and fast inference times, making it highly suitable for applications like autonomous driving, where real-time performance is crucial. YOLOv8 is the latest iteration of the You Only Look Once (YOLO) family of models, designed to address the need for both speed and accuracy in detecting objects within images. The model’s efficiency and high accuracy are essential when detecting traffic signs, where real-time analysis is required to ensure safe and effective navigation.\n\nSpeed and Efficiency\nYOLO has always been known for its real-time performance, making it a preferred choice for autonomous systems. YOLO processes images in a single pass, significantly faster than traditional two-stage models like Faster R-CNN, which split the detection process into separate proposal and classification steps. The ability to perform both object localization and classification in a single network makes YOLOv8 highly efficient and suitable for environments that require low latency, such as autonomous driving and real-time surveillance.\n\n \n\nIn addition, YOLOv8 offers improved speed compared to previous YOLO versions, capable of processing images at 45 to 155 frames per second (fps) depending on the model version and hardware. This capability ensures that the model can operate in real-time, an essential feature when deploying object detection models for fast-moving objects like traffic signs and vehicles.\n\nLightweight Architecture with High Accuracy\nThe YOLOv8 architecture has been optimized for both speed and accuracy. The key components of YOLOv8 include:\n\nBackbone: CSPDarkNet - a lightweight yet powerful backbone designed to efficiently extract features from images without compromising performance.\nNeck: PANet (Path Aggregation Network) - a feature aggregation network that enhances the ability to capture context across different layers, improving the model’s ability to handle objects of various scales.\nHead: YOLOv8’s head handles bounding box and class predictions, making it effective for 2D detection tasks like traffic sign detection.\n\nYOLOv8’s flexible design enables it to work across different hardware platforms, from low-power edge devices to high-performance GPUs. This adaptability is particularly important when deploying in varying environments with different hardware constraints.\n\nSimplified Detection Pipeline\nUnlike traditional models that require multiple stages for object detection, YOLOv8 performs detection in a single, unified pipeline. This integrated approach reduces complexity and computational overhead, which leads to faster inference times. The traditional multi-stage models often involve separate components for region proposal generation, classification, and bounding box regression. However, YOLO’s streamlined method directly predicts bounding box coordinates and class probabilities in a single step, making it more efficient for real-time applications.\n\nGrid-based Approach\nYOLOv8 divides an input image into a grid of cells, where each cell predicts bounding boxes and class probabilities for the objects it contains. This grid-based spatial approach simplifies the detection task, allowing each grid cell to focus on a smaller region of the image, reducing overall processing time. Although the grid-based approach could struggle with detecting small objects or objects spanning multiple cells, YOLOv8 mitigates this challenge by incorporating anchor boxes and using multiple grid scales, improving the model’s ability to handle various object sizes.\n\nDirect Bounding Box Regression\nAnother advantage of YOLOv8 is its direct bounding box regression, meaning the model directly predicts bounding box coordinates and class labels from the grid cells. This contrasts with models like Faster R-CNN, which require an additional region proposal network to hypothesize potential bounding box locations before classification. By simplifying the detection pipeline and predicting bounding boxes alongside class probabilities, YOLOv8 achieves faster inference times and more consistent predictions.\n\nReal-Time Performance and Versatility\nThe most compelling reason for choosing YOLOv8 is its real-time performance. YOLOv8 is capable of processing images at speeds that make it suitable for time-sensitive applications such as autonomous driving, where quick decision-making is critical. While other models like Faster R-CNN may provide slightly higher accuracy, they typically process images at slower rates, making them unsuitable for environments where speed is a priority.\n\n \n\nMoreover, YOLOv8’s versatility allows for the adaptation of the model to various platforms with different computational capabilities. It offers different model sizes, allowing it to scale depending on the hardware available, making it suitable for both edge devices with limited resources and high-performance servers.\n\nAdaptability to Real-World Scenarios\nGiven the complexity and variability of real-world environments, YOLOv8’s flexibility in handling different object scales, various lighting conditions, and backgrounds makes it an ideal choice for traffic sign detection. With the right training and dataset augmentation, YOLOv8 can effectively handle diverse traffic sign scenarios in complex driving environments.\n\nYOLOv8: Key Features Summary\nSingle-Stage Detection: YOLOv8 performs both object localization and classification in a single pass, allowing for fast and efficient real-time processing.\nUnified Detection Pipeline: By combining all detection tasks in a single network, YOLOv8 reduces complexity and training overhead compared to traditional multi-stage models.\nGrid-Based Spatial Constraints: The grid-based approach simplifies the detection task, while multi-scale anchor boxes improve its ability to handle different object sizes.\nReal-Time Performance: YOLOv8 is optimized for real-time inference, making it ideal for applications like autonomous driving.\nVersatility and Flexibility: YOLOv8 adapts to different hardware configurations and can scale depending on the resources available.\nVoxelNeXt for 3D Detection\n\nVoxelNeXt is an advanced deep learning architecture designed for 3D object detection using voxelized representations of LiDAR point cloud data. This approach leverages the 3D nature of the data, making it ideal for applications in autonomous driving, where understanding the spatial relationships between objects is critical.\n\nVoxelization: Point cloud data is first converted into a voxel grid, enabling the model to process the data efficiently by reducing the dimensionality while maintaining essential spatial information.\nVoxel Feature Encoding: VoxelNeXt uses specialized encoding layers to extract features from each voxel, capturing both the spatial structure and object information.\n3D Convolutional Neural Networks (CNNs): After voxel feature encoding, 3D CNN layers process the voxel grid to detect objects by learning spatial hierarchies from the data.\nSparse Convolutions: To enhance computational efficiency, sparse convolutions are employed, focusing processing on non-empty voxels, which reduces overhead while maintaining accuracy.\nRegion Proposal Network (RPN): The RPN generates candidate regions of interest where objects are likely to be located, which are refined in subsequent stages.\nBounding Box Regression and Classification: The final stages of the network refine object proposals by predicting 3D locations and classifying objects (e.g., car, pedestrian, cyclist).\nKey Features of VoxelNeXt:\nVoxel-Based Representation: VoxelNeXt uses voxel grids to represent point cloud data, which is more efficient for large-scale 3D data processing compared to traditional point-based methods.\nSparse Convolutions: These layers reduce computation by focusing only on non-empty voxels, essential for real-time applications like autonomous driving.\n3D Convolutional Layers: These layers enable the model to understand complex spatial relationships in 3D, crucial for detecting objects and understanding their positions relative to others.\nRegion Proposal Network (RPN): This feature improves the model’s detection accuracy and speed by efficiently narrowing down areas where objects are likely to be located.\nScalability: VoxelNeXt is highly scalable, capable of processing large, dense point clouds, making it suitable for real-time applications in autonomous driving.\n4. Simulation\n4.1 Simulate Reality\n\nWhen evaluating models, traditional metrics such as accuracy, precision, recall, and mean average precision (mAP) provide a quantitative assessment of performance in controlled environments. These metrics help in comparing different models and tracking improvements. However, real-world performance often differs due to unpredictable factors like environmental conditions, sensor noise, and hardware limitations. To truly assess how well a 3D object detection model will function, testing it in a realistic setting is crucial.\n\nChallenges of Real-World Testing\n\nWhile it might seem ideal to set up sensors on a vehicle and drive around to collect real-world data, this approach is impractical and ethically risky. Testing an untested model in a high-risk environment, such as a populated area, can be dangerous and could result in accidents or unintended consequences. Therefore, before live testing, ensuring a model performs well in controlled conditions is essential.\n\nOne possible option is using closed test tracks, where vehicles with sensors can operate in safer, contained environments. However, this method is costly for many individuals and smaller teams, as it requires substantial investment in vehicles, sensors, and specialized equipment. Even large corporations may find frequent physical tests inefficient, wasting both time and money.\n\nThe Role of Simulators\n\nThis is where simulators become invaluable. A simulator provides a virtual environment that mimics real-world complexities in a safe and controlled manner. High-fidelity simulators allow models to be tested under various scenarios, such as different weather conditions, times of day, or traffic levels, without any physical risk or the need for expensive equipment. Through simulation, we can introduce environmental factors that would be difficult or dangerous to recreate in real life, such as vehicle detection in extreme weather or simulating high-speed driving in dense urban traffic.\n\nAdvantages of Simulation\n\nOne significant advantage of simulation is the ability to accelerate time. Instead of spending hours navigating city traffic to evaluate a model's performance, a simulator can compress time, enabling hours of real-world driving to be simulated in a fraction of the time. This efficiency allows developers to run more tests, gather data faster, and iterate on models more quickly.\n\nMoreover, simulators offer a consistent, reproducible environment for testing, which is invaluable for debugging and fine-tuning models. In real-world tests, replicating identical conditions for each test can be nearly impossible. However, in simulation, every aspect of the environment can be controlled, enabling precise comparisons between different model configurations or versions.\n\n4.2 Carla Simulator\n\nCARLA Simulator was chosen for this project because it is open-source, offering flexibility for customization and integration into our research. Unlike proprietary systems with high licensing fees, CARLA allows us to modify its code to meet the specific needs of our project, making it ideal for academic research and experimental development.\n\nFigure 4. Carla Simulator Logo\n\nCARLA Features\n\nRealistic Urban Simulation:\nCARLA simulates urban environments, making it perfect for testing autonomous vehicle models. It uses the Unreal Engine for accurate visuals and physics, including gravity, collisions, and road friction. This ensures that vehicles behave realistically, similar to how they would in the real world.\n\nActors:\nIn CARLA, actors are all the entities in the simulation, like vehicles, pedestrians, and traffic signs. These actors can follow traffic rules, interact with each other, and simulate real-world driving behaviours, making it ideal for testing object detection models.\n\nMaps and Customization:\nCARLA provides detailed maps of urban and suburban areas, which can be customized to fit specific testing scenarios. Users can create new environments that mimic real-world locations, enhancing the model's ability to generalize in various conditions.\n\nSensor Suite:\nCARLA simulates essential sensors used in autonomous vehicles, such as cameras, LiDAR, radar, and GPS. These sensors provide synthetic data that closely matches real-world sensor inputs, which is crucial for training and testing 3D object detection models.\n\nTraffic Simulation:\nCARLA also simulates traffic systems, including vehicles and pedestrians following traffic laws. This creates realistic conditions for testing how models handle busy intersections, lane changes, and other complex traffic scenarios.\n\nTime Acceleration:\nOne key advantage of CARLA is the ability to speed up time during testing, allowing hours of real-world driving to be condensed into a much shorter period. This accelerates the development process and allows for rapid model iteration. Additionally, testing in a simulator eliminates the ethical risks of real-world testing, where untested models could cause accidents.\n\n4.3 Client-Server Architecture\n\nCARLA Simulator operates on a scalable client-server architecture, which is crucial for its flexibility and performance. The server handles all core tasks of the simulation, including rendering sensors, calculating physics, updating the environment and actors, and more. For optimal performance, especially when using machine learning models, it's recommended to run the server on a dedicated GPU. This helps process computationally demanding tasks, such as rendering detailed 3D environments and handling large sensor data (e.g., from LiDAR and cameras) without slowing down the system.\n\nFigure 5. Carla API workflow\n\nThe client manages the logic of the actors (e.g., vehicles, pedestrians) and sets the conditions of the world. Clients communicate with the server using the CARLA API, available in both Python and C++, allowing users to control the simulation, manipulate the environment, and retrieve data from sensors. The API is regularly updated, making CARLA highly adaptable for autonomous driving research.\n\nKey CARLA Components\n\nTraffic Manager:\nThis built-in system controls all vehicles in the simulation except the ego vehicle (the one being tested or trained). It ensures that vehicles behave realistically, following traffic rules and responding to events like intersections and pedestrian crossings.\n\nSensors:\nCARLA offers a rich set of sensors, including RGB cameras, depth cameras, LiDAR, radar, and GPS. These sensors mimic real-world autonomous vehicle sensors and can be attached to vehicles in the simulation. The data collected can be streamed or stored for later analysis, making it easier to train and evaluate models.\n\nRecorder:\nThe recorder feature tracks the state of every actor in the simulation, enabling users to replay events frame by frame. This is especially useful for debugging, as it allows users to trace actions and interactions during the simulation.\n\nROS Bridge and Autoware Integration:\nCARLA supports integration with Robot Operating System (ROS) and Autoware, an open-source autonomous driving stack. These integrations allow CARLA to interact with other simulation tools and real-time environments, broadening testing capabilities.\n\nOpen Assets:\nCARLA includes a variety of assets, such as urban maps, weather conditions, and actor blueprints. These assets are customizable, allowing users to create tailored environments. The ability to control weather and lighting conditions adds realism, enabling simulations of diverse driving scenarios, including rain, fog, or night driving.\n\nScenario Runner:\nCARLA includes predefined driving scenarios, such as urban routes and common traffic situations. These scenarios are used in the CARLA Challenge, an open competition where participants test their autonomous driving solutions. Scenario Runner automates test setup, allowing vehicles to repeatedly encounter specific situations to improve their responses.\n\n4.3.1 Server-Side Simulation\n\nTown 10 is the default map for server-side simulation. It combines suburban and urban areas with multiple intersections, providing a realistic testing environment. The map includes:\n\nFigure 6. Town 10 view\n\n50 vehicles using autopilot, following a waypoint algorithm to simulate everyday traffic with actions like stopping, turning, and interacting with intersections.\nPedestrians are introduced at five times the number of vehicles, simulating modern cities where foot traffic is common. Pedestrians follow random paths but can also be programmed for specific behaviours, like crossing during a red light.\n\nTo ensure accurate sensor data, the simulation is set to synchronous mode, aligning all actions and sensor readings at fixed time intervals. This setup guarantees reliable data for testing and model training.\n\n4.3.2 Client-Side Simulation\n\nThe ego vehicle in the simulation is an Audi A2, a compact hatchback commonly seen in Europe. It’s equipped with LiDAR and four RGB cameras, which provide critical data for object detection and navigation.\n\nLiDAR Sensor Specifications\nRange: 100 meters (with a 75-meter detection model buffer for smoother transitions).\nRotation Frequency: 60 FPS, synchronized with the world FPS, ensuring real-time data collection.\nChannels: 64 channels for balanced vertical resolution.\nPoints per Second: 1 million points for detailed point cloud data.\nFields of View:\nUpper FOV: 10° for detecting elevated objects like traffic lights and bridges.\nLower FOV: -30° for detecting obstacles on or below the road surface.\nHorizontal FOV: 360° for full 360-degree coverage around the vehicle.\nRGB Camera Sensor Specifications\nResolution: 1920x1080 pixels for high-definition image quality.\nPosition: 2 meters above the vehicle’s centre for optimal field of view.\nOrientation: Four cameras (front, rear, left, right) for 360-degree visual coverage, similar to real-world autonomous vehicles.\n\nThese sensors provide the necessary data for object detection and scene understanding, ensuring the simulation accurately reflects real-world driving conditions.\n\n4.4 Real-time Processing in 3D\n\nReal-time processing in autonomous vehicle simulations presents a significant challenge, primarily due to the need to parse and compute vast amounts of data within a very limited time frame. Achieving real-time performance requires optimizing each step of the data pipeline to minimize execution time, ensuring that sensor data can be processed, interpreted, and visualized quickly enough to make decisions in real-time. This involves not only efficient data handling but also high-performance visualization tools to interpret complex data outputs, such as 3D point clouds and camera feeds, all within milliseconds.\n\nTo meet these requirements, the simulation and data transformation processes are handled using Python, a high-level language known for its flexibility. Python enables easy integration with high-performance code written in lower-level languages like C, C++, or Rust, where necessary, to maximize efficiency while retaining Python’s user-friendly nature. The simulation leverages both the CARLA Simulator API and the PyTorch API to handle the simulation and machine learning inference in real time.\n\nFigure 7. Real time system architecture\n\nThe CARLA Simulator API allows for direct communication between the simulation environment and the vehicle’s sensors. However, in addition to controlling the simulation, real-time results need to be processed from the machine learning models driving the perception system. This is achieved by directly interfacing with the PyTorch API, which allows the inference model to be called and applied to the live sensor data. PyTorch is responsible for running the object detection model, taking in sensor data (such as camera images or LiDAR point clouds), and outputting the detected objects, classifications, and bounding boxes.\n\nBy calling the PyTorch API from within the Python environment, sensor data from CARLA can be directly fed into the object detection model for processing. This allows real-time inferences to be made on the incoming data streams, delivering immediate feedback on what the model detects in the environment. The flexibility of PyTorch enables fast computation on both CPU and GPU, ensuring that the processing pipeline remains optimized for performance. This approach eliminates the need for post-processing delays, as the model inference happens in sync with the simulation, allowing for continuous data flow and decision-making.\n\nSince CARLA Simulator separates the client-side data processing from the server-side simulation, it becomes critical to visualize the data in real-time. Humans rely heavily on visual inputs for interpretation, so displaying sensor data as it’s captured by the vehicle is crucial for understanding how the system responds to its environment. The integration of the PyTorch API allows for this data to be processed and rendered in real-time, giving immediate insight into how the model is interpreting the sensor data.\n\nA Flask Application serves as the core for handling real-time data outputs from the autonomous driving model. After some preprocessing, the data is adapted for rendering in the front end. The Flask application manages the data flow between the model and the visual interface, offering a lightweight but efficient framework for serving real-time data. This modular approach allows for flexibility in data handling and processing, separating the simulation data gathering from its visualization.\n\nThe camera data from CARLA is initially received in BGR format (Blue, Green, Red), which is the standard image format returned by the simulator. This data must then be processed and transformed into RGB format to align with standard display requirements, ensuring correct color representation. Each camera feed produces an array of shape (1080, 1920, 3), corresponding to the height, width, and three color channels (red, green, and blue). By processing these camera feeds in real-time, it becomes possible to visualize multiple views simultaneously, which is critical for understanding the vehicle’s surroundings from various perspectives.\n\nAdditionally, LiDAR data is transformed from its raw float32 format, which represents the position and intensity of each point in the cloud, into an integer format of shape (N, 4). This transformation compresses the data into a manageable form for rendering and analysis, where each point consists of its X, Y, Z coordinates, and intensity value. Handling LiDAR data efficiently is key to ensuring the vehicle has a precise understanding of its environment in real-time, especially in dense urban settings where point cloud data must be processed quickly.\n\nFor the 3D visualization, the Flask application uses Plotly.js as the backend to render the real-time 3D data. Plotly.js is a robust library that supports high-performance 3D plotting, enabling users to interact with the data through zooming, panning, and rotating the view without suffering rendering slowdowns. This interactivity is essential for evaluating the performance of the autonomous vehicle model in a complex 3D environment, providing insights into how the vehicle processes point cloud data and detects objects. Websockets are used to facilitate real-time communication between the Flask application and the 3D rendering, ensuring that updates are delivered with minimal latency.\n\nThe use of websockets enables seamless, two-way communication between the data processing module and the rendering interface. This separation of concerns allows the data processing to happen in one module, while the visualization runs independently, providing a smooth, fluid user experience. Data is sent asynchronously between the server (which runs the simulation) and the client (which handles the real-time visualization), ensuring that sensor data is immediately available for interpretation.\n\nThe following three visual outputs illustrate the same simulation frame from different perspectives:\n\nCamera Grid: A grid display of the four RGB camera feeds from the vehicle, providing comprehensive visual coverage of the vehicle’s surroundings. The grid view helps visualize what the car’s cameras are capturing in real-time, offering insights into how the vehicle sees the road, pedestrians, and other obstacles.\n\nFigure 8. Camera views on real time app\n\n3D Plot of LiDAR Data: A real-time 3D plot displaying point cloud data from the LiDAR sensor. This visual can demonstrate how objects are detected and represented in 3D space, including the processed voxels and the identification of objects surrounding the vehicle. This is essential to visualize how the environment is interpreted in 3D, with objects like pedestrians and vehicles plotted in real time.\n\nFigure 9. LiDAR point cloud view on real time app\n\nSimulator Screenshot: A screenshot taken directly from the CARLA simulation, showing the vehicle in its environment on the server side. This gives context to how the vehicle navigates through the virtual world, interacting with other actors like cars and pedestrians in real time.\n\nFigure 10. Screenshot of server side simulation scene\n\n4.5 Scalability and Resource Optimization\n\nThe modular design of the simulation framework, which leverages both the CARLA Simulator API and PyTorch for machine learning inference, allows for significant scalability and resource optimization. This approach simplifies the development process by separating different components (such as simulation, sensor data processing, and machine learning model inference), facilitating expansion and optimization without requiring major structural changes.\n\nScalability\n\nScalability is achieved through the modularity of the system, enabling individual components, like data collection, inference, and visualization, to be distributed across different systems or scaled up as needed. For instance, sensor data from multiple vehicles can be processed simultaneously, with each instance running independently and communicating with the centralized model via websockets. This setup is adaptable and can handle increased data flow without overloading the system, making it suitable for larger-scale simulations with numerous vehicles and pedestrians.\n\nResource Optimization\n\nSeveral strategies can be implemented to improve resource optimization:\n\nMultithreading:\n\nMultithreading enhances processing efficiency by parallelizing tasks, such as sensor input handling, model inference, and visualization updates. The system can fully utilize modern multi-core processors, ensuring that no single process becomes a bottleneck. For example, sensor data processing for cameras and LiDAR can run on separate threads, while PyTorch model inference is processed on another. This improves real-time performance by reducing wait times between stages.\n\nSpecialized Hardware:\n\nGPUs and TPUs (Tensor Processing Units) are used for tasks requiring parallel processing, such as rendering simulated environments in CARLA or running neural network computations in PyTorch. Offloading tasks to dedicated hardware allows the system to process large amounts of data efficiently, enabling higher-fidelity simulations. Additionally, specialized processors like FPGAs (Field-Programmable Gate Arrays) and ASICs (Application-Specific Integrated Circuits) can accelerate specific computations, such as LiDAR point cloud processing or image classification tasks, improving both speed and energy efficiency.\n\nLoad Balancing and Distributed Processing:\n\nAs the simulation grows more complex, tasks like simulation, model inference, and data visualization may need to be distributed across multiple machines. Load balancing ensures tasks are evenly distributed across available hardware, improving system efficiency and preventing any machine from becoming a bottleneck. This approach optimizes resource usage, maintaining high performance even with larger simulations involving multiple vehicles, pedestrians, and complex environments.\n\nBy employing these strategies, the system can scale to handle larger simulations without sacrificing performance, ensuring smooth real-time processing, even as simulation complexity increases.\n\n4.6 Integration with 2D Systems\n\nIntegrating 3D object detection results from LiDAR data with 2D camera views is crucial for creating a unified perception system, often called sensor fusion. This involves projecting the 3D bounding boxes (bbox) predicted by the model from LiDAR point clouds onto the 2D image plane of the vehicle’s cameras using a projection matrix. This enables a more comprehensive view of the environment, where 3D detections from LiDAR can be visualized within the 2D camera feed, similar to how objects are rendered in video games.\n\nProjection Process\n\nThe projection process relies on both intrinsic and extrinsic camera parameters:\n\nIntrinsic parameters: These define the camera’s internal characteristics, such as its focal length, sensor size, and the principal point (center of the image). They are essential for mapping 3D points onto the 2D image plane and controlling how the scene appears in the camera’s view.\n\nExtrinsic parameters: These describe the camera’s position and orientation relative to the vehicle or LiDAR sensor. They define the transformation required to convert 3D points from the LiDAR’s coordinate system into the camera’s coordinate system.\n\nOnce the transformation is completed, the 3D bounding boxes can be represented on the 2D image plane of the cameras by applying the appropriate projection matrix.\n\nPractical Application\n\nThis process works similarly to how objects in 3D games are rendered on a 2D screen. In a 3D scene, objects are represented with depth (X, Y, Z coordinates), but when displayed on a 2D screen, these objects must be projected according to the camera’s viewpoint.\n\nIn autonomous driving, 3D bounding boxes (for detected cars, pedestrians, etc.) are projected into the camera images, allowing both 2D and 3D information to be merged for a better understanding of the vehicle’s surroundings.\n\n5. Conclusions and Further Research\n\nThe research and implementation conducted throughout this project offer valuable insights into the design, testing, and optimization of autonomous driving systems, particularly with regard to integrating 3D and 2D data, real-time processing, and scalability. These systems play a critical role in ensuring that autonomous vehicles can navigate and interact safely with complex environments, utilizing a range of sensors such as LiDAR and cameras to perceive the world around them. This section outlines the conclusions drawn from the research and offers suggestions for further investigation and development in the field.\n\n5.1 Data Analysis\n\nAfter conducting simulations with the YOLO model and VoxelNeXt, a reflective analysis of the results provides critical insights into the performance and limitations of these models in real-world autonomous driving applications.\n\nYOLO Model Performance\n\nThe YOLO model demonstrated a strong ability to detect smaller objects, such as traffic signs, at a distance. Specifically, its capability to detect small traffic signs early, even from afar, was one of the key strengths observed during the simulations. This is crucial for autonomous systems where early detection of traffic signs (like stop signs or speed limits) affects vehicle decision-making. However, an issue arises when the model encounters scenes with more than five traffic signs within a single image. In such cases, the detection accuracy starts to decline, likely due to the inherent complexity of processing multiple objects within a constrained computational framework.\n\nTo address this issue, one possible optimization strategy would be to implement a two-stage detection system. In this approach, a lighter model could first be specialized to detect traffic signs only, cropping their bounding boxes, while a second model could handle other objects in the scene. This modular system could theoretically improve detection accuracy by assigning dedicated resources to specific tasks. However, this approach introduces additional complexity and would increase overall inference time, as it involves running multiple models in sequence.\n\nGiven the constraints of real-time processing, such as minimizing inference time, the decision was made to use a single YOLO model with data augmentation techniques. This approach offers a balance between speed and accuracy, ensuring that the system remains efficient while maintaining reasonable performance in multi-object detection scenarios. Data augmentation helped to improve the model’s ability to generalize across diverse scenarios, reinforcing the choice to prioritize a single model.\n\nVoxelNeXt Performance\n\nThe simulations conducted using VoxelNeXt revealed different strengths and weaknesses compared to the YOLO model. One of the primary challenges observed was class confusion at longer ranges, especially beyond 40 meters. At these distances, the model sometimes struggled to accurately differentiate between objects, leading to classification errors. This is likely due to the nature of LiDAR data at long distances, where the point cloud becomes increasingly sparse. When fewer data points represent an object, the model’s ability to infer precise characteristics, such as shape and class, is diminished. Additionally, missed detections were more common in the 40+ meter range. This issue again ties back to the sparsity of data points in LiDAR detection, which causes the model to lose precision when detecting small or distant objects. Despite these challenges, VoxelNeXt performed well in closer ranges, where point density was higher, enabling accurate and consistent object detection.\n\n5.2 ONNX & TensorRT\n\nONNX (Open Neural Network Exchange) and TensorRT are two advanced tools widely used to optimize and deploy machine learning models, especially in real-time applications that demand low latency and high performance, such as autonomous driving. As models become increasingly complex, especially in fields like 3D object detection or scene understanding, it becomes crucial to ensure that they can be deployed efficiently without sacrificing speed or accuracy. These tools allow developers to streamline the deployment process while maintaining the performance necessary for real-time inference.\n\nONNX\n\nONNX (The Linux Foundation, 2019) is an open-source format for representing machine learning models, developed by Microsoft and Facebook. The core idea behind ONNX is to enable interoperability between different machine learning frameworks. This means that models trained in frameworks like PyTorch or TensorFlow can be converted into ONNX format, allowing them to be easily transferred to other platforms (such as Caffe2, MXNet, or TensorRT) without needing to retrain or rewrite the model. This flexibility facilitates smoother transitions between research and production environments.\n\nAdvantages of ONNX:\n\nInteroperability: By providing a unified format, ONNX enables seamless transitions between different frameworks, reducing dependency on a single tool or environment during development.\nFlexibility: Developers can train models in their preferred framework and then export them to other environments more suited for real-time or production applications.\nOptimization: ONNX includes various optimization techniques that help reduce model size and improve inference speed, making it particularly suitable for deployment on resource-constrained hardware.\nTensorRT\n\nTensorRT (Nvidia Corporation, 2019) is a high-performance deep learning inference engine developed by NVIDIA, specifically designed to optimize machine learning models for deployment on NVIDIA GPUs. TensorRT takes models (often exported in ONNX format from frameworks like PyTorch or TensorFlow) and applies several optimization techniques to accelerate inference. These optimizations are essential in real-time applications, such as autonomous driving, where low-latency detection and decision-making are critical.\n\nAdvantages of TensorRT:\n\nHigh Performance: TensorRT employs techniques such as precision calibration (using FP16 or INT8) to reduce memory usage and improve inference speed, drastically cutting down on latency.\nNVIDIA Hardware Optimization: TensorRT is highly optimized for NVIDIA GPUs, allowing it to fully leverage the parallel computing power of these devices.\nReal-Time Inference: TensorRT is ideal for scenarios where minimal latency is crucial, such as detecting objects in real-time while a vehicle is in motion.\n\nWhile neither ONNX nor TensorRT were implemented within the scope of this research, these tools offer significant advantages that could enhance future deployments of the models. For instance, models like YOLO or VoxelNeXt, which were developed and trained using PyTorch, could be converted into ONNX format. From there, the models could be imported into TensorRT for real-time optimization on NVIDIA GPUs. This would result in reduced inference time, making these models ideal for real-world, real-time applications where rapid decision-making is essential, such as autonomous vehicle navigation.\n\nHowever, implementing ONNX and TensorRT requires additional considerations and infrastructure that go beyond the current focus of this project, which primarily aimed to develop and test models within the simulation environment. Future research could explore how these tools can be integrated to optimize model performance for real-world deployment.\n\n5.3 Practical Applications and Impact\n\nThe outcomes of this research have significant implications for both data-driven systems and the future of autonomous driving. These findings offer practical applications in refining object detection, data labeling, and real-time processing, all of which are critical to ensuring that autonomous vehicles can navigate complex environments safely and efficiently.\n\nBy leveraging advanced models like YOLO and VoxelNeXt, which integrate 2D and 3D data for object detection, this research allows for more precise interaction between autonomous systems and their surroundings. Autonomous driving systems can use this technology to detect objects such as traffic signs, pedestrians, and other vehicles with greater accuracy. Early detection of traffic signs, for instance, enables vehicles to make better-informed decisions, significantly improving response times and reducing the likelihood of accidents. This not only enhances the individual vehicle's safety but also facilitates smoother interaction between multiple autonomous systems, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication networks.\n\nThe enhanced ability to predict data patterns with high accuracy could transform the way autonomous driving systems interact with other systems and with each other. In the future, this research could help establish a framework where autonomous vehicles operate within a connected ecosystem, optimizing traffic flow, reducing congestion, and improving safety by sharing real-time data.\n\nBeyond autonomous driving, the potential impact of this research extends to any domain that relies on accurate, real-time detection and processing of sensor data, such as robotics, industrial automation, and healthcare. As these models improve in both accuracy and efficiency, they can be adapted to various applications that require sophisticated real-time decision-making based on complex sensor inputs.\n\n5.4 Limitations of Current Development\n\nWhile the research has made significant progress in developing reliable 2D and 3D object detection systems, several limitations remain. The YOLO-based 2D system performs well in detecting small and low-resolution objects, even in challenging environments, but struggles with multiple labels or densely packed scenes. Detection accuracy begins to decline when more than five objects are present within the same frame, likely due to computational limitations and the complexity of managing multiple bounding boxes.\n\nThe 3D system, which uses LiDAR and other depth sensors to manage millions of points in real-time, has shown strong performance at close and medium ranges. However, detection accuracy begins to degrade at distances over 40 meters, where the data from LiDAR becomes sparse. This sparseness makes it difficult for the system to accurately classify and detect objects, resulting in missed detections or misclassifications.\n\nAnother critical limitation is the size of the datasets required to train these models. Managing hundreds of gigabytes (and potentially terabytes) of data poses significant challenges, particularly in terms of storage, processing power, and time. The computational cost of parsing and training on these large datasets is immense, often taking several weeks or even months to complete a full training cycle. While smaller subsets of data can be used for testing purposes, training on the full dataset is necessary to achieve the highest level of accuracy. Unfortunately, even small errors during this phase can lead to significant penalties in terms of time, as retraining the entire model can further delay the process.\n\nThese limitations suggest that while the current systems are functional and offer significant promise, there is still much work to be done to improve scalability, computational efficiency, and performance, particularly in handling large datasets and real-world complexities.\n\n5.5 Further Research Directions\n\nWhile the current work presents valuable insights into autonomous driving systems, several areas remain ripe for further research and development. The following sections outline potential directions for future work to build on the foundation laid by this project.\n\nAdvanced Fusion Techniques: The integration of 2D and 3D data has proven to enhance object detection, but there is room for improving sensor fusion methods. Future research could focus on developing more sophisticated algorithms for combining the strengths of LiDAR, camera, radar, and other sensors. By improving the fusion pipeline, autonomous systems could better handle ambiguous or occluded objects, which are often challenging to detect using a single modality.\n\nHandling Sparse Data in Long-Range LiDAR Detection: As mentioned, LiDAR data becomes sparse at greater distances, leading to decreased detection accuracy. Further research could explore techniques for overcoming this limitation, such as using more advanced filtering or interpolation techniques to enhance long-range detection. Alternatively, integrating high-resolution LiDAR systems or combining multiple sensors could address some of these challenges.\n\nReal-Time Optimization with ONNX and TensorRT: Future work could explore the full potential of ONNX and TensorRT for optimizing model deployment. While these tools were not fully integrated into this project, they could significantly improve the real-time inference speed and scalability of complex models. Research into deploying YOLO and VoxelNeXt models using ONNX and TensorRT on various hardware configurations (such as NVIDIA GPUs) would be valuable, especially for large-scale and resource-constrained applications.\n\nMulti-Agent and Vehicle-to-Vehicle Communication: In the realm of autonomous driving, communication between vehicles (V2V) and between vehicles and infrastructure (V2I) is an essential feature for increasing situational awareness and reducing traffic hazards. Future research could explore methods for enhancing V2V communication, using real-time data from the sensor networks of multiple vehicles to improve decision-making algorithms. This would contribute to the development of a more interconnected and collaborative autonomous vehicle network.\n\nDataset Expansion and Management: The computational costs of training models on large datasets remain a challenge. Future research could focus on developing more efficient methods for dataset augmentation, dataset management, and distributed training. Exploring techniques like federated learning, where model training occurs on decentralized devices while maintaining data privacy, could also be an avenue worth investigating.\n\nAutonomous System Testing in Dynamic Environments: While this project focused on simulations, further research should aim to validate the proposed systems in real-world, dynamic environments. Conducting large-scale testing involving various traffic scenarios, different weather conditions, and varying vehicle types would help identify edge cases and ensure that the system can handle the unpredictability of real-world driving.\n\nEnergy Efficiency and Sustainability: Finally, a growing focus on sustainable and energy-efficient systems in autonomous driving is needed. Research could explore how to reduce the power consumption of sensors, onboard processors, and communication systems while maintaining high performance. Given the computational load of running models like YOLO and VoxelNeXt in real-time, optimizing the system's energy consumption would be a key development in advancing autonomous driving technologies.\n\n5.6 Prospective Development\n\nFurther development of these systems will need to focus on deeper integration with the vehicle’s architecture, particularly through optimizing communication pathways such as the Controller Area Network (CAN-bus). The CAN-bus serves as the central communication hub within a vehicle, ensuring that sensors, processors, and actuators can communicate with one another in real-time. To fully realize the potential of systems like YOLO and VoxelNeXt in autonomous driving, it is critical to ensure that the data processed by these models is relayed quickly and accurately to the vehicle’s control systems, such as those responsible for braking, steering, and throttle control.\n\nThe current architecture allows for robust detection of objects, but future improvements should focus on minimizing latency and ensuring seamless communication between the detection models and the car’s distributed systems. For instance, the CAN-bus needs to handle high data rates while maintaining low latency to ensure that the vehicle can react to sudden changes in its environment. This is particularly important in high-speed scenarios where every millisecond counts.\n\nIn addition to enhancing the internal vehicle architecture, future research could explore the use of reinforcement learning to fine-tune the system’s ability to adapt to complex driving scenarios. By continuously learning from real-world sensor data, the models could be trained to anticipate and react to various traffic situations, such as predicting pedestrian movement or identifying anomalies in road conditions. Reinforcement learning could also help optimize resource allocation, allowing the system to dynamically adjust its processing power based on the complexity of the environment.\n\nLastly, ensuring that these systems include built-in redundancy and fail-safes is crucial. In the event of sensor failure or misinterpretation of data, the system must have alternative pathways to ensure safe vehicle operation. These fail-safe mechanisms would ensure that, even in the event of partial system failure, the vehicle can continue to operate safely.\n\n5.7 Reflection on Ethical and Social Implications\n\nThe development of autonomous driving systems presents not only technical challenges but also ethical and social considerations that must be addressed. Autonomous vehicles operate without human drivers, meaning that their decision-making processes must be programmed into their systems in a way that accounts for the potential consequences of those decisions. However, machines lack the ability to reflect on moral dilemmas or consider the broader ethical implications of their actions.\n\nA classic ethical problem, known as the Trolley Problem, highlights the type of decisions autonomous vehicles might face. In this scenario, a person must decide whether to let a trolley continue on its current track, where it will kill five people, or divert it to another track, where it will kill one person. Autonomous vehicles might encounter similar dilemmas in real-world scenarios: for example, choosing between swerving to avoid a pedestrian who has illegally crossed the street and potentially colliding with another vehicle or pedestrian who is following the rules. These split-second decisions have life-or-death consequences, yet autonomous systems cannot consider the moral implications in the same way a human might.\n\nFigure 11. The trolley problem\n\nMoreover, the introduction of autonomous driving into society raises questions about accountability and responsibility. If an autonomous vehicle is involved in an accident, who is responsible? Is it the manufacturer, the designer of the AI, or the owner of the vehicle? These questions highlight the need for new regulations and ethical frameworks that can guide the deployment of these systems in a way that aligns with societal values.\n\nAutonomous driving also has broader social implications, particularly concerning employment. As more advanced autonomous systems are developed, there is the potential for job losses in industries such as transportation and logistics. Balancing technological progress with the social impact of automation will require careful planning, including initiatives that ensure displaced workers have opportunities to transition into new roles.\n\n5.8 Final Conclusions\n\nThe research and development carried out throughout this project offer a promising foundation for the future of autonomous driving systems. By integrating 2D and 3D detection models with real-time processing capabilities, the systems developed here provide the necessary building blocks for a more advanced and reliable autonomous driving framework. However, the research also underscores the significant challenges that remain, particularly regarding data processing, system scalability, and ethical considerations. As further research continues to refine these technologies, there is great potential for these systems to be further developed, ensuring that autonomous vehicles can safely navigate complex environments and adapt to the evolving challenges of the transportation landscape.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAdvancing Autonomous Driving: Deep Learning Innovations for Enhanced Perception and Decision-Making\n\nAbstract\n\nIntroduction\nRelated Works\n\nTraditional Image Processing Techniques\n\nAdvances in Deep Learning for Object Detection\n\nYOLO (You Only Look Once):\n\nVoxel-based 3D Detection:\n\nHybrid Approaches:\n\nEvaluation Metrics and Comparative Analysis\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nFiles\nAdvance Autonomous Driving- Deep Learning Innovations for Enhanced Perception and Decision-Making.pdf\nYou might be interested\nSimple Autonomous Driving Vehicle\nK\nDec 26, 202423 reads\nADASAI+9\nVision-Based Perception Systems for Autonomous Vehicles\nMost Promising Innovation\nF\nA\nA\nI\nF\nDec 30, 202472 reads\nautonomousAutonomous vehicles+16\nAutonomous Driving - Car Detection\nDec 30, 202412 reads\ncar detection systemNon-max Suppression+1\nAdvanced Self Driving Car for Egyptian Streets (V2X)\nW\nDec 24, 202416 reads\nADASBehavior Cloning+12",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "aerosage-generative-ai-for-trusted-aviation-maintenance-1rOJ0tZbFKBA",
    "username": "s@sekharchennaiah12345",
    "license": "MIT License",
    "title": "AeroSage: Generative AI for Trusted Aviation Maintenance",
    "publication_description": "Back to publications\nDec 29, 2024\n●\n86 reads\n●\nMIT License\nAeroSage: Generative AI for Trusted Aviation Maintenance\nColplai-Gemma-Streamlit\nDeep Learning\nGenAI\nMulti-Modal-RAG\nRAG\nVision LLM\nS\n@sekharchennaiah12345\nLike\nBookmark\nShare\nMaking Aircraft Maintenance Safer Through Visual AI\n\nTransforming Technical Documentation for the People Who Keep Us Flying\n\nIntroduction: When Every Second Counts\n\nPicture this: A maintenance technician stands before a complex aircraft component, manual in hand, knowing that every minute the aircraft stays grounded costs thousands of dollars...\n\nUnderstanding the Challenge: A Day in the Life\n\nIn modern aircraft maintenance, precision isn't just important—it's critical for safety. When a maintenance technician approaches a Bell Model 412 helicopter for routine maintenance, they face a complex web of challenges that directly impact both safety and operational efficiency. These challenges begin before they even reach for their first tool.\n\nConsider the daily reality of an aircraft maintenance technician:\n\nThey must navigate through technical documentation exceeding 1000 pages\nEvery component they work on must be precisely identified among similar-looking parts\nEach procedure must be followed in exact sequence, with no room for error\nAll this must be accomplished under significant time pressure to maintain flight schedules\n\nOur comprehensive research has uncovered statistics that highlight the urgency of this situation:\n\nTechnicians spend approximately 30% of their valuable maintenance time simply searching through documentation\nA typical aircraft maintenance manual contains over 1000 pages of dense technical information\nDocumentation-related errors can result in costs exceeding $100M\nMost critically, many safety procedures lack proper visual validation, forcing technicians to rely solely on text descriptions\nCurrent Solutions Fall Short\n\nTo understand why we need a new approach, let's examine existing solutions and their limitations. Each current approach attempts to solve the documentation challenge but falls short in critical ways.\n\nTraditional Search Systems (70% Time Waste)\n\nThink of traditional search like trying to find a specific recipe in a massive cookbook without any pictures. When a technician searches \"main rotor bolt torque specifications\", they face several challenges:\n\nManual Lookup Process\nEvery search requires multiple steps: finding the right section, scanning through pages, cross-referencing with other sections. This process wastes up to 70% of valuable maintenance time.\n\nNo Visual Context\nEven when technicians find the right text, they lack visual confirmation. Imagine being told to \"tighten the third bolt from the left\" without a picture showing which bolt array is being referenced.\n\nMulti-Step Verification\nTechnicians must constantly cross-reference their findings with physical components, technical diagrams, and other documentation sections to ensure accuracy.\n\nPure Language Models (15% Error Rate)\n\nUsing pure language models is like getting maintenance advice from someone who has memorized the manual but has never seen the actual aircraft. This approach introduces several risks:\n\nHallucination Risk\nLanguage models can generate plausible-sounding but incorrect specifications, leading to a 15% error rate in critical parameters.\n\nNo Visual Validation\nThese systems cannot confirm whether a technician is looking at the correct component, creating a dangerous disconnect between instructions and reality.\n\nTrust Issues\nMaintenance technicians, understandably, show low confidence in AI-generated answers that lack visual validation, leading to additional time spent on verification.\n\nBasic RAG Systems (40% Context Loss)\n\nBasic RAG systems try to bridge the gap between search and AI but still fall short of what maintenance technicians need:\n\nComplex Pipeline Overhead\nCurrent implementations suffer from a 2.5x processing overhead, making real-time interactions challenging.\n\nVisual Loss\nThese systems typically lose about 40% of critical visual context when processing documentation, missing crucial diagram details.\n\nLimited Integration\nInformation often becomes fragmented between text and visual components, forcing technicians to mentally reconstruct the complete picture.\n\nTo illustrate these limitations, consider a real-world scenario:\n\nA technician needs to inspect the main rotor assembly. Using current solutions:\n\nTraditional Search: Returns 50+ pages containing \"main rotor inspection\"\nPure LLM: Provides a procedure but can't verify component identification\nBasic RAG: Finds relevant text but loses critical visual assembly details\n\nBasic Workflow of the existing approaches\n\n\nUnderstanding ColPali: The Bridge Between Visual and Textual Understanding\n\nThink of ColPali like an expert mentor who can both read technical manuals and see what you're working on...\n\nThe ColPali Pipeline: A Detailed Walkthrough\n\nColPali processes information in two distinct phases: document processing (offline) and query processing (online). Let's examine each step in detail:\n\nDocument Processing Phase (0.39s/page)\n\nPage Image Input\n\nSystem takes raw PDF pages as input\nEach page is treated as a single high-resolution image\nNo OCR or text extraction needed\n\nVision Encoder\n\nDivides each page into 1030 distinct patches\nEach patch represents a specific region of the page\nPatches capture both text and visual elements\n\nLinear Projection (First Stage)\n\nTransforms visual features into a format suitable for language processing\nReduces computational overhead\nMaintains spatial relationships between patches\n\nLLM Processing\n\nProcesses projected patches through language model\nEnriches visual features with semantic understanding\nCreates context-aware representations\n\nFinal Linear Projection\n\nConverts processed features into 128-dimensional vectors\nEach page ends up with 1030 patch embeddings\nEach embedding is a 128-dimensional vector\n\nThe result: Each page is represented by a 1030×128 matrix, where each row represents a distinct patch's understanding of that region of the page.\n\nQuery Processing Phase (30ms/query)\n\nText Query Input\n\nTechnician submits maintenance query\nExample: \"Show me the brake system inspection procedure\"\n\nLLM Encoder\n\nProcesses query through language model\nCreates semantic understanding of the request\nGenerates query embeddings in same space as document patches\n\nSimilarity Computation\nLet's see exactly how ColPali matches queries to documents:\n\nGiven:\n\nQuery embeddings: e_query (text understanding)\nPage embeddings: e_image (1030 patches × 128 dimensions)\n\nThe matching process follows four precise steps:\n\nStep 1: Token-Patch Similarity\n\nsimilarity = torch.matmul(e_query, e_image.T)\n# Results in similarity scores between each query token \n# and each document patch\n\nStep 2: Best Patch Selection\n\nmax_similarities = similarity.max(dim=1)[0]\n# For each query token, find the best matching patch\n\nStep 3: Final Score Computation\n\nfinal_score = max_similarities.sum()\n# Sum up the best matches to get page relevance\n\nFor example, if we have:\n\nQuery token 1: [0.5, 0.1, 0.7, 0.3]\nDocument patch 1: [0.3, 0.2, 0.6, 0.5]\n\nThe similarity calculation would be:\n\n0.5×0.3 + 0.1×0.2 + 0.7×0.6 + 0.3×0.5 = 0.74\nResponse Generation\n\nTop matching patches are identified\nRelevant page sections are extracted\nGemini Flash processes the combined information\nGenerates contextually accurate response\n\nColPali in Practice: Understanding the Document Processing Pipeline\n\nTo truly understand how ColPali transforms aircraft maintenance documentation, let's walk through each stage of its processing pipeline, examining how it converts complex technical documents into searchable, visual-aware representations.\n\nUnderstanding Patch-Based Processing\n\nWhen ColPali receives a maintenance manual page, it processes it much like how a technician would scan a document - by breaking it into manageable sections and understanding each part in context. Here's how:\n\nPage Segmentation\n   # Each page is divided into 1030 patches\n   patches = vision_encoder.segment_page(document_page)\n   # Shape: [1030, initial_features]\n\nThink of this like dividing a maintenance diagram into a grid, where each cell can capture text, diagrams, or both. The number 1030 wasn't chosen randomly - it provides the optimal balance between detail and processing efficiency.\n\nInitial Feature Extraction\n# Vision encoder processes each patch\nvisual_features = vision_encoder(patches)\n# Shape: [1030, visual_dimension]\n\nDuring this stage, ColPali identifies visual elements like:\n\nComponent diagrams and their details\nWarning symbols and safety indicators\nText formatting and layout\nSpatial relationships between elements\nFeature Enrichment Through Language Understanding\n# Project visual features to language space\nprojected_features = linear_projection_1(visual_features)\n\n# Process through language model\nenriched_features = language_model(projected_features)\n# Shape: [1030, llm_dimension]\n\n# Final projection for efficient storage\nfinal_embeddings = linear_projection_2(enriched_features)\n# Shape: [1030, 128]\n\nLet's examine a specific example of how this works with real numbers:\nConsider a maintenance manual page showing a brake system diagram. One patch might contain both text (\"Maximum pressure: 3000 psi\") and part of the diagram. ColPali processes this as:\n\n# Single patch processing example\nvisual_features = [0.8, 0.3, 0.6, ...]  # Initial visual understanding\nprojected = [0.7, 0.4, 0.5, ...]        # Projected to language space\nenriched = [0.9, 0.6, 0.8, ...]         # Enhanced with semantic understanding\nfinal = [0.85, 0.55, 0.75, ...]         # Compressed to efficient representation\n\nQuery Processing and Matching\nWhen a technician submits a query, ColPali uses a sophisticated matching system:\n\nQuery Encoding\n# Process query text through language model\nquery_embedding = language_model(query_text)\n# Shape: [query_length, 128]\nSimilarity Calculation\nFor each document page, ColPali computes:\n# Computing similarity between query and all patches\nsimilarities = torch.matmul(query_embedding, page_embeddings.transpose(0, 1))\n# Shape: [query_length, 1030]\n\n# Finding best matching patch for each query term\nbest_matches = similarities.max(dim=1)[0]\n# Shape: [query_length]\n\n# Computing final page score\npage_score = best_matches.sum()\n# Single value representing page relevance\n\nFor example, if a technician searches for \"brake system pressure check\":\n\nQuery tokens -> Individual embeddings:\n\"brake\"    -> [0.9, 0.2, 0.7, ...]\n\"system\"   -> [0.6, 0.5, 0.4, ...]\n\"pressure\" -> [0.8, 0.3, 0.9, ...]\n\"check\"    -> [0.5, 0.7, 0.3, ...]\n\nEach token is matched against all 1030 patches per page\nBest matches are combined to rank document relevance\nBeyond Basic Matching: Intelligent Response Generation\n\nWhile matching documents accurately is crucial, ColPali goes further by understanding the context and generating helpful, accurate responses. Let's explore how this works in practice.\n\nThe Dual RAG System: Combining Visual and Textual Understanding\n\n\n\n\nOur implementation uses a unique dual-panel approach that combines ColPali's capabilities with traditional text processing:\n\nMulti-Modal RAG Panel (Left Side)\n   # Process visual information first\n   visual_context = colpali_processor.analyze(\n       query=maintenance_query,\n       page_embeddings=relevant_page_embeddings\n   )\n   # Returns both relevant patches and their spatial relationships\n\nThis panel provides:\n\nVisual confirmation of components\nSpatial context for maintenance procedures\nHighlighted safety-critical areas\nReal-time validation of component identification\nText-Based RAG Panel (Right Side)\n# Process textual information\ntext_context = text_processor.analyze(\n    query=maintenance_query,\n    matched_pages=relevant_pages\n)\n# Returns structured procedural information\n\nThis panel delivers:\n\nStep-by-step maintenance procedures\nTechnical specifications\nSafety warnings and prerequisites\nCross-references to related procedures\n\nReal-World Example: Brake System Inspection\nLet's see how this works in a real maintenance scenario:\nWhen a technician queries \"Show me the brake system inspection procedure\", the system:\n\nInitial Processing\n# Query gets processed through both pipelines\nvisual_results = visual_pipeline.process(query)\ntext_results = text_pipeline.process(query)\n\n# Both results are synchronized\ncombined_results = synchronize_results(\n    visual=visual_results,\n    text=text_results\n)\nVisual Validation\nIdentifies relevant brake system diagrams\nHighlights inspection points\nShows component relationships\nMarks safety-critical areas\nProcedural Guidance\nLists inspection steps in order\nProvides specific torque values\nNotes safety precautions\nIndicates required tools\nSynchronized Display\n# Ensure visual and textual elements align\nfor step in inspection_steps:\n    highlight_component(step.component_id)\n    show_procedure(step.instructions)\n    validate_safety_requirements(step.safety_checks)\n\nThis dual approach ensures that technicians:\n\nSee exactly what they're looking for\nUnderstand precisely where to look\nKnow exactly what to do\nCan validate their work visually\nPerformance and Accuracy Metrics\n\nOur system achieves impressive performance metrics that directly impact maintenance efficiency:\n\nProcessing Speed\nDocument indexing: 0.39s per page\nQuery response: 30ms average\nVisual validation: Near real-time\nAccuracy\nComponent identification: 98% accuracy\nProcedure matching: 95% precision\nSafety validation: 100% coverage\nResource Efficiency\n60% lower computational requirements\nOptimized patch processing\nEfficient memory utilization\nSystem Benefits and Impact: Transforming Aircraft Maintenance\n\nThe real value of our ColPali-based system becomes clear when we examine how it transforms daily maintenance operations. Let's explore the concrete benefits and their impact on safety, efficiency, and technical operations.\n\nSafety Improvements: Protecting Lives and Equipment\n\nOur system has achieved a remarkable 95% reduction in documentation-related errors. To understand the significance of this improvement, let's break down how it prevents common maintenance mistakes:\n\nComponent Identification Accuracy\nBefore our system, technicians might spend valuable minutes or hours ensuring they were looking at the correct component in a complex assembly. Now, the visual validation system provides instant confirmation. For example:\n\nWhen inspecting a landing gear assembly:\n\nTraditional method: Technician cross-references multiple manual pages and diagrams\nOur system: Instantly highlights the exact component and shows its relationship to surrounding parts\nResult: Zero ambiguity about which component needs attention\n\nProcedural Compliance\nThe system ensures 100% visual validation for critical steps through:\n\nReal-time visual guides showing exact component locations\nStep-by-step confirmation of procedure completion\nAutomatic flagging of safety-critical steps\nVisual verification of correct tool placement and usage\n\nFor instance, when working on the rotor system:\n\n# Safety validation example\nsafety_checks = {\n    'component_verified': True,    # Visual match confirmed\n    'tools_correct': True,        # Required tools identified\n    'sequence_validated': True,    # Steps in correct order\n    'safety_equipment': True      # Required safety gear confirmed\n}\nEfficiency Gains: Time is Safety\n\nThe 70% reduction in search and verification time translates to real operational benefits:\nProcessing Speed Improvements\n\nDocument processing: 0.39s per page (compared to 7.22s traditional)\nQuery response: 30ms average (compared to 22s traditional)\nVisual validation: Near instantaneous\n\nLet's see this in practice:\n\n# Time savings calculation for typical maintenance task\ntraditional_time = {\n    'document_search': 15,    # minutes\n    'verification': 10,       # minutes\n    'cross_reference': 5      # minutes\n}\n\nnew_system_time = {\n    'document_search': 4,     # minutes\n    'verification': 3,        # minutes\n    'cross_reference': 1      # minutes\n}\n\ntotal_time_saved = sum(traditional_time.values()) - sum(new_system_time.values())\n# Results in 22 minutes saved per task\nTechnical Advantages: Smarter Resource Usage\n\nOur system's technical improvements lead to better resource utilization:\nComputational Efficiency\nThe 60% reduction in computational requirements comes from:\n\nEfficient patch-based processing\n# Example of efficient patch processing\npage_patches = 1030  # Optimal number of patches\nfeature_dimension = 128  # Compressed representation\nmemory_per_page = page_patches * feature_dimension * 4  # bytes\n# Results in efficient memory usage while maintaining accuracy\nSmart caching system\nFrequently accessed procedures stay readily available\nIntelligent pre-loading of related documents\nOptimized memory management\n\nResponse Accuracy\nThe 98% response accuracy is achieved through:\n\nMulti-stage verification\nVisual component matching\nTextual procedure confirmation\nCross-reference validation\nContinuous learning\nSystem tracks successful matches\nAdapts to technician feedback\nImproves accuracy over time\n\nFor example, when processing a maintenance query:\n\nconfidence_metrics = {\n    'visual_match': 0.98,        # Component identification\n    'procedure_match': 0.97,     # Correct maintenance step\n    'context_relevance': 0.99,   # Appropriate to situation\n    'safety_validation': 1.00    # Critical safety checks\n}\nFuture Work: Pioneering Visual-First Aircraft Maintenance\n\nWhile our current dual RAG system represents a significant advancement in maintenance documentation, we're excited to share a groundbreaking extension that fundamentally transforms how technicians interact with technical information: direct image-based search capability. This innovation, currently under review by the ColPali development team, represents the next evolution in maintenance documentation interaction.\n\nRevolutionary Image-Based Search\n\nImagine a technician encountering an unfamiliar component or unusual wear pattern. Instead of trying to describe what they see in words, they can simply:\n\nTake a photo of the component\nLet the system find matching documentation instantly\nReceive relevant maintenance procedures\n\nLet's understand how this works through a detailed example:\n\nCurrent Workflow vs. Image-Based Innovation:\n\nTraditional Query:\nTechnician: \"Show me maintenance procedures for cylindrical component\nwith three mounting brackets near the landing gear\"\n\nNew Visual Approach:\nTechnician: takes photo of component\nSystem: instantly matches visual patterns and retrieves relevant documentation\n\nTechnical Innovation Deep Dive\n\nOur image-based search uses the same ColPali architecture but in a novel way:\n\nImage Processing Stage\nquery_image → 1030 patches → visual embeddings\n[Patch1: visual features] → projection → [128-dim vector]\n[Patch2: visual features] → projection → [128-dim vector]\n...\n[Patch1030: visual features] → projection → [128-dim vector]\n\nSimilarity Matching\nJust as with text queries, but now comparing visual patterns:\nImage Patches          Document Page Patches\n[1030 x 128]       vs      [1030 x 128]\n\nMaxSim Operation\nFor each query patch, find the best matching document patch:\nMaxSim(query_patch, document_patches) → highest similarity score\nSum(best_matches) → final document relevance score\n\nValidation and Impact\n\nWe've validated this approach using a jewelry catalog dataset, achieving:\n\n98% accuracy in component matching\nSub-second processing time\nZero need for text query formulation\n\nThis innovation addresses several critical maintenance challenges:\n\nImmediate Recognition\nNo need to describe complex components in words\nInstant identification of similar parts\nReduction in misidentification risk\nVisual Pattern Matching\nIdentifies similar components across documentation\nMatches wear patterns and damage indicators\nFinds relevant maintenance procedures based on visual cues\nTime Savings\nEliminates need for complex text searches\nReduces documentation navigation time\nSpeeds up maintenance procedures\nEnterprise Evolution\n\nBuilding on these innovations, we're developing a comprehensive enterprise solution:\n\n\nConclusion: Transforming Aircraft Maintenance Through Visual Intelligence\n\nThe journey from traditional documentation to an intelligent visual-first system represents more than just technological advancement—it marks a fundamental shift in how maintenance technicians interact with critical information. Through our research and implementation, we've demonstrated that combining visual and textual understanding can dramatically improve both safety and efficiency in aircraft maintenance.\n\nImpact on Aircraft Maintenance Safety\n\nOur dual RAG system, powered by ColPali, has achieved significant improvements in critical safety metrics:\n\nThe 95% reduction in documentation-related errors means:\n\nTechnicians can confidently identify correct components\nMaintenance procedures are followed with precision\nSafety-critical steps receive visual validation\nThe risk of misinterpretation is virtually eliminated\n\nConsider the real-world impact: When a technician approaches a complex system like the landing gear assembly, our solution provides immediate visual confirmation of each component, ensuring that crucial maintenance steps are performed on exactly the right parts in precisely the right sequence.\n\nTransformation of Maintenance Efficiency\n\nThe system's ability to reduce search time by 70% translates into tangible benefits:\n\nAircraft return to service more quickly\nTechnicians focus on maintenance rather than documentation\nTraining time for new technicians is reduced\nResource utilization is optimized\n\nOur innovative image search capability, currently under review by the ColPali team, promises to push these boundaries even further. By enabling technicians to simply photograph components for immediate documentation access, we're not just improving efficiency—we're reimagining how maintenance information can be accessed and utilized.\n\nFuture Vision and Impact\n\nLooking ahead, our commitment to advancing this technology continues through:\n\nEnhanced Visual Intelligence\n\nDirect image-based documentation retrieval\nReal-time component recognition\nPredictive maintenance through visual pattern analysis\n\nEnterprise Integration\n\nScalable, secure deployment across maintenance operations\nIntegration with existing maintenance management systems\nComprehensive audit trails and compliance tracking\n\nContinuous Innovation\n\nOngoing collaboration with the ColPali development team\nRegular integration of user feedback\nAdaptation to emerging maintenance challenges\n\n🚨 For technical details, implementation guide visit our Colab Notebook: [\nhttps://colab.research.google.com/drive/18E4Bla2SXzKah0qGxKu8J6HSvnKNFFCJ?usp=sharing\n]\n\nOpen Source Contribution\n\nOur proposal (Pull Request #160) to the ColPali repository introduces direct image-based search capabilities. This feature emerged from our real-world experience with aircraft maintenance challenges and has been validated through extensive testing with a jewelry catalog dataset.\n\nFor those interested in contributing to or learning more about this innovation:\n\nReview the complete proposal: \nhttps://github.com/illuin-tech/colpali/issues/160\nExamine our proof-of-concept implementation\nConsider how this approach might benefit other technical documentation use cases\n\nThe collaboration with the ColPali team demonstrates the power of open-source development in advancing critical safety technologies. While our current implementation focuses on aircraft maintenance, the underlying technology could benefit any field requiring precise visual component identification and documentation retrieval.\n\nLooking Forward: A Community-Driven Approach to Innovation\n\nBuilding on our successful implementation and the encouraging response from the ColPali team, we envision a future where the maintenance community collaboratively advances visual documentation technology. Our open-source contribution not only proposes new features but invites broader participation in shaping the future of technical documentation.\n\nWhen we shared our image search innovation with the ColPali team, we emphasized several key benefits that resonated with the maintenance community:\n\nImmediate Practical Impact\nThe ability to initiate searches directly from component photographs addresses a daily challenge faced by maintenance technicians worldwide. As one technician noted during testing: \"This is exactly what we've needed - being able to show the system what we're looking at rather than trying to describe it.\"\n\nCross-Industry Potential\nWhile our implementation focused on aircraft maintenance, the underlying technology can benefit any field requiring precise technical documentation. For example:\n\nMedical equipment maintenance\nIndustrial machinery repair\nAutomotive diagnostics\nManufacturing quality control\n\nCollaborative Development Path\nOur pull request (\nhttps://github.com/illuin-tech/colpali/issues/160\n) provides:\n\nDetailed implementation documentation\nProof-of-concept validation\nPerformance metrics\nIntegration guidelines\nFinal Thoughts: Transforming Maintenance Through Vision\n\nAs we conclude this presentation of our work, it's worth reflecting on the journey from traditional documentation to an intelligent visual system. Our solution, combining ColPali's powerful vision-language model with innovative dual RAG architecture, has demonstrated that we can fundamentally transform how maintenance technicians interact with critical information.\n\nThe metrics tell a compelling story:\n\n95% reduction in documentation errors means safer aircraft\n70% faster information retrieval means more efficient maintenance\n98% accuracy in component identification means confident technicians\n100% visual validation means verified safety procedures\n\nBut perhaps the most significant achievement lies in what these numbers represent: a maintenance environment where technicians can focus entirely on their expertise rather than wrestling with documentation. Each improvement in our system translates directly to enhanced safety and efficiency in aircraft maintenance operations.\n\nCall to Action\n\nWe invite the broader maintenance and technical documentation community to:\n\nExplore our implementation and findings\nContribute to the ongoing development via our GitHub repository\nReview and comment on our ColPali feature proposal\nShare experiences and suggest improvements\n\nThe future of technical documentation is evolving, and through collaborative effort, we can ensure it evolves in a direction that best serves the needs of maintenance professionals worldwide. By combining the power of visual AI with the practical wisdom of maintenance experts, we're not just improving documentation—we're reimagining how technical knowledge can be shared and applied.\n\nDemo :-\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nFiles\npb.ipynb",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "aether-a-retrieval-augmented-generation-assistant-for-ready-tensor-publications-R5kOoJTKUPsC",
    "username": "kKennedy Kiragu Maina",
    "license": "MIT License",
    "title": "Aether: A Retrieval-Augmented Generation Assistant for Ready Tensor Publications",
    "publication_description": "Back to publications\nJul 14, 2025\n●\n48 reads\n●\nMIT License\nAether: A Retrieval-Augmented Generation Assistant for Ready Tensor Publications\nAAIDC2025\nChromaDB\nFastAPI\nGoogle Generative AI\nIntent Recognition\nLangChain\nRe-ranking\nK\nKennedy Kiragu Maina\nLike\nBookmark\nShare\n\nAbstract\n\nThis publication introduces Aether, a conversational AI assistant designed to provide users with a seamless and intuitive way to access and understand information from the \"Ready Tensor\" collection of publications. Aether leverages a Retrieval-Augmented Generation (RAG) architecture to deliver accurate and contextually relevant answers to a wide range of user queries. This document details the project's architecture, implementation, and practical applications, serving as a comprehensive guide for developers, researchers, and anyone interested in the intersection of conversational AI and information retrieval.\n\n1. Introduction\n1.1. Purpose and Objectives\n\nThe primary purpose of this project is to develop and showcase a sophisticated, yet user-friendly, conversational AI assistant named Aether. The core objective is to provide a tool that can accurately and efficiently answer questions about the \"Ready Tensor\" publications, demonstrating the practical application of Retrieval-Augmented Generation (RAG) models in creating intelligent systems that can comprehend and converse about specialized knowledge domains. To achieve this, the project focuses on building a robust backend service using FastAPI and LangChain to orchestrate the RAG pipeline, complete with a re-ranking step to enhance search result relevance. This is complemented by an intuitive Next.js frontend, an efficient data ingestion process using a Chroma vector database, and an intent detection mechanism to provide tailored responses while maintaining conversation history for a natural, context-aware user experience.\n\n1.2. Intended Audience and Use Case\n\nThis publication is designed for a broad technical audience, including developers and AI engineers interested in building conversational AI, researchers and data scientists exploring RAG models, and students or enthusiasts learning about large language models and vector databases. The primary use case for Aether is to serve as an intelligent search and summarization tool for the \"Ready Tensor\" publications. It allows users to ask questions in natural language and receive concise, accurate answers, eliminating the need to manually sift through lengthy documents.\n\n2. Methodology\n2.1. System Architecture\n\nAether's architecture consists of a backend service and a frontend web application that work in tandem. The backend, built with FastAPI, exposes a RESTful API and uses LangChain to orchestrate the entire RAG pipeline, from data ingestion to response generation. The \"Ready Tensor\" publications are indexed into a Chroma vector database using Google's embedding-001 model, while the gemini-2.5-pro model generates the final natural language responses. The frontend is a responsive Next.js application that provides a clean, intuitive chat interface for users to interact with Aether, sending queries to the backend and displaying the assistant's responses.\n\n2.2. The RAG Pipeline\n\nThe core of Aether's functionality is its sophisticated RAG pipeline, which processes user queries to generate accurate, context-aware answers. When a user submits a query, it is first assessed by a lightweight gemini-1.5-flash model to determine the user's intent—for instance, whether they are asking a general question, requesting a list of publications, or seeking a summary of a specific document.\n\nBased on this intent, the system follows one of two paths. For straightforward requests, such as \"list all publications,\" Aether takes a \"fast path,\" directly querying a JSON file of publication data to return a precise answer without engaging the more complex RAG components. For more nuanced or open-ended questions, the system initiates the \"slow path.\" This begins with a semantic search in the Chroma vector database to retrieve text chunks whose meanings are closest to the user's query.\n\nTo ensure the highest relevance, these retrieved chunks undergo a re-ranking step using a Cross-Encoder model, which meticulously re-orders the results to prioritize the most contextually appropriate information. Finally, the top-ranked document chunks are combined with the original question and fed into the powerful gemini-2.5-pro model. This model synthesizes the information to generate a comprehensive, well-formatted, and friendly response. Throughout this process, Aether maintains a conversation history, allowing it to understand follow-up questions and provide a more natural, flowing conversational experience.\n\n3. Implementation\n3.1. Getting Started\n\nTo run Aether on your local machine, you will need Python (3.8+), Node.js (14.0+), and their respective package managers (pip and npm) installed. The setup process for the backend and frontend is as follows:\n\n# Backend Setup:\n# 1. Clone the project repository from GitHub.\ngit clone <repository_url>\n\n# 2. Navigate to the foundoune directory.\ncd foundoune\n\n# 3. Install the required Python packages:\npip install -r requirements.txt\n\n# 4. Set the DATA_PATH environment variable to the path of your project_1_publications.json file.\nexport DATA_PATH=/path/to/your/project_1_publications.json\n\n# 5. Add the GEMINI and GOOGLE api keys to the project’s environment.\nexport GEMINI_API_KEY=\"YOUR_GEMINI_API_KEY\"\nexport GOOGLE_API_KEY=\"YOUR_GOOGLE_API_KEY\"\n\n# 6. Start the FastAPI server:\nuvicorn main:app --reload\n# Frontend Setup:\n# 1. Navigate to the aether-frontend directory.\ncd aether-frontend\n\n# 2. Install the required npm packages:\nnpm install\n\n# 3. Start the Next.js development server:\nnpm run dev\n\nOnce both services are running, you can access the Aether chat interface by navigating to http://localhost:3000 in your web browser.\n\n3.2. Live Demo\n\nLive URL: \nhttps://aether.alkenacode.dev\n\n4. Results and Discussion\n4.1. Performance Evaluation\n\nTo validate the effectiveness and reliability of the Aether assistant, a systematic evaluation of its core RAG capabilities was conducted. The assessment focused on three critical metrics that define the quality of any retrieval-augmented system:\n\nContext Precision/Recall: The system's ability to retrieve the correct and most relevant documents from the knowledge base in response to a query.\nFaithfulness: The degree to which the generated answer is factually consistent with the retrieved context, ensuring the model does not invent or \"hallucinate\" information.\nAnswer Relevancy: The extent to which the final, synthesized answer directly addresses the user's original question.\n\nThe test suite for this evaluation included a diverse set of queries targeting specific topics within the knowledge base, such as the implementation of YOLO models, the nuances of software licenses, and best practices for open-source contributions.\n\nThe results of this evaluation were definitive. The system demonstrated exceptional performance, achieving a perfect 5/5 score across all three metrics for every test case. This outcome confirms that the system not only excels at retrieving the correct documents but also at synthesizing their content into answers that are both factually accurate and directly relevant to user queries.\n\nThe figure below provides a visual summary of this high performance, illustrating the consistent success of the RAG pipeline across the test suite.\n\nThis rigorous validation confirms that Aether's RAG architecture provides a robust and trustworthy foundation for information retrieval and response generation.\n\n4.2. Use Cases and Examples\n\nWith its performance validated, Aether excels at handling a wide variety of queries about the \"Ready Tensor\" publications. For example, a user can request a high-level overview by asking, \"Can you summarize the main points of the 'Hands on Computer Vision' publication?\" For more targeted information, a user might ask, \"What are the key features of the MIT License?\" The assistant also supports comparative analysis, answering questions like, \"What are the key differences between the 'Transformers' and 'RNNs' publications?\" These examples illustrate Aether's ability to function as a versatile and reliable information retrieval tool.\n\n4.3. Future Work\n\nBuilt upon a successful and validated foundation, Aether is well-positioned for future enhancements. The most immediate step is to expand the knowledge base beyond the \"Ready Tensor\" publications to include a wider range of documents, thereby increasing its utility. Concurrently, the intent detection mechanism could be refined to handle a broader and more complex spectrum of user queries with greater precision. Finally, future development can focus on improving Aether's ability to handle complex questions that require multi-step reasoning and inference, moving it closer to a truly comprehensive research assistant.\n\n5. Conclusion\n\nThe successful evaluation of Aether confirms its efficacy as a powerful and versatile conversational AI assistant. The results demonstrate that the system reliably grounds its responses in the provided knowledge base, effectively bridging the gap between information retrieval and natural language generation. By implementing a robust RAG architecture, Aether provides a seamless and intuitive way for users to access and understand specialized knowledge domains with a high degree of confidence in the accuracy of the information provided.\n\nThe project is open-source and available for exploration and contribution. Feedback and collaborative efforts from the community are welcome. The project can be accessed at \nhttps://github.com/Kiragu-Maina/aether-rag-assistant\n.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\n1.1. Purpose and Objectives\n\n1.2. Intended Audience and Use Case\n\nMethodology\n\n2.1. System Architecture\n\n2.2. The RAG Pipeline\n\nImplementation\n\n3.1. Getting Started\n\n3.2. Live Demo\n\nView all\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "agentconnect-decentralized-collaboration-framework-for-independent-ai-agents-RLFuglEDiwwS",
    "username": "aAkshat Joshi",
    "license": null,
    "title": "AgentConnect: Decentralized Collaboration Framework for Independent AI Agents",
    "publication_description": "Back to publications\nMar 31, 2025\n●\n337 reads\n●\nApache 2.0\nWinner of\nBest Overall Project\nat the\nAgentic AI Innovation Challenge 2025\nAgentConnect: Decentralized Collaboration Framework for Independent AI Agents\nAgent Discovery\nAgent Framework\nAgentConnect\nAI Interoperability\nAutonomous Agents\nCapability-Based Discovery\nDecentralized AI\nInter-Agent Communication\nLLM\nMulti-Agent Systems (MAS)\nOpen Source\nScalable AI Systems\nSecure Communication\nA\nAkshat Joshi\nR\nRavikant Agrawal\nLike\nBookmark\nShare\nTL;DR\n\nAgentConnect provides a novel, decentralized communication and discovery layer enabling independent AI agents—potentially possessing their own complex internal structures—to dynamically find, interact, and collaborate securely. Unlike traditional, centrally controlled multi-agent systems, AgentConnect fosters a peer-to-peer ecosystem where agents built using diverse tools and frameworks can interoperate based on advertised capabilities. It features capability-based discovery via a decentralized registry, secure message routing through a communication hub, robust security protocols, multi-provider LLM support, and integration with LangSmith for monitoring, paving the way for truly scalable and flexible collaborative AI applications.\n\nIntroduction: The Interoperability Challenge for Independent Agents\n\nImagine building a specialized AI agent for financial analysis, while another developer creates a powerful web research agent. How can these two independently developed and operated agents discover each other's capabilities and collaborate on a task requiring both skills? This fundamental challenge of interoperability hinders the development of complex, emergent AI ecosystems. Existing multi-agent frameworks often operate within closed, centrally managed environments, limiting agent autonomy and preventing seamless collaboration between agents from different origins or built with different technologies.\n\nAgentConnect tackles this challenge head-on. It is not another framework for building internal multi-agent workflows within a single system (though agents using AgentConnect can certainly employ such internal structures). Instead, AgentConnect provides a crucial decentralized communication and discovery layer above individual agent implementations. It enables true peer-to-peer collaboration in a scalable, secure, and flexible manner, fostering a \"universe\" of interoperable AI agents where specialized capabilities can be dynamically discovered and leveraged across boundaries.\n\nThis publication introduces the AgentConnect framework, details its innovative architecture, highlights key features, showcases practical examples, and discusses its potential impact on the future of collaborative AI.\n\nThe Problem: Limitations of Centralized Multi-Agent Systems\n\nTraditional approaches to multi-agent AI face significant limitations, particularly when attempting to connect independently developed agents:\n\nCentralized Control & Bottlenecks: Most multi-agent systems rely on a central orchestrator, limiting individual agent autonomy and creating single points of failure that hinder scalability.\nLack of Interoperability: Agents built with different frameworks or by different teams typically cannot communicate or collaborate, creating siloed ecosystems. Integration is often manual, brittle, and non-scalable.\nStatic Connections: Interactions are often pre-defined, preventing dynamic discovery of new capabilities or agents joining the network.\nScalability Challenges: Centralized architectures struggle to manage communication and coordination efficiently as the number of agents grows significantly.\nSecurity Concerns: Establishing trust and secure communication channels between independently managed agents in a decentralized way is complex without a dedicated framework.\n\nThese limitations prevent the realization of a truly dynamic and open ecosystem where diverse AI agents can seamlessly collaborate.\n\nAgentConnect: A Decentralized Solution for Inter-Agent Collaboration\n\nFigure 1: AgentConnect Ecosystem - The four foundational pillars of AgentConnect: Decentralized Discovery for finding collaborators without central control, Secure Communication for verified messaging, Agent Autonomy preserving independent operation, and Tool-Based Interaction that agents dynamically invoke based on reasoning when they cannot complete tasks independently.\n\nAgentConnect provides the necessary infrastructure for a decentralized network of autonomous agents:\n\nDecentralized Network Topology: Eliminates central control, allowing agents to operate as independent peers.\nDynamic Capability-Based Discovery: Agents find collaborators based on what they can do, facilitated by a Decentralized Agent Registry.\nSecure & Standardized Communication: A Communication Hub routes cryptographically signed messages using defined protocols, ensuring trust and interoperability.\nPreservation of Agent Autonomy: AgentConnect provides the connectivity layer; it does not dictate internal agent logic, implementation, or decision-making. Agents can use any internal architecture (e.g., LangGraph, custom code).\nHorizontal Scalability: Designed to support a vast network of interacting agents without performance degradation associated with central coordinators.\nKey Features\n\nAgentConnect offers a rich set of features designed for building robust and flexible decentralized agent networks:\n\nFigure 2: AgentConnect Key Features Flow - This comprehensive diagram illustrates the complete agent collaboration lifecycle. It shows how agents with their own internal multi-agent systems can discover others through the Registry based on capabilities, securely exchange messages through the Communication Hub, and make autonomous decisions about collaboration. The numbered flow demonstrates the end-to-end process from user input to final response.\n\nDynamic Agent Discovery: Benefit: Enables flexible, ad-hoc collaboration based on real-time needs, not static configurations. Supports exact and semantic matching.\nDecentralized Communication: Benefit: Ensures scalability and resilience by avoiding central bottlenecks. Promotes true peer-to-peer interaction logic.\nAgent Autonomy: Benefit: Maximum implementation freedom for developers. Allows agents to encapsulate complex internal processes (including their own MAS) while participating externally.\nSecure Communication: Benefit: Guarantees message authenticity and integrity using cryptographic signing (DID-based) and standardized protocols, crucial for trust in a decentralized setting.\nMulti-Provider LLM Support: Benefit: Flexibility to choose the best AI model (OpenAI, Anthropic, Groq, Google AI) for an agent's specific needs, fostering diverse capabilities within the network.\nIntegrated Monitoring (LangSmith): Benefit: Provides deep observability into complex inter-agent interactions, aiding debugging, performance analysis, and optimization.\nArchitecture Deep Dive\n\nAgentConnect's power stems from its three core pillars: the \nDecentralized Agent Registry\n, the \nCommunication Hub\n, and the principle of Independent Agent Systems. This architecture is specifically designed to facilitate interaction between distinct agent entities, regardless of their internal complexity.\n\nDecentralized Agent Registry\n\nThe \nRegistry\n serves as the network's decentralized directory. It stores AgentRegistration information, including agent IDs, verifiable identities (DIDs), and detailed capabilities. Crucially, it performs \nidentity verification\n upon registration requests (forwarded by the Hub) and facilitates \ncapability-based discovery\n when queried by agent Tools. It does not control agents but acts as a verifiable information source.\n\nCapability Discovery Service\nAgent Registry\nsearch_for_agents Tool\nInternal Workflow\nAgent A\nCapability Discovery Service\nAgent Registry\nsearch_for_agents Tool\nInternal Workflow\nAgent A\nAsync Discovery Process\nAsynchronous tool execution\nNon-blocking operation\nPerforms exact matching or\nsemantic vector search\nAgent can now initiate secure collaboration with discovered agents\nNeeds agent with\nspecific capability\nsearch_for_agents(capability=\"data_analysis\")\nquery_registry(capability=\"data_analysis\")\nfind_by_capability_name() or\nfind_by_capability_semantic()\n[matched_agents_with_scores]\n[agent_metadata, ...]\nReturn matching agents\nanalyze_results()\nSelect appropriate agent\nfor collaboration\n\nFigure 3: Agent Discovery Flow - This diagram illustrates how an agent's internal workflow uses the Registry to find peers with specific capabilities, demonstrating the dynamic discovery mechanism that enables ad-hoc collaboration.\n\nCommunication Hub\n\nThe \nHub\n is the connection and message routing layer. Agents connect directly to the Hub to join the network.\n\nRecipient Agent\nCommunication Hub\nsend_collaboration_request Tool\nSender Agent\nRecipient Agent\nCommunication Hub\nsend_collaboration_request Tool\nSender Agent\nSecure Asynchronous Communication Process\nCryptographic Security Verification\nProtocol Validation\nAsynchronous Message Routing\nNon-blocking delivery\nusing asyncio.create_task()\nAsynchronous Internal Processing\nResponse Correlation\nComplete request-response cycle with protocol-verified secure communication\nsend_collaboration_request(recipient_id, task)\nroute_message(message)\n1. Verify sender identity (via DID)\n2. Verify receiver identity (via DID)\n3. Verify message signature (cryptographic)\n4. Validate protocol compliance\n1. Check protocol version compatibility\n2. Validate message type against protocol\n3. Apply protocol-specific rules\ncreate_task(deliver_message())\nreceive_message(message)\n1. Queue message (asyncio.Queue)\n2. Process in worker task\n3. Invoke internal workflow\nresponse_message\nMatch response to original request ID\nresponse\ncollaboration_result\n\nFigure 4: Agent Communication Flow - Illustrating AgentConnect's communication feature, this diagram shows how agents securely exchange messages through the Hub, which handles verification, routing, and response correlation.\n\nRegistration: When an agent connects, the Hub coordinates with the Registry to verify the agent's identity and record its registration details.\n\n# Example: Agent Registration (Developer facing code)\n# (Inside an async function)\nfrom agentconnect.agents import AIAgent\nfrom agentconnect.communication import CommunicationHub\nfrom agentconnect.core.registry import AgentRegistry\n# ... other imports: AgentIdentity, Capability, etc.\n\n# 1. Initialize Hub and Registry\nregistry = AgentRegistry()\nhub = CommunicationHub(registry)\n\n# 2. Create Agent with Identity and Capabilities\nai_identity = AgentIdentity.create_key_based()\nai_capabilities = [Capability(name=\"summarize\", description=\"Summarizes text\", ...)]\nai_agent = AIAgent(\n    agent_id=\"ai_summarizer\",\n    identity=ai_identity,\n    capabilities=ai_capabilities,\n    # ... other parameters: provider, model, api_key ...\n)\n\n# 3. Register the agent with the Hub\nif await hub.register_agent(ai_agent):\n    print(f\"Agent {ai_agent.agent_id} registered successfully.\")\n    # Agent is now discoverable and can communicate\nelse:\n    print(f\"Failed to register agent {ai_agent.agent_id}.\")\n\nSecure Routing: The Hub ensures secure message delivery between verified agents. When a message is sent (typically via a Tool calling Hub methods like send_collaboration_request), the Hub verifies the sender's signature against their registered identity before forwarding the message to the recipient agent's receive_message method. It also manages the asynchronous request-response lifecycle. The Hub guarantees transport security but does not dictate agent behavior or interpret message content beyond verification needs.\n\nIndependent Agent Systems and the Role of Tools\n\nAgents\n are autonomous entities. The \nBaseAgent\n class defines the core structure and the interface required to interact with the Hub.\n\n# Example BaseAgent interface methods developers implement/use\nclass BaseAgent(ABC):\n    # ... __init__ ...\n\n    # Called BY THE HUB to deliver a message\n    async def receive_message(self, message: Message):\n        \"\"\"Puts an incoming message onto the agent's internal queue.\"\"\"\n        await self.message_queue.put(message)\n        # ...\n\n    # Implemented BY THE DEVELOPER (subclass like AIAgent)\n    @abstractmethod\n    async def process_message(self, message: Message) -> Optional[Message]:\n        \"\"\"Contains the agent's core logic for handling a message from its queue.\"\"\"\n        pass\n\n    # Optionally Implemented BY THE DEVELOPER (subclass like AIAgent)\n    async def run(self):\n        \"\"\"Agent's main loop: gets messages from queue, processes them via process_message.\"\"\"\n        # ... loop structure ...\n        pass\n\n    # Agent's Identity property (used BY THE HUB for verification)\n    @property\n    def identity(self) -> AgentIdentity:\n        # ... returns the agent's identity object ...\n        pass\n    # ... other base methods ...\n\nCrucially, complex agents like \nAIAgent\n typically implement their process_message logic by invoking an internal reasoning workflow (e.g., a ReAct agent built with LangGraph, using workflows defined in the \nPrompts module\n). This workflow makes decisions and interacts with the outside world using Tools.\n\nThe Role of \nTools\n: Tools bridge the agent's internal reasoning loop with the AgentConnect framework's capabilities:\n\nDiscovery: The workflow might decide it needs an agent with a specific capability. It calls the search_for_agents Tool. This tool interacts with the AgentRegistry to find suitable peers and returns the information to the workflow.\nCollaboration: The workflow formulates a request for another agent. It calls the send_collaboration_request Tool. This tool interacts with the CommunicationHub to securely send the request message and manage the response.\nInternal Tasks: Tools like decompose_task help the agent manage its own process.\n\nThis Tool-based approach keeps the core Agent classes focused on state management and the basic communication interface, while the dynamic, intelligent interaction logic resides within the agent's configurable workflow and the specialized Tools it uses.\n\nUse Cases & Examples\n\nAgentConnect enables a wide range of collaborative agent applications across different domains. Our examples demonstrate the framework's versatility:\n\nFigure 5: AgentConnect Use Case Implementation - This diagram illustrates how the framework enables specialized independent agents to collaborate. Each agent offers distinct capabilities (web searching, data analysis, content processing, and user interface) while maintaining full autonomy. The \nexamples\n directory provides working implementations of these agents, demonstrating dynamic discovery and secure communication through the AgentConnect hub.\n\nResearch & Knowledge Tasks\n\nBuild assistants that can dynamically discover and collaborate with specialized knowledge agents. These systems can retrieve information from multiple sources, analyze findings, and synthesize comprehensive responses to complex research queries by leveraging the unique capabilities of different agents in the network.\n\nData Analysis & Insights\n\nCreate data-focused applications where agents specializing in different analytical techniques collaborate to process, visualize, and extract insights from datasets. One agent might handle data preparation, another specialized in statistical analysis, and a third focused on generating visualizations or natural language explanations.\n\nMulti-Stage Complex Problem Solving\n\nImplement sophisticated workflows where a coordinating agent breaks down complex tasks, discovers specialized agents with relevant capabilities, and orchestrates their collaboration to solve multi-stage problems that would be difficult for a single agent to handle.\n\nIntegration with External Platforms & Services\n\nConnect agent networks to external platforms like messaging services, allowing users to interact with a front-end agent that can seamlessly leverage capabilities from other agents in the network. This enables building sophisticated user-facing applications with rich, distributed functionality behind the scenes.\n\nThe framework's flexible architecture supports many more use cases, from creative content generation to complex decision support systems. For detailed examples and code, see the \nExamples Documentation\n and \nExamples on GitHub\n.\n\nImplementation Highlights\n\nAgentConnect is built using robust and modern technologies:\n\nCore Logic: Python 3.11+, leveraging asyncio for non-blocking agent operations and concurrent message processing\nAI Integration: Built with LangChain for multi-provider LLM support and LangSmith integration for tracing agent interactions\nSecurity: Implements message signing and verification using Python's cryptography library and DID-based identity verification\nModularity: Well-structured codebase with clean separation between core components for maintainability\nTool-Based Interaction: Flexible agent capabilities implemented through standardized tool interfaces\nProtocol Support: Extensible communication protocols enabling secure, standardized agent interactions\nProvider Flexibility: Support for multiple LLM providers (OpenAI, Anthropic, Groq, Google AI)\n\nNote: For complete implementation details and API references, please refer to the \nAgentConnect GitHub repository\n and the \nOfficial Documentation\n.\n\nInnovation & Impact\n\nInnovation:\n\nAgentConnect's primary innovation lies in its decentralized approach to inter-agent discovery and communication. It moves beyond monolithic, centrally controlled MAS to enable an open ecosystem where independently developed agents can collaborate as peers. This capability-based dynamic discovery and secure, framework-agnostic communication layer is a novel contribution to the field of agentic AI.\n\nPotential Impact:\n\nAgent Marketplaces: Facilitates the creation of marketplaces where specialized agents can offer their capabilities as services.\nComplex Problem Solving: Enables tackling problems requiring diverse expertise by composing workflows across independent, specialized agents.\nIncreased AI Automation: Lowers the barrier for integrating different AI tools and services developed by different teams or vendors.\nFoundation for Emergent Systems: Creates an environment where complex collaborative behaviors can emerge from the interactions of autonomous agents.\nRoadmap\n\nOur vision for AgentConnect's future development:\n\n🔐 Agent ID and Reputation System\nUnique, verifiable identity for each agent\nDecentralized reputation tracking via performance metrics\nHistorical interaction records for trust and accountability\n🔍 Enhanced Agent Discovery Marketplace\nMarketplace-style discovery with detailed filtering\nAgent capability and performance metrics as search parameters\nIntelligent matching for optimal agent collaboration\n⚡ Anthropic MCP Server Integration\nSeamless integration with Anthropic's MCP server\nAdvanced reasoning and memory management\nEnhanced coordination for complex multi-agent tasks\n🧩 Structured Agent SDK\nStandardized schemas for agent I/O and capabilities\nConsistent definition of parameters and SLAs\nImproved interoperability and discovery efficiency\n💰 Decentralized Payment Infrastructure\nSecure on-chain micropayments via Coinbase's AgentKit\nSupport for pay-per-use and subscription models\nAutonomous economic operation with crypto wallets\n\nThese developments will transform AgentConnect from a powerful communication framework into a complete ecosystem for autonomous agent collaboration.\n\nResources\nGitHub Repository\n - Source code and development\nDocumentation\n - Complete API references and guides\nExamples\n - Working implementations and use cases\nContribution Guide\n - How to get involved\nQuickStart Guide\n - Get up and running quickly\nConclusion\nOvercoming Traditional Limitations\n\nBy eliminating central control points and enabling direct peer-to-peer communication, AgentConnect resolves the fundamental limitations of conventional multi-agent systems. Its decentralized registry and secure communication hub create a resilient network that scales horizontally without traditional bottlenecks.\n\nEmpowering Developer Innovation\n\nThe framework provides developers with unprecedented freedom to build specialized agents that can discover and collaborate with other agents dynamically. This capability-based approach allows developers to focus on their agents' core competencies while leveraging complementary capabilities from the network.\n\nFostering an Open Ecosystem\n\nAgentConnect lays the foundation for a truly open AI ecosystem where independently developed agents can seamlessly interoperate. This interoperability will drive innovation, specialization, and the emergence of new collaborative patterns that would be impossible in siloed frameworks.\n\nBuilding the Future of Collaborative AI\n\nAs AI systems become more sophisticated, the ability for independent agents to collaborate effectively becomes increasingly crucial. AgentConnect provides the vital infrastructure layer for this next generation of AI applications, enabling complex workflows across organizational and technical boundaries.\n\nWe invite the community to join us in exploring and expanding the possibilities of AgentConnect. By contributing to its development, building new agents, and sharing insights, we can collectively shape the future of decentralized, collaborative artificial intelligence.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nTL;DR\n\nIntroduction: The Interoperability Challenge for Independent Agents\n\nThe Problem: Limitations of Centralized Multi-Agent Systems\n\nAgentConnect: A Decentralized Solution for Inter-Agent Collaboration\n\nKey Features\n\nArchitecture Deep Dive\n\nDecentralized Agent Registry\n\nCommunication Hub\n\nIndependent Agent Systems and the Role of Tools\n\nUse Cases & Examples\n\nView all\nComments\n(2)\nMost recent\n\n✨ Want to join the conversation?\n\nSign in or create an account to comment.\nB\nBHABANI SANKAR BREDEKA\nLast month\n\nSome one is required to explain this huge description\n\nLike\nReply\nA\n@anasboss270\n2 months ago\n\nCertificate\n\nLike\nReply\nYou might be interested\nWhat is Agentic AI (AAIDC-Week1-Lesson-1)\nMay 15, 20255357 reads\nAAIDCAgentic AI+5\nWeek 6 Preview: Architecting Multi-Agent Systems (AAIDC-Week6-Preview)\nJun 22, 2025144 reads\nAAIDCAgent Collaboration+9\nArchitecting Intelligence: Design Patterns for Multi-Agent AI Systems (AAIDC-Week6-Lesson-1)\nJun 22, 2025205 reads\nAAIDCAgent Collaboration+9\nAI Agent Penetration Testing Workflow\nC\nMar 31, 202517 reads",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "agentic-ai-a-step-towards-artificial-general-intelligence-7kM1peUvZW9y",
    "username": "aAmina Javaid",
    "license": "MIT License",
    "title": "Agentic AI - A Step towards Artificial General Intelligence",
    "publication_description": "Back to publications\nMar 26, 2025\n●\n95 reads\n●\nMIT License\nWinner of\nDistinguished Technical Deep-Dive\nat the\nAgentic AI Innovation Challenge 2025\nAgentic AI - A Step towards Artificial General Intelligence\nAgentic AI\nAgents\nagno\nArtificial General Intelligence\nFastAPI\nGenerative AI\nLLMs\nMulti-agent Systems\nReAct\nResearch Assistant\nResearch Copilot\nStreamlit\nTools\nA\nAmina Javaid\nLike\nBookmark\nShare\n\nIntroduction\n\nArtificial Intelligence is the ability of a machine to think and behave as close as possible to humans. It enables a system to learn, reason, and solve problems just like humans do. AI agents are at the next level of artificial intelligence. There has been a lot of discussion about AI agents since the beginning of this year as we have been hearing statements from the CEOs of Meta, Microsoft, and Nvidia about the rise of AI agents and the dominance of autonomous AI systems all over the world, potentially reshaping the technological landscape. These big tech giants are cutting down their work force by embedding agentic behavior into their routine tasks. Many companies are rapidly integrating AI agents into their workflows, transforming their processes through intelligent, autonomous systems in order to increase their productivity.\n\nAI agents are considered as the biggest transformation in the history of Artificial Intelligence. They are set to revolutionize the way work gets done in any specific domain.\n\nAI agents have the potential to redefine how work is done across industries. The first question that comes to mind is what exactly are AI agents and how is agentic AI different from traditional AI, specifically generative AI? In this article, I'll explore AI agents in detail, defining them in the simplest possible way and taking them to the next level, exploring agentic patterns, architecture and workflows along with some real-world examples of AI agents and their implementation.\n\nWhat are AI agents?\n\nAn AI agent is an intelligent system that solves a real-world problem by automating tasks, eventually making one's life easier and improving productivity.\n\nThink of AI as raw intelligence. It has got the potential to behave intelligently but it's not really useful until you give it a job. It needs a clear direction in order to utilize its full potential, but where does all this intelligence reside? Well, large language models (LLMs) possess this vast amount of intelligence. They have been trained on huge corpuses of text and are becoming more and more intelligent with every day passing by. Despite all this intelligence, LLMs don't take action on their own. This is where an AI agent steps in. An AI agent is a system which helps achieve a useful goal by using all the intelligence of the LLMs.\nAI agents expand beyond the capabilities of a generative AI model alone. Unlike traditional AI models that generate responses based on prompts, AI agents go a step further. They are provided with clear goals and objectives, and they can autonomously make decisions, interact with external systems, and execute tasks whether its answering questions, managing workflows, or automating daily tasks. Its the difference between knowing how to do something and actually doing it.\n\nHere's a simple way to think about it:\n\nAI = Brain (Knowledge and Intelligence)\nAI Agent = The Executor (The one who makes things happen)\n\nAn AI agent follows a structured process:\n\nUnderstand - Recognize the task at hand.\nThink - Process information and determine the best course of action.\nAct - Execute the task or deliver what is required whether it's retrieving data, solving a problem, or interacting with an external system.\n\nLet's think of an analogy to understand an AI agent:\n\nImagine you return from a long vacation and find your inbox with hundreds of emails waiting for you to go through. You get overwhelmed. You obviously don't have time to read everything.\nNow, think of an AI agent as your smart assistant. It\n\nScans all emails and filters out the most important ones.\nSummarizes key messages so you get a quick overview.\nFlags urgent emails that require immediate action before your meeting.\n\nYou give instructions to your agent according to your preferences and it performs tasks accordingly, saving your time and effort. That's exactly how an AI agent works. Developers provide these agents with detailed instructions so they know exactly how to respond in different situations. Some agents come programmed with basic instructions like an experienced assistant capable of performing routine tasks. Others can be customized to fit your needs like giving the assistant some special rules according to your needs and preferences.\n\nA more formal definition of an AI agent could be:\n\nAn AI agent is a software system which is capable of performing tasks autonomously in order to achieve a specific goal by leveraging the capabilities of various tools and external systems such as APIs and databases, and by interacting with other agents.\n\nAn AI agent takes your goal, processes information, makes decisions, and executes actions - automating tedious tasks so you don't have to.\nAgents are not as simple as we think they are. They are complex systems involving\n\nadvanced problem-solving,\nfull-stack development, and\nintelligent decision-making.\n\nNot everything is an AI agent. Nowadays, every developer wants to develop an AI agent whether it's useful or not.\n\nBe cautious! Every single thing IS NOT and SHOULD NOT be an AI agent.\n\nWe have to be very thoughtful while putting in efforts developing an AI agent. It should solve a real-world problem and be intelligent enough to help humans in their routine tasks enhancing their productivity. It can also be helpful in specialized tasks using its intelligence and decision-making capabilities. Otherwise, it's just a piece of complicated software.\n\nImportance of AI Agents\n\nAgentic AI is the future!\n\nHowever, the concept of agents is not new - it has already been there since decades.\n\nIn their book \"Artificial Intelligence - A Modern Approach\", Stuart J. Russel & Peter Norvig define an agent as:\n\nAn agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\n\nToday, when we talk about AI agents, we are usually referring to LLM-powered AI agents.\n\nLLMs on their own have certain limitations like they are constrained by the data they have been trained on. They are capable of performing a variety of tasks like summarizing documents, creating drafts for emails, and generating reports to name a few. Fine-tuning LLMs requires a substantial amount of investment in data and resources. This is where AI agents take the charge. The real magic happens when we start building systems around LLMs, essentially taking the LLM and integrating it into our processes and workflows.\n\nFor example, imagine an autonomous AI agent that scans your inbox to summarize important emails, highlighting the ones requiring urgent attention, saving a lot of time that can be used to focus on more productive tasks.\n \nTaking this a step further, think of a collaborative research copilot, a team of AI agents working together to achieve the research goals. \n\nA paper extraction agent fetches relevant research papers.\nA summarization agent generates concise summaries of the papers.\nAn interactive chat agent allows meaningful interaction with those research papers.\n \nThese agents, working autonomously and in coordination could help generate meaningful literature reviews, enabling the researchers focus on experimentation and core research objectives.\nSo, what's the role of LLMs in AI agents? They serve as the main intelligence layer, helping the agents plan, organize, and make decisions about task execution. The role of LLMs is to help the agents achieve the level of autonomy they require, adapting them to tasks dynamically rather than following rule-based workflows.\n\nThe future of AI isn't just about more powerful models - it's about intelligent, autonomous systems that enhance productivity and decision-making. Systems that automate repetitive tasks, taking your business to the next level. That's the power of Agentic AI.\n\nAgentic AI is potentially being considered as the biggest transformation we'll see in our lifetime. Those who embrace AI agents now will have a major advantage over those who don't. The world of AI is changing at a fast pace and we must understand the importance of AI agents and their use in our particular scenario, otherwise we might be left behind in this rapidly changing world. The key is identifying the right agent type based on our particular requirements.\n\nAgentic AI vs Generative AI\n\nHow is an AI agent different from a traditional AI system?\n\nThe key difference lies in its autonomy- the ability to think independently (to a certain extent) in order to make decisions and give intelligent suggestions.\n\nAccording to the Agents whitepaper by Google,\n\nThe combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent.\n\nAn AI agent has a goal to achieve.\nIt utilizes a generative AI model to perform some of its basic tasks.\nIt learns from historical data, improving its capabilities over time.\nIt is aware of its environment and can access various tools to gather and process real-time information.\n\nWhile a traditional AI system completes its task as directed, an AI agent adds more intelligence to traditional AI capabilities, behaving more like a human.\n\nLet's say we want to automate the process of ordering food at a restaurant. A traditional AI chatbot would interact with the user, take the order, extract important information and place that order according to the user's requirement. However, an AI agent would remember the returning users, recall what they ordered from their past experiences, and give them valuable suggestions or recommendations to make their experience more pleasant. That sounds more like an intelligent human who is conscious and aware of his returning customers.\n\nGenerative AI is all about LLM models. Big tech giants have been competing to build the best LLM models for the past few years. 2024 has been the year to build custom chatbots using LLMs like OpenAI, Llama3, and others. These LLMs help generate content. A query by the user is sent to the LLM in the form of a prompt which generates a response as an output. These LLMs can also be further fine-tuned for specific tasks.\n\nAgentic AI, on the other hand, are autonomous AI systems which are able to perform tasks on their own. 2025 is the year to build autonomous AI agents. They have a specific goal to achieve. These AI systems work independently without any human intervention to achieve their goal. You can integrate as many tools into these systems as you want. The goal is mostly based on a business outcome. They have complex workflows and can fine-tune themselves to improve their performance.\n\nWhere do LLMs stand in the scope of AI Agents?\n\nLLMs are the brain of AI agents. They provide the agents with the autonomy they require to perform specific tasks. They are smart enough to break the bigger task into smaller sub-tasks and decide a plan of action to execute dynamically. They enable the agent to perform in a specific manner using memory, knowledge, and tools.\n\nAgentic workflows have LLM calls chained together where an agent lets the LLM decide how many times to run. It may continue the loop until it finds a resolution. For example, talking to a customer for customer support or iterating on code changes. When you don't know the number of steps to complete a task, it is an autonomous workflow where we don't predefine a path. All this has been possible with the evolution of large language models (LLMs). LLMs and tools are getting better and smarter every day making the agents more capable and prevalent.\n\nImportance of Prompt for an AI Agent\n\nPrompt is the most important part in giving instructions to an AI agent. It should be clear and thoughtful. It must define the goal that wants to be achieved and specify the methodology to achieve that goal. The tools that are given to the model must be properly documented. This helps the LLM in deciding which tool to use for a particular task. The prompt should be detailed enough but we cannot deny the importance of properly documented tools. How can you use tools and function calling if you don't know what the parameters mean and what that tool actually does? Tools should be described properly to understand their purpose. Writing a good tool description influences other parts of the agent prompt.\n\nControl logic of a compound AI system is the path to answer a query. Previously, this control logic had been defined by hard coded rules. Another way to control the logic of a compound AI system is to put the LLM in charge. This has only become possible because we are seeing a lot of improvement in the reasoning capabilities of large language models. You can feed complex problems to LLMs and then prompt them to break the problem into smaller problems and come up with a plan on how to tackle them.\n\nIf we control the logic programmatically, we are designing our system to stick with the instructions being given without deviating from a set path. On the other hand, if we control the logic through LLM, then we are designing our system to break the problem down into sub-problems, make a plan to solve them, try to come up with a solution, and see if we need to readjust the plan accordingly.\n\nAgentic AI approach is putting LLMs in charge of the whole system.\n\nEvolution of AI Agents in Recent Years\n\nSince the release of ChatGPT in November, 2022, there have been various shifts in the field of Generative AI.\n\nModels\n\n2022 was the year of LLMs. Models on their own are limited by the data they have been trained on. This impacts what they know about the world and what tasks they may solve. So they have limited knowledge and are hard to adapt to new changes in the real-world. Information is being generated at the speed of light. Data is all around and new data keeps on being added to the existing data. One option is to fine tune the models but that requires an investment in data, resources, and time. Models on their own could be useful for a variety of tasks like summarizing documents, creating first drafts of emails, or creating reports.\n\nCompound AI Systems\n\nIn 2023, compound AI systems started to emerge. Certain problems are better solved when you apply the principles of system design. Building an AI system around the model makes it possible to solve many complex real-world problems where context is important.\n\nAccomplishing a task through system design is inherently easier to solve than tuning a model. Systems are faster, quicker, and easier to adapt. Systems have programmatic components such as output verifiers, query breakers, database search engines, and customized tools. We can pick the right components to solve our problem.\n\nRetrieval Augmented Generation (RAG) is one of the most popular and commonly used compound AI systems. It utilizes the capabilities of LLMs and adds context to the information, also trying to resolve the hallucination of LLMs.\n\nAI Agents\n\nIn 2024, all focus shifted to building AI agents, LLM agents to be specific. AI Agents are systems built around the large language models (LLMs), actually taking the model and integrating into the processes that we have.\n\nComponents of AI Agents:\n\nFollowing are the core components of an AI agent:\n\nModel - Large Language Model (LLM)\nTools - An agent uses tools to fetch the latest information. They can be databases, APIs, or external systems.\nMemory - Short term and long term memory\nKnowledge - Databases\nReasoning - The ability to think, divide a task into sub-tasks, and plan to execute them autonomously.\n\nEvery agent defines its role, description, and specific instructions to carry out tasks. It specifies the model (LLM) and available tools to be used at various stages during its course of action. Similarly, it also specifies the memory and knowledge base to be used for storing conversations, interactions, and other information.\n\nRole\n\nThe role of an agent defines its personality. It allows the agent to behave in a specific manner appropriate to its assigned task.\n\nModel\n\nLarge language models (LLMs) are the basic building blocks of an AI agent. They are the brain of the agent allowing it to think, understand, reason, plan, and act accordingly. In addition to that, they serve as the generative component of the agent as well, helping it to generate meaningful responses.\n\nTools\n\nTools are external resources that an agent can use to achieve its goal. They can be as simple as Python functions, or APIs, databases, or other external systems. Tools enhance the capabilities of an agent by helping it access information and interact with its environment. Agents are smart enough to understand the functionalities of tools and utilize them as required during their workflow. Examples of tools include:\n\nSearch (web or database),\nCalculator (mathematical calculations),\nCoding agent (to manipulate a database),\nLLM (summarization, translation), and\nAPIs (weather, google maps).\nMemory\n\nAgents possess short term and long term memory for specific purposes. Short term memory is used for immediate interactions with the environment whereas long term memory is used for storing information and conversations. The agents can maintain context, learn from historical data and past experiences, and improve their performance. They are adaptive and learn from past interactions.\n\nKnowledge\n\nKnowledge base helps agents store the information and context which is required to perform their tasks.\n\nReasoning\n\nAgents possess the ability to think and reason utilizing the large language models (LLMs) at their disposal. This helps them achieve the level of autonomy required to complete various tasks moving towards their goal.\n\nReal-World Agentic Use Cases\nAI Trip Planning Assistant\n\nImagine you are going on a 5-day business trip to Netherlands next month. You want to pack efficiently, considering the weather, your meetings, and any formal events.\n\nYou ask an AI agent:\n\nI have a 5-day business trip to Netherlands next month. I need to know what clothes to pack based on the weather, my meetings, and evening events. Can you please help?\n\nAn LLM alone won't be able to solve this problem for you because it needs your specific travel information along with some real-time weather information which it might acquire through APIs.\n\nBreaking Down the Problem\nTrip Duration & Schedule - The agent retrieves your travel dates from your calendar.\nWeather Forecast- It checks the expected temperature, chances of rain, and humidity in Netherlands for that week.\nEvent Types - It scans your calendar for meetings, formal dinners, and casual outing plans, and asks if you would need business attire and whether you would be attending any special events requiring formal wear.\nPacking Strategy - The agent suggests outfit combinations to reduce unnecessary packing. It also considers the number of shoes & accessories needed.\nAdditional Essentials - It reminds you to bring: travel-size toiletries (if your airline has liquid restrictions), chargers, adapters, and documents.\nFinal Packing List - The agent compiles a checklist tailored to your needs. It even reminds you when to pack based on your departure time.\n\nThis is a modular, multi-step planning problem, where the agent retrieves external data, personalizes recommendations, and optimizes the process, making it a great AI agent use case!\n\nHere are a few other agentic AI use-cases which help solve real-world problems.\n\n1. AI Coding Assistant\n\nThink of an agent which writes a code and then tests it to pass all unit tests, corrects the code and continues this loop making the program perfect in terms of quality. When the coding agent sees the error during each step of the loop, it helps the model converge towards the right solution sooner after getting this feedback.\n\n2. Customer Service Chatbot\n\nThink of a chatbot which remembers the customer, knows all the information about particular customers and serves them according to their preferences.\n\n3. AI Healthcare Assistant\n\nA personal healthcare assistant which remembers everything about a person's health records, nutrition requirements, fitness and well-being. It has access to a person's vitals through fitness trackers. It advises a person about regular physical and mental examination based on his/her current condition.\n\n4. Emergency Response System\n\nThink of an automated system which triggers important alarms automatically assessing an emergency situation through its sensors. It might call the appropriate helplines and rescue operations when in need.\n\n5. Email Manager\n\nAn agent to review emails and prepare tasks on a digital to-do list based on the highest priority emails.\n\n6. Environmental Monitoring System\n\nAn environmental sustainability agent that uses weather data, satellite images, and other information to decide how to take care of plants and their environment.\n\n7. AI Travel Planner\n\nA travel planning agent that takes a user's preferences and needs into account, researches things like destinations, hotels, and activities, and then suggests an itinerary for the user. It might use a collaborative agent to book everything taking the user into confidence.\n\nHow do AI Agents Work?\n\nAgents have a clear and specific goal to achieve. Every step they take moves them closer to their end goal. They plan and divide the goal into sub-tasks. They process information iteratively, making decisions at every step about the next action to take based on the output from the previous step. The orchestration layer is responsible for the overall execution of an agent delegating responsibilities to various components of the agentic architecture. Prompts define the role and task of an agent. The better the prompt, the easy it is for the agent to achieve the required objective. The evolution of prompt engineering and task planning for language models helps make the agents much more better with the passage of time.\n\nHere are a few prompt engineering frameworks considered as best for the agents:\n\nReAct - Reason & Act\nCoT - Chain of Thoughts\nToT - Tree of Thoughts\nReAct - Reason & Act\n\nThe ReAct framework is a technique for building AI agents that combine the reasoning capabilities of LLMs with the ability to take actions, allowing them to solve complex problems.\n\nCoT - Chain of Thoughts\n\nChain-of-thought prompting is a technique that enhances the reasoning capabilities of LLMs by guiding them to generate step-by-step explanations before arriving at a final answer.\n\nToT - Tree of Thoughts\n\nTree-of-thoughts framework is a problem-solving technique that allows LLMs to explore multiple reasoning paths simultaneously, and evaluating different solution paths.\n\nAgents can utilize these frameworks or other techniques to choose the next best action based on a user query.\n\nThe quality of an agent's response highly depends on the quality of the prompt it receives.\n\nThese prompts enable the agent to think and plan their tasks in an effective manner. They help them decide which tools or external resources to use in order to complete their assigned tasks properly.\nAn AI agent is as good as the model you are using. You can totally customize your agents with agentic frameworks by providing various tools. Docstring of a function or tool is extremely important. When you register a tool, the LLM understands the purpose of that tool by going over the docstring. The agent gives the docstring to the LLM which can understand it and map it to the intent of the user query. The agent basically uses docstrings to enhance its understanding of its tools. Based on the docstring, the LLM is smart enough to map the function or tool to the user query and also give the required arguments to the function or tool.\n\nTypes of AI Agents\n\nThere are many types of AI agents based on the level of autonomy, task execution, and integration with tools and memory. Here are a few common types of agents:\n\n1. Simple Reflex Agents \n\nThey are the simplest form of automation. They execute preprogrammed instructions without thinking and adaptation. They are mostly efficient but not much flexible. These agents are best suited for repetitive tasks such as email autoresponders and invoice processing agents.\n\n2. Model-based Reflex Agents\n\nThese are LLM-enhanced agents for contextual understanding. They are simple but intelligent and mostly suitable for low-complexity, high-volume tasks such as filtering emails and AI-enhanced content generation.\n\n3. Goal-based Agents\n\nThey have a clear, specific goal to achieve. They instruct the LLM to achieve that goal which thinks and makes a plan of action. They utilize tools, knowledge, and memory to achieve intermediate tasks and iterate until the goal is complete. Examples include coding copilots and automated data analysis systems.\n\n4. Learning Agents\n\nThese agents learn, adapt, evolve, and improve themselves over time. They don't need constant human intervention. They use reasoning, memory, and autonomous learning capabilities such as complex research and simulation agents.\n\n5. ReAct Agents\n\nThese agents are a combination of reasoning and action. They involve strategic thinking for task decomposition and multi-step decision-making. They are the closest to human problem-solving behavior dynamically adjusting their approach. Their examples include project planning tools and content generation agents which produce and critique their content in an iterative manner in order to improve the response in every step of the way. Real-time external knowledge combined with a ReAct agent makes it even more powerful for precision critical tasks.\n\n6. Memory-Enhanced Agents\n\nThese agents use historical context, task history, and previous interactions to remember user preferences. Examples include customer service chatbots and personalized recommendation systems for shopping, travel, and other things.\n\nBasic Agentic Workflow\n\nAn AI agent is a combination of two things:\n\nAn interface\nA workflow\n\nThe interface is how you interact with an AI agent. It may be through a chat window, a voice assistant or a button on a website.\n\nBehind that interface is the workflow. You ask the AI agent a question or give it a command through the interface. From there, an AI agent follows a series of steps in a workflow. These steps could involve accessing information from a database, making a decision based on what it finds or even performing a task like sending an email. The steps in this workflow may change depending on what you ask or what the agent needs to do next. The AI agent works through a whole process to get you the best result.\n\nAt a higher level, an agentic workflow is a 4-step process:\n\nInput - You ask a question or give a command to the agent.\nProcessing - The AI agent thinks about what you have asked and searches for the best way to respond.\nAction - The AI agent gives you an answer, finds information or performs a task.\nLearning - The more you use it, the smarter it gets, learning from past interactions to improve.\n\nAt a lower level, an AI agent works in the following way:\n\nThe user gives instructions to an AI agent about what he wants to achieve.\nThe agent is provided with a detailed prompt based on the user's query.\nThe agent breaks down the task into smaller subtasks and tries to accomplish them by interacting with different tools or other agents.\n\nBefore LLMs, these agents had defined rules which helped them make decisions on the go. LLMs help the agents achieve that level of autonomy which is required to make decisions in real-time based on the current situation and responses from the agent itself.\n\nAutonomous AI agents work by simplifying and automating complex tasks.\n\nBasic agentic workflow includes:\n\n1. Planning\n\nThe agent receives a specific instruction or goal from the user. Planning involves:\n\nDetermining the goal.\nPlanning tasks.\nBreaking down tasks into smaller actionable tasks.\nPerforming the tasks in a specific order to achieve the goal.\n2. Information Acquisition\n\nIn order to act on the planned tasks, agents may acquire information in the following ways:\n\nExtracting conversation logs,\nSearching the internet to retrieve information, and\nInteracting with other agents or ML models to access or exchange information.\n3. Task Implementation\n\nTask implementation involves accomplishing a task and moving to the next one in plan. The agent needs to evaluate if the goal has been achieved before moving on to the next task using external feedback or inspecting its own logs. This might require creating and acting on more tasks as the agent moves towards its goal.\n\nWorkflow Prompt vs Agent Prompt\n\nWorkflow prompt is a prompt which you feed to an LLM, then take its response and feed it to the same or another LLM, and continue this pattern until you are done. You might have a fixed number of steps or you may check the intermediate results to decide of they are good enough to stop. Each of these prompts is very specific in terms of taking one input and transforming it into an output.\n\nIn contrast, an agent prompt is much more open-ended and usually gives the model tools and multiple things to check. Feedback is very important in order for the agent to converge to the right answer.\n\nIn multiagent collaborative systems, we may use a combination of workflow prompt and agent prompts. Every single agent which is a part of the system is given an open-ended prompt to accomplish its task. These single agents can then be put into a workflow in which the input of the next agent depends on the output of the previous agent and so on. This helps agents achieve a bigger goal in a collaborative manner while achieving their defined tasks autonomously.\n\nAgentic Patterns\n\nAI agents can be configured in many ways. The configuration of an AI agent in a specific manner is called an agentic pattern.\n\nMost popular agentic patterns include:\n\nReflection Pattern\nTool Use Pattern\nPlanning Pattern\nMulti-agent Pattern\nReflection Pattern\n\nThe reflection pattern involves a Generate agent and a Reflect agent both working in a loop. One agent generates the content and the other agent critiques or reflects upon its response including its own feedback and sending the response back to the generate agent so that it can improve or make it better. When an agent iteratively reflects on its response to improve it, we eventually get far better results.\n\nLarge language models can become effective AI agents by reflecting on their own behavior.\n\nReflection Workflow:\nUser provides a prompt such as \"Generate an article about something…\" using the Generate agent.\nThe article is generated as a text.\nThe text is sent to the Reflect agent which provides some critique to the Generate agent. It also provides some suggestions and modifications to the original content.\nThis critique is again sent to the Generate agent which generates another version of the article by taking into consideration all the critique and feedback that has been sent to it.\nThe loop continues for 'n' number of steps or until the Reflect agent is satisfied by the response. This pattern improves the results of your LLM calls.\nTool Use Pattern\n\nA tool is a way for an LLM to access the outside world. It is like a Python function that the LLM can access and run to fetch some relevant result. e.g.\n\nusing an API, or\nparsing web content.\n\nHow can we make this function (tool) available to an LLM? We use system prompt in which we tell the LLM to behave as a function calling AI model.\nTool use agent has the capability of using tools. It has a list of tools and it selects the right tool for the question that we have asked. Then it runs the tool to fetch the relevant information from the outside world. It ultimately returns this information in natural language as a response.\n\nPlanning Pattern\n\nThis pattern is also called ReAct - reason and act. It tries to improve the learning and reasoning capabilities of LLMs.\n\nPlanning Workflow:\nAction step is basically running a tool when the agent decides to run a tool based on its task decomposition.\nObservation step is where the LLM makes an observation about the output of the tool.\nThought step is where the agent reflects upon the observation and decides to take another action or return the response if the goal has been achieved.\n\nCore agents implement the ReAct technique because it is one of the most known techniques for improving the capabilities of LLMs.\n\nMulti-agent Pattern\n\nThe main idea of this pattern is to divide a task into smaller, simpler subtasks that are executed by different agents. Each agent adopts a role and solves a smaller task. When all small tasks have been completed, we may say that the bigger task has been achieved by the crew of agents or multi-agents. There is a dependency between the agents in a multi-agent system.\n\nAgentic Architecture\n\nAccording to the Agents whitepaper by Google, the three essential components in an agent runtime from user query to agent response include:\n\nModel\nTools\nOrchestration\nModel\n\nLarge language model (LLM) is the main decision-maker or task-planner in an agent. \n\nThere can be one or multiple models in an agentic workflow. \nModels may be small or large depending on the required task at hand.\nLLMs are capable of following reasoning frameworks such as ReAct, Chain-of-Thoughts, and Tree-of-Thoughts. \nThey can be general-purpose, multimodal, or fine-tuned.\nThe chosen LLM should best fit your required application goals.\nTools\n\nFoundation models are constrained by their inability to interact with the outside world. This brings the importance of using tools in agentic architecture.\n\nTools enable the agent to interact with external data and services, enhancing its capabilities.\nExamples of tools include weather API, database, or an external system.\nTools help access and process real-world information.\nThey bridge the gap between the agent's internal capabilities and external world unlocking a broader range of possibilities.\nThey can be functions, extensions, data stores, or plugins.\nOrchestration\n\nOrchestration is an iterative process where an agent takes information, does internal reasoning, and makes the next decision. The loop continues until the agent has reached its goal or a stopping point. It can be simple (calculations with decision rules) or complex (chained logic, additional machine learning algorithms, probabilistic reasoning techniques) depending on the goal and tasks.\n\nModel vs Agent\n\nLet's differentiate between a model and an agent.\n\nA model has limited knowledge - the one it has been trained on whereas an agent can access and process extended knowledge via tools interaction.\nA model generates a single inference based on user query while an agent maintains session history and context to support long-term interaction.\nA model does not have any tools while and agent implements tools or accesses information through already built tools like APIs, databases, etc.\nA model has no logical layer. It is capable of handling simple to complex prompts. On the other hand, an agent has reasoning frameworks in the architecture such as ReAct, CoT, etc.\nAgent vs Workflow\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\nPopular Agentic Frameworks\n\nFollowing are a few agentic frameworks used to build cutting-edge AI agents for real-world use-cases.\n\nCrewAI\nLangChain\nMicrosoft AutoGen\nAgno\nLangFlow\nLangGraph\nBenefits of AI Agents\nAgents save a lot of time and effort enabling us to focus on more important tasks.\nThey help automate routine tasks and repetitive workflows. \nAgents help reduce the cost of workforce by taking charge of business processes.\nQuality of responses improves by using agentic patterns like ReAct.\nThey help improve customer experience through customer service chatbots providing accurate and efficient responses. \nThey are more flexible and versatile than traditional computer programs because of LLMs.\nAgents are great for complex and unpredictable tasks.\nThey improve the overall productivity and efficiency of a business process.\nChallenges\n\nDespite having a lot of benefits, agents pose some significant challenges which include:\n\nData privacy concerns\nEthical challenges\nTechnical complexities\nLimited compute resources\nMultiagent dependencies\nInfinite feedback loops\nComputational complexity\nWhen to build AI agents?\n\nConfused about whether you should build an AI agent or not for your use case? Consider the following aspects.\n\nWhat level of autonomy you require in your system? If it's a simple task, then going through the programmatic approach might be a better option instead of overcomplicating the system with agentic behavior.\nFor complex tasks, like trying to solve GitHub issues independently and to handle a variety of queries, an agentic route could be helpful because it would take you too much effort to configure every single path in the system.\nAgentic behavior also requires human in the loop to some extent depending on the task in hand. So you need to consider this aspect as well while designing an AI agent.\n\nAgents are highly beneficial when tasks require complex decision-making, autonomy, and adaptability. For example:\n\nEmail autoresponders\nCustomer inquiry chatbots\nComplex data analysis & report generation\n\nAgents are not useful for straight-forward, infrequent, and minimal automation tasks or tasks requiring deep domain-specific knowledge. They are also not suitable for high-stake decision-making tasks involving financial transactions or data security and privacy concerns.\n\nCase Study - Building AI Agents\n\nLet's build two AI agents - a simple AI research agent and a multi-agent collaborative system - in order to understand the agentic workflow process. We'll use agno to build our AI agents. I'll take you step by step explaining each step in detail so that you get an in-depth understanding of the agentic workflow.\n\nPrerequisites\nYou must have a basic understanding of programming and some experience with coding in Python.\nIts good to have some knowledge of creating and working in virtual environments.\nagno - Framework to build AI Agents\n\nAgno is an open-source Python framework for building multi-modal AI agents. It claims to be the fastest framework for building AI agents. It is simple, intuitive, efficient, and easy to learn. It enhances LLMs with memory, knowledge, tools, and reasoning as required.\n\nProblem Statement\n\nFinding and reading research papers is a tedious task which requires days and months of effort. Understanding the papers and writing literature reviews is a very time-consuming task. Automating this task can make the lives of students, researchers, and AI professionals easier in terms of understanding and reviewing research papers.\n\nWe'll build the following two AI agents to understand the agentic process. \n\nSimple Research Agent - A single agent research assistant\nResearch Copilot - A multi-agent collaborative system\nBuilding a Simple AI Agent\nSimple Research Agent\n\nLet's build a simple research agent which finds and extracts research papers on a given topic, extracts their metadata, summarizes them, and generates a comprehensive literature review using a single research agent.\n\nImplementation\nProject Structure\nsimple-research-agent/           # Main project folder\n│── frontend/                    # Frontend directory\n│   ├── app.py                   # Streamlit application\n│\n│── backend/                     # Backend directory\n│   ├── main.py                   # FastAPI backend\n│   ├── pdf.py                    # Converts output to PDF\n│   ├── utils.py                  # Utility functions\n│   ├── research_agent.py         # Simple Research Agent\n│\n│── requirements.txt              # Dependencies\n│── .env                          # Environment variables\n│── LICENSE                       # License file\n│── README.md                     # Documentation\nSetting up the environment\n\nCreate a new virtual environment and install the dependencies using requirements.txt file. \n\nrequirements.txt\n\nstreamlit\nuvicorn\nfastapi\nagno\ngroq\nopenai\nrequests\narxiv\npypdf\nreportlab\nhtml2text\n\n.env\nCreate a .env file and add your API key.\n\nTOGETHER_API_KEY=\"your_api_key_here\"\nFRONTEND\n\nThe frontend of the simple research agent looks like this:\n\nUser Input: User enters a research topic and the number of papers to fetch from arXiv.\n\nOutput: Literature review of the fetched papers.\n\napp.py\nThis is the main entry point of the application built in Streamlit. It uses requests to interact with the FastAPI endpoints at the backend.\n\nimport streamlit as st\nimport requests\nimport json\n\n# Set Streamlit page configuration\nst.set_page_config(page_title=\"AI Research Agent\", layout=\"wide\")\n\n# Initialize session state variables\nif \"result_json\" not in st.session_state:\n    st.session_state.result_json = None  # Stores fetched research paper results\n\nif \"research_topic\" not in st.session_state:\n    st.session_state.research_topic = \"\"  # Stores user-entered research topic\n\nif \"max_papers\" not in st.session_state:\n    st.session_state.max_papers = 5  # Stores user-defined number of papers to fetch\n\n# Sidebar UI elements\nwith st.sidebar:\n    st.image(\"../images/research_logo.png\", width=150)  # Display research agent logo\n    st.subheader(\"Search and Generate Literature Reviews\")\n    st.header(\"🔍 Search Papers\")\n    \n    # User input fields for topic and number of papers\n    topic = st.text_input(\"Enter Research Topic:\", key=\"topic_input\", value=st.session_state.research_topic)\n    max_papers = st.number_input(\"Number of Papers to Fetch:\", min_value=1, max_value=10, \n                                 value=st.session_state.max_papers, key=\"papers_input\")\n    \n    # Button columns for fetching papers and refreshing\n    col_fetch, col_refresh = st.columns([2, 1])\n\n    with col_fetch:\n        if st.button(\"Fetch Papers & Generate Review\"):\n            if topic:\n                st.session_state.research_topic = topic  # Save topic in session state\n                try:\n                    with st.spinner(\"Fetching papers...\"):\n                        API_URL = \"http://127.0.0.1:8000/fetch_papers/\"  # Backend API endpoint\n                        response = requests.post(API_URL, json={\"topic\": topic, \"max_papers\": max_papers})\n\n                    if response.status_code != 200:\n                        st.error(\"Error: Invalid response from server.\")\n                        st.stop()\n\n                    result_decoded = response.content.decode(\"utf-8\")\n                    st.session_state.result_json = json.loads(result_decoded)  # Store API response in session state\n\n                except json.JSONDecodeError:\n                    st.error(\"Error: Received an unreadable response from the server.\")\n                    st.stop()\n                except requests.exceptions.RequestException:\n                    st.error(\"Error: Could not connect to the server.\")\n                    st.stop()\n\n    with col_refresh:\n        if st.button(\"🔄 Refresh\"):\n            st.session_state.result_json = None  # Clear previous results\n            st.session_state.research_topic = \"\"  # Reset topic field\n            st.session_state.max_papers = 5  # Reset number of papers\n            st.rerun()  # Force a page refresh\n\n# Main content layout\ncol1, col2 = st.columns([1, 11])\n\n# Page title\nst.title(\"AI Research Agent\")\n\n# Initial message when no results are available\nif st.session_state.result_json is None:\n    st.info(\"Enter a research topic and click 'Fetch Papers & Generate Review' to get started.\")\n\n# Display results if available\nif st.session_state.result_json:\n    research_topic = st.session_state.research_topic\n    research_topic = ' '.join(word.capitalize() for word in research_topic.split())  # Capitalize topic words\n    \n    st.header(f\"📖 Literature Review: {research_topic}\")\n\n    # If a PDF path is provided in the response, display download option\n    if \"pdf_path\" in st.session_state.result_json:\n        pdf_path = st.session_state.result_json[\"pdf_path\"]\n        filename = f\"{topic.replace(' ', '_')}_literature_review.pdf\"\n\n        # Create a row layout for success message + download button\n        col_success, col_download = st.columns([7, 1])\n\n        with col_success:\n            st.success(\"Literature Review Generated!\")\n\n        with col_download:\n            st.download_button(label=\"📥 Download\", \n                               data=open(pdf_path, \"rb\"), \n                               file_name=filename, \n                               mime=\"application/pdf\")\n        \n        # Display the generated literature review text\n        st.write(st.session_state.result_json.get(\"response\", \"No review available.\"))  \n    else:\n        st.error(\"No papers found!\")\nBACKEND\nmain.py\nThis is the main backend file with the following FastAPI endpoint.\nfetch_papers - This endpoint fetches the research papers on a given topic from arXiv, extracts and saves their metadata and generates a comprehensive literature review of the papers through the research agent. It saves the generated literature review in PDF format.\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom agno.agent import RunResponse\nfrom research_agent import research_agent\nfrom utils import extract_metadata, save_paper_metadata, generate_pdf\nimport os\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Define request model for research paper fetching\nclass ResearchRequest(BaseModel):\n    topic: str  # Research topic provided by the user\n    max_papers: int = 5  # Default number of papers to fetch\n\n# Define API endpoint to fetch research papers\n@app.post(\"/fetch_papers/\")\nasync def fetch_papers(request: ResearchRequest):\n    # Call the research agent to search for relevant papers and generate a literature review\n    response: RunResponse = research_agent.run(\n        f\"Search for {request.max_papers} most relevant papers on {request.topic} and generate a literature review.\"\n    )\n    \n    if response:\n        # Extract metadata from the response (e.g., title, authors, publication year, etc.)\n        metadata = extract_metadata(response.content)\n        \n        # Save extracted metadata to a file for future reference\n        metadata_file = save_paper_metadata(request.topic, metadata)\n        \n        # Generate a PDF containing the literature review\n        generate_pdf(request.topic, response.content)\n        \n        # Define the PDF file path based on the topic name\n        pdf_path = os.path.join(\"../literature_reviews\", f\"{request.topic.replace(' ', '_')}_literature_review.pdf\")\n        \n        # Return the generated literature review details\n        return {\n            \"message\": \"Literature review generated!\", \n            \"pdf_path\": pdf_path, \n            \"response\": response.content\n        }\n    else:\n        # Return an error message if no papers were found\n        return {\"error\": \"No papers found\"}\npdf.py\nThe PDFDocument class is used to convert the literature review generated by the research agent into a PDF format.\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib.units import inch\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.colors import blue, black, gray\nimport re\n\nclass PDFDocument:\n    def __init__(self, filename, title):\n        \"\"\"\n        Initializes the PDF document with the given filename and title.\n        \"\"\"\n        self.canvas = canvas.Canvas(filename, pagesize=letter)\n        self.canvas.setTitle(title)\n        self.width, self.height = letter\n\n    def add_page_number(self, page_num):\n        \"\"\"\n        Adds a page number at the bottom of each page.\n        \"\"\"\n        self.canvas.setFont('Helvetica', 10)\n        self.canvas.drawRightString(self.width - inch, 0.5 * inch, f\"Page {page_num}\")\n        self.canvas.line(inch, 0.6 * inch, self.width - inch, 0.6 * inch)\n\n    def draw_text(self, text, x, y, size=12, line_height=14, indent=0):\n        \"\"\"\n        Draws formatted text onto the PDF, handling bold, italic, and links.\n        \"\"\"\n        def get_chunks(text):\n            \"\"\"\n            Splits the text into chunks based on formatting (bold, italic, links).\n            \"\"\"\n            patterns = [r'\\*\\*(.*?)\\*\\*', r'\\*(.*?)\\*', r'(https?://\\S+)']\n            cursor = 0\n            for match in re.finditer('|'.join(patterns), text):\n                if cursor < match.start():\n                    yield text[cursor:match.start()], 'Helvetica', black\n                if match.group().startswith('**'):\n                    yield match.group()[2:-2], 'Helvetica-Bold', black\n                elif match.group().startswith('*') and not match.group().startswith('**'):\n                    yield match.group()[1:-1], 'Helvetica', black\n                elif re.match(r'https?://', match.group()):\n                    yield match.group(), 'Helvetica-Oblique', blue\n                cursor = match.end()\n            if cursor < len(text):\n                yield text[cursor:], 'Helvetica', black\n\n        cursor_x = x + indent\n        cursor_y = y\n        line_width = self.width - 2 * inch - indent\n\n        for chunk, style, color in get_chunks(text):\n            self.canvas.setFont(style, size)\n            self.canvas.setFillColor(color)\n\n            words = chunk.split()\n            for word in words:\n                word_width = self.canvas.stringWidth(word, style, size)\n                if cursor_x + word_width > x + line_width:\n                    cursor_y -= line_height\n                    cursor_x = x + indent\n                self.canvas.drawString(cursor_x, cursor_y, word)\n                cursor_x += word_width + self.canvas.stringWidth(' ', style, size)\n\n            self.canvas.setFillColor(black)\n\n        return x, cursor_y - line_height\n\n    def create_pdf(self, topic, content):\n        \"\"\"\n        Generates a literature review PDF from the provided topic and content.\n        \"\"\"\n        page_num = 1\n        y = self.height - 1.5 * inch\n        sections = content.split(\"\\n\")\n        bullet = u\"\\u2022\"  # Unicode for bullet point\n        list_counter = 1\n\n        # Add Title at the top\n        self.canvas.setFont('Helvetica-Bold', 20)\n        self.canvas.drawString(inch, y, f\"Literature Review: {topic}\")\n        y -= 0.5 * inch  # Space below the title\n\n        for section in sections:\n            # Add new page if the section is a new heading and space is low\n            if section.startswith(\"### \") and y < self.height - 2 * inch:\n                self.add_page_number(page_num)\n                self.canvas.showPage()\n                page_num += 1\n                y = self.height - inch\n\n            # Add new page if reaching bottom of the page\n            if y < 2 * inch:\n                self.add_page_number(page_num)\n                self.canvas.showPage()\n                page_num += 1\n                y = self.height - inch\n\n            # Handle headings\n            if section.startswith(\"### \"):\n                y -= 0.5 * inch\n                self.canvas.setFont('Helvetica-Bold', 18)\n                self.canvas.drawString(inch, y, section[4:].strip())\n                y -= 0.3 * inch\n                self.canvas.line(inch, y, self.width - inch, y)\n                y -= 0.4 * inch\n            # Handle bold text\n            elif section.startswith(\"**\"):\n                y -= 0.3 * inch\n                _, y = self.draw_text(section, inch, y, 14)\n            # Handle numbered lists\n            elif section.startswith(\"* \"):\n                section = f\"**{list_counter}.** {section[2:]}\"\n                list_counter += 1\n                _, y = self.draw_text(section, inch, y, 12, 14, 10)\n            # Handle bullet points\n            elif section.startswith(\"  + \") or section.startswith(\"  - \"):\n                section = f\"{bullet} {section[4:]}\"\n                _, y = self.draw_text(section, inch, y, 12, 14, 20)\n            # Handle normal text\n            else:\n                _, y = self.draw_text(section, inch, y)\n\n            y -= 0.1 * inch  # Add slight space between sections\n\n        # Add final page number and save PDF\n        self.add_page_number(page_num)\n        self.canvas.save()\nutils.py\nThese are helper functions to extract and save metadata from the research papers and generate PDF.\nimport os\nimport json\nimport re\nfrom pdf import PDFDocument\n\ndef extract_metadata(response_content: str):\n    metadata = []\n    paper_sections = response_content.split(\"## \")[1:]  # Splitting by section titles\n    \n    for section in paper_sections:\n        lines = section.strip().split(\"\\n\")\n        title = lines[0].strip()\n        \n        # Skip non-research sections\n        if title.lower() in [\"conclusion\", \"references\"]:\n            continue\n\n        authors = re.search(r\"\\*\\*Authors\\*\\*: (.+)\", section)\n        publication_date = re.search(r\"\\*\\*Publication Date\\*\\*: (.+)\", section)\n        keywords = re.search(r\"\\*\\*Keywords\\*\\*: (.+)\", section)\n        source_link = re.search(r\"(http[s]?://[^\\s]+)\", section)\n\n        metadata.append({\n            \"title\": title,\n            \"authors\": authors.group(1) if authors else \"\",\n            \"publication_date\": publication_date.group(1) if publication_date else \"\",\n            \"keywords\": [kw.strip() for kw in keywords.group(1).split(\",\")] if keywords else [],\n            \"source_link\": source_link.group(1) if source_link else \"\"\n        })\n\n    return metadata\n\n# Save paper metadata as JSON\ndef save_paper_metadata(topic, papers):\n    PAPERS_DIR = \"../papers_metadata\"\n    os.makedirs(PAPERS_DIR, exist_ok=True)\n\n    file_path = os.path.join(PAPERS_DIR, f\"{topic}_papers.json\")\n    with open(file_path, \"w\") as f:\n        json.dump(papers, f, indent=4)\n    return file_path\n\ndef generate_pdf(topic, response):\n    PDF_DIR = \"../literature_reviews\"\n    os.makedirs(PDF_DIR, exist_ok=True)\n\n    pdf_path = os.path.join(PDF_DIR, f\"{topic.replace(' ', '_')}_literature_review.pdf\")\n\n    print(\"Generating PDF...\")\n\n    # Initialize PDFDocument\n    pdf = PDFDocument(pdf_path, f\"Literature Review: {topic}\")\n\n    # Generate PDF content using the class method\n    pdf.create_pdf(topic, response)\n\n    print(f\"PDF successfully generated: {pdf_path}\")\n    return pdf_path\nresearch_agent.py\nThis is the AI agent powered by LLM which does all the autonomous work of finding, summarizing, and review generation of the research papers from arXiv. \n\nWe are using the Agent class from the agno framework. It uses a model (LLM) and a tool (ArXivTools) to accomplish its task. The LLM is used through the Together API provided in the framework. Similarly, ArxivTools are being used from built in toolkits in the framework to search and find research papers from arXiv. The description and instructions provided to the agent are very important here. We give a comprehensive set of instructions which help the agent autonomously decide which course of actions to take in order to achieve its goal. First, it uses ArxivTools to search the papers and bring their content including metadata and summaries. Then it utilizes those summaries to generate a comprehensive literature review.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.tools.arxiv import ArxivTools\nfrom agno.utils.pprint import pprint_run_response\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the research agent\nresearch_agent = Agent(\n    name=\"research-agent\",\n    model=Together(\n        id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", \n        api_key=os.getenv(\"TOGETHER_API_KEY\")),\n    tools=[ArxivTools()],\n    description=\n    '''\n        You are a research agent that will fetch papers from arXiv based on a given topic and maximum number of papers.\n        You will generate a comprehensive literature review of these papers.\n    ''',\n    instructions=[\n        \"Search for research papers on the given topic from arXiv.\",\n        \"If the number of papers is not specified, fetch 5 by default.\",\n        \"Process papers one at a time to reduce token usage.\",\n        \"For each paper, extract the following metadata: title, abstract, authors, publication date, keywords, and source link.\",\n        \"Limit extracted text to at most 2000 tokens per paper.\",\n        \"Generate a concise summary (max 300 tokens) for each paper.\",\n        \"After all papers are processed, generate a literature review using only stored summaries.\",\n        \"The final response should include the literature review of each research paper in a separate paragraph.\"\n        \"Each paper review should start with the number and title of the paper as a subheading.\"\n        \"The metadata of each paper should be displayed after the title.\",\n        \"Follow the following format for the metadata of each paper in normal text displaying each metadata item on a new line:\",\n            \"**Authors**: [Authors of the Paper]\",\n            \"**Publication Date**: [Date of Publication]\",\n            \"**Keywords**: [Keywords of the Paper]\",\n        \"Follow the following format for the review of each paper in italic text:\",\n            \"**Review**: *[Literature review of the Paper]*\",\n        \"Do not include a Literature Review heading in the response.\",\n        \"Include a conclusion section providing a combined summary of all papers.\",\n        \"Include a references section with citations and source links at the end of the literature review.\",\n        \"Include numbers for each paper in front of their titles.\",\n        \"Generate the literature review by processing the summaries of the papers in smaller chunks instead of one long response.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\nDEMO - Simple Research Agent\n\nHere is a demonstration of using the simple research agent.\n\nRunning the Simple Research Agent\nClone the repository from GitHub. The link has been provided in the References section.\nNavigate to the backend folder and start the FastAPI server:\ncd backend\nuvicorn main:app --reload\nNavigate to the frontend folder and start the Streamlit application:\ncd frontend\nstreamlit run app.py\n\nAccess the application through the following link.\n\nhttp://localhost:8501\nBuilding a Multi-Agent Collaborative System\nResearch Copilot\n\nLet's build a multi-agent collaborative system by enhancing the simple research agent built previously. This system finds and extracts research papers on a given topic, extracts their metadata, and summarizes them using a summarization agent, and generates a comprehensive literature review using a review generation agent. Both these agents work in an agentic workflow as the review generation agent is dependent on the response from the summarization agent.\n\nImplementation\nProject Structure\nresearch-copilot/               # Main project folder\n│── frontend/                   # Frontend directory\n│   ├── app.py                   # Streamlit application\n│\n│── backend/                     # Backend directory\n│   ├── main.py                   # FastAPI backend\n│   ├── pdf_from_json.py          # Converts JSON output to PDF\n│   ├── utils.py                  # Utility functions\n│   │\n│   ├── agents/                   # Agents folder\n│   │   ├── summarization_agent.py\n│   │   ├── review_generation_agent.py\n│   │\n│   ├── tools/                    # Tools folder\n│   │   ├── paper_download_tool.py\n│   │\n│   ├── workflows/                # Workflows folder\n│   │   ├── research_workflow.py\n│\n│── requirements.txt              # Dependencies\n│── .env                          # Environment variables\n│── LICENSE                       # License file\n│── README.md                     # Documentation\nSetting up the environment\n\nCreate a new virtual environment and install the dependencies using requirements.txt file.\n\nrequirements.txt\nstreamlit\nuvicorn\nfastapi\nagno\nrequests\narxiv\npypdf\nsqlalchemy\nreportlab\nhtml2text\npymupdf\n.env\nCreate a .env file and add your API key.\nTOGETHER_API_KEY=\"your_api_key_here\"\nFRONTEND\n\nThe frontend of the research copilot looks like this:\n\nUser Input: User enters a research topic and the number of papers to fetch from arXiv.\n\nOutput: Literature review of the fetched papers in JSON and PDF format.\n\napp.py\n\nThis is the main entry point of the application built in Streamlit. It uses requests to interact with the FastAPI endpoints at the backend.\n\nimport streamlit as st\nimport requests\nimport json\n\n# Set Streamlit page configurations\nst.set_page_config(page_title=\"AI Research Copilot\", layout=\"wide\")\n\n# Initialize session state variables to maintain user input and results\nif \"result_json\" not in st.session_state:\n    st.session_state.result_json = None\nif \"research_topic\" not in st.session_state:\n    st.session_state.research_topic = \"\"\nif \"max_papers\" not in st.session_state:\n    st.session_state.max_papers = 5\nif \"selected_paper\" not in st.session_state:\n    st.session_state.selected_paper = None\nif \"chat_history\" not in st.session_state:\n    st.session_state.chat_history = []\n\n# Define API URL for backend communication\nAPI_URL = \"http://127.0.0.1:8000\"\n\n# Sidebar UI for user inputs and controls\nwith st.sidebar:\n    st.image(\"../images/research_copilot_logo.png\", width=150)  # Display logo\n    st.subheader(\"Search and Generate Literature Reviews\")\n    st.header(\"🔍 Search Papers\")\n    \n    # Input fields for research topic and number of papers\n    topic = st.text_input(\"Enter Research Topic:\", key=\"topic_input\", value=st.session_state.research_topic)\n    max_papers = st.number_input(\"Number of Papers to Fetch:\", min_value=1, max_value=10, value=st.session_state.max_papers, key=\"papers_input\")\n    \n    col_btn1, col_btn2 = st.columns([2, 1])  # Layout for buttons\n    \n    with col_btn1:\n        # Fetch papers button\n        if st.button(\"Fetch Papers & Generate Review\"):\n            if topic:\n                st.session_state.research_topic = topic\n                try:\n                    with st.spinner(\"Fetching papers from arXiv...\"):\n                        response = requests.post(API_URL+\"/fetch_papers/\", json={\"topic\": topic, \"max_papers\": max_papers})\n                    \n                    if response.status_code != 200:\n                        st.error(\"Error: Invalid response from server.\")\n                        st.stop()\n                    \n                    # Parse response JSON\n                    result_decoded = response.content.decode(\"utf-8\")\n                    st.session_state.result_json = json.loads(result_decoded)\n                \n                except json.JSONDecodeError:\n                    st.error(\"Error: Received an unreadable response from the server.\")\n                    st.stop()\n                except requests.exceptions.RequestException:\n                    st.error(\"Error: Could not connect to the server.\")\n                    st.stop()\n    \n    with col_btn2:\n        # Refresh button to clear results and reset inputs\n        if st.button(\"🔄 Refresh\"):\n            st.session_state.result_json = None\n            st.session_state.research_topic = \"\"\n            st.session_state.max_papers = 5\n            st.rerun()\n\n# Display main application title\nst.title(\"AI Research Copilot\")\n\n# Show instructions if no results are available\nif st.session_state.result_json is None:\n    st.info(\"Enter a research topic and click 'Fetch Papers & Generate Review' to get started.\")\n\n# Display results if available\nif st.session_state.result_json:\n    research_topic = st.session_state.research_topic.title()  # Capitalize topic words\n    st.header(f\"📖 Literature Review: {research_topic}\")\n    \n    if \"pdf_path\" in st.session_state.result_json:\n        pdf_path = st.session_state.result_json[\"pdf_path\"]\n        filename = f\"{topic.replace(' ', '_')}_literature_review.pdf\"\n        \n        # Layout for success message and download button\n        col_success, col_download = st.columns([5, 1])\n        \n        with col_success:\n            st.success(\"Literature Review Generated!\")\n        \n        with col_download:\n            st.download_button(label=\"📥 Download\", \n                               data=open(pdf_path, \"rb\"), \n                               file_name=filename, \n                               mime=\"application/pdf\")\n    \n    # Extract and display literature review details\n    response = st.session_state.result_json.get(\"response\")\n    data = json.loads(response)\n    papers = data.get(\"papers\", [])\n    conclusion = data.get(\"conclusion\", \"\")\n    references = data.get(\"references\", [])\n    \n    if papers:\n        # Iterate through each retrieved paper and display details\n        for i, paper in enumerate(papers):\n            with st.container():\n                st.subheader(f\"📄 {i+1}. {paper['title']}\")\n                st.markdown(f\"**🖊️ Authors:** {paper['authors']}\")\n                st.markdown(f\"**📅 Publication Date:** {paper['publication_date'][:10]}\")\n                st.markdown(f\"**🔑 Keywords:** {paper['keywords']}\")\n                st.markdown(\"**📌 Summary:**\")\n                st.write(paper[\"summary\"])\n                st.markdown(\"**📝 Review:**\")\n                st.write(paper[\"review\"])\n                st.divider()\n        \n        # Display conclusion section\n        st.subheader(\"🧐 Conclusion\")\n        st.write(conclusion)\n        \n        # Display references section\n        st.subheader(\"📚 References\")\n        for ref in references:\n            st.markdown(f\"- {ref}\")\n    else:\n        st.error(\"No papers found!\")\nBACKEND\nmain.py\n\nThis is the FastAPI backend of the application. It has the following endpoint:\nfetch_papers - This endpoint fetches the research papers on a given topic from arXiv, extracts and saves their metadata and generates a comprehensive literature review of the papers using the research workflow. It saves the generated literature review in PDF format and returns the response in JSON format to be displayed at the frontend.\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom agents.chat_agent import chat_agent\nfrom workflows.research_workflow import research_workflow\nfrom agno.agent import RunResponse\nfrom knowledge.knowledge_base import pdf_knowledge_base\nfrom utils import extract_metadata, save_paper_metadata, generate_pdf\nimport os\n\n# Initialize FastAPI application\napp = FastAPI()\n\n# Define request model for research paper fetching\nclass ResearchRequest(BaseModel):\n    topic: str  # Research topic to fetch papers for\n    max_papers: int = 5  # Default number of papers to fetch\n\n@app.post(\"/fetch_papers/\")\nasync def fetch_papers(request: ResearchRequest):\n    \"\"\"\n    Endpoint to fetch research papers and generate a literature review.\n    Executes the research workflow and processes the retrieved papers.\n    \"\"\"\n    print(\"Executing workflow...\")\n    \n    # Run the research workflow to fetch relevant papers\n    response: RunResponse = research_workflow.run(topic=request.topic, max_papers=request.max_papers)\n\n    if response:\n        # Extract metadata from the response\n        metadata = extract_metadata(response.content)\n        metadata_file = save_paper_metadata(request.topic, metadata)\n        \n        # Generate and save the literature review PDF\n        generate_pdf(request.topic, response.content)\n        pdf_path = os.path.join(\"../literature_reviews\", f\"{request.topic.replace(' ', '_')}_literature_review.pdf\")\n\n        return {\n            \"message\": \"Literature review generated!\",\n            \"pdf_path\": pdf_path,\n            \"response\": response.content\n        }\n    else:\n        return {\"error\": \"No papers found\"}\npdf_from_json.py\nThis script converts the JSON response from the literature review agent into PDF format.\nfrom reportlab.lib.pagesizes import A4\nfrom reportlab.lib import colors\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n\ndef generate_pdf_from_json(topic, json_data, output_filename=\"research_report.pdf\"):\n    \"\"\"\n    Generate a PDF from a JSON response containing research papers, a conclusion, and references.\n\n    Args:\n        json_data (dict): JSON response with papers, conclusion, and references.\n        output_filename (str): Name of the output PDF file.\n    \"\"\"\n    doc = SimpleDocTemplate(output_filename, pagesize=A4)\n    elements = []\n\n    # Define styles with Unicode font\n    styles = getSampleStyleSheet()\n    title_style = ParagraphStyle(name=\"Title\", fontName=\"Helvetica-Bold\", fontSize=16, spaceAfter=10)\n    heading_style = ParagraphStyle(name=\"Heading2\", fontName=\"Helvetica\", fontSize=14, spaceAfter=8, textColor=colors.darkblue)\n    body_style = ParagraphStyle(name=\"BodyText\", fontName=\"Helvetica\", fontSize=12, spaceAfter=6)\n    bold_style = ParagraphStyle(name=\"Bold\", parent=body_style, fontName=\"Helvetica-Bold\")\n\n    # Add Title\n    elements.append(Paragraph(f\"Literature Review: {topic}\", title_style))\n    elements.append(Spacer(1, 12))\n\n    # Process Papers Section\n    if \"papers\" in json_data:\n        elements.append(Paragraph(\"Papers\", heading_style))\n        elements.append(Spacer(1, 6))\n\n        for idx, paper in enumerate(json_data[\"papers\"], 1):\n            elements.append(Paragraph(f\"{idx}. {paper['title']}\", bold_style))\n            elements.append(Paragraph(f\"Authors: {paper['authors']}\", body_style))\n            elements.append(Paragraph(f\"Publication Date: {paper['publication_date'][:10]}\", body_style))\n            elements.append(Paragraph(f\"Keywords: {', '.join(paper['keywords'])}\", body_style))\n            elements.append(Paragraph(f\"Source: <a href='{paper['source_link']}'>{paper['source_link']}</a>\", body_style))\n            elements.append(Spacer(1, 6))\n\n            # Add Abstract\n            # elements.append(Paragraph(\"Abstract:\", bold_style))\n            # elements.append(Paragraph(paper[\"abstract\"], body_style))\n            # elements.append(Spacer(1, 6))\n\n            # Add Summary\n            elements.append(Paragraph(\"Summary:\", bold_style))\n            elements.append(Paragraph(paper[\"summary\"], body_style))\n            elements.append(Spacer(1, 12))\n\n            # Add Review (if available)\n            if \"review\" in paper and paper[\"review\"]:\n                elements.append(Paragraph(\"Review:\", bold_style))\n                elements.append(Paragraph(paper[\"review\"], body_style))\n                elements.append(Spacer(1, 12))\n\n    # Conclusion Section\n    if \"conclusion\" in json_data:\n        elements.append(Paragraph(\"Conclusion\", heading_style))\n        elements.append(Spacer(1, 6))\n        elements.append(Paragraph(json_data[\"conclusion\"], body_style))\n        elements.append(Spacer(1, 12))\n\n    # References Section\n    if \"references\" in json_data:\n        elements.append(Paragraph(\"References\", heading_style))\n        elements.append(Spacer(1, 6))\n\n        # Format references as a table\n        ref_data = [[f\"{idx}. {ref}\"] for idx, ref in enumerate(json_data[\"references\"], 1)]\n        ref_table = Table(ref_data, colWidths=[500])\n\n        # Style the table\n        ref_table.setStyle(TableStyle([\n            (\"TEXTCOLOR\", (0, 0), (-1, -1), colors.black),\n            (\"ALIGN\", (0, 0), (-1, -1), \"LEFT\"),\n            (\"FONTNAME\", (0, 0), (-1, -1), \"Helvetica\"),  # ✅ Change to built-in font\n            (\"FONTSIZE\", (0, 0), (-1, -1), 10),\n            (\"BOTTOMPADDING\", (0, 0), (-1, -1), 6),\n            (\"GRID\", (0, 0), (-1, -1), 1, colors.black),\n        ]))\n\n        elements.append(ref_table)\n\n    # Build PDF\n    doc.build(elements)\n    print(f\"✅ PDF generated successfully: {output_filename}\")\nutils.py\n\nThese are utility functions to extract and save metadata and generate PDF.\n\nimport os\nimport json\nimport re\nfrom pdf_from_json import generate_pdf_from_json\n\ndef extract_metadata(response_content: str):\n    metadata = []\n    paper_sections = response_content.split(\"## \")[1:]  # Splitting by section titles\n    \n    for section in paper_sections:\n        lines = section.strip().split(\"\\n\")\n        title = lines[0].strip()\n        \n        # Skip non-research sections\n        if title.lower() in [\"conclusion\", \"references\"]:\n            continue\n\n        authors = re.search(r\"\\*\\*Authors\\*\\*: (.+)\", section)\n        publication_date = re.search(r\"\\*\\*Publication Date\\*\\*: (.+)\", section)\n        keywords = re.search(r\"\\*\\*Keywords\\*\\*: (.+)\", section)\n        journal = re.search(r\"\\*\\*Journal\\*\\*: (.+)\", section)\n        review_match = re.search(r\"\\*\\*Review\\*\\*: \\*(.+?)\\*\", section, re.DOTALL)\n        source_link = re.search(r\"(http[s]?://[^\\s]+)\", section)\n\n        metadata.append({\n            \"title\": title,\n            \"authors\": authors.group(1) if authors else \"\",\n            \"publication_date\": publication_date.group(1) if publication_date else \"\",\n            \"keywords\": [kw.strip() for kw in keywords.group(1).split(\",\")] if keywords else [],\n            \"journal\": journal.group(1) if journal else \"\",\n            \"source_link\": source_link.group(1) if source_link else \"\"\n        })\n\n    return metadata\n\n# Save paper metadata as JSON\ndef save_paper_metadata(topic, papers):\n    PAPERS_DIR = \"../papers_metadata\"\n    os.makedirs(PAPERS_DIR, exist_ok=True)\n\n    file_path = os.path.join(PAPERS_DIR, f\"{topic}_papers.json\")\n    with open(file_path, \"w\") as f:\n        json.dump(papers, f, indent=4)\n    return file_path\n\ndef generate_pdf(topic, response):\n    PDF_DIR = \"../literature_reviews\"\n    os.makedirs(PDF_DIR, exist_ok=True)\n\n    pdf_path = os.path.join(PDF_DIR, f\"{topic.replace(' ', '_')}_literature_review.pdf\")\n\n    print(\"Generating PDF...\")\n\n    # Generate PDF from JSON\n    json_response = json.loads(response)\n    generate_pdf_from_json(topic, json_response, pdf_path)\n\n    print(f\"PDF successfully generated: {pdf_path}\")\n    return pdf_path\npaper_download_tool.py\n\nThis tool helps download the research papers from arXiv into a local directory for further reference and processing.\n\nimport requests\nimport os\n\ndef download_arxiv_papers(topic, links):\n    \"\"\"\n    Downloads PDFs from given arXiv source links and saves them locally.\n\n    Args:\n        topic (str): Topic of the papers (used as the directory name).\n        links (list): List of arXiv paper URLs (e.g., \"https://arxiv.org/abs/2403.12345\").\n        \n    Returns:\n        list: A list of dictionaries containing 'link', 'file_path' (or 'error' if any).\n    \"\"\"\n    save_dir = f\"downloaded_papers/{topic}\"\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)  # Create directory if it doesn't exist\n\n    results = []\n\n    for link in links:\n        result_entry = {\"link\": link}  # Initialize entry with the link\n\n        try:\n            # Convert abstract page URL to PDF URL\n            if \"arxiv.org/abs/\" in link:\n                pdf_url = link.replace(\"arxiv.org/abs/\", \"arxiv.org/pdf/\") + \".pdf\"\n            elif \"arxiv.org/pdf/\" in link and not link.endswith(\".pdf\"):\n                pdf_url = link + \".pdf\"\n            else:\n                pdf_url = link  # Assume it's already a direct PDF link\n\n            paper_id = pdf_url.split(\"/\")[-1]  # Extract paper ID\n            file_path = os.path.join(save_dir, f\"{paper_id}\")\n\n            # Download the PDF\n            response = requests.get(pdf_url, stream=True)\n            response.raise_for_status()  # Raise error for failed requests\n\n            with open(file_path, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=1024):\n                    file.write(chunk)\n\n            result_entry[\"file_path\"] = file_path  # Store success result\n\n        except requests.exceptions.RequestException as e:\n            result_entry[\"error\"] = str(e)  # Store error message\n\n        results.append(result_entry)  # Append the result entry to the list\n\n    return results\nsummarization_agent.py\n\nThis is the summarization agent which fetches papers from arXiv based on a given topic and maximum number of papers. It extracts the metadata, summary, and a list of source links of the papers and downloads them in PDF format into a local directory.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.tools.arxiv import ArxivTools\nfrom tools.paper_download_tool import download_arxiv_papers\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the summarization agent\nsummarization_agent = Agent(\n    name=\"summarization-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\")),\n    tools=[ArxivTools(), download_arxiv_papers],\n    role=\"Download papers from arXiv and save them for further processing.\",\n    description=\n    '''\n        You are a summarization agent that will fetch papers from arXiv based on a given topic and maximum number of papers.\n        You will extract the metadata, summary, and a list of source links of the papers and download them in pdf format.\n    ''',\n    instructions=[\n        \"Search for the latest and most relevant research papers on the given topic from arXiv.\",\n        \"If the number of papers is not specified, fetch 5 by default.\",\n        \"Extract the metadata, summary, and a list of source links of the papers for downloading.\",\n        \"For each paper, extract the following metadata: title, abstract, authors, publication date, keywords, and source link.\",\n        \"Download the papers in pdf format for processing by other agents.\",\n        \"Your response should include a list of the extracted papers including the following information for each paper:\"\n        \"1. Metadata: Title, Authors, Abstract,Publication Date, Keywords, Source Link\",\n        \"2. Summary: Abstract or a concise summary of the paper\",\n        \"3. Paths to the downloaded papers for further processing\",\n        \"The information for each paper must be included in the following format.\",\n            \"Title: [Title of the Paper]\",\n            \"Authors: [Authors of the Paper]\",\n            \"Abstract: [Abstract of the Paper]\",\n            \"Publication Date: [Date of Publication]\",\n            \"Keywords: [Keywords of the Paper]\",\n            \"Source Link: [Source Link of the Paper]\",\n            \"Summary: [Summary of the Paper]\",\n            \"PDF Path: [Path to the Downloaded Paper]\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\nreview_generation_agent.py\n\nThis is the review generation agent which generates a comprehensive literature review of the research papers using the provided metadata and summaries of papers generated by the summarization agent. It saves the literature review in PDF format for downloading.\n\nfrom agno.agent import Agent\nfrom agno.models.together import Together\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the review generation agent\nreview_generation_agent = Agent(\n    name=\"review-generation-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\")),\n    description=\n    '''\n        You are a review generation agent that will generate a comprehensive literature review of research papers using \n        the provided metadata and summaries of papers generated by the Summarization Agent.\n    ''',\n    instructions=[\n        \"Generate a literature review using the metadata, abstracts, and summaries of the papers from the Summarization Agent.\",\n        \"The final response should include the literature review of each research paper in a separate paragraph.\",\n        \"Each paper review should start with the title of the paper as a subheading, along with metadata of that paper at the beginning.\"\n        \"Follow the following format for the metadata of each paper in normal text displaying each metadata item on a new line:\",\n            \"**Authors**: [Authors of the Paper]\",\n            \"**Publication Date**: [Date of Publication]\",\n            \"**Keywords**: [Keywords of the Paper]\",\n            \"**PDF Path**: [Path to the Downloaded Paper]\",\n        \"Display each item of the metadata on a new line.\",\n        \"Follow the following format for the review of each paper in italic text:\",\n            \"**Review**: *[Literature review of the Paper]*\",\n        \"Do not include a Literature Review heading in the response.\",\n        \"Include a conclusion section providing a combined summary of all papers.\",\n        \"Include a references section at the end of the literature review which includes a list of citations with source links.\",\n        \"Generate the literature review by processing the summaries of the papers in smaller chunks instead of one long response.\"\n        \"Important: You final response must be in JSON format with the following structure:\",\n            \"{\"\n            \"    'papers': [\"\n            \"        {\"\n            \"            'title': 'Title of the Paper',\"\n            \"            'authors': 'Authors of the Paper',\"            \n            \"            'abstract': 'Abstract of the Paper',\"\n            \"            'publication_date': 'Date of Publication',\"\n            \"            'keywords': 'Keywords of the Paper',\"\n            \"            'source_link': 'Source Link of the Paper',\"\n            \"            'summary': 'Summary of the Paper',\"\n            \"            'review': 'Literature Review of the Paper'\",\n            \"            'pdf_path': 'Path to the Downloaded Paper'\"\n            \"        },\"\n            \"        {...}\"\n            \"    ]\",\n            \"    'conclusion': 'Combined summary of all papers',\"\n            \"    'references': 'List of citations with source links'\"\n            \"}\"\n        \"The above JSON response must include all papers with required information, conclusion, and references.\",\n        \"The response MUST be in proper JSON format with keys and values in double quotes.\",\n        \"The final response MUST not include anything else other than the JSON response.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\nresearch_workflow.py\n\nThis is the agentic workflow of this multi-agent collaborative system. It runs the summarization agent and the review generation agent step by step as the input of one agent depends on the output of the other agent. It is an end to end workflow that helps the agents collaborate and work together effectively to achieve their goal.\n\nfrom agno.agent import Agent\nfrom agno.workflow import Workflow, RunResponse, RunEvent\nfrom agno.storage.workflow.sqlite import SqliteWorkflowStorage\nfrom agno.utils.pprint import pprint_run_response\nfrom agno.utils.log import logger\n\nfrom agents.summarization_agent import summarization_agent\nfrom agents.review_generation_agent import review_generation_agent\n\n# Define the ResearchCopilot workflow \nclass ResearchCopilot(Workflow):\n    summarization_agent: Agent = summarization_agent\n    review_generation_agent: Agent = review_generation_agent\n\n    def run(self, topic: str, max_papers: int = 5) -> RunResponse:\n        \"\"\"\n            Executes the research workflow:\n            1. Searches for research papers related to the topic.\n            2. Generates a literature review based on the extracted papers.\n        \"\"\"\n        logger.info(f\"Generating a literature review of {max_papers} research papers from arXiv on: {topic}\")\n\n        # Step 1: Search arXiv for research papers on the topic and summarize them\n        extracted_papers = self.get_extracted_papers(topic, max_papers)\n        \n        # If no extracted papers are found for the topic, end the workflow\n        if extracted_papers is None:\n            return RunResponse(\n                event=RunEvent.workflow_completed,\n                content=f\"Sorry, could not find any research papers on the topic: {topic}\",\n            )           \n        \n        print(\"Extracted papers:\", extracted_papers)\n\n        # Step 2: Generate a literature review of the extracted papers\n        literature_review: RunResponse = self.review_generation_agent.run(extracted_papers)\n        if literature_review is None:\n            return RunResponse(\n                event=RunEvent.workflow_completed,\n                content=\"Sorry, could not generate a literature review of the research papers.\",\n            )\n        \n        print(\"Literature review:\", literature_review.content)\n        return RunResponse(\n            event=RunEvent.workflow_completed,\n            content=literature_review.content,\n        )\n\n    def get_extracted_papers(self, topic: str, max_papers: int):\n        \"\"\"Get the search results for a topic.\"\"\"\n\n        MAX_ATTEMPTS = 3\n\n        for attempt in range(MAX_ATTEMPTS):\n            try:\n                prompt = f\"Search for {max_papers} most relevant papers on {topic}.\"\n                summarizer_response: RunResponse = self.summarization_agent.run(prompt)\n\n                # Validate response content\n                if not summarizer_response or not summarizer_response.content:\n                    logger.warning(f\"Attempt {attempt + 1}/{MAX_ATTEMPTS}: Empty searcher response\")\n                    continue\n\n                logger.info(f\"Found papers on the topic {topic} in attempt {attempt + 1}\")\n\n                print(\"Extracted papers:\", summarizer_response.content)\n                return summarizer_response.content\n            \n            except Exception as e:\n                logger.warning(f\"Attempt {attempt + 1}/{MAX_ATTEMPTS} failed: {str(e)}\")\n\n        logger.error(f\"Failed to get extracted papers after {MAX_ATTEMPTS} attempts\")\n        return None\n    \n\n# Initialize the Research Copilot workflow with SQLite storage\nresearch_workflow = ResearchCopilot(\n    session_id=f\"generate-literature-review\",\n    storage=SqliteWorkflowStorage(\n        table_name=\"generate_literature_review_workflows\",\n        db_file=\"workflows/db/workflows.db\",\n    ),\n)\nDEMO - Research Copilot\n\nThis is a demonstration of using the research copilot.\n\nRunning the Research Copilot\nClone the repository from GitHub. The link has been provided in the References section.\nNavigate to the backend folder and start the FastAPI server:\ncd backend\nuvicorn main:app --reload\nNavigate to the frontend folder and start the Streamlit application:\ncd frontend\nstreamlit run app.py\n\nAccess the application through the following link.\n\nhttp://localhost:8501\nLimitations & Considerations\nBoth the Simple Research Agent and the Research Copilot use Llama 3.3 70B Instruct turbo Free as an LLM from the Together API. This model has certain limitations in terms of token usage.\nThe prompts given to the agents have been specifically designed to generate short summaries and reviews in order to limit the token usage as the basic purpose of building these agents is to understand the whole agentic AI workflow.\nThe Simple Research Agent returns the review generated by the LLM without any formatting, therefore, the layout of its response would be different every time.\nThe Research Copilot has been explicitly instructed to generate the response in JSON format so that it can be displayed at the frontend in a consistent manner.\nPotential future enhancements could be adding an interactive chat agent to chat with individual research papers, integrating other research paper repositories like Semantic Scholar and IEEE Xplore, and introducing enhanced citation formatting.\nA paper assessment agent could be added to facilitate effective research paper writing.\nA paper analysis agent could be integrated to critically analyze different sections of a research paper, finding potential discrepancies and suggesting new research directions.\nPersonal Tips & Tricks\n\nAgents are no magic. They are complex software systems powered by LLMs. Here are a few tips from my personal experience for building effective agents:\n\nYou must have a clear idea about what you are going to build. If you start with the right problem formulation, you'll end up building the right agentic system.\nAn agent must have a clear, specific goal to achieve.\nGive every agent a small specific task so that it doesn't get confused or it may start hallucinating as a result. As the agent builds upon an LLM, there is a fair chance that it might get off track and does not produce the right output. The more focused a single agent is, the better it is.\nPrompt is the most important thing in the agentic workflow. If it is not correct or thoughtfully prepared, the agent won't work as required. \nTest different models (LLMs) and tool combinations to find the right fit for your specific use-case.\nFuture of AI Agents\n\nAI agents are going to totally transform how we live and work especially for local businesses. Imagine AI handling routine tasks, freeing business owners to focus on growth and innovation.\n\nLocal businesses would be able to provide smarter customer service and more efficient operations with the integration of AI agents.\nService providers would be able to deliver more value with less effort.\n\nAI agents are set to revolutionize how work gets done with a huge impact in every domain. They can easily take up routine tasks freeing you to focus on more important things.\n\nPowerful agentic AI systems have been possible because of the recent advancements in LLMs and their reasoning capabilities. The future of AI agents seems to be very bright incorporating multi-agent collaborative systems into business processes.\n\nThere will be an increase in businesses adopting AI agents to automate their routine tasks.\nMulti-agent collaborative systems would be working in production environments.\nMultiple LLMs would be playing their roles in agentic workflows, each targeting its own domain-specific task.\nThere is going to be a rise in business development agents improving productivity like never before.\nNext Steps…\n\nThis article serves as a starting point for anyone who wants to understand the concept of AI agents from a very basic level along with real-world use cases and implementation. To further enhance your understanding and taking it to the next level, consider:\n\nFocusing on different agentic patterns and understanding their strengths and weaknesses in detail. \nExploring different agentic AI frameworks and understanding their key features and limitations.\nDoing hands-on projects solving real-world problems through AI agents.\nConclusion\n\nAI agents are the most powerful innovation in the history of artificial intelligence. They are software applications that use large language models (LLMs) to autonomously perform specific tasks, ranging from answering research questions to handling backend services. They work towards achieving a specific goal in an optimized way marking their true impact while making our lives easier. AI agents are incredibly useful for tasks that demand complex decision making, autonomy, and adaptability. LLMs, tools, and orchestration are the core components of an agentic architecture. AI agents leverage AI models (LLMs) to understand goals, generate tasks, and go about completing them in an efficient manner. Examples of AI agents include coding agents, customer service chatbots, and personalized learning management systems.\n\nReferences\nAgents white paper - Google\nMastering AI Agents e-book - Galileo\nChapter 2: Intelligent Agents from Artificial Intelligence - A Modern Approach by Stuart J. Russel & Peter Norvig\nIntro to AI Agents - YouTube: Google Cloud Tech\nDocumentation: agno\nWhat are AI Agents? - aws\nWhat are AI Agents ? - IBM\nWhat is an AI Agent? - Google Cloud\nBuilding Effective Agents - Anthropic\nAgentic Patterns from Scratch - The Neural Maze\nSimple Research Agent - A Single Agent\nResearch Copilot - A Multi-Agent Collaborative System\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nIntroduction\n\nWhat are AI agents?\n\nImportance of AI Agents\n\nAgentic AI vs Generative AI\n\nWhere do LLMs stand in the scope of AI Agents?\n\nImportance of Prompt for an AI Agent\n\nEvolution of AI Agents in Recent Years\n\nModels\n\nCompound AI Systems\n\nAI Agents\n\nView all\nComments\nCode\nYou might be interested\nAgentic AI Applications\nDistinguished Technical Deep-Dive\nY\nMar 24, 202583 reads\nAgentic AIArtificial intelligence+3\nAn Intelligent Research Paper Search Agent\nA\nMar 19, 202525 reads\nAcademic Research Supportagentic-ai+7\nBuilding an Agentic AI: A Practical Guide with a Task Management Example\nDec 30, 202416 reads\nMulti-Agent Research Assistant\nJul 31, 202511 reads\nLangchainMULTI-AGENT+2",
    "awards": [
      "distinguished technical deep-dive"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "agentic-ai-applications-EWPgeS4fdJlB",
    "username": "yYash Nikumbh",
    "license": "There are some Access and Availability Status that should be considered before using any frameworks or tools -\nLicense Agreements: Some frameworks or tools (like Azure OpenAI, or certain paid APIs) may require users to accept specific license agreements before use.\nAPI Keys & Authentication: Many tools, especially cloud-based ones like Azure OpenAI, will require users to create accounts and obtain API keys. Make sure to follow the authentication steps outlined in their respective documentation.\nBeta Versions: Be aware that some frameworks may be in beta or experimental stages. These might offer advanced features but could come with stability issues.",
    "title": "Agentic AI Applications",
    "publication_description": "Back to publications\nMar 24, 2025\n●\n83 reads\n●\nApache 2.0\nWinner of\nDistinguished Technical Deep-Dive\nat the\nAgentic AI Innovation Challenge 2025\nAgentic AI Applications\nAgentic AI\nArtificial intelligence\nAzureOpenAI\nLLM\nOrchestration\nY\nYash Nikumbh\nLike\nBookmark\nShare\nIntroduction\n\nHey folks, a quick introduction about the content that I'll be sharing here today. This publication will help you deeply understand the application of Agentic AI programmatically. I will be covering a few popular frameworks which will help you get started with Agentic AI Frameworks.\n\n\nAbstract\n\nThis paper looks at agentic artificial intelligence (AI) frameworks, which allow AI systems to make decisions, pursue goals, and learn on their own. Agentic AI is a big step forward in how intelligent systems work, enabling them to handle complex tasks with little human input. In this paper, we explore the basics of agentic AI and how it can be used in areas like robotics, virtual assistants, and decision-making tools. We also dive into how developers can code these frameworks, offering practical examples and code snippets to help bring agentic AI to life. By exploring various case studies and providing practical coding examples, this research aims to bridge the gap between conceptual understanding and practical implementation, equipping developers and researchers with the tools needed to harness the power of agentic AI. Even simple applications of agentic AI can add significant value by automating everyday tasks and improving efficiency. Application of these frameworks will show how automation can be achieved for small intermediate steps in business.\n\nMethodology\n\nAs all industries are moving towards AI-powered solutions, it is important for developers to approach the integration of agentic AI with responsibility and foresight. As creators of these systems, developers must ensure that the AI's autonomy is balanced with ethical considerations, safety protocols, and transparency. By doing so, we can create AI that not only enhances productivity but also aligns with societal values and benefits. The few frameworks that this paper would be covering is Autogen, LangGraph, Crewai, SmolAgents and LlamaIndex . By first understanding these key terms, we lay the groundwork for better understanding how agentic AI frameworks function in practice;\n[Ref - \nPrepare for agentic AI\n]\n\nTo keep the content concise, references are included throughout the paper. Please feel free to navigate to them for a deeper understanding of each concept.\n\nAgentic AI - Agentic AI uses sophisticated reasoning and iterative planning to solve complex, multi-step problems.\n[Ref - \nWhat is Agentic AI?\n]\n\nAgentic AI frameworks - Agentic frameworks are the building blocks for developing, deploying and managing AI agents.\n[Ref - \nWhat is Agentic AI?\n]\n\nAgent - An artificial intelligence (AI) agent is a software program that can interact with its environment, collect data, and use the data to perform self-determined tasks to meet predetermined goals. Humans set goals, but an AI agent independently chooses the best actions it needs to perform to achieve those goals.\n[Ref - \nWhat are AI agents?\n]\n\nTool Use - Interaction with external systems like APIs, databases, calculators, and even web searches.\n[Ref - \nTool calling Agents\n]\n\nKnowledge Graph - A collection of Entities, Entity Types, and Entity Relationship Types that enable reasoning and inference.\n[Ref - \nTool calling Agents\n]\n\nMulti-Agent systems - A multi-agent system (MAS) is composed of multiple interacting intelligent agents - autonomous entities that can sense, learn models of their environment, make decisions and act upon them.\n[Ref - \nConcept of Multi-agentic system.\n]\n\nOrchestration - Artificial intelligence (AI) agent orchestration is the process of coordinating multiple specialized AI agents within a unified system to efficiently achieve shared objectives.\n[Ref - \nAI agent Orchestration\n]\n\nHuman-in-loop - A human acting as a mediator/orchestrator helps agent gather feedback. This integration allows human to provide guidance and oversight before tasks are executed, ensuring that the agent stays on the right track and aligns with the intended goals.\n[Ref - \nHuman-in-loop\n]\n\nPrompt - In the context of AI, a prompt is the input you provide to the model to elicit a specific response. This can take various forms, ranging from simple questions or keywords to complex instructions, code snippets, or even creative writing samples. The effectiveness of your prompt directly influences the quality and relevance of the AI's output.\n[Ref - \nAI agent Orchestration\n]\n\nTokens - Tokens can be thought of as pieces of words. Before the API processes the request, the input is broken down into tokens. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words.\n[Ref - \nWhat are tokens?\n]\n\nNow that we have a solid understanding of the key concepts in agentic AI, let's dive into the frameworks that enable the development and deployment of such systems. These frameworks provide the structure and tools necessary for building autonomous AI systems that can make decisions, learn from experience, and interact with their environments effectively.\n\nThere are some Access and Availability Status that should be considered before using any frameworks or tools -\nLicense Agreements: Some frameworks or tools (like Azure OpenAI, or certain paid APIs) may require users to accept specific license agreements before use.\nAPI Keys & Authentication: Many tools, especially cloud-based ones like Azure OpenAI, will require users to create accounts and obtain API keys. Make sure to follow the authentication steps outlined in their respective documentation.\nBeta Versions: Be aware that some frameworks may be in beta or experimental stages. These might offer advanced features but could come with stability issues.\n\nBefore we jump into the use cases and how these frameworks work in practice, let’s quickly go over what you’ll need to get started. Here's a list of the essentials to set everything up and make the most of these tools:\n\nAny local IDE - VS code, Pycharm, etc\nPython (Version 3.10 or higher)\nVirtual Environment Setup (Optional but Recommended) -\npython -m venv myenv\nsource myenv/bin/activate\nEnvironment Configuration and Setup(using .env)\n\nHere is the summary of access and availability status for each framework:\n\nAutoGen\nAccess: AutoGen is available through the official GitHub repository. It is an open-source framework and can be freely accessed and integrated into your projects.\nAvailability: As of now, AutoGen is actively maintained and frequently updated. Make sure to check the repository for the latest release.\nLimitations: Users need an Azure account to use certain services within AutoGen (e.g., language models). Some features may require specific Azure permissions.\n\nLangChain\nAccess: LangChain is also open-source and can be accessed via the LangChain GitHub repository. It's widely used for chaining together different components, such as LLMs and other tools.\nAvailability: LangChain is actively maintained, with regular updates and a large community of contributors. The framework is publicly available and free to use.\nLimitations: While LangChain itself is free to use, certain tools and services (such as APIs or cloud-based integrations) may require separate licensing or API keys.\n\nCrewai\nAccess: Crewai is available as part of the CrewAI package. The library is open-source and free to use.\nAvailability: Crewai is currently stable but has periodic updates. It's available for integration into custom workflows and projects.\nLimitations: Crewai’s functionality can be limited by the complexity of the tasks it is set to perform. Users need to ensure they are using compatible tools and agents for optimal performance.\n\nSmolAgents\nAccess: SmolAgents is available via GitHub. It’s an experimental framework with some parts still under active development.\nAvailability: SmolAgents is publicly available, though some features may be subject to changes as it evolves. Be sure to check the repository for any breaking changes or updates.\nLimitations: SmolAgents is not fully stable in every scenario, especially for larger projects. Users may experience issues related to scalability and integration with certain APIs.\n\nLlamaIndex\nAccess: LlamaIndex is an open-source project and can be found on GitHub. It is widely used in LLM-powered applications.\nAvailability: The framework is stable and maintained, and it is supported by a large user community.\nLimitations: While LlamaIndex is freely available, users may face limitations in terms of scalability when handling large data sets. Performance might be affected based on the deployment scale.\n\nIn this section, we’ll explore several widely-used agentic AI frameworks, examining their core features, practical applications, and how developers can implement them to create intelligent, goal-directed systems. Each framework has its unique strengths and use cases, and we’ll discuss how they can be leveraged across various industries, from robotics and virtual assistants to complex decision-making systems.\n\nSmart Orchestration for Local Code Generation and Execution using AutoGen -\n\nAutogen\n\n\n\nVersion Used - 0.047\nFramework description -\nAutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. AutoGen aims to provide an easy-to-use and flexible framework for accelerating development and research on agentic AI.\n\n\nKey Features :\n\nAsynchronous messaging\nModular and extensible\nObservability and debugging\nScalable and distributed\nBuilt-in and community extensions\nCross-language support\nFull type support\n\nArchitecture of class demonstrated(MagenticOneGroupChat)\n\n\nMagentic-One is a versatile and generalist multi-agent system designed to solve open-ended tasks, both web-based and file-based, across a variety of domains. It was built using the autogen-core library but has since been upgraded to work with autogen-agentchat, offering a more modular and user-friendly interface. The system is powered by an orchestrator called MagenticOneGroupChat, which now functions as an AgentChat team, supporting various agents.\n\nInstallation and Usage Instructions :\n\nIt is recommended to create a virtual environment on your local machine before installing any packages.\n\nPlease refer to \ninstallation guide\n for getting started.\n\nLocalCommandLineCodeExecutor functionality is not recommended by Autogen. Due to resource limitation I have used local executor. Consider using docker!\n\nProblem Statement Addressed - The need for a self-healing, autonomous orchestration system for local code generation and execution, capable of operating without human intervention.\n\n## Note - AutoGen internally uses a lot of packages which might need to download externally\n# importing required libraries\nimport asyncio\n\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\nfrom autogen_ext.agents.magentic_one import MagenticOneCoderAgent\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\nfrom autogen_ext.agents.file_surfer import FileSurfer\nfrom autogen_ext.tools.code_execution import PythonCodeExecutionTool\n\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent\n\nfrom autogen_core import SingleThreadedAgentRuntime\n\n# Using AutoGen with Async as it is supported in >=v0.4\nasync def main() -> None:\n    # Setting runtime for the team which we are using - MagenticOneGroupChat\n    runtime = SingleThreadedAgentRuntime()\n    runtime.start()\n\n    # Enabling Local code Execution supported in AutoGen\n    ## WARNING - Functionality is not recommended by Autogen. Due to resource limitation I have used local executor. Consider using docker!\n    executor = LocalCommandLineCodeExecutor(work_dir=\"output\")\n    python_tool = PythonCodeExecutionTool(executor=executor)\n\n    # Start off by setting up model client which is essential for defining an agent \n    model_client = AzureOpenAIChatCompletionClient(\n    model='gpt-4o',\n    azure_endpoint='https://example.deployment.azure.com/',\n    azure_deployment='deployment_name',\n    api_version='2024-08-01-preview',\n    api_key='API_key',\n    )\n\n    # Define agents \n    web_surfer = MultimodalWebSurfer(\n    \"webSurfer\",\n    model_client=model_client,\n    headless=True,\n    to_save_screenshots=True,\n    use_ocr=True,\n    debug_dir=\"./output\"\n    )\n\n    coder = MagenticOneCoderAgent(\n    \"coder\",\n    model_client=model_client,\n    )\n\n\n    unit_tester = AssistantAgent(\n    \"tester\",\n    model_client=model_client,\n    description=\"An AI assistant that helps in creating unit test cases and conduct a security scan for the code.\",\n    system_message=\"\"\"create and execute 3 unit test cases using the following format:\n    Unit test case - 1: Test case description\n    Input: Input values\n    Expected Output: Expected output.\n    Code: Unit test code\n    Repeat the same for all test cases.\n    \"\"\",\n    tools=[python_tool],\n    )\n\n    local_file_surfer = FileSurfer(\n    \"fileSurfer\",\n    model_client=model_client,\n    description=\"A local file surfer that can access files on the local system.\",\n    )\n\n    code_executor = CodeExecutorAgent(\n    \"CoderExecutor\",\n    code_executor=executor,\n    )\n\n    critic_agent = AssistantAgent(\n    \"critic\",\n    model_client=model_client,\n    description=\"An AI assistant that helps in reviewing the code and ensure it is executing properly.\",\n    )\n\n    # Use agents as participants in group chats\n    participants = [unit_tester, local_file_surfer, code_executor, web_surfer, coder, critic_agent]\n\n    # Use agents as participants in group chats\n    team = MagenticOneGroupChat(\n    participants=participants,\n    model_client=model_client,\n    max_turns=50,\n    runtime=runtime,\n    )\n\n    # try running user query\n    await Console(team.run_stream(task=\"write a code to scrape web pages using python\"), output_stats=True)\n    await runtime.stop()\n\nasyncio.run(main())\n\nCode Explanation -\nThe provided code sets up an asynchronous environment to manage a team of AI agents, each with a specific role, using the SingleThreadedAgentRuntime. This setup allows each agent to perform distinct tasks, such as web surfing, code execution, file browsing, and unit testing, working together to complete a larger goal. The main function initializes these agents, starts the runtime, and triggers a task using the console interface.\n\nImports:\nThe code begins by importing various necessary modules and classes that help set up the agents and their functionalities. These imports include tools for code execution, interacting with models, and managing agent behaviors.\n\nMain Function:\nAn asynchronous main function is defined to initialize and execute the agents. It sets up the overall workflow of the agents in the environment, ensuring that tasks are carried out as expected.\n\nRuntime Initialization:\nThe SingleThreadedAgentRuntime is initialized and started within the main function. This runtime serves as the backbone for managing agent operations, ensuring that each task is processed sequentially and efficiently in a single-threaded environment.\n\nCode Executor:\nThe LocalCommandLineCodeExecutor is set up to execute code locally, specifically within the \"output\" directory. This executor facilitates running code on the local machine rather than relying on remote servers. It ensures that any code generated by the agents is executed in a controlled, local environment. The PythonCodeExecutionTool is created using this executor, enabling the execution of Python code tasks.\n\nModel Client:\nThe AzureOpenAIChatCompletionClient is initialized with necessary parameters to interact with the Azure OpenAI service. This client allows the agents to communicate with a language model (such as GPT) to generate code, solve problems, or perform various tasks that require natural language processing.\n\nAgents Initialization:\nSeveral agents are initialized, each serving a unique role;\n\nWeb Surfer(The MultimodalWebSurfer is configured to carry out web surfing tasks, allowing it to browse the web and gather information.)\n\nCoder(The MagenticOneCoderAgent is initialized to handle coding tasks, such as writing scripts or solving coding challenges.)\n\nUnit Tester(The AssistantAgent is configured to create and run unit tests, ensuring that the code functions correctly.)\n\nFile Surfer(The FileSurfer agent is set up to navigate local files, accessing documents, configurations, or code that may be needed.)\n\nCode Executor(The CodeExecutorAgent is set up to handle the execution of the code generated by the other agents.)\n\nCritic Agent(Another AssistantAgent is used as a critic to review and verify the correctness of the code execution.)\n\nTeam Setup:\nA list of agents (referred to as participants) is created to collaborate on the task. The MagenticOneGroupChat orchestrates the interactions between these agents, enabling them to work together seamlessly. It is initialized with the participants, model client, and the runtime environment, ensuring that all agents are ready to cooperate.\n\nTask Execution:\nUsing the console interface, a task is run where the agents work together to write Python code for scraping web pages. The task involves multiple agents interacting with each other—web surfing agents collect information, coding agents write the code, and unit testing agents ensure the code works as intended. Once the task is completed, the runtime is stopped.\n\nRun Main Function:\nFinally, the asyncio.run(main()) method is used to run the main function asynchronously, initiating the entire process and allowing the agents to execute their respective roles in the workflow.\n\nSample Output -\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an example of an implementation where the system can autonomously generate and execute code, create and run test cases, and even heal itself when issues arise. You can add or remove agents as per your requirements. This system can implement any library/framework as it has access to web it can gather relevant information and try out a sample implementation.\n\nLimitations/Drawbacks : The orchestration class can be token-hungry, especially when the Group Chat exceeds 8-10 messages, often leading to token shortages. This becomes a significant challenge as the system scales with more complex interactions.\nGaps in existing research : While the central orchestrator approach works well, it may not suit all use cases. For more process-oriented approaches, you might want to explore other orchestration models, such as Swarm (for structured hand-offs), SelectorGroupChat (for LLM-based speaker selection), SocietyOfMind (where agents collaborate to produce a single output), and RoundRobinGroupChat (based on a Round Robin approach for task allocation). These alternatives offer unique strengths depending on your project's needs. Human-in-loop approches are also supported in AutoGen which can be helpful if human intervention is needed at intermediate steps.\n\nAutomating Data Visualization using LangGraph -\n\nLangGraph\n\n\n\nVersion Used - 0.3.16\nFramework description -\nLangGraph simplifies the creation of stateful, multi-actor applications using LLMs. Building on LangChain, it adds the ability to create and manage cyclical graphs, which are crucial for developing advanced agent runtimes. The core features of LangGraph include its graph structure, state management, and coordination capabilities.\n\n\nKey Features :\n\nModular Architecture\nFramework customizability\nUnified Interface\nExtensive Documentation and Examples\n\nDescription of class implementated(create_pandas_dataframe_agent)\nLangChain's Pandas DataFrame Agent allows language models (LLMs) to interact with and analyze data in Pandas DataFrames using natural language queries, making complex data tasks like filtering, aggregation, and visualization more accessible for data scientists and analysts. It streamlines workflows by combining the power of LLMs with the flexibility of Pandas. However, since the agent uses a Python REPL to execute code, it introduces potential security risks, such as executing malicious code, if not carefully managed. To mitigate these risks, users must opt-in to this feature by setting a specific parameter to True, ensuring they understand the security implications and have safeguards in place.\n\nInstallation and Usage Instructions :\n\nIt is recommended to create a virtual environment on your local machine before installing any packages.\n\nPlease refer to \ninstallation guide\n for getting started.\n\ncreate_pandas_dataframe_agent is an experimental function. Use it at your own risk. Enabling parameters like allow_dangerous_code can pose security risks, so proceed with caution and use them at your discretion.\n\nProblem Statement Addressed - The need for automating data visualization processes, enabling efficient and dynamic creation of visual representations from data.\n\nimport httpx\nimport pandas as pd\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain.agents import StructuredChatAgent, AgentExecutor, Tool\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n\nfrom langgraph.graph import StateGraph\n\n# Create an HTTP client with SSL verification disabled\nclient = httpx.Client(verify=False)\n\n# Initialize the AzureChatOpenAI language model\nllm = AzureChatOpenAI(\n    model='gpt-4o',\n    azure_endpoint='https://your.endpoint.azure.com/',\n    azure_deployment='your_deployment',\n    api_version='2024-08-01-preview',\n    api_key='YOUR_API_KEY' \n)\n\n# Function to create an agent with specified tools\ndef create_agent(csv_path, llm):\n    # Initialize the DuckDuckGo search tool\n    search = DuckDuckGoSearchRun(max_results=3, verify=False)\n    \n    # Create a pandas dataframe agent\n    pd_agent = create_pandas_dataframe_agent(\n        llm,\n        pd.read_csv(csv_path),\n        verbose=True,\n        allow_dangerous_code=True,\n        agent_executor_kwargs={\"handle_parsing_errors\": True},\n    )\n    \n    # Define the tools available to the agent\n    tools = [\n        Tool(\n            name=\"Search\",\n            func=search.run,\n            description=\"\"\"\n    Useful for when you need to find information related to Exploratory Data Analysis(EDA) on web.\n    Input should be a search query.\n    \"\"\"\n        ),\n        Tool(\n            name=\"CSV_Agent\",\n            func=pd_agent.run,\n            description=\"\"\"\n    Useful when you need to carry out EDA with insights from the dataset provided from.\n    Input should be a search query.\n    \"\"\"\n        ),\n    ]\n\n    # Define the prefix, format instructions, and suffix for the agent's responses\n    PREFIX = \"\"\"[INST]Respond to the human as helpfully and accurately as possible. You have access to the following tools:\"\"\"\n    FORMAT_INSTRUCTIONS = \"\"\"Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n\n    Valid \"action\" values: \"Final Answer\" or {tool_names}\n    while passing query to the tool make sure not to pass multiple queries.\n    Provide only ONE action per $JSON_BLOB, as shown:\n\n    ```\n    {{{{\n    \"action\": $TOOL_NAME,\n    \"action_input\": $INPUT\n    }}}}\n    ```\n\n    Follow this format:\n\n    Question: input question to answer\n    Thought: consider previous and subsequent steps\n    Action:\n    ```\n    $JSON_BLOB\n    ```\n    Observation: action result\n    ... (repeat Thought/Action/Observation N times)\n    Thought: I know what to respond\n    Action:\n    ```\n    {{{{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"Final response to human\"\n    }}}}\n    ```[/INST]\"\"\"\n    SUFFIX = \"\"\"Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\n    Thought:[INST]\"\"\"\n\n    HUMAN_MESSAGE_TEMPLATE = \"{input}\\n\\n{agent_scratchpad}\"\n\n    # Create the structured chat agent\n    agent = StructuredChatAgent.from_llm_and_tools(\n        llm,\n        tools,\n        prefix=PREFIX,\n        suffix=SUFFIX,\n        human_message_template=HUMAN_MESSAGE_TEMPLATE,\n        format_instructions=FORMAT_INSTRUCTIONS,\n    )\n    \n    # Create the agent executor\n    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n    return agent_executor\n\n# Path to the CSV file\ncsv_path = 'path_to_your_CSV_dataset'\n\n# Define the state structure\nclass State(TypedDict):\n    messages: List\n\n# Initialize the state graph\ngraph_builder = StateGraph(State)\n\n# Define the chatbot function\ndef chatbot(state: State):\n    chatbot = create_agent(csv_path, llm)\n    return {\"messages\": [chatbot.invoke(state[\"messages\"])]}\n\n# Add the chatbot node to the graph\ngraph_builder.add_node(\"chatbot\", chatbot)\n\n# Set the entry and finish points for the graph\ngraph_builder.set_entry_point(\"chatbot\")\ngraph_builder.set_finish_point(\"chatbot\")\n\n# Compile the graph\ngraph = graph_builder.compile()\n\n# Main loop to interact with the user\nwhile True:\n    user_input = input(\"User: \")\n    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n        print(\"Goodbye!\")\n        break\n    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n        for value in event.values():\n            print(\"Message structure:\", value[\"messages\"][-1])\n            print(\"Assistant:\", value[\"messages\"][-1])\n        print('-' * 50)\n\nCode Explanation -\nThe provided code sets up a chatbot system that utilizes the langchain library to interact with users in an intelligent, dynamic way. The chatbot uses multiple tools, such as a web search tool (DuckDuckGo) and a pandas dataframe agent, to respond accurately to user queries. The system operates based on a state graph that manages the flow of conversation, helping to keep track of interactions and guide the user through a series of actions.\n\nLanguage Model Initialization:\nThe AzureChatOpenAI language model is initialized with specific parameters. This model is responsible for processing the natural language inputs from the user and generating appropriate responses. By using this model, the chatbot can handle complex language tasks and provide insightful answers.\n\nAgent Creation Function:\nThe code defines a function to create a chatbot agent that is equipped with various tools for enhanced functionality;\n\nDuckDuckGoSearchRun(This tool allows the agent to perform web searches using DuckDuckGo, which is useful for retrieving up-to-date information or answering questions that require browsing the web.)\n\ncreate_pandas_dataframe_agent(This tool enables the agent to interact with data stored in pandas dataframes, making it capable of analyzing and manipulating structured data.)\n\nTool(This is a general definition for the tools that are available to the agent, providing flexibility in adding different types of functionalities.)\n\nStructuredChatAgent(This creates an agent with the specified tools and sets response format instructions, ensuring that the agent responds in a structured and predictable manner.)\n\nAgentExecutor(This is responsible for executing the agent's tasks using the defined tools and ensuring the proper sequence of operations.)\n\nCSV Path:\nThe path to the CSV file used by the pandas dataframe agent is specified. This file provides structured data that the agent can reference, making it capable of performing tasks like data analysis or answering questions based on the data.\n\nState Definition:\nThe state structure is defined, outlining the different states or stages in the conversation. This structure helps track where the conversation is and guides the flow of interaction based on user input.\n\nState Graph Initialization:\nThe state graph is initialized with the defined state structure. The state graph is crucial because it manages the transitions between different states, ensuring that the chatbot can handle multi-turn conversations effectively by keeping track of the context.\n\nChatbot Function:\nA function is defined to create and invoke the chatbot with the current state. This function integrates all the components—language model, tools, and state graph—into a cohesive system that can respond to user inputs.\n\nGraph Configuration:\nThe state graph is further configured by adding a chatbot node, setting the entry point (where the conversation starts), and specifying the finish point (where the conversation ends). This configuration ensures that the chatbot knows how to begin and end interactions with the user.\n\nMain Loop:\nThe main loop continuously interacts with the user. It accepts user input, processes it through the state graph to determine the appropriate response, and then outputs the assistant’s reply. The loop keeps running until the user types a command like \"quit\", \"exit\", or \"q\", at which point the chatbot ends the session.\n\nSample Output -\n\n\nThis simple agentic implementation helps visualizing any dataset you have. It also helps find outliers and assists in selecting the most relevant features for your machine learning model. Alternatively, you can create a \ngraph for tools\n and enable the agent to interact with the tools as needed based on the specific requirements of the task.\n\nLimitations/Drawbacks: Managing the state of the graph is challenging, especially since the state function changes based on the use case. If proper state management is not implemented, errors may arise later in the process, leading to instability.\nGaps in Existing Research: This approach is highly use case-specific, which limits its broader applicability. To improve flexibility, some approaches incorporate a tool node, allowing the agent to select tools based on specific requirements. Additionally, adding graphs under nodes for a more use case-centric design could provide greater adaptability and scalability.\n\nCode Review and Technology stack analysis using CrewAI -\n\nCrewAI\n\n\n\nVersion Used - 0.102.0\nFramework description -\nCrewAI is a fast, lightweight Python framework built from the ground up, independent of LangChain or other agent frameworks, offering developers both simplicity and fine-grained control. It is designed for creating autonomous AI agents that can be customized for any use case. CrewAI provides two key features: Crews, which allow developers to create AI teams where each agent has specific roles, tools, and goals to collaborate effectively, and Flows, which offer event-driven control, enabling precise task orchestration with single LLM calls, while also supporting Crews natively. This combination makes CrewAI a flexible solution for building complex, collaborative AI systems.\n\n\nKey Features :\n\nWorkflow management\nFlexible tools\nTask Management\nIntelligent collaboration\n\nDescription of class implemented(sequential process)\nThe sequential process ensures tasks are completed in a specific, linear order, making it ideal for projects that require clear, step-by-step execution. Key features include a linear task flow that ensures tasks follow a predetermined sequence, simplicity for projects with straightforward tasks, and easy monitoring for tracking task completion. To implement the sequential process, users can assemble their AI crew and define the tasks in the order they need to be executed, ensuring an efficient and organized approach to project completion.\n\nInstallation and Usage Instructions :\n\nIt is recommended to create a virtual environment on your local machine before installing any packages.\n\nPlease refer to \ninstallation guide\n for getting started. Things get a bit complicated when it comes to CrewAI. This is due to the folder structure that the library follows. Strictly refer to the installation guide provided above. Before making any further changes try executing \nsample code\n and make sure the flow works end-too-end. In case you get any errors while running the crew please refer to the \ndiscussions\n. Make sure you have the following directory structure :\n\nProblem Statement Addressed - The need for automating code review and technology stack analysis, enabling efficient evaluation and assessment of code quality and technology choices.\n\nCodeReviewCrew/\n├── .env\n├── .gitignore\n├── pyproject.toml\n├── README.md\n├── uv.lock\n├── knowledge/\n│ └── user_preference.txt\n├── output/\n├── src/\n│ └── CodeReviewCrew/\n│ ├── pycache/\n│ ├── config/\n│ │ ├── agents.yaml\n│ │ └── agents.yaml\n│ ├── tools/\n│ │ ├── init.py\n│ │ └── custom_tool.py\n│ ├── main.py\n│ ├── init.py\n│ ├── crew.py\n├── tests/\n└── ...\n\nOnce you are have the crew running and directory in place make following changes:\n\nChange your .env file to -\n\nMODEL=gpt-4o\nAZURE_API_KEY=your_api_key\nAZURE_API_BASE=\nhttps://example.baseURL.azure.com/\n\nAZURE_API_VERSION=2024-05-01-preview\nAZURE_SDK_TRACING_IMPLEMENTATION=opentelemetry\n\nEnsure that you include the exact same parameters in your environment file (do not modify the names or remove the uppercase letters). For tracing, in some versions, if \"opentelemetry\" doesn't work, try replacing it with \"opentelemetry\".\n\nAdd some context to the user preference file -\nUser name is XXXX.\nUser is an AI Engineer.\nUser is interested in AI Agents.\nUser creates documents based on the code provided for more code readability.\nSetting up the agent configs -\n\nAdd this to agents.yaml\n\ncode_interpretor:\n  role: >\n    {topic} Senior Python coder\n  goal: >\n    Create high level description of code based on {topic} provided.\n  backstory: >\n    You are an experienced code reviewer with strong code readability and analyzing skills.\n  llm: azure/gpt-4o\n \ndocumentation_expert:\n  role: >\n    {topic} Documentation Expert\n  goal: >\n    Create document based on information provided on {topic} to create a detailed report. \n  backstory: >\n    You are an experienced documentation expert with strong writing and analytical skills. You are capable of creating detailed reports based on the information provided.\n  llm: azure/gpt-4o\n \ntechnology_expert:\n  role: >\n    {topic} Technology Expert\n  goal: >\n    Understand the technology stack used in {topic}.\n    Gather information from web and provide detailed report on the technology stack.\n  backstory: >\n    You are an experienced technology expert with strong knowledge of the technology stack used in {topic}. You are capable of providing detailed reports on the technology stack.\n  llm: azure/gpt-4o\n\n\nAdd this to tasks.yaml\n\ncode_interpretor_task:\n  description: >\n    Review the code you got and create a high-level description of the code, integrating it into a full section for a report.\n    Ensure the report is detailed, includes all relevant code snippets, and provides insights into the code structure and functionality.\n  expected_output: >\n    A fully fledged report on explainability of the code, including code snippets, code structure, and functionality.\n  agent: code_interpretor\n \ndocumentation_expert_task:\n  description: >\n    Review the context you got and document the information provided to create a detailed report.\n    Ensure the report is detailed, includes all relevant information, and provides insights into the {topic}.\n  expected_output: >\n    A fully fledged report with detailed information, analysis, and insights into the {topic}.\n  agent: documentation_expert\n \n \ntechnology_expert_task:\n  description: >\n    Review the context you got and provide a detailed report on the technology stack used in {topic}.\n    Do thorough research on the web to gather information on the technology stack.\n    Ensure the report is detailed, includes all relevant information, and provides insights into the technology stack.\n  expected_output: >\n    A fully fledged report with detailed information, analysis, and insights into the technology stack used in {topic}.\n  agent: technology_expert\n\nAdding a custom tool -\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom crewai.tools import BaseTool\n\n\nclass MyCustomDuckDuckGoTool(BaseTool):\n    name: str = \"DuckDuckGo Search Tool\"\n    description: str = \"Search the web for a given query.\"\n\n    def _run(self, query: str) -> str:\n        # Ensure the DuckDuckGoSearchRun is invoked properly.\n        duckduckgo_tool = DuckDuckGoSearchRun()\n        response = duckduckgo_tool.invoke(query)\n        return response\n\n    def _get_tool():\n        # Create an instance of the tool when needed\n        return MyCustomDuckDuckGoTool()\n\nChange the main.py and crew.py\n# Replace main.py with the following code\n\nimport sys\nimport warnings\n\nfrom datetime import datetime\n\nfrom CodeReviewCrew.crew import CodeReviewCrew\n\nwarnings.filterwarnings(\"ignore\", category=SyntaxWarning, module=\"pysbd\")\n\n# This main file is intended to be a way for you to run your\n# crew locally, so refrain from adding unnecessary logic into this file.\n# Replace with inputs you want to test with, it will automatically\n# interpolate any tasks and agents information\n\ndef run():\n    \"\"\"\n    Run the crew.\n    \"\"\"\n    inputs = {\n        'topic': '''\n#python code\nfrom swarm import Agent\n\n\ndef process_refund(item_id, reason=\"NOT SPECIFIED\"):\n    \"\"\"Refund an item. Refund an item. Make sure you have the item_id of the form item_... Ask for user confirmation before processing the refund.\"\"\"\n    print(f\"[mock] Refunding item {item_id} because {reason}...\")\n    return \"Success!\"\n\n\ndef apply_discount():\n    \"\"\"Apply a discount to the user's cart.\"\"\"\n    print(\"[mock] Applying discount...\")\n    return \"Applied discount of 11%\"\n\n\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"Determine which agent is best suited to handle the user's request, and transfer the conversation to that agent.\",\n)\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"Be super enthusiastic about selling bees.\",\n)\nrefunds_agent = Agent(\n    name=\"Refunds Agent\",\n    instructions=\"Help the user with a refund. If the reason is that it was too expensive, offer the user a refund code. If they insist, then process the refund.\",\n    functions=[process_refund, apply_discount],\n)\n\n\ndef transfer_back_to_triage():\n    \"\"\"Call this function if a user is asking about a topic that is not handled by the current agent.\"\"\"\n    return triage_agent\n\n\ndef transfer_to_sales():\n    return sales_agent\n\n\ndef transfer_to_refunds():\n    return refunds_agent\n\n\ntriage_agent.functions = [transfer_to_sales, transfer_to_refunds]\nsales_agent.functions.append(transfer_back_to_triage)\nrefunds_agent.functions.append(transfer_back_to_triage)\n''',\n        'current_year': str(datetime.now().year)\n    }\n    \n    try:\n        CodeReviewCrew().crew().kickoff(inputs=inputs)\n    except Exception as e:\n        raise Exception(f\"An error occurred while running the crew: {e}\")\n\n\ndef train():\n    \"\"\"\n    Train the crew for a given number of iterations.\n    \"\"\"\n    inputs = {\n        \"topic\": \"AI LLMs\"\n    }\n    try:\n        CodeReviewCrew().crew().train(n_iterations=int(sys.argv[1]), filename=sys.argv[2], inputs=inputs)\n\n    except Exception as e:\n        raise Exception(f\"An error occurred while training the crew: {e}\")\n\ndef replay():\n    \"\"\"\n    Replay the crew execution from a specific task.\n    \"\"\"\n    try:\n        CodeReviewCrew().crew().replay(task_id=sys.argv[1])\n\n    except Exception as e:\n        raise Exception(f\"An error occurred while replaying the crew: {e}\")\n\ndef test():\n    \"\"\"\n    Test the crew execution and returns the results.\n    \"\"\"\n    inputs = {\n        \"topic\": \"AI LLMs\"\n    }\n    try:\n        CodeReviewCrew().crew().test(n_iterations=int(sys.argv[1]), openai_model_name=sys.argv[2], inputs=inputs)\n\n    except Exception as e:\n        raise Exception(f\"An error occurred while testing the crew: {e}\")\n\n# Replace crew.py with the following code\nfrom crewai import Agent, Crew, Process, Task\nfrom crewai.project import CrewBase, agent, crew, task\n# from crewai_tools import (\n#     DirectoryReadTool,\n#     FileReadTool,\n#     SerperDevTool,\n#     WebsiteSearchTool\n# )\nfrom CodeReviewCrew.tools.custom_tool import MyCustomDuckDuckGoTool\n\n\nDuck_search = MyCustomDuckDuckGoTool()\n# If you want to run a snippet of code before or after the crew starts, \n# you can use the @before_kickoff and @after_kickoff decorators\n# https://docs.crewai.com/concepts/crews#example-crew-class-with-decorators\n\n@CrewBase\nclass CodeReviewCrew():\n\t\"\"\"CodeReviewCrew crew\"\"\"\n\n\t# Learn more about YAML configuration files here:\n\t# Agents: https://docs.crewai.com/concepts/agents#yaml-configuration-recommended\n\t# Tasks: https://docs.crewai.com/concepts/tasks#yaml-configuration-recommended\n\tagents_config = 'config/agents.yaml'\n\ttasks_config = 'config/tasks.yaml'\n\t\n\n\t# If you would like to add tools to your agents, you can learn more about it here:\n\t# https://docs.crewai.com/concepts/agents#agent-tools\n\t@agent\n\tdef code_interpretor(self) -> Agent:\n\t\treturn Agent(\n\t\t\tconfig=self.agents_config['code_interpretor'],\n\t\t\tverbose=True\n\t\t)\n\n\t@agent\n\tdef documentation_expert(self) -> Agent:\n\t\treturn Agent(\n\t\t\tconfig=self.agents_config['documentation_expert'],\n\t\t\tverbose=True\n\t\t)\n\t\n\t@agent\n\tdef technology_expert(self) -> Agent:\n\t\treturn Agent(\n\t\t\tconfig=self.agents_config['technology_expert'],\n\t\t\tverbose=True,\n\t\t\ttools=[Duck_search]\n\t\t)\n\n\t# To learn more about structured task outputs, \n\t# task dependencies, and task callbacks, check out the documentation:\n\t# https://docs.crewai.com/concepts/tasks#overview-of-a-task\n\t@task\n\tdef code_interpretor_task(self) -> Task:\n\t\treturn Task(\n\t\t\tconfig=self.tasks_config['code_interpretor_task'],\n\t\t\toutput_file='output/Code_report.md'\n\t\t)\n\n\t@task\n\tdef documentation_expert_task(self) -> Task:\n\t\treturn Task(\n\t\t\tconfig=self.tasks_config['documentation_expert_task'],\n\t\t\toutput_file='output/code_document_report.md'\n\t\t)\n\t\n\t@task\n\tdef technology_expert_task(self) -> Task:\n\t\treturn Task(\n\t\t\tconfig=self.tasks_config['technology_expert_task'],\n\t\t\toutput_file='output/technology_stack_report.md'\n\t\t)\n\n\t@crew\n\tdef crew(self) -> Crew:\n\t\t\"\"\"Creates the CodeReviewCrew crew\"\"\"\n\t\t# To learn how to add knowledge sources to your crew, check out the documentation:\n\t\t# https://docs.crewai.com/concepts/knowledge#what-is-knowledge\n\n\t\treturn Crew(\n\t\t\tagents=self.agents, # Automatically created by the @agent decorator\n\t\t\ttasks=self.tasks, # Automatically created by the @task decorator\n\t\t\tprocess=Process.sequential,\n\t\t\tverbose=True,\n\t\t\t# process=Process.hierarchical, # In case you wanna use that instead https://docs.crewai.com/how-to/Hierarchical/\n\t\t)\n\nRunning the crew -\n\nTo run the completed crew navigate to the root of your project\n\ncd .\\CodeReviewCrew\\\n\nOnce you navigate to the root of your project run the crew\n\ncrewai run\nResults -\nCheck the output folder for results. LLM powered reports will be generated by each agent in .md format.\n\nSample Output -\n\n\n\n\n\n\nThis implementation automates code reviews and technology stack analysis by taking advantage of CrewAI’s advanced features, including built-in document-saving capabilities. This implementation helps in the process of identifying code quality issues, optimizing performance, and assessing compatibility across various technologies within a project.\n\nLimitations/Drawbacks: There are recurring telemetry issues, despite attempts to resolve them using OpenTelemetry or other fixes. While these issues don’t impact the output of the crew, they negatively affect the clarity of the terminal output, which can be problematic for troubleshooting and monitoring.\nGaps in Existing Research: This implementation includes the sequential process approach to address the problem statement; however, a hierarchical process could also be a valuable alternative, depending on the complexity of the tasks. Additionally, there is potential to configure custom flows tailored to specific use cases, and agents could be trained over time to improve their performance and adaptability.\n\nAgentic RAG using HuggingFace SmolAgents -\n\nSmolAgents\n\n\n\nVersion Used - 1.11.0\nFramework description -\nFor certain low-level agentic tasks, such as chains or routers, writing all the code yourself can give you full control and a better understanding of your system. However, as the complexity increases—like when you want an LLM to call a function (tool calling) or run a while loop (multi-step agent)—you’ll need some abstractions. SmolAgents is a lightweight library developed by Hugging Face that makes it easier to create and run powerful agents. Its minimalist design sets it apart, with the entire agent logic contained in just about 1,000 lines of code. This simple yet efficient approach ensures that users can quickly get started while still benefiting from the full functionality needed for building robust agents.\n\n\nKey Features :\n\nSimplicity\nHub Integrations\nCode Agent Support\nMulti LLM support\n\nDescription of class implemented(ToolCallingAgent)\nThe ToolCallingAgent is a special agent in the smolagents library that helps solve tasks by using predefined tools or managed agents. It works by creating and using structured JSON-like tool calls, which help it communicate clearly with external systems like APIs or databases. Key features of the ToolCallingAgent include dynamically executing tools, managing intermediate results, keeping track of its actions through logging, and offering transparency in decision-making. It also uses a fallback mechanism to provide a final answer if it can't solve the task within a certain number of steps. Essentially, it makes interacting with tools more organized and efficient.\n\nInstallation and Usage Instructions :\n\nIt is recommended to create a virtual environment on your local machine before installing any packages.\n\nPlease refer to \ninstallation guide\n for getting started.\n\nProblem Statement Addressed - The need for automating Retrieval-Augmented Generation (RAG) processes, enabling intelligent and context-aware generation of responses.\n\nfrom smolagents import AzureOpenAIServerModel\nfrom smolagents import ToolCallingAgent\nfrom smolagents import Tool\n\nfrom langchain_core.vectorstores import VectorStore\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n\nfrom transformers import AutoTokenizer\n\nfrom dotenv import load_dotenv\nimport fitz\nfrom tqdm import tqdm\n\n\n# Load environment variables\nload_dotenv()\n\n\n# Initialize the Azure OpenAI model\nmodel = AzureOpenAIServerModel(\n    model_id=\"gpt-4o\",\n    azure_endpoint=\"https://example.endpoint.azure.com/\",\n    api_key=\"your_api_key\",\n    api_version=\"2024-08-01\"\n)\n\n# Function to extract text from a PDF file\ndef extract_text_from_pdf(pdf_path):\n    doc = fitz.open(pdf_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    return text\n\n# Path to your PDF file\npdf_path = \"path_to_your_pdf\"\n\n# Extract text from the PDF\npdf_text = extract_text_from_pdf(pdf_path)\n\n# Create a Document object from the extracted text\npdf_document = Document(page_content=pdf_text, metadata={\"source\": \"pdf_file\"})\n\n# Use the extracted PDF document as the source document\nsource_docs = [pdf_document]\n\n# Initialize the text splitter\ntext_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n    chunk_size=200,\n    chunk_overlap=20,\n    add_start_index=True,\n    strip_whitespace=True,\n    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n)\n\n# Split documents and keep only unique ones\nprint(\"Splitting documents...\")\ndocs_processed = []\nunique_texts = {}\nfor doc in tqdm(source_docs):\n    new_docs = text_splitter.split_documents([doc])\n    for new_doc in new_docs:\n        if new_doc.page_content not in unique_texts:\n            unique_texts[new_doc.page_content] = True\n            docs_processed.append(new_doc)\n\n# Embed the processed documents\nprint(\"Embedding documents...\")\nembedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\nhuggingface_doc_vector_db = FAISS.from_documents(\n    documents=docs_processed,\n    embedding=embedding_model,\n    distance_strategy=DistanceStrategy.COSINE,\n)\n\n# Define the RetrieverTool class\nclass RetrieverTool(Tool):\n    name = \"retriever\"\n    description = \"Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n    inputs = {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n        }\n    }\n    output_type = \"string\"\n\n    def __init__(self, vectordb: VectorStore, **kwargs):\n        super().__init__(**kwargs)\n        self.vectordb = vectordb\n\n    def forward(self, query: str) -> str:\n        assert isinstance(query, str), \"Your search query must be a string\"\n\n        docs = self.vectordb.similarity_search(\n            query,\n            k=7,\n        )\n\n        return \"\\nRetrieved documents:\\n\" + \"\".join(\n            [f\"===== Document {str(i)} =====\\n\" + doc.page_content for i, doc in enumerate(docs)]\n        )\n\n# Initialize the retriever tool and agent\nhuggingface_doc_retriever_tool = RetrieverTool(huggingface_doc_vector_db)\nretriever_agent = ToolCallingAgent(\n    tools=[huggingface_doc_retriever_tool],\n    model=model,\n)\n\n# Function to run the agentic RAG process\ndef run_agentic_rag(question: str) -> str:\n    question_prompt = f\"\"\"Using the information contained in your knowledge base, which you can access with the 'retriever' tool,\n    give a comprehensive answer to the question below.\n    Respond only to the question asked, response should be concise and relevant to the question.\n    If you cannot find information, do not give up and try calling your retriever again with different arguments!\n    Make sure to have covered the question completely by calling the retriever tool several times with semantically different queries.\n\n    Question:\n    {question}\"\"\"\n\n    return retriever_agent.run(question_prompt)\n\n\nquestion = \"Explain core capabilities of autogen\"\n\nanswer = run_agentic_rag(question)\nprint(f\"Question: {question}\")\nprint(f\"Answer: {answer}\")\n\nCode Explanation -\nThis script is designed to extract text from a PDF, break it down into smaller pieces, embed these pieces into a vector database, and then use a retriever tool to find the most relevant documents based on a query. The code relies on several libraries and tools such as smolagents, langchain, transformers, and fitz to process the PDF and perform the retrieval tasks.\n\nImports and Environment Setup:\nThe script begins by importing the necessary libraries and modules required for the entire process. It also loads environment variables using the dotenv library, ensuring that any sensitive data, like API keys or configuration details, are securely managed and accessed.\n\nModel Initialization:\nThe script initializes the Azure OpenAI model with specific parameters. This model will be responsible for processing the query and interacting with the data, allowing the agent to generate answers based on the retrieved documents.\n\nPDF Text Extraction:\nA function is defined to extract text from a given PDF file using the fitz library, which is a part of the PyMuPDF package. The function takes in a PDF file, processes it, and converts the extracted text into a Document object, making it easier to handle and manipulate.\n\nText Splitting:\nOnce the text is extracted, a text splitter is initialized using a tokenizer from HuggingFace. This step breaks the long text into smaller, manageable chunks, ensuring that each chunk is unique. This is crucial because smaller chunks make it easier to work with the data and improve the quality of retrieval results.\n\nDocument Embedding:\nThe text chunks are then embedded into a vector database using the FAISS library, which is used for efficient similarity search. A HuggingFace embedding model is applied to transform the text chunks into vector representations. These embeddings allow the system to understand the semantic meaning of the text and perform more accurate retrieval when a query is made.\n\nRetriever Tool Definition:\nThe code defines a RetrieverTool class that is responsible for retrieving documents from the vector database. It does this by comparing the semantic similarity between the query and the embedded text chunks. This ensures that the documents retrieved are the most relevant to the given query.\n\nAgent Initialization:\nA ToolCallingAgent is initialized with the retriever tool and the Azure OpenAI model. This agent will manage the interaction between the user’s query and the retrieval system, ensuring that the query is processed and the correct documents are fetched from the database.\n\nQuestion Answering Function:\nThe function run_agentic_rag is defined to handle the process of enhancing the query and using the retriever agent to find the most relevant documents. The function takes the question, processes it, and retrieves the documents that are most related to it, ultimately returning the best possible answer based on the information stored in the vector database.\n\nExecution:\nFinally, a sample question is defined and passed into the run_agentic_rag function to retrieve an answer. The script prints out the question along with the answer retrieved from the vector database, providing an example of how the system works in practice.\n\nSample Output -\n\n\n\n\n\n\n\n\n\n\nThe script automates the process of querying a PDF document for information using AI. It extracts text, processes it into manageable chunks, converts the text into vector embeddings, and then uses a semantic search mechanism to find relevant answers to user queries. Agentic approach helps gather information multiple times for different relevant input prompt allowing more accurate answer.\n\nLimitations/Drawbacks: The MultiStepAgent class encounters frequent issues related to Git, as the framework is still experimental and subject to frequent changes in functions, methods, and classes. This instability can create challenges when trying to maintain consistency.\nGaps in Existing Research: This approach lacks central orchestration or process integrations, primarily due to limited community support and insufficient examples in the documentation. While there are some multi-agent handling capabilities, they remain underexplored and are still in development. The CodeAgent class, however, is the most stable and reliable agent for code-related use cases.\n\nAI powered Research and report generation using LlamaIndex -\n\nLlamaIndex\n\n\n\nVersion Used - 0.12.22\nFramework description -\nLlamaIndex is a framework for creating LLM-powered agents and applications that enhance the ability of language models (LLMs) to interact with various types of data. Agents are assistants powered by LLMs that use tools to perform tasks such as research or data extraction. They can range from simple question-answering to more complex tasks involving decision-making and actions. Context augmentation is the process of making private or specific data accessible to LLMs so they can use it to solve problems. LlamaIndex helps with context-augmentation by allowing users to ingest, index, and process data from various sources like APIs, databases, PDFs, and more. LlamaIndex provides a comprehensive framework for building advanced LLM applications, combining agents, workflows, and context-augmentation to interact with diverse data sources. It supports a range of use cases, from simple question-answering to complex multi-modal applications and autonomous agents.\n\n\nKey Features :\n\nDistributed Service Oriented Architecture\nEase of deployment\nScalability and resource management\nCustomized orchestration flows\n\nDescription of class implemented(AgentWorkflow)\nThe AgentWorkflow is a tool that helps manage and run a system with one or more agents. It acts as an orchestrator, coordinating how agents interact and complete tasks. It ensures that tasks are completed in an orderly and efficient manner. It is especially useful for handling workflows that involve multiple steps or agents working together.\n\nInstallation and Usage Instructions :\n\nIt is recommended to create a virtual environment on your local machine before installing any packages.\n\nPlease refer to \ninstallation guide\n for getting started.\n\nProblem Statement Addressed: The need for automating AI-powered research and report generation, enabling efficient gathering, processing, and summarization of information to produce detailed reports.\n\nimport asyncio\nfrom llama_index.llms.azure_openai import AzureOpenAI\nfrom llama_index.core.agent.workflow import (\n    AgentOutput,\n    ToolCall,\n    ToolCallResult,\n)\nfrom llama_index.core.workflow import Context\nfrom llama_index.core.agent.workflow import FunctionAgent\nfrom llama_index.core.agent.workflow import AgentWorkflow\n\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\n\n# Azure OpenAI API credentials\naoai_api_key = \"ede5ff1d10f04521b9db46a6e15c19ff\"\naoai_endpoint = \"https://azureaieu6682939841.cognitiveservices.azure.com/\"\naoai_api_version = \"2024-08-01-preview\"\n\n# Initialize the Azure OpenAI LLM\nllm = AzureOpenAI(\n    engine=\"gpt-4o-Auto-Agent\",\n    model=\"gpt-4o\",\n    api_key=aoai_api_key,\n    azure_endpoint=aoai_endpoint,\n    api_version=aoai_api_version,\n)\n\n# Define the search_web tool\nasync def search_web(query: str) -> str:\n    \"\"\"Useful for using the web to answer questions.\"\"\"\n    web_search = DuckDuckGoSearchRun()\n    return str(web_search.run(query))\n\n# Define the record_notes tool\nasync def record_notes(ctx: Context, notes: str, notes_title: str) -> str:\n    \"\"\"Useful for recording notes on a given topic. Your input should be notes with a title to save the notes under.\"\"\"\n    current_state = await ctx.get(\"state\")\n    if \"research_notes\" not in current_state:\n        current_state[\"research_notes\"] = {}\n    current_state[\"research_notes\"][notes_title] = notes\n    await ctx.set(\"state\", current_state)\n    return \"Notes recorded.\"\n\n# Define the write_report tool\nasync def write_report(ctx: Context, report_content: str) -> str:\n    \"\"\"Useful for writing a report on a given topic. Your input should be a markdown formatted report.\"\"\"\n    current_state = await ctx.get(\"state\")\n    current_state[\"report_content\"] = report_content\n    await ctx.set(\"state\", current_state)\n    return \"Report written.\"\n\n# Define the review_report tool\nasync def review_report(ctx: Context, review: str) -> str:\n    \"\"\"Useful for reviewing a report and providing feedback. Your input should be a review of the report.\"\"\"\n    current_state = await ctx.get(\"state\")\n    current_state[\"review\"] = review\n    await ctx.set(\"state\", current_state)\n    return \"Report reviewed.\"\n\n# Define the ResearchAgent\nresearch_agent = FunctionAgent(\n    name=\"ResearchAgent\",\n    description=\"Useful for searching the web for information on a given topic and recording notes on the topic.\",\n    system_prompt=(\n        \"You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic. \"\n        \"Once notes are recorded and you are satisfied, you should hand off control to the WriteAgent to write a report on the topic. \"\n        \"You should have at least some notes on a topic before handing off control to the WriteAgent.\"\n    ),\n    llm=llm,\n    tools=[search_web, record_notes],\n    can_handoff_to=[\"WriteAgent\"],\n)\n\n# Define the WriteAgent\nwrite_agent = FunctionAgent(\n    name=\"WriteAgent\",\n    description=\"Useful for writing a report on a given topic.\",\n    system_prompt=(\n        \"You are the WriteAgent that can write a report on a given topic. \"\n        \"Your report should be in a markdown format. The content should be grounded in the research notes. \"\n        \"Once the report is written, you should get feedback at least once from the ReviewAgent.\"\n    ),\n    llm=llm,\n    tools=[write_report],\n    can_handoff_to=[\"ReviewAgent\", \"ResearchAgent\"],\n    handoff_logic=lambda state: \"ReviewAgent\" if state.get(\"report_content\") != \"Not written yet.\" else \"ResearchAgent\"\n)\n\n# Define the ReviewAgent\nreview_agent = FunctionAgent(\n    name=\"ReviewAgent\",\n    description=\"Useful for reviewing a report and providing feedback.\",\n    system_prompt=(\n        \"You are the ReviewAgent that can review the write report and provide feedback. \"\n        \"Your review should either approve the current report or request changes for the WriteAgent to implement. \"\n        \"If you have feedback that requires changes, you should hand off control to the WriteAgent to implement the changes after submitting the review.\"\n    ),\n    llm=llm,\n    tools=[review_report],\n    can_handoff_to=[\"WriteAgent\"],\n)\n\n# Define the agent workflow\nagent_workflow = AgentWorkflow(\n    agents=[research_agent, write_agent, review_agent],\n    root_agent=research_agent.name,\n    initial_state={\n        \"research_notes\": {},\n        \"report_content\": \"Not written yet.\",\n        \"review\": \"Review required.\",\n    },\n)\n\n# Main function to run the agent workflow\nasync def main():\n    handler = agent_workflow.run(\n        user_msg=(\n            \"Write me a report on the Top 5 Agentic Frameworks. \"\n            \"Briefly describe the all the frameworks in detail and give sample code implementations in python \"\n            \"Also give technology stack used in each framework.\"\n        )\n    )\n\n    current_agent = None\n    current_tool_calls = \"\"\n\n    # Process events from the agent workflow\n    async for event in handler.stream_events():\n        if (\n            hasattr(event, \"current_agent_name\")\n            and event.current_agent_name != current_agent\n        ):\n            current_agent = event.current_agent_name\n            print(f\"\\n{'='*50}\")\n            print(f\"🤖 Agent: {current_agent}\")\n            print(f\"{'='*50}\\n\")\n        # if isinstance(event, AgentStream):\n        #     if event.delta:\n        #         print(event.delta, end=\"\", flush=True)\n        # elif isinstance(event, AgentInput):\n        #     print(\"📥 Input:\", event.input)\n        elif isinstance(event, AgentOutput):\n            if event.response.content:\n                print(\"📤 Output:\", event.response.content)\n            if event.tool_calls:\n                tool_names = [call.tool_name for call in event.tool_calls]\n                current_tool_calls += f\"🛠️  Planning to use tools: {tool_names}\\n\"\n                print(\"🛠️  Planning to use tools:\", tool_names)\n        elif isinstance(event, ToolCallResult):\n            tool_result = (\n                f\"🔧 Tool Result ({event.tool_name}):\\n\"\n                f\"  Arguments: {event.tool_kwargs}\\n\"\n                f\"  Output: {event.tool_output}\\n\"\n            )\n            current_tool_calls += tool_result\n            print(tool_result)\n        elif isinstance(event, ToolCall):\n            tool_call = (\n                f\"🔨 Calling Tool: {event.tool_name}\\n\"\n                f\"  With arguments: {event.tool_kwargs}\\n\"\n            )\n            current_tool_calls += tool_call\n            print(tool_call)\n\n    # Print a summary of tool calls\n    print(\"\\nSummary of tool calls:\")\n    print(current_tool_calls)\n\n# Run the main function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nCode Explanation -\nThis code defines an agent-based workflow utilizing the llama_index and langchain_community libraries to create a collaborative environment where three agents work together to generate a report on a given topic. The workflow is designed to have each agent perform specific tasks in a sequential manner, with each agent's output serving as the input for the next one.\n\nAzure OpenAI Initialization:\nThe script begins by initializing the Azure OpenAI LLM (Language Model) using the provided API credentials. This sets up the model that will power the agents, allowing them to generate natural language responses and process queries.\n\nTool Definitions:\nSeveral tools are defined to assist the agents in carrying out their tasks;\nsearch_web(Uses DuckDuckGo to search the web for information related to the topic at hand.)\nrecord_notes(A tool that helps record notes about the topic, which will later be used by the writing agent to create the report.)\nwrite_report(A tool designed to write a report on the given topic using the recorded notes.)\nreview_report(A tool that reviews the generated report and provides feedback for improvement.)\n\nAgent Definitions:\nThe script defines three agents, each with a specific role;\nResearchAgent(This agent is responsible for searching the web for relevant information and recording notes.)\nWriteAgent(The WriteAgent uses the research notes provided by the ResearchAgent to write a detailed report on the topic.)\nReviewAgent(After the report is written, this agent reviews the content and provides feedback or suggests improvements to ensure the quality of the report.)\n\nAgent Workflow:\nThe agent workflow is structured to start with the ResearchAgent, which performs the web search and note-taking. After the research phase, the WriteAgent takes over, utilizing the notes to write the report. Finally, the ReviewAgent reviews the report, ensuring that it meets the desired quality before completion. This collaborative process allows the agents to hand off tasks and work together to create a polished, comprehensive report.\n\nMain Function:\nThe main function runs the agent workflow and processes events as the agents perform their tasks. It prints the outputs and tool calls, allowing you to track the workflow and understand how each agent interacts with the tools and contributes to the final result. The agents' outputs are passed from one to the next in a smooth sequence, making the process of generating the report automated and efficient.\n\nSample Output -\n\n\n\n\n\n\nA simple multi-agentic approach to perform research, write reports, and review them, utilizing a combination of tools such as web search and note recording. The code demonstrates a multi-agent workflow to research a topic, generate a report, and refine it through feedback. The agents work together to handle the process from gathering information to producing a final reviewed report.\n\nLimitations/Drawbacks: The system is heavily dependent on OpenAI, as it internally utilizes the entire OpenAI ecosystem. Additionally, there is a lack of conceptual documentation—while the documentation offers numerous code examples, it doesn't provide a clear understanding of the architectural framework.\nGaps in Existing Research: For this problem statement, I used the workflow concept from LlamaIndex, but there are many other orchestration deployment patterns available. The framework also supports older agent classes (such as OldOpenAIAgent and OldReActAgent), making it easier to migrate between different versions of the framework.\n\nDeployment Considerations -\nWhen implementing these agentic AI frameworks in a production environment, security considerations are crucial to ensure safe and reliable operation. For instance, frameworks like LangGraph that execute Python code via a REPL environment need strict safeguards to prevent the execution of malicious code. It's important to opt-in to certain features and use access controls to limit exposure to security risks. Additionally, frameworks such as SmolAgents and Autogen, which interact with external tools and APIs, must ensure secure communication and handle potential vulnerabilities in data exchanges. Crewai's sequential process must be closely monitored to avoid unauthorized task execution, while LlamaIndex workflows must maintain integrity to prevent malicious agents from hijacking task execution. In production, these frameworks should be deployed with comprehensive logging, access control mechanisms, and continuous security auditing to minimize risks and ensure safe deployment.\n\nReader next steps -\n\nNow that you have an understanding of the prerequisites and requirements, the next step is to get hands-on with the frameworks. Start by setting up your development environment, installing the necessary libraries, and obtaining any required API keys. Once everything is set up, try experimenting with some basic examples to get familiar with how each framework works. Here are some more frameworks that you can consider exploring :\n\nMicrosoft - \nSemantic Kernel Agents\nMicrosoft - \nPrompt Flow\nAzureAI - \nAgent Service\n\nMicrosoft provide industry scalable solutions that you can directly integrate with your existing system or flow. You can also try out enterprise versions for including agentic frameworks into your organization.\n\nResults\n\nAs we reach the end of this research and its findings, I’d like to note that while I initially intended to explore more frameworks, I focused on these key ones—Autogen, LangGraph, Crewai, SmolAgents, and LlamaIndex—to showcase their unique functionalities and applications in agentic AI.\n\nFor Autogen, I implemented the MagenticOneGroupChat system, which is a versatile, multi-agent architecture capable of tackling open-ended tasks across various domains. This system, built on the autogen-core library and upgraded with autogen-agentchat, is orchestrated through the MagenticOneGroupChat, which supports collaborative agent teams for task execution. This provided a highly modular and user-friendly interface.\n\nIn LangGraph, I worked with the create_pandas_dataframe_agent class, which allows language models to seamlessly interact with data in Pandas DataFrames using natural language queries. This feature greatly simplifies complex tasks like filtering, aggregation, and visualization. However, a potential security concern arises from executing Python code in a REPL environment, requiring users to opt-in and manage potential risks carefully.\n\nFor Crewai, I focused on implementing the sequential process class. This class ensures tasks are executed in a specific, linear order, which is perfect for projects needing clear, step-by-step execution. It’s simple yet effective for straightforward tasks, with easy monitoring for tracking progress.\n\nIn SmolAgents, I worked with the ToolCallingAgent, a specialized agent that facilitates task completion by invoking predefined tools or managed agents. The agent uses structured JSON-like tool calls to interact with external systems, such as APIs or databases, offering dynamic tool execution, state management, and transparent decision-making, which makes the entire process more efficient and organized.\n\nLastly, for LlamaIndex, I explored the AgentWorkflow, a tool that orchestrates and manages systems with multiple agents. This class enables smooth coordination between agents, ensuring that tasks are carried out in an orderly, efficient manner, making it ideal for workflows that involve several agents working together in a system.\n\nThrough these implementations, we’ve observed how each framework addresses different aspects of agentic AI, from task automation and data handling to security and workflow management. While each framework has its strengths, the results also highlight areas for improvement, such as scalability and adaptability, which are crucial for real-world applications.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nIntroduction\n\nAbstract\n\nMethodology\n\nSmart Orchestration for Local Code Generation and Execution using AutoGen -\n\nAutomating Data Visualization using LangGraph -\n\nCode Review and Technology stack analysis using CrewAI -\n\nAgentic RAG using HuggingFace SmolAgents -\n\nResults\n\nComments\nCode",
    "awards": [
      "distinguished technical deep-dive"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "agentic-ai-developer-certification-production-langgraph-orchestrated-research-assistant-Vr7NF6G51fjN",
    "username": "c@chibueze.k.muoneke",
    "license": "/Agentic_AI_Developer_Certification_Project3-main\n├── LICENSE\n├── README.md                  # Project overview and instructions\n├── deployment.md              # Deployment guide for various environments\n├── Dockerfile                 # Instructions for Docker image config\n├── requirements-test.txt      # Development and test dependencies\n├── requirements.txt           # Project dependencies\n├── .gitignore                 # Ignored files and folders\n├── .env.example               # Example environment file for API keys\n├── data/\n│   ├── project_1_publications.json  # Sample Ready Tensor dataset\n│   ├── sample_publications/         # Input publication .txt files\n├── docs/\n│   ├── Untitled diagram _ Mermaid Chart-2025-07-09-115351.png\n│   ├── langgraph_flowchart.mmd\n│   ├── publication_flowchart.png\n├── examples_screens/                # Streamlit interface screenshots\n├── logs/                            # Runtime log output\n│   └── pipeline.log\n├── outputs/                         # Validated profiles and comparison results\n│   ├── profiles/\n│   └── comparisons/\n├── src/                             # Source code\n│   ├── app.py                       # Main Streamlit App\n│   ├── explorer.py                  # LLM-based publication comparison engine\n│   ├── generate_flowchart_graphviz.py\n│   ├── generate_flowchart_mermaid.py\n│   ├── loader.py                    # Converts JSON into individual .txt files\n│   ├── paths.py                     # Centralized path definitions\n│   ├── utils.py                     # Helper functions\n│   ├── logger.py                    # Centralized log configuration\n│   ├── docs/\n│   │   ├── langgraph_flowchart.mmd\n│   │   ├── publication_flowchart.png\n│   ├── rails/                       # Guardrails XML schemas\n│   │   ├── profile_extraction.rail\n├── tests/\n│   ├── conftest.py                # Reusable fixtures for explorer and sample publications\n│   ├── test_app.py                # Checks app entrypoint doesn’t break imports\n│   ├── test_explorer.py           # Validates the core pipeline\n│   ├── test_guardrails.py         # Ensures Guardrails schema works\n├── pytest_output.log",
    "title": "Agentic AI Developer Certification: Production LangGraph-Orchestrated Research Assistant",
    "publication_description": "Back to publications\nAug 10, 2025\n●\n44 reads\nAgentic AI Developer Certification: Production LangGraph-Orchestrated Research Assistant\nAAIDC2025\nAgentic AI\nAI Agents\nDeployment\nGuardrails\nLangChain\nLangGraph\nMulti-Agent Systems\nProduction Systems\nSecurity\nStreamlit\nTesting\nUI\nC\n@chibueze.k.muoneke\nMichela Agostini\nJ\n@joshua_abok\nLike\nBookmark\nShare\nTL;DR\n\nThis production-ready evolution of the LangGraph-Orchestrated Research Assistant builds on the initial Ready Tensor prototype with significant upgrades in safety, reliability, and usability. The system now incorporates strong guardrails, including input and output validation, prompt protection, and structured response handling. Observability has been improved through advanced logging, user feedback capture, and performance monitoring, ensuring that both errors and efficiency can be tracked effectively. On the deployment side, the assistant is delivered through a Streamlit application, supported by Docker for reproducible builds and cloud-hosting options that allow for scalable adoption. Finally, comprehensive documentation accompanies the release, offering detailed usage guides, step-by-step walkthroughs, and transparent notes on limitations and safety\n\nPurpose and Objectives\n\nThe primary purpose of this project is to evolve the initial Ready Tensor prototype into a production-ready LangGraph-Orchestrated Research Assistant. While the prototype demonstrated feasibility, this work focuses on building the robustness, safety, and usability required for real-world deployments.\n\nThe main objectives are:\n\nTo establish guardrails that ensure safe, validated, and reliable agent interactions.\nTo introduce observability through structured logging, monitoring, and feedback collection.\nTo provide comprehensive documentation for ease of adoption and transparency.\nTo provide a comprehensive testing suite to confirm correctness, resilience, and robustness of the system under both normal and adversarial conditions.\nSignificance of the Research\n\nThis work is significant because it moves beyond proof-of-concept implementations of LangGraph systems and demonstrates how to integrate production best practices such as error handling, monitoring, and deployment pipelines. By bridging the gap between prototype and production, the project contributes to the growing field of agentic AI development and provides a reference architecture for building safe, reliable, and maintainable AI assistants.\n\nTool Overview & Architecture\nSystem Architecture Goals\n\nThe upgraded system transforms the original LangGraph-Orchestrated Research Assistant prototype into a production-grade, reliable, and user-friendly multi-agent AI application. Built on LangChain and LangGraph, it combines structured agent orchestration, strong safety measures, and robust deployment capabilities to handle real-world usage at scale.\n\nThe architecture is driven by four primary production-focused goals:\n1. Safety & Reliability Guardrails\n\nThe system integrates multiple safeguards to ensure both safe and reliable operation. All inputs and outputs undergo validation to guarantee that only well-structured data is processed. In addition, prompt protection mechanisms are built in to shield the agents from adversarial inputs such as prompt injection attacks. Responses are also constrained to adhere to predefined structures—typically JSON or table-based formats—so that downstream components can consistently parse and use the generated outputs.\n\n2. Observability & Monitoring\n\nRobust observability features make the system easier to monitor, debug, and improve over time. Enhanced logging captures each agent’s decision-making process, along with request–response cycles and any error events that occur during execution. Users are also able to provide direct feedback, submitting corrections or suggestions for improvements. Furthermore, the system includes instrumentation to collect performance metrics, which can be analyzed to optimize both execution time and accuracy.\n\n3. Scalable Deployment & Accessibility\n\nThe project is designed for flexible and scalable deployment. A Streamlit web application provides an interactive interface that makes the tool accessible even to non-technical users. To ensure reproducibility, Docker support is included, enabling containerized builds that behave consistently across environments. For broader adoption, the system also supports deployment to multiple cloud platforms, allowing it to scale seamlessly as demand increases.\n\n4. Comprehensive Documentation & Onboarding\n\nTo reduce the learning curve, the project ships with comprehensive documentation. Usage guides walk new users through installation and day-to-day interaction with the system. Detailed, step-by-step walkthroughs explain how to perform key tasks and workflows. The documentation also openly addresses the system’s limitations and safety considerations, ensuring transparency around what the tool can and cannot currently do.\n\nWorkflow\n\nThe production-ready workflow mirrors the original LangGraph orchestration but with added guardrails and observability hooks at each stage:\n\nUser Input is validated and preprocessed.\nLangGraph State Machine routes requests to specialized agents (extraction, comparison, enrichment, fact-checking).\nGuardrails filter and constrain outputs before passing them downstream.\nStreamlit UI displays results alongside orchestration diagrams generated via Mermaid and Graphviz.\nLogs & Metrics are captured for monitoring and future tuning.\nProductionizing the Multi-Agent System\nGuardrails\n\nGuardrails-AI\n enforces structured, validated outputs from LLMs.\n\nComparison: Without vs. With Guardrails-AI\nFeature\tWithout Guardrails-AI ❌\tWith Guardrails-AI ✅\nOutput format\tFree-form, unstructured text\tSchema-validated JSON\nInput flexibility\tHigh\tModerate (requires schema)\nRetry on invalid\tNo\tYes\nProduction-safety\tRisk of LLM drift\tRobust\nDev speed\tFast\tRequires setup effort\nIntegration Steps\nInstall guardrails-ai.\nCreate a .rail schema for publication profiling.\nUpdate analysis functions (e.g., PublicationExplorer.analyze_pub1(), PublicationExplorer.analyze_pub2() in src/explorer.py).\nValidate outputs before updating state.\n\nValidated output includes:\n\ntools: Frameworks/libraries extracted from publication\nevaluation_methods: Metrics or strategies described\ndatasets: Benchmarks used\ntask_types: Types of tasks addressed\nresults: Key findings\nAll fields must be present for validation. Structured, machine-readable JSON enables downstream analytics and reproducibility.\nTesting\n\nThe Research Assistant includes a comprehensive test suite implemented with pytest. These tests confirm correctness, resilience, and robustness of the system under both normal and adversarial conditions.\n\nTest Organization\n\nUnit Tests\nIndividual components such as analyze_pub1, compare, and react_agent_tool are tested in isolation with mocked LLM responses, ensuring reproducibility.\n\nIntegration Tests\nThe Streamlit interface (app.py) is tested with AppTest to confirm the UI initializes correctly and responds without crashing.\n\nSchema Validation Tests\nGuardrails-AI .rail schemas are tested with both well-formed and malformed JSON to verify strict schema adherence.\n\nResilience Tests\nMissing or malformed fields are automatically repaired using fallback mechanisms, guaranteeing that all required fields (tools, evaluation_methods, datasets, task_types, results) are always present.\n\nExample Test Cases\nEnsure generated publication profiles always contain all required fields.\nValidate that malformed JSON (e.g., missing fields) is recovered into schema-compliant output.\nConfirm the Streamlit app launches and renders the expected title within 10 seconds.\nVerify comparison logic detects dataset usage across two publications.\nObservability\n\nObservability is essential for monitoring, tracing, and debugging during real-world deployment. Logging and monitoring use Python’s logging module and node tracing.\n\nKey Observability Features\nFeature\tTool/Technique\tPurpose\nStructured Logs\tPython’s logging + JSON formatting\tCentralized log collection & readability\nTracing\tuuid session ID + node info\tTrack LLM invocation paths & failures\nImplementation Steps\n\nThe implementation begins with the installation of the required logging tools, followed by the creation of a dedicated logger.py module. Within this setup, structured logging is introduced through the use of Loguru, which provides a cleaner and more flexible logging interface. To improve traceability, each session is assigned a unique UUID, allowing logs to be linked back to specific execution runs. Logging is configured to be centralized, capturing events both in the console for real-time feedback and in persistent log files for later analysis. Finally, logging is applied consistently across all major graph nodes, ensuring that system activity can be fully monitored and debugged when needed.\n\nDeployment\n\nThe application runs locally via \nStreamlit\n, or can be deployed to cloud platforms (Streamlit Cloud, \nDocker\n, \nHugging Face Spaces\n).\n\nImprovements Over the Prototype\nFeature\tBefore (Prototype)\tAfter (Production)\nGuardrails\t❌ None\t✅ .rail schema with fallback\nLogging\t⚠️ Basic\t✅ Structured logs with timestamps\nOutput Separation\t❌ Mixed outputs\t✅ Separate folders: outputs/profiles/, outputs/comparison/\nValidation\t❌ Ad-hoc\t✅ Schema-driven, strict validation\nResilience\t❌ Fragile\t✅ Robust with fallbacks and logs\nDeployment\t❌ Prototype\t✅ Docker & cloud ready\nInterface\t⚠️ Exposed API key\t✅ Clean sidebar info, no key exposure\nDocumentation\t⚠️ Incomplete\t✅ README, deployment.md, code docstrings\n\nNote: \"Before\" refers to the workflow in the original publication.\n\nRepository Structure\n\n/Agentic_AI_Developer_Certification_Project3-main\n├── LICENSE\n├── README.md # Project overview and instructions\n├── deployment.md # Deployment guide for various environments\n├── Dockerfile # Instructions for Docker image config\n├── requirements-test.txt # Development and test dependencies\n├── requirements.txt # Project dependencies\n├── .gitignore # Ignored files and folders\n├── .env.example # Example environment file for API keys\n├── data/\n│ ├── project_1_publications.json # Sample Ready Tensor dataset\n│ ├── sample_publications/ # Input publication .txt files\n├── docs/\n│ ├── Untitled diagram _ Mermaid Chart-2025-07-09-115351.png\n│ ├── langgraph_flowchart.mmd\n│ ├── publication_flowchart.png\n├── examples_screens/ # Streamlit interface screenshots\n├── logs/ # Runtime log output\n│ └── pipeline.log\n├── outputs/ # Validated profiles and comparison results\n│ ├── profiles/\n│ └── comparisons/\n├── src/ # Source code\n│ ├── app.py # Main Streamlit App\n│ ├── explorer.py # LLM-based publication comparison engine\n│ ├── generate_flowchart_graphviz.py\n│ ├── generate_flowchart_mermaid.py\n│ ├── loader.py # Converts JSON into individual .txt files\n│ ├── paths.py # Centralized path definitions\n│ ├── utils.py # Helper functions\n│ ├── logger.py # Centralized log configuration\n│ ├── docs/\n│ │ ├── langgraph_flowchart.mmd\n│ │ ├── publication_flowchart.png\n│ ├── rails/ # Guardrails XML schemas\n│ │ ├── profile_extraction.rail\n├── tests/\n│ ├── conftest.py # Reusable fixtures for explorer and sample publications\n│ ├── test_app.py # Checks app entrypoint doesn’t break imports\n│ ├── test_explorer.py # Validates the core pipeline\n│ ├── test_guardrails.py # Ensures Guardrails schema works\n├── pytest_output.log\n\nInstallation Instructions\nPrerequisites\nPython 3.10+\nOpenAI API key\nTavily API key\nSet as OPENAI_API_KEY and TAVILY_API_KEY environment variables\nInstallation\n\nClone the repository\n\ngit clone https://github.com/micag2025/Agentic_AI_Developer_Certification_Project3\ncd Agentic_AI_Developer_Certification_Project3\n\nInstall dependencies\n\npip install -r requirements.txt\n# For testing:\npip install -r requirements-test.txt\n\nNote: Test dependencies are separated from runtime dependencies.\n\nSet up environment variables\n\nCopy .env.example to .env and add your OpenAI and Tavily API keys.\n\n(Recommended) Use a virtual environment\n\npython3 -m venv .venv\nsource .venv/bin/activate   # Linux/macOS\n.\\.venv\\Scripts\\activate    # Windows\nRunning the Application\n\nEnsure project_1_publications.json is present in data/.\n\nThe sample dataset is available in the \"Datasets\" section of the related publication.\n\nLaunch the Streamlit app:\n\nstreamlit run src/app.py\n\nOpen your browser to the local Streamlit URL (usually \nhttp://localhost:8501\n).\n\nYou can now interact with the LangGraph-Orchestrated Research Assistant for Ready Tensor!\n\nOutput Locations\n\nValidated Profiles: outputs/profiles/*.json\nComparison Reports: outputs/comparisons/*.json and .html\nLog Files: logs/*.log\n\nDownload the latest validated profile and log file directly from the Streamlit interface.\n\nDebugging Guardrails Integration\n\nRun the app and monitor the terminal for raw vs. validated outputs. The pipeline will:\n\nTrigger the PublicationExplorer\nInvoke analyze_pub1 and analyze_pub2\nPrint both raw and validated outputs in the terminal\nUsage Examples\nExample: Validated Profile (Guardrails-AI)\n\nSample output in outputs/ when query_type=\"evaluation_method:\n\n{\n  \"tools\": [\"LangGraph\", \"Microsoft AutoGen\"],\n  \"evaluation_methods\": [],\n  \"datasets\": [],\n  \"task_types\": [\"AI Agent\", \"Autonomous Agents\", \"Multi-Agent System\"],\n  \"results\": []\n}\n\nIf evaluation_methods is empty, this means the publication lacks evaluation details or the model did not extract any. Guardrails validated the output using the .rail schema.\n\nExample: Observability (Logging, Tracing, Monitoring)\n\nLogs are stored in the /logs directory, and the system records events with the detail and rigor expected of a production-grade application. Each log entry captures the function, line, module, process, and thread involved, along with precise timestamps. Informative tags such as 📊, 📈, 📝, 🔍, and 🤖 make it easier to quickly distinguish between different types of events. Both raw model responses and validated outputs are logged, ensuring that every stage of the workflow can be audited and traced when necessary.\n\nExample: Deployment (Streamlit Cloud, Docker, CLI)\n\nBefore deployment, the system requires that the outputs/profiles/, outputs/comparisons/, and logs/ directories exist and are writable. The Streamlit interface provides direct access to validated profiles and logs, which can be downloaded from the user interface for further analysis. For containerized setups, Docker is fully supported, allowing local volumes to be mapped as needed to maintain persistent outputs and logs across environments.\n\nStreamlit App Example\nLaunch the UI:\nstreamlit run src/app.py\nNavigate to \nhttp://localhost:8501\n to access the Research Assistant.\n\nChoose two publications from the dropdown menus. Now, the two publications are selected from the Ready Tensor dataset and enter a Query\nE.g., “Evaluation Methods” or “Dataset comparisons”.\nAnd the publications are analyzed independently to extract relevant metadata.\n\nOverall final output\nFinally, we get a structured response ready for user presentation off of the workflow\n\nReferences\nLangGraph\nLangChain\nOpenai API\nStreamlit\nTavily\nPytest\nMermaid\nGuardrails\nReady Tensor Certifications\nTechnical Evaluation Rubric\nEngage and Inspire: Best Practices for Publishing on Ready Tensor\nContributing\n\nWe welcome enhancements and issue reports:\n\nFork the repo\nCreate a feature branch (git checkout -b feature-name)\nCommit & Push your changes\nSubmit a Pull Request\n\nPlease adhere to existing code style and update documentation as needed.\n\nFuture Implementations\n\nWe encourage and welcome community contributions to help make this assistant even more robust and versatile. Here are several potential areas for future improvement, along with example contributions for each:\n\nIntegrate Additional LLM Providers\n\nExpand the system to support multiple large language model backends (e.g., OpenAI, Anthropic Claude, Google Gemini), allowing users to select or compare across different models.\n\nExample: Add support for Anthropic Claude by creating a new API wrapper in src/utils.py and updating explorer.py to enable backend selection.\nExtend the UI for Richer User Interaction\n\nEnhance the user interface to provide more flexibility and usability.\n\nExample: Add a sidebar widget for filtering comparison results by publication topic, or implement functionality for uploading new publication files directly via the Streamlit interface.\nDesign New Guardrail Schemas for Various Research Domains\n\nAdapt and extend validation schemas to better handle domain-specific needs.\n\nExample: Create a new .rail schema for extracting information from specific types of publications, and update the validation logic in explorer.py to accommodate new domain-specific fields.\nIntegrate Fine-Tuned or Domain-Adapted LLMs\n\nImprove accuracy in extraction, summarization, and reasoning tasks by supporting fine-tuned or specialized LLMs.\n\nExample: Add configuration options for loading and selecting fine-tuned models, and develop benchmarking tools to evaluate their performance in the publication profiling pipeline.\nInvestigate and Address Missing Evaluation Methods\n\nEnhance reliability by investigating why the system sometimes fails to extract evaluation methods from publications.\n\nExample:\nLLM Extraction Limitations: The LLM may miss evaluation methods due to vague or implicit phrasing in the source text.\nPreprocessing (Truncation): If publication text is truncated (e.g., MAX_CHARS = 12000), key evaluation sections may be omitted from analysis. Improving data preprocessing or LLM prompts could mitigate this issue.\nLicense\n\nLicensed under the \nMIT License\n\nContact\n\nFor questions or feedback, please contact the authors:\n\nchibueze.k.muoneke@gmail.com\nmichelaagostini73@gmail.com\nnyajuaya.j.a@gmail.com\nAcknowledgments\n\nThis work is part of the Agentic AI Developer Certification program by\n\nReadyTensor\n Special thanks to the Ready Tensor developer community for guidance and feedback.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nTL;DR\n\nPurpose and Objectives\n\nSignificance of the Research\n\nTool Overview & Architecture\n\nSystem Architecture Goals\n\nSafety & Reliability Guardrails\nObservability & Monitoring\nScalable Deployment & Accessibility\nComprehensive Documentation & Onboarding\n\nWorkflow\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "agentic-ai-for-smart-agriculture-a-multimodal-rag-system-for-crop-disease-detection-diagnosis-mLRSML10lEYd",
    "username": "rRobert Yaw Agyekum Addo",
    "license": "MIT License",
    "title": "Agentic AI for Smart Agriculture: A Multimodal RAG System for Crop Disease Detection & Diagnosis",
    "publication_description": "Back to publications\nMar 17, 2025\n●\n102 reads\n●\nMIT License\nAgentic AI for Smart Agriculture: A Multimodal RAG System for Crop Disease Detection & Diagnosis\nComputer Vision\nLarge Language Models (LLMs)\nMultimodal RAG\nNatural Language Processing (NLP\nObject Detection\nOpenCV\nPyTorch\nQdrant\nRAGAS\nRetrieval Augmented Generation (\nVector Database\nVision Language Models (VLMs)\nYOLOv8\nR\nRobert Yaw Agyekum Addo\nLike\nBookmark\nShare\nIntroduction: Empowering Agriculture with Agentic AI\n\nAgriculture is the backbone of global food security, yet it faces an ever-growing challenge—crop diseases. Every year, plant infections and fungal outbreaks lead to devastating losses, threatening the livelihoods of millions of farmers and impacting food production. Traditional disease detection methods rely on human expertise, which can be costly, time-consuming, and often inaccessible in rural regions. With the rise of artificial intelligence, a new opportunity emerges: empowering farmers with AI-driven disease detection systems that provide instant, accurate, and actionable insights.\n\nIn this project, we introduce an Agentic AI-powered Crop Disease Detection and Diagnosis System, designed to revolutionize precision agriculture. By integrating advanced computer vision with Retrieval-Augmented Generation (RAG), the system can identify crop diseases from images, retrieve expert knowledge from a database, and generate human-like explanations and treatment recommendations. What sets this solution apart is its ability to adapt autonomously, refine responses based on user feedback, and operate in multiple languages, ensuring accessibility for farmers across diverse regions.\n\nThis innovation aligns perfectly with the goals of the Agentic AI Innovation Challenge 2025—leveraging AI’s agentic capabilities to solve real-world problems. By harnessing the power of YOLOv8 for image-based disease detection, Qdrant for vector search, and Groq-powered LLMs for contextual reasoning, our system goes beyond simple classification. It understands, explains, and suggests actionable solutions in a way that is both scalable and user-friendly.\n\nAs we explore the core components of this project, we will showcase how Agentic AI can bridge the gap between cutting-edge research and practical agricultural solutions, creating a future where farmers have instant, expert-level disease diagnostics at their fingertips.\n\nThe Innovation: AI-Powered Crop Disease Detection\n\nBridging the Gap Between AI and Agriculture\n\nModern agriculture is undergoing a technological transformation, and artificial intelligence is at the forefront of this revolution. The ability to detect crop diseases early and provide accurate, data-driven recommendations can mean the difference between a thriving harvest and a devastating loss. However, many existing solutions lack adaptability, multilingual support, and explainability—three critical factors for real-world deployment, especially in underserved agricultural regions.\n\nOur Agentic AI-powered Crop Disease Detection and Diagnosis System bridges this gap by combining state-of-the-art computer vision with Retrieval-Augmented Generation (RAG), offering an interactive, autonomous, and knowledge-driven approach to plant disease management. This innovation allows farmers and agricultural experts to capture an image of an infected crop, receive real-time disease predictions, and obtain scientifically validated treatment suggestions—all in a single workflow.\n\nHow This System Works: A Multi-Stage AI Pipeline\n\nAt the heart of this innovation lies a multi-stage AI pipeline, designed to process images, retrieve relevant agronomic knowledge, and generate human-like responses tailored to the user's query. The system operates in three key steps:\n\n1. Image-Based Disease Detection with YOLOv8\nFarmers upload an image of their affected crops.\nA YOLOv8 object detection model, trained on a diverse dataset of crop diseases, identifies infected areas and classifies diseases with high accuracy.\nConfidence scores and bounding boxes highlight the affected areas, allowing users to visualize the disease’s severity.\n2. Knowledge Retrieval via Retrieval-Augmented Generation (RAG)\nInstead of relying solely on the LLM’s pretrained knowledge, we retrieve relevant disease-specific information from a structured database.\nA vector search engine (Qdrant) powered by SentenceTransformers ensures that the system fetches the most relevant disease descriptions, symptoms, and treatment guidelines.\nThis retrieval-based approach improves accuracy, making the system more reliable than purely generative AI models that might hallucinate incorrect facts.\n3. Generating Contextual and Actionable Insights\nThe retrieved knowledge is then processed by a Groq-powered large language model (LLM) to generate a detailed yet easy-to-understand explanation.\nThe system tailors responses based on user queries, allowing for dynamic conversations about disease symptoms, environmental conditions, and treatment recommendations.\nText-to-Speech (TTS) integration converts the response into audio, ensuring accessibility for farmers with low literacy levels.\nMultilingual Support using Google Translate ensures that farmers can receive disease analysis in their preferred local languages, increasing adoption and usability.\nA Truly Agentic AI System\n\nUnlike traditional AI tools, this system is agentic—meaning it actively retrieves, adapts, and refines its responses based on user interactions and real-world conditions. By continuously learning from feedback, improving knowledge retrieval, and offering dynamic disease diagnostics, this project represents a step forward in intelligent agricultural assistants.\n\nBy leveraging agentic AI principles, this solution goes beyond static predictions, offering a self-improving, farmer-friendly, and scalable AI-powered crop disease detection system that can transform the future of agriculture.\n\nHow the System Works\nA Seamless AI Workflow for Farmers\n\nOur Agentic AI-powered Crop Disease Detection and Diagnosis System follows a three-stage process—image-based disease detection, intelligent knowledge retrieval, and dynamic response generation. This end-to-end pipeline ensures accurate, explainable, and actionable insights for farmers, researchers, and agricultural extension officers.\n\nStep 1: Image-Based Disease Detection with YOLOv8\n\nThe process begins with the farmer or user uploading an image of a diseased plant. The system leverages YOLOv8, a state-of-the-art object detection model, to:\n✅ Identify affected areas within the image.\n✅ Classify the detected disease(s) with high confidence.\n✅ Display bounding boxes around infected areas for clear visualization.\n\nThis real-time analysis allows farmers to quickly assess the severity of plant infections without needing expert intervention. The use of pretrained deep learning models ensures high accuracy, making the system a reliable early-warning tool for crop health monitoring.\n\nStep 2: Intelligent Knowledge Retrieval with RAG\n\nOnce the disease is detected, the system takes a groundbreaking approach to knowledge retrieval using Retrieval-Augmented Generation (RAG). Instead of generating responses solely from an LLM’s pretrained knowledge, this system:\n✅ Retrieves domain-specific insights from a structured agricultural knowledge base.\n✅ Uses SentenceTransformer embeddings to fetch the most relevant documents about the detected disease.\n✅ Employs Qdrant, a high-speed vector database, to efficiently match queries with disease-specific information.\n\nThis ensures scientific accuracy and prevents hallucinations, making the model more trustworthy than standard generative AI solutions. Farmers receive not just generic answers, but precise, expert-backed treatment strategies for their detected crop diseases.\n\nStep 3: Generating Actionable Insights with LLMs\n\nWith retrieved knowledge in place, the system employs Groq-powered large language models (LLMs) to:\n✅ Generate an in-depth explanation of the disease, including its causes, symptoms, and environmental triggers.\n✅ Provide recommended treatments and preventive measures to help mitigate future outbreaks.\n✅ Allow users to ask follow-up questions, making the interaction more dynamic and personalized.\n\nStep 4: Multilingual Support and Accessibility Features\n\nUnderstanding that many farmers may not speak English fluently, the system integrates Google Translate API to:\n✅ Translate disease information into local languages like Swahili, Twi, Hausa, or French.\n✅ Support voice output using Text-to-Speech (TTS), making it accessible to users with low literacy levels.\n✅ Ensure cultural and linguistic adaptability, promoting widespread adoption.\n\nA Fully Automated and Agentic AI System\n\nThis pipeline doesn’t just detect diseases—it retrieves, explains, and evolves based on user interactions. The agentic nature of the system ensures that:\n✅ Responses improve over time with user feedback loops.\n✅ Disease knowledge is updated dynamically for continuous learning.\n✅ The model adapts to real-world agricultural challenges without human intervention.\n\nBy combining computer vision, intelligent retrieval, generative AI, and multilingual accessibility, this project redefines precision agriculture, equipping farmers with a powerful, autonomous AI assistant for crop health management.\n\nKey Features & Technical Advancements\nHarnessing Advanced AI Models for Precision Agriculture\n\nOur Agentic AI-powered Crop Disease Detection and Diagnosis System integrates cutting-edge models to deliver accurate, efficient, and accessible solutions for farmers. This section delves into the technical components that make this system innovative and effective.\n\n1. YOLOv8: Real-Time Image-Based Disease Detection\n\nAt the forefront of our system is YOLOv8 (You Only Look Once, Version 8), a state-of-the-art object detection model renowned for its speed and accuracy. Trained on extensive datasets of crop images, YOLOv8 enables the system to:\n\nReal-Time Detection: Process images swiftly, allowing for immediate identification of diseases.\n\nHigh Precision: Accurately detect and localize multiple diseases within a single image, even in complex backgrounds.\n\nScalability: Adapt to various crops and disease types by retraining on new datasets, ensuring broad applicability.\n\nThe integration of YOLOv8 ensures that farmers receive prompt and precise diagnostics, facilitating timely interventions.\n\n2. Retrieval-Augmented Generation (RAG): Intelligent Knowledge Retrieval\n\nTo provide comprehensive and accurate information, our system employs Retrieval-Augmented Generation (RAG), a technique that combines retrieval systems with generative models. This approach allows the system to:\n\nAccess Up-to-Date Information: Retrieve the latest research and treatment protocols from agricultural databases.\n\nEnhance Response Accuracy: Ground AI-generated responses in verified data, reducing the risk of misinformation.\n\nPersonalize Interactions: Tailor responses based on specific user queries and contextual factors, such as local climate conditions.\n\nBy leveraging RAG, the system ensures that users receive detailed and relevant information, enhancing decision-making in crop management.\n\n3. Kokoro-82M: Efficient Text-to-Speech (TTS) Integration\n\nAccessibility is a cornerstone of our system, achieved through the integration of Kokoro-82M, an open-weight Text-to-Speech (TTS) model with 82 million parameters. Despite its compact size, Kokoro-82M delivers high-quality speech synthesis, making it ideal for:\n\nMultilingual Support: Capable of synthesizing speech in multiple languages and voices, enhancing usability across diverse linguistic groups.\n\nResource Efficiency: Operates efficiently on devices with limited computational power, ensuring accessibility in resource-constrained environments.\n\nOpen-Source Flexibility: Licensed under Apache 2.0, allowing for broad deployment and customization to meet specific user needs.\n\nThe inclusion of Kokoro-82M ensures that information is not only accessible but also delivered in a user-friendly auditory format, catering to users with varying literacy levels.\n\n4. Qdrant: High-Performance Vector Search\n\nEfficient and accurate information retrieval is facilitated by Qdrant, a high-performance vector search engine. In our system, Qdrant is utilized to:\n\nPerform Semantic Searches: Quickly find relevant information based on the semantic content of user queries.\n\nScale Seamlessly: Handle large volumes of data, ensuring that the system remains responsive as the knowledge base expands.\n\nEnhance Retrieval Accuracy: Utilize advanced indexing and search algorithms to deliver precise results, improving the quality of information provided.\n\nBy integrating Qdrant, the system maintains high-speed and accurate knowledge retrieval, essential for timely decision-making in agriculture.\n\n5. SentenceTransformers: Contextual Embedding for Enhanced Understanding\n\nTo comprehend and process user queries effectively, the system employs SentenceTransformers, models that generate contextual embeddings of text. This capability allows the system to:\n\nUnderstand Nuanced Queries: Capture the context and intent behind user inputs, leading to more accurate responses.\n\nImprove Information Retrieval: Enhance the matching of queries to relevant documents, ensuring that users receive pertinent information.\n\nSupport Multilingual Processing: Handle queries in various languages, aligning with the system's goal of broad accessibility.\n\nThe use of SentenceTransformers ensures that the system interprets and responds to user queries with a high degree of understanding and relevance.\n\n6. RAGAS: Ensuring Response Faithfulness\n\nMaintaining the accuracy and reliability of AI-generated responses is critical. Our system incorporates RAGAS, a framework for evaluating the faithfulness of Retrieval-Augmented Generation systems. Through RAGAS, we:\n\nAssess Response Accuracy: Regularly evaluate the correctness of the information provided to users.\n\nIdentify Improvement Areas: Detect and address discrepancies or inaccuracies in the system's outputs.\n\nMaintain User Trust: Ensure that users can rely on the information provided, fostering confidence in the system.\n\nBy implementing RAGAS, we commit to delivering trustworthy and precise information, essential for effective crop disease management.\n\n7. User Feedback Loop: Continuous Learning and Adaptation\n\nAn essential feature of our system is its ability to learn and adapt over time. We have established a user feedback loop that enables the system to:\n\nIncorporate User Insights: Learn from user interactions to refine and improve future responses.\n\nUpdate Knowledge Bases: Stay current with emerging crop diseases and treatment methods by integrating new information.\n\nEnhance User Experience: Adapt to user preferences and feedback, ensuring that the system remains relevant and user-friendly.\n\nThis continuous learning approach ensures that the system evolves alongside agricultural advancements and user needs.\n\nReal-World Impact & Use Cases\nBringing AI-Powered Disease Diagnosis to the Farming Community\n\nCrop diseases continue to pose a significant challenge to global agriculture, leading to reduced yields, economic losses, and food insecurity. Traditional diagnosis methods often require expert knowledge and laboratory testing, making them inaccessible and costly for many farmers.\n\nOur Agentic AI-powered Crop Disease Detection and Diagnosis System directly addresses these issues by providing an affordable, real-time, and multilingual solution that farmers, agribusinesses, and agricultural extension officers can use with just a smartphone or computer. Below are key real-world applications of this technology.\n\n🌱 1. Smallholder Farmers: Instant Disease Detection & Treatment Advice\nChallenge:\nMany farmers struggle to identify plant diseases due to a lack of expert guidance.\nMisdiagnosis leads to incorrect treatments, worsening crop conditions.\nLanguage barriers and literacy levels make it difficult for some farmers to access expert knowledge.\nHow the AI System Helps:\n\n✅ Farmers can upload an image of a diseased plant and receive an immediate diagnosis.\n✅ The system retrieves scientifically verified disease descriptions and treatment options.\n✅ Text-to-Speech (TTS) and multilingual support enable farmers to receive guidance in their preferred language.\n\nImpact:\n\n🌾 Faster, cost-effective disease diagnosis, reducing crop losses and improving productivity.\n\n🏭 2. Agribusiness & Commercial Farmers: Scalable Crop Health Monitoring\nChallenge:\nLarge-scale farms need efficient ways to diagnose diseases affecting different crops.\nRelying on human inspections across vast fields is time-consuming and prone to errors.\nQuick access to reliable treatment recommendations is essential to prevent disease spread.\nHow the AI System Helps:\n\n✅ Batch Image Processing: Farmers can upload multiple images for simultaneous analysis.\n✅ The AI model detects multiple diseases in a single image, streamlining the diagnosis process.\n✅ Retrieval-Augmented Generation (RAG) technology ensures that treatment recommendations are backed by verified agricultural knowledge.\n\nImpact:\n\n🌾 Time-efficient, AI-powered disease monitoring, helping commercial farmers take action before major crop losses occur.\n\n📚 3. Agricultural Extension Officers: Improving Farmer Assistance\nChallenge:\nExtension officers often serve thousands of farmers, making personalized disease diagnosis difficult.\nMany farmers lack internet access, making AI-based solutions harder to deploy in rural areas.\nHow the AI System Helps:\n\n✅ Offline Access: Officers can use pre-downloaded disease knowledge to help farmers in remote areas.\n✅ Farmers can upload images to a centralized system, allowing extension workers to review and provide expert recommendations.\n✅ The system generates easy-to-understand explanations, reducing the need for highly technical expertise.\n\nImpact:\n\n🌾 Extension officers can assist more farmers efficiently, leading to widespread disease control and improved farming practices.\n\n🔬 4. Agricultural Researchers & Institutions: Enhancing Disease Data Collection\nChallenge:\nAgricultural scientists require large datasets of crop diseases for research.\nExisting disease datasets lack real-time updates and localized case studies.\nHow the AI System Helps:\n\n✅ The system logs and categorizes detected diseases, creating a growing database of plant infections.\n✅ Researchers can use the retrieved knowledge base to compare new disease patterns with historical cases.\n✅ The AI can evaluate the faithfulness and relevancy of generated responses using RAGAS metrics.\n\nImpact:\n\n🌾 Accelerated agricultural research, leading to improved disease-resistant crops and treatment methodologies.\n\n📲 5. AI-Powered Agricultural Apps & Digital Farming Services\nChallenge:\nExisting farming advisory services lack AI-driven disease diagnosis capabilities.\nMany farmers use basic mobile apps, which often provide static disease information rather than interactive responses.\nHow the AI System Helps:\n\n✅ The system can be integrated into existing agricultural platforms as an AI-powered assistant.\n✅ Conversational AI allows for interactive Q&A sessions about crop diseases and treatments.\n✅ Multilingual NLP and Text-to-Speech (TTS) support make disease diagnosis more accessible.\n\nImpact:\n\n🌾 Farmers gain direct access to AI-powered agricultural insights, reducing reliance on manual research.\n\n🌍 A Step Towards Smarter, AI-Driven Agriculture\n\nBy integrating YOLOv8, Retrieval-Augmented Generation (RAG), Qdrant, and multimodal AI, this system provides a scalable, real-time, and knowledge-driven solution for crop disease management. Its applications directly benefit farmers, agribusinesses, extension officers, and researchers, ensuring data-driven decision-making in agriculture.\n\nWith agentic AI capabilities, the system continuously improves based on user interactions, making it a self-evolving assistant for the future of precision agriculture.\n\nKey Takeaways from the Real-World Applications:\n\n✔ Smallholder farmers get instant disease diagnosis and treatment suggestions.\n✔ Commercial farmers benefit from scalable crop health monitoring.\n✔ Agricultural extension officers can support more farmers efficiently.\n✔ Researchers and institutions gain access to real-time disease data.\n✔ AI-powered farming apps can integrate the system for wider accessibility.\n\nHere’s Section 6, focusing on the challenges faced and possible future enhancements for your AI-powered crop disease detection system.\n\n** Challenges & Future Enhancements**\nAddressing Current Limitations and Expanding Capabilities\n\nWhile the Agentic AI-powered Crop Disease Detection and Diagnosis System brings a transformative approach to agricultural disease management, it still faces technical, data-related, and deployment challenges. This section highlights key limitations and outlines future enhancements to improve accuracy, scalability, and accessibility.\n\n🔴 Current Challenges\n1. Limited Disease Coverage\n\n✅ Challenge: The model currently detects only the diseases it has been trained on. Any new or rare plant diseases outside the training dataset go undetected.\n✅ Impact: This reduces the system’s generalizability to all farming conditions.\n✅ Proposed Solution:\n\nExpand the training dataset by incorporating more crop images and diverse plant diseases.\nUse active learning to fine-tune the model continuously based on new farmer-submitted images.\n2. Potential for Incorrect Predictions\n\n✅ Challenge: Despite high accuracy, AI models are not infallible—there's still a chance of false positives or misclassifications.\n✅ Impact: Incorrect predictions can mislead farmers into applying unnecessary or incorrect treatments.\n✅ Proposed Solution:\n\nIntroduce confidence-based recommendations—if the model is uncertain, it can suggest seeking human expert validation.\nImprove RAGAS evaluation metrics to ensure that retrieved knowledge aligns with the detected disease.\n3. Computational Resource Requirements\n\n✅ Challenge: AI models, especially YOLOv8 and LLM-based retrieval systems, require high computational power for inference.\n✅ Impact: Farmers in low-resource settings may struggle to access the system due to hardware or internet limitations.\n✅ Proposed Solution:\n\nDevelop lightweight, mobile-friendly models optimized for on-device processing (e.g., TensorFlow Lite).\nEnable edge AI processing, reducing dependency on cloud-based inference.\n4. Multilingual Support Limitations\n\n✅ Challenge: The system relies on translation models for multilingual support, which may not always produce perfect translations for agricultural terms.\n✅ Impact: Certain technical agricultural concepts might get lost in translation, reducing the clarity of disease explanations.\n✅ Proposed Solution:\n\nWork with local agricultural experts to fine-tune translations for key farming terms.\nIntegrate custom-trained NLP models for more context-aware translations.\n5. Lack of Real-Time Feedback from Farmers\n\n✅ Challenge: While the system generates insights, it does not currently incorporate direct farmer feedback to refine future responses.\n✅ Impact: The AI model does not learn from misclassifications or improve based on user experiences.\n✅ Proposed Solution:\n\nImplement a user feedback loop where farmers confirm or correct AI predictions.\nUse this data for continuous retraining, improving detection accuracy over time.\n🚀 Future Enhancements & Expansion Plans\n1. Continuous Learning with Human-in-the-Loop AI\n\n✅ Farmers and agricultural experts will be able to validate AI-generated results, helping the model refine its predictions.\n✅ Adaptive AI updates will ensure that the system improves over time with real-world usage data.\n\n2. Expansion of the Disease Knowledge Base\n\n✅ Currently, the system relies on a predefined database of crop diseases.\n✅ Future versions will incorporate live updates from agricultural research institutions, keeping treatment recommendations current and scientifically validated.\n\n3. Integration with Agricultural Extension Services\n\n✅ Work with agricultural extension officers to integrate the AI assistant into national farming advisory programs.\n✅ Ensure that AI-generated insights align with best farming practices in specific regions.\n\n4. Optimizing for Low-Resource Devices\n\n✅ Deploy lightweight AI models optimized for offline use.\n✅ Allow farmers to store and access disease detection results without an internet connection.\n\n🌍 Towards the Future of Smart Agriculture\n\nThe Agentic AI-powered Crop Disease Detection and Diagnosis System represents a pioneering step toward AI-driven agriculture, but continuous improvements are necessary to maximize its impact. By enhancing model accuracy, expanding disease coverage, refining multilingual capabilities, and enabling real-time feedback, this system can evolve into an even more robust, farmer-friendly AI assistant.\n\nThrough strategic research collaborations, better user adaptation, and scalable AI integration, the future of intelligent crop disease management is within reach.\n\nConclusion: AI-Powered Agriculture for a Sustainable Future\n\nAgriculture stands at the crossroads of innovation and necessity. The threat of crop diseases, compounded by climate change, lack of early detection, and limited access to expert knowledge, continues to challenge food production worldwide. However, with advancements in Agentic AI, computer vision, and Retrieval-Augmented Generation (RAG), we now have the ability to empower farmers, researchers, and agricultural institutions with real-time, data-driven, and explainable disease diagnosis.\n\nThe Agentic AI-powered Crop Disease Detection and Diagnosis System presents a scalable, multilingual, and user-friendly solution that combines:\n✅ YOLOv8 for real-time crop disease detection\n✅ Retrieval-Augmented Generation (RAG) for precise, knowledge-backed responses\n✅ Multimodal AI with Text-to-Speech (TTS) for accessibility\n✅ RAGAS evaluation metrics for improving response quality\n✅ A feedback loop for continuous learning and adaptation\n\n🌍 A Future Where Every Farmer Has an AI Assistant\n\nThis system is more than an AI tool—it is a farmer’s intelligent companion in the fight against crop diseases. By providing instant, expert-level insights, it eliminates barriers to agricultural expertise, allowing smallholder farmers, commercial agribusinesses, and extension officers to make informed, science-driven decisions.\n\nWith further enhancements in disease coverage, model optimization for low-resource environments, and integration into agricultural advisory services, this innovation has the potential to transform global farming.\n\n🏆 Why This Innovation Aligns with the Agentic AI Innovation Challenge 2025\n\nThe Agentic AI Innovation Challenge 2025 seeks AI solutions that go beyond automation, actively engaging with users and adapting to real-world conditions. This project aligns perfectly with that vision because it:\n✔ Actively retrieves, generates, and refines information, rather than relying on static AI predictions.\n✔ Learns from user interactions, improving over time through real-world feedback.\n✔ Bridges the gap between AI research and real-world agricultural challenges, ensuring accessibility for all farmers.\n✔ Embodies Agentic AI principles, where the system acts as an autonomous assistant, making intelligent, informed decisions.\n\n🚀 The Path Ahead: Scaling AI-Driven Crop Disease Management\n\nBy integrating Agentic AI principles into agriculture, this system lays the foundation for a future where:\n🔹 Farmers no longer struggle with unknown crop diseases.\n🔹 AI-driven diagnosis prevents crop losses before they happen.\n🔹 AI-powered solutions bridge the gap between knowledge and accessibility.\n\nThis is just the beginning. With further development, collaboration, and scaling, AI will redefine how we protect global food security.\n\nYOLOv8: State-of-the-Art Computer Vision Model\nAvailable: \nhttps://yolov8.com/\n\nWhat is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector\nAvailable: \nhttps://arxiv.org/abs/2408.15857\n\nA Review on YOLOv8 and Its Advancements\nAvailable: \nhttps://www.researchgate.net/publication/377216968_A_Review_on_YOLOv8_and_Its_Advancements\n\nRAGAS: Automated Evaluation of Retrieval Augmented Generation\nAvailable: \nhttps://arxiv.org/abs/2309.15217\n\nOverview of Metrics - Ragas\nAvailable: \nhttps://docs.ragas.io/en/stable/concepts/metrics/overview/\n\nList of Available Metrics - Ragas\nAvailable: \nhttps://docs.ragas.io/en/stable/concepts/metrics/available_metrics/\n\nAI Agent Evaluation with RAGAS - YouTube\nAvailable: \nhttps://www.youtube.com/watch?v=-_52DIIOsCE\n\nRAGAS for RAG in LLMs: A Comprehensive Guide to Evaluation Metrics\nAvailable: \nhttps://dkaarthick.medium.com/ragas-for-rag-in-llms-a-comprehensive-guide-to-evaluation-metrics-3aca142d6e38\n\nEvaluation of RAG Metrics using RAGA\nAvailable: \nhttps://medium.com/walmartglobaltech/evaluation-of-rag-metrics-using-raga-0cd9bf001a76\n\nMetrics-Driven Agent Development | Pinecone\nAvailable: \nhttps://www.pinecone.io/learn/series/rag/ragas/\n\nYOLOv8-Based Visual Detection of Road Hazards: Potholes, Sewer Covers, and Manholes\nAvailable: \nhttps://arxiv.org/abs/2311.00073\n\nImproved YOLOv8 Algorithms for Small Object Detection in Aerial Imagery\nAvailable: \nhttps://www.sciencedirect.com/science/article/pii/S1319157824002027\n\nVERA: Validation and Evaluation of Retrieval-Augmented Systems\nAvailable: \nhttps://arxiv.org/abs/2409.03759\n\nPaper of YOLOv8 · Issue #2572 - GitHub\nAvailable: \nhttps://github.com/ultralytics/ultralytics/issues/2572\n\nWhy can't I find the YOLOv8 white paper? : r/computervision - Reddit\nAvailable: \nhttps://www.reddit.com/r/computervision/comments/1765dcc/why_cant_i_find_the_yolov8_white_paper/\n\nReal-Time Flying Object Detection with YOLOv8 - arXiv\nAvailable: \nhttps://arxiv.org/abs/2305.09972\n\nEnhancing YOLOv8 for Small Object Detection in Traffic Scenes\nAvailable: \nhttps://paperswithcode.com/paper/sod-yolov8-enhancing-yolov8-for-small-object\n\nReview: YOLOv8 (Object Detection) | by Sik-Ho Tsang - Medium\nAvailable: \nhttps://sh-tsang.medium.com/review-yolov8-object-detection-5214fa105731\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nIntroduction: Empowering Agriculture with Agentic AI\n\nThe Innovation: AI-Powered Crop Disease Detection\n\nBridging the Gap Between AI and Agriculture\n\nHow This System Works: A Multi-Stage AI Pipeline\n\n1. Image-Based Disease Detection with YOLOv8\n\n2. Knowledge Retrieval via Retrieval-Augmented Generation (RAG)\n\n3. Generating Contextual and Actionable Insights\n\nA Truly Agentic AI System\n\nHow the System Works\n\nA Seamless AI Workflow for Farmers\n\nView all\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "agentic-rag-dynamic-workflow-orchestration-for-hallucination-reduced-knowledge-systems-VMbZXRt4V7Ji",
    "username": "aAbdullah Ansari",
    "license": "MIT License",
    "title": "Agentic RAG: Dynamic Workflow Orchestration for Hallucination-Reduced Knowledge Systems",
    "publication_description": "Back to publications\nAug 18, 2025\n●\n122 reads\n●\nMIT License\nAgentic RAG: Dynamic Workflow Orchestration for Hallucination-Reduced Knowledge Systems\nagentic-ai\nai-safety\nknowledge-graphs\nlanggraph\nllm-orchestration\nmultimodal-rag\nselfcorrecting-systems\nA\nAbdullah Ansari\nLike\nBookmark\nShare\nIntroduction\n\nIn the rapidly evolving landscape of artificial intelligence, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for combining large language models with external knowledge sources. This publication presents an Advanced Agentic RAG System that leverages multiple specialized agents working in concert to provide accurate, fact-checked, and safe responses to user queries.\n\nFigure 1: Main interface of the Advanced Agentic RAG System showing configuration options, knowledge sources, and chat interface\n\nThe system represents a significant advancement over traditional RAG implementations by incorporating self-correction mechanisms, fact-checking capabilities, and safety protocols through a sophisticated multi-agent architecture built on LangGraph.\n\nSystem Architecture\nCore Components\n\nThe Advanced Agentic RAG System is built upon a robust technology stack:\n\nStreamlit: Provides an intuitive web interface for user interaction\n\nLangChain: Offers comprehensive RAG capabilities and tool integration\n\nLangGraph: Enables sophisticated multi-agent workflow orchestration\n\nChroma: Serves as the vector store for efficient document retrieval\n\nGoogle Generative AI Embeddings: Powers semantic search capabilities\n\nGroq: Delivers high-performance LLM inference with Llama 3\n\nTavily Search: Provides real-time web search capabilities\n\nMulti-Agent Design\n\nThe system employs nine specialized agents, each with distinct responsibilities:\n\n1. Router Agent: Determines the optimal workflow path based on query analysis\n\n2. Retrieval Agent: Fetches relevant documents from the knowledge base\n\n3. Reformulation Agent: Improves query formulation for better retrieval\n\n4. Web Search Agent: Accesses real-time information when needed\n\n5. Synthesis Agent: Combines information from multiple sources\n\n6. Generation Agent: Creates coherent final responses\n\n7. Fact Check Agent: Verifies factual claims in generated content\n\n8. Safety Agent: Ensures content appropriateness and harm prevention\n\n9. Clarification Agent: Requests additional information when needed\n\nFigure 2: System workflow showing the multi-agent collaboration and decision flow\n\nKey Features and Capabilities\n1. Intelligent Query Routing\n\nThe system analyzes each query to determine the most appropriate processing path:\n\n\ndef router_agent(state: AgentState) -> dict:\n\n    \"\"\"Decide initial workflow path based on query and history\"\"\"\n\n    model = ChatGroq(\n\n        temperature=st.session_state.temperature, \n\n        model_name=\"llama3-70b-8192\",\n\n        groq_api_key=GROQ_API_KEY\n\n    )\n\n    prompt = PromptTemplate(\n\n        template=\"\"\"As the Router Agent, analyze the user's question and conversation history to determine the best next step.\n\n        Conversation History:\n\n        {history}\n\n        Current Question: {question}\n\n        Choose one of these actions:\n\n        - \"retrieve\": If question can be answered with known documents\n\n        - \"reformulate\": If query needs refinement for better retrieval\n\n        - \"web_search\": If question requires current/realtime information\n\n        - \"clarify\": If question is ambiguous or needs more details\n\n        - \"generate\": If question is simple or conversational\n\n        Only respond with the action word.\"\"\",\n\n        input_variables=[\"question\", \"history\"]\n\n    )\n\n2. Self-Correction Mechanisms\n\nThe system implements multiple layers of self-correction:\n\nQuery Reformulation: Automatically improves poorly formulated queries\n\nDocument Grading: Evaluates retrieved document relevance\n\nFact-Checking: Verifies claims against external sources\n\nSafety Filtering: Ensures content appropriateness\n\n3. Flexible Knowledge Sources\n\nUsers can integrate multiple knowledge sources:\n\n\n# Load from URLs\n\nfor url in urls:\n\n    try:\n\n        docs.extend(WebBaseLoader(url).load())\n\n    except Exception as e:\n\n        st.error(f\"Failed to load {url}: {str(e)}\")\n\n# Load from uploaded files\n\nfor uploaded_file in uploaded_files:\n\n    try:\n\n        ext = os.path.splitext(uploaded_file.name)[1].lower()\n\n        if ext == \".txt\":\n\n            loader = TextLoader(temp_file_path, encoding=\"utf-8\")\n\n        elif ext == \".pdf\":\n\n            loader = PyPDFLoader(temp_file_path)\n\n        elif ext == \".docx\":\n\n            loader = Docx2txtLoader(temp_file_path)\n\n        # ... process documents\n\n4. Advanced Fact-Checking\n\nThe system includes a sophisticated fact-checking agent:\n\n\ndef fact_check_agent(state: AgentState) -> dict:\n\n    \"\"\"Fact-check generated answer using web search\"\"\"\n\n    # Identify factual claims\n\n    claims_prompt = \"\"\"Identify factual claims in this text. List them as bullet points:\n\n    {text}\n\n    Respond ONLY with the bullet points of claims.\"\"\"\n\n    # Verify each claim with web search\n\n    for claim in claims_list[:3]:\n\n        search_query = claim[2:] if claim.startswith('- ') else claim\n\n        results = st.session_state.web_search_tool.invoke({\"query\": f\"Verify: {search_query}\"})\n\n        verification_prompt = f\"\"\"Based on these sources, is this claim true?\n\n        Claim: {claim}\n\n        Sources: {sources}\n\n        Respond with \"TRUE\" or \"FALSE\" and a brief explanation.\"\"\"\n\n        verdict = model.invoke(verification_prompt).content\n\n        verified_claims.append(f\"{claim} → {verdict}\")\n\nTechnical Implementation\nState Management\n\nThe system uses a comprehensive state management approach:\n\n\nclass AgentState(BaseModel):\n\n    messages: List[BaseMessage] = Field(default_factory=list)\n\n    chat_history: List[BaseMessage] = Field(default_factory=list)\n\n    reformulation_count: int = 0\n\n    current_query: Optional[str] = None\n\n    retrieved_docs: List[Dict[str, Any]] = Field(default_factory=list) \n\n    generated_answer: Optional[str] = None\n\n    next_step: Optional[str] = None\n\nWorkflow Orchestration\n\nThe LangGraph workflow coordinates agent interactions:\n\n\nworkflow = StateGraph(AgentState)\n\n# Add specialized agent nodes\n\nworkflow.add_node(\"router\", router_agent)\n\nworkflow.add_node(\"retrieve\", retrieve_agent)\n\nworkflow.add_node(\"reformulate_query\", reformulate_agent)\n\nworkflow.add_node(\"web_search\", web_search_agent)\n\nworkflow.add_node(\"synthesize\", synthesize_agent)\n\nworkflow.add_node(\"generate\", generate_agent)\n\nworkflow.add_node(\"fact_check\", fact_check_agent)\n\nworkflow.add_node(\"safety_check\", safety_agent)\n\nworkflow.add_node(\"ask_clarification\", ask_clarification)\n\n# Add edges and conditional routing\n\nworkflow.add_edge(START, \"router\")\n\nworkflow.add_conditional_edges(\n\n    \"router\",\n\n    route_decision,\n\n    {\n\n        \"retrieve\": \"retrieve\",\n\n        \"reformulate\": \"reformulate_query\",\n\n        \"web_search\": \"web_search\",\n\n        \"clarify\": \"ask_clarification\",\n\n        \"generate\": \"generate\"\n\n    }\n\n)\n\nVector Store Integration\n\nThe system uses Chroma for efficient document retrieval:\n\n\n# Create vector store\n\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", \n\n                                         google_api_key=GOOGLE_API_KEY)\n\nvectorstore = Chroma.from_documents(\n\n    documents=doc_splits,\n\n    collection_name=\"rag-chroma\",\n\n    embedding=embeddings,\n\n)\n\n# Create retriever tool\n\nretriever_tool = create_retriever_tool(\n\n    retriever_instance,\n\n    \"retrieve_knowledge\",\n\n    \"Search and return information from the provided knowledge sources.\",\n\n)\n\nPerformance and Benefits\nHigh-Speed Inference\n\nLeveraging Groq's LPU inference engine, the system achieves exceptional performance:\n\nToken Processing: Up to 241 tokens per second for Llama 3 70B\n\nLow Latency: Sub-second response times for most queries\n\nConsistent Quality: No performance degradation under load\n\nEnhanced Accuracy\n\nThe multi-agent approach significantly improves response accuracy:\n\nDocument Relevance Grading: Ensures only relevant information is used\n\nQuery Reformulation: Improves retrieval success rates\n\nFact-Checking: Reduces hallucinations and inaccuracies\n\nSafety Filtering: Prevents harmful or inappropriate content\n\nScalability and Flexibility\n\nThe system architecture supports:\n\nMultiple Knowledge Sources: URLs, documents, and real-time web search\n\nConfigurable Parameters: Chunk size, retrieval K, temperature\n\nExtensible Design: Easy to add new agents or modify workflows\n\nMulti-User Support: Isolated sessions and state management\n\nReal-World Applications\nEnterprise Knowledge Management\n\nThe system excels in enterprise environments where:\n\nDocument Volume: Large collections of internal documents need to be searchable\n\nAccuracy Requirements: High-stakes decisions require verified information\n\nSecurity: Sensitive information requires controlled access\n\nCompliance: Responses must meet regulatory requirements\n\nCustomer Support\n\nFor customer support applications, the system provides:\n\nConsistent Responses: Standardized answers based on official documentation\n\nReal-time Updates: Integration with current product information\n\nMulti-language Support: Handling queries in various languages\n\nEscalation Path: Clear protocols for complex queries\n\nResearch and Analysis\n\nResearchers benefit from:\n\nLiterature Review: Efficient searching across academic papers\n\nFact Verification: Cross-referencing claims against multiple sources\n\nSynthesis Capabilities: Combining information from diverse sources\n\nCitation Tracking: Automatic source attribution\n\nFuture Enhancements\nPlanned Improvements\n\n1. Multi-modal Support: Integration of image and video processing\n\n2. Advanced Analytics: Detailed performance metrics and insights\n\n3. Collaborative Filtering: Learning from user feedback patterns\n\n4. Enhanced Security: Role-based access control and audit trails\n\n5. Deployment Options: Cloud-native and on-premises deployment packages\n\nResearch Directions\n\n1. Agent Specialization: More granular agent roles and responsibilities\n\n2. Memory Optimization: Efficient long-term conversation memory\n\n3. Cross-lingual Capabilities: Seamless multi-language processing\n\n4. Real-time Learning: Continuous improvement from user interactions\n\nGetting Started\nPrerequisites\n\nPython 3.8+\n\nStreamlit\n\nLangChain\n\nLangGraph\n\nChroma\n\nGoogle Generative AI API key\n\nGroq API key\n\nTavily API key\n\nGithub Repo Link\n\nhttps://github.com/Abdullah-47/Multi-Agentic-RAG/tree/main\n\nInstallation\ngit clone https://github.com/Abdullah-47/Multi-Agentic-RAG/tree/main\npip install -r requirements.txt\n\nConfiguration\n\nSet up environment variables:\n\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"your_langchain_api_key\"\n\nos.environ[\"TAVILY_API_KEY\"] = \"your_tavily_api_key\"\n\nGOOGLE_API_KEY = \"your_google_api_key\"\n\nGROQ_API_KEY = \"your_groq_api_key\"\n\nRunning the Application\nstreamlit run app.py\n\nStreamlit Hosted App Link\nhttps://dynamic-agentic-rag.streamlit.app/\n\nReferences and Resources\nCore Technologies\n\n1. LangGraph Multi-Agent Systems - \nOfficial Documentation\n\n2. Streamlit for RAG Applications - \nMedium Article\n\n3. Chroma Vector Store - \nAnalytics Vidhya Guide\n\n4. Google Generative AI Embeddings - \nOfficial Documentation\n\nAdvanced RAG Techniques\n\n1. Advanced RAG Architectures - \nMachine Learning Mastery\n\n2. Self-Reflective RAG with LangGraph - \nLangChain Blog\n\n3. Corrective RAG (CRAG) - \nLangGraph Tutorial\n\nPerformance and Optimization\n\n1. Groq Inference Performance - \nOfficial Benchmarks\n\n2. Tavily Search for RAG - \nMedium Article\n\n3. Fact-checking with LLMs - \nResearch Paper\n\nCommunity and Support\n\n1. LangChain Community - \nDiscord Server\n\n2. Streamlit Community - \nForum\n\n3. Groq Developer Community - \nGitHub\n\nConclusion\n\nThe Advanced Agentic RAG System represents a significant leap forward in the field of intelligent document processing and question-answering systems. By leveraging multiple specialized agents, self-correction mechanisms, and state-of-the-art technologies, it provides a robust, accurate, and scalable solution for organizations seeking to harness the power of AI with their knowledge bases.\n\nThe system's modular architecture, combined with its comprehensive safety and fact-checking features, makes it suitable for deployment in enterprise environments where accuracy, security, and reliability are paramount. As the field of AI continues to evolve, this system provides a solid foundation for future enhancements and adaptations to emerging requirements and technologies.\n\nFor organizations looking to implement advanced RAG capabilities, this system offers a proven architecture that balances performance, accuracy, and usability while maintaining the flexibility to adapt to specific use cases and requirements.\n\nThis publication is part of an ongoing effort to advance the state of RAG systems and multi-agent AI architectures. For the latest updates and contributions, please refer to the project repository and documentation.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nIntroduction\n\nSystem Architecture\n\nCore Components\n\nMulti-Agent Design\n\nKey Features and Capabilities\n\nIntelligent Query Routing\nSelf-Correction Mechanisms\nFlexible Knowledge Sources\nAdvanced Fact-Checking\n\nTechnical Implementation\n\nView all\nComments\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "ai-assistant-vui-VcG8wcVNZul2",
    "username": "mMax Goltzsche",
    "license": null,
    "title": "AI Assistant VUI",
    "publication_description": "Back to publications\nMar 31, 2025\n●\n147 reads\n●\nApache 2.0\nWinner of\nBest AI Tool Innovation\nat the\nAgentic AI Innovation Challenge 2025\nAI Assistant VUI\nai\nai-assistant\nchatbot\ngo\ngolang\njarvis\nlive-transcription\nspeech\nspeech-recognition\nvad\nvoice\nvoice-assistant\nvoice-chat-bot\nvoice-chatbot\nvoice-control\nvoice-user-interface\nvui\nM\nMax Goltzsche\nLike\nBookmark\nShare\n\nAI Assistant VUI\n\nAn experimental voice user interface (VUI) to interact with an AI assistant.\n\nProject link\n\nIt is as a client CLI that connects to an OpenAI API-compatible server (that is served locally by \nLocalAI\n).\nTo answer a user request it can decide to use tools in form of running configurable, dockerized functions.\n\nFor voice activity detection (VAD) \nsilero-vad\n is built into the client.\nFor chat completion, speech-to-text (STT) and text-to-speech (TTS) capabilities the client leverages the API server.\nIn order to detect whether the AI assistant is addressed, a wake word can be configured.\nThough, wake word support is implemented by matching the STT (whisper) output string against the wake word, requiring all voice communication to be STT processed, at least for now.\n\nMotivation\n\nOne day I was in the kitchen cooking when my \nMopidy\n-based streamer started playing a song that I didn't like.\nI wanted to skip the song or at least turn the volume down but, since I had dirty fingers, I couldn't simply use my phone or computer mouse, unless I'd clean them.\nHaving a freely configurable VUI would be a great addition to a graphical user interface (GUI) in such a situation and for IoT devices in general since the user wouldn't have to touch a screen or mouse to skip the song.\nSimilarly, such a VUI backed by a Large Language Model (LLM) could allow disabled and old people to use computers (more intuitively).\nPotentially there are also other use-cases such as a personal advisor or a translator (also see \nBabelfish\n) for everyone.\nWhile there are already commercial, cloud-based products such as Amazon Alexa available that address some of these problems to some degree, they are not as capable and flexible as the latest LLMs, not freely configurable, not open source but they come with a vendor lock-in, require an internet connection, listen to everything that anyone around them says and send it to 3rd party servers, impacting their users' privacy negatively.\nGiven the latest advancements of AI technology, I was curious whether a VUI could already be implemented based on open source software without those limitations, preferably in Go, and started researching.\n\nFeatures/requirements\nA voice-controlled AI assistant that can interact with the real world using preconfigured tools: e.g. can decide to run a docker container to change the music volume.\nLow latency/near-realtime response to support a fluent, natural conversation.\nVerbally interruptable system in order to appear responsive and not waste the user's time by talking about irrelevant information. When interrupted, the assistant stops talking after it finished the current sentence and for consistency only what it really said ends up within the message history.\nConfigurable wake word support to prevent the AI from responding to every voice communication (e.g. between humans) which is annoying otherwise. Any sentence the user says that does not contain the wake word is ignored by the AI.\nSave energy/API calls and avoid hallucination of the STT system by STT-processing only audio signals that contain voice activity (VAD).\nRelated work\nHistorically dialog systems such as \nCMU Sphinx\n exist since a while already. However, these were quite unflexible since they required the user to say specific preconfigured phrases in order to trigger a response and they weren't as good at recognizing those as modern LLM-based solutions which are also more flexible since they do not require every possible dialog to be preconfigured.\nCommercial cloud/SaaS-based, closed-source voice-controlled assistants such as Amazon's Alexa, Google's Assistant, Apple's Siri. However, these products require an internet connection and send the recorded audio data to 3rd party servers, negatively impacting their users' privacy.\nOpenAI's \nwhisper\n is the state-of-the-art speech recognition (STT) model and \nSaaS API\n.\nCommercial AI SaaS products such as OpenAI's \nChatGPT\n started to support voice interaction recently.\nfaster-whisper\n: An open source reimplementation of OpenAI's whisper that is even faster than the original.\nSilero VAD\n: An open source Voice Activity Detection (VAD) model, allowing to detect voice activity within an audio signal. This is useful to reduce hallucination of the STT/whisper model and also to reduce the computational load and therefore energy consumption since not every audio signal needs to be STT-processed.\nLocalAIVoiceChat\n: A Python-based conversational AI to talk to but without the ability to let the AI call user-defined functions.\ngo-whisper-cpp-server-example\n: An STT-based translator written in Go.\nollama\n: An open source LLM engine that implements an OpenAI-compatible chat completion API that can be run locally.\nLocalAI\n: An open source, OpenAI-compatible LLM API server that integrates ollama and faster-whisper along with other open source AI projects to support chat completion, STT and TTS locally. Therefore LocalAI is very well suited as the server of the AI Assistant VUI.\nSystem requirements\nProcessor: Intel Core i3/AMD Ryzen 3 or better.\nRAM: 4GB minimum, 8GB recommended.\nGraphics card with 8GB RAM (AMD or Nvidia).\nStorage: 1GB free space.\nAudio: Working microphone and speakers/headphones.\nOperating system: Linux (tested on an Ubuntu 24.04 host), adaptable for MacOS and Windows.\nBuild\n\nClone the repo:\n\ngit clone https://github.com/mgoltzsche/ai-assistant-vui.git\ncd ai-assistant-vui\n\nTo build the Linux container image, run the following command within the project's root directory (requires \nDocker\n to be installed):\n\nmake\nRun\nStart the \nLocalAI\n API server (LLM server) by running the following within the project's root directory:\nmake run-localai\n\nBrowse the LocalAI web GUI at \nhttp://127.0.0.1:8080/browse/\n and search and install the models you want to use. When using the \ndefault AI Assistant VUI\n configuration, you need to install whisper-1 (STT), localai-functioncall-qwen2.5-7b-v0.5 (chat) and voice-en-us-amy-low (TTS).\n\nRun the VUI (within another terminal):\n\nmake run-vui INPUT_DEVICE=\"KLIM Talk\" OUTPUT_DEVICE=\"ALC1220 Analog\"\n\nYou will likely have to replace the values of INPUT_DEVICE and OUTPUT_DEVICE with the names or IDs of your audio devices (available devices are listed in the log).\nYou may not be able to use an audio device when another program (e.g. your browser) is already using it.\nIn that case, please close other programs, wait a few seconds and then re-run the VUI.\n\nYou need to mention the configured wake word (defaults to \"Computer\") with each request to the AI assistant.\nFor instance you can ask \"Computer, what's the capital of Germany?\"\n\nImplementation details\n\nThe application is written in Go, leveraging its static typing and concurrency features.\n\nSchematic sequence diagram:\n\n\nLimitations\nThe wake word must be recognized by the whisper model - this could be improved potentially using a specialized wake word model.\nContext size and storage:\nTo keep the context size minimal and speed up inference, tool results of the previous user request are deleted from the chat history with every new user request.\nThe chat history is currently stored in-memory only and is therefore lost when restarting the application.\nThe chat history is infinite / not truncated which therefore exceeds the LLM's context size at some point and then requires an application restart.\nDue to a LocalAI LLM bug function calls are often repeated infinitely - this is detected and prevented after the 2nd call.\nAudio device usage: The container does not work with pulseaudio but ALSA and therefore requires no other application to use the same audio devices it uses.\nOther people can also give the AI commands (e.g. somebody on the street shouting through the window) - voice recognition could protect against that.\nRoadmap\nContext size and storage:\nChat history retention: Detect when the maximum context size would be exceeded and delete old messages in that case, starting with the first user request and assistant response.\nTo support multiple rooms and a client for each as well as remote access, the context/conversation history could be stored on a (local) server.\nAdd Retrieval-Augmented Generation (RAG) support to kind of support an infinite context size: write conversations (and other personal information) into a vector database, query it for every user request to find related information and add it to the message history before sending it to the chat completion endpoint.\nPrevent function call repetition.\nAdd a wake word engine in order to save energy/STT API requests.\nTo improve compatibility, it would be good to make the container connect to pulseaudio.\nAuthentication via voice recognition to make the assistant aware of who is talking and to protect against other people commanding the assistant.\nCredits\n\nWhen I searched the web for an existing implementation of such an application or similar to start with, I found the STT translator \ngo-whisper-cpp-server-example\n from where I copied and adjusted a couple of code snippets.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAI Assistant VUI\n\nMotivation\n\nFeatures/requirements\n\nRelated work\n\nSystem requirements\n\nBuild\n\nRun\n\nImplementation details\n\nLimitations\n\nRoadmap\n\nView all\nComments\n(1)\nMost recent\n\n✨ Want to join the conversation?\n\nSign in or create an account to comment.\nDark Haven Forge\n5 months ago\n\nThe **AI Assistant VUI** project innovatively enhances hands-free control and accessibility using open-source AI technologies, improving user experience and privacy. It offers valuable learning opportunities while also facing challenges in wake word\n\nLike (1)\nReply\nFiles\nai-assistant-vui.zip\nYou might be interested\nAgentic CallBot Using Twilio and OpenAi Realtime\nO\nFeb 24, 202527 reads\nagentic aiagents+4\nAI Voice Assistant: Enhancing Developer Productivity\nFeb 27, 202521 reads\nAIAssistant+5\nBuilding My Own Jarvis: An AI Adventure\nJ\nFeb 27, 202515 reads\nChatGPTCoffee+4\nGemini Based Live Voice to Voice Todo Agent Assistant\nO\nMar 31, 202513 reads\naigemin+5",
    "awards": [
      "best ai tool innovation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "ai-dot-artificial-intelligence-driven-options-trading-bot-jojuSlCdBrvw",
    "username": "Abhishek",
    "license": "MIT License",
    "title": "AI-DOT: Artificial Intelligence Driven Options Trading Bot",
    "publication_description": "Back to publications\nJul 29, 2025\n●\n70 reads\n●\nMIT License\nAI-DOT: Artificial Intelligence Driven Options Trading Bot\nai in finance\nalgorithmic trading\nann model\nartificial intelligence\nautomated trading system\nbacktesting strategies\nbroker api integration\nbuy sell signals\ndata\ndeep learning\nfinance-ml pipeline\nfinancial forecasting\nkeras model\nmachine learning\nmodel evaluation metrics\nneural networks\noptions data analytics\noptions trading\npredictive modeling\npython trading bot\nquant-finance\nquantitative trading\nrealtime trading\nrnn lstm\nstock price prediction\ntechnical indicators\ntensorflow\ntime series prediction\nyfinance\nAbhishek\nLike\nBookmark\nShare\n🤖 AI-DOT: Artificial Intelligence Driven Options Trading Bot\n\nAI-DOT is an intelligent options trading bot developed using Artificial Neural Networks (ANN) and Recurrent Neural Networks (RNN). It automatically analyzes options data, generates call/put buy/sell signals, and integrates with broker APIs for real-time execution. This project demonstrates the full pipeline from data acquisition to model training, evaluation, signal generation, and live deployment.\n\n📄 \nRead the Full Report (PDF)\n\n📦 Dataset & Features\nData Source: \nyFinance\nScope:\nHistorical stock prices\nOption chain data (call/put)\nImplied volatility, open interest\nPreprocessing:\nHandling NaNs and outliers (Box & Line plots)\nShort/long moving averages\nStandardScaler normalization\nFeature Types:\nTechnical Indicators (MA, volatility, price patterns)\nDerived options data (IV, call/put ratios)\n\n\n\n\n⚙️ Modeling Approach\nAlgorithms Used:\nANN: For regression-based price prediction\nRNN (LSTM): To capture temporal patterns\nArchitecture:\nDense layers with ReLU activation\nRegularization: L1/L2\nOptimizer: Adam (lr=0.0001)\nEarlyStopping & ReduceLROnPlateau used\nTraining Details:\nEpochs: 600 (ANN), 200 (RNN)\nLoss: MAE, RMSE\n\n\n\n\n📊 Model Evaluation\nModel\tMAE\tRMSE\nANN\t0.37\t0.47\nRNN\t1.18\t1.58\n\nANN outperformed RNN in both accuracy and consistency. RNN showed stable learning but struggled with prediction precision.\n\n\n\n\n📈 Learning Curve\n\nTraining and validation loss decreased smoothly with no overfitting observed.\n\n\n\n\n💹 Trading Signal Generation\n\nThe trained models were used to generate trading signals:\n\nGreen: Buy Call Signal\nRed: Buy Put Signal\n\nSignals were overlaid on live price charts for intuitive visualization and backtesting.\n\n\n\n\n🧪 Testing Workflow\nTest Stage\tStatus\nData preprocessing and cleaning\t✅ Passed\nANN & RNN Model training\t✅ Passed\nModel validation with MAE & RMSE\t✅ Passed\nPrediction histogram and alignment\t✅ Passed\nSignal generation from model outputs\t✅ Passed\nVisual verification of trading points\t✅ Passed\n🧠 Key Insights\nANN provided more precise and stable predictions than RNN.\nBoth models generalized well on unseen data with minimal overfitting.\nBuy/sell signal alignment with price trends confirmed model robustness.\nVisual inspection helped refine strategies and filter noise.\n🚀 Future Scope\nIntegrate live news sentiment analysis to enhance prediction accuracy.\nExpand models using hybrid CNN-RNN or Transformer-based architectures.\nDeploy a Streamlit or Flask-based dashboard with real-time brokerage execution.\nEnable multi-asset support (index options, forex derivatives, etc.)\n📝 Citation\n\nMalaviya, A., Dhumale, N., Kotian, N., & Vala, J. (2024). AI-DOT: Artificial Intelligence Driven Options Trading Bot. Shah and Anchor Kutchhi Engineering College.\n\nFull Report PDF\n\n📂 Resources\n💻 \nGitHub Repository\n📈 \nReport PDF\n🔗 Broker API Integration (planned in real-time module)\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\n🤖 AI-DOT: Artificial Intelligence Driven Options Trading Bot\n\n📦 Dataset & Features\n\n⚙️ Modeling Approach\n\n📊 Model Evaluation\n\n📈 Learning Curve\n\n💹 Trading Signal Generation\n\n🧪 Testing Workflow\n\n🧠 Key Insights\n\n🚀 Future Scope\n\n📝 Citation\n\nView all\nComments\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "ai-filefinder-Sjfw9WmPowZj",
    "username": "Botir Bakhtiyarov",
    "license": "MIT License",
    "title": "Ai FileFinder",
    "publication_description": "Back to publications\nJul 29, 2025\n●\n112 reads\n●\nMIT License\nAi FileFinder\nAI\nfilefinder\nBotir Bakhtiyarov\nLike\nBookmark\nShare\nAI File Finder📂\n\nSmart File Finder & Chat Assistant v2 is a powerful Python-based desktop application designed to streamline file management and information retrieval for Chinese-speaking users. With a sleek, Chinese-localized interface, this tool leverages advanced AI to search, summarize, and interact with documents and images.\n\nBuilt with customtkinter for a modern GUI and Flask for a robust API backend, it’s perfect for professionals, researchers, and teams handling large file collections in Chinese environments. 🚀\n\nInstall Setup File\n\nDownload FileFinderSetup.exe from \nReleases\n.\n\n✨ Features\nSemantic File Search 🔍: Find documents (DOCX, PDF, TXT) and images (JPG, PNG) using natural language queries in Chinese.\nAI-Driven Insights 🤖: Generate embeddings for text and images with models optimized for Chinese content, powered by SentenceTransformers and ChineseCLIP.\nOCR Excellence 📸: Extract and summarize text from images and PDFs using Tesseract OCR, tailored for Chinese characters.\nSmart Chat Assistant 💬: Engage in contextual conversations or summarize files with a Chinese-focused AI, supporting both Normal and RAG modes.\nChinese Interface 🇨🇳: Fully localized UI with intuitive navigation, designed for native Chinese speakers.\nDark/Light Mode 🌗: Toggle themes for comfortable use in any lighting.\nSystem Tray Integration 🖥️: Minimize to tray with Alt+Q hotkey to restore.\nContinuous Indexing 🔄: Automatically updates file indexes for monitored directories.\n📸 Screenshots\n\nTo be updated with v2’s Chinese interface.\n\nSearch Tab:\n\nChat Assistant:\n\nOCR Summary:\n\n🚀 Installation\nPrerequisites\nPython 🐍: 3.10 or higher\nTesseract OCR 📖: For Chinese text extraction\nWindows: Download, add to PATH, ensure chi_sim data is installed\nLinux: sudo apt install tesseract-ocr tesseract-ocr-chi-sim\nmacOS: brew install tesseract\nPoppler 📄: For PDF processing\nWindows: Download, add to PATH\nLinux: sudo apt install poppler-utils\nmacOS: brew install poppler\nSetup Steps\n# Clone the Repository 📥\ngit clone https://github.com/BotirBakhtiyarov/Filefinder.git\ncd Filefinder\n\n# Set Up Virtual Environment 🛠️\npython -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n.venv\\Scripts\\activate     # Windows\n\n# Install Dependencies 📦\npip install -r requirements.txt\nConfigure AI Models ⚙️\n\nUpdate MODEL_DIR in api_server.py:\n\nMODEL_DIR = \"models/\"  # Path to Chinese-optimized models\n\nMake sure models like:\n\nparaphrase-multilingual-MiniLM-L12-v2\nchinese-clip-vit-base-patch16\n\nare available or downloaded.\n\nLaunch the App\n# Start the API server 🌐\npython api_server.py\n\n# Run the main application 🎉\npython app.py\n🖱️ Usage\nFirst Launch 🚀\n\nA Chinese-language setup wizard will guide you to:\n\nEnter API keys/URLs\nSet document/image directories\nChoose theme (Dark/Light)\nSearch Files 🔎\nGo to the “搜索” tab.\nEnter Chinese queries (e.g., 年度报告概述).\nView results and:\n打开: Open file\n摘要: AI-generated summary\nOCR 摘要: OCR-based summary for images\nChat Assistant 💬\nGo to the “聊天” tab\nSelect:\nNormal Mode: For general chat\nRAG Mode: Upload and ask about specific files\nAsk questions in Chinese and get summarized answers\nSystem Tray 🖥️\nMinimize to tray by closing window\nPress Alt+Q to restore\n📁 Project Structure\nFileFinder_v2/\n├── .venv/              # Virtual environment\n├── assets/\n│   ├── ui/             # Chinese-localized GUI\n│   │   ├── chat_frame.py\n│   │   ├── main_app.py\n│   │   ├── search_frame.py\n│   │   ├── setup_wizard.py\n│   │   └── summary_window.py\n│   └── utils/\n│       └── file_utils.py\n├── app.py              # App entry point\n├── api_server.py       # Flask backend\n├── requirements.txt    # Dependencies\n└── myicon.ico          # App icon\n\n🌐 API Endpoints\n\napi_server.py includes:\n\n/embed_text: Embed Chinese text\n/embed_image: Embed image\n/embed_clip_text: CLIP-based text-image embedding\n/extract_pdf_with_ocr: OCR for Chinese PDFs\n/extract_image_ocr: OCR for Chinese images\n🤝 Contributing\n\nWe welcome contributions! 🌟\n\n# Fork and branch\ngit checkout -b feature/YourFeature\n\n# Commit and push\ngit commit -m \"Add YourFeature\"\ngit push origin feature/YourFeature\nFollow PEP 8\nMaintain Chinese localization\n⚠️ Troubleshooting\nFile Access Errors: Verify file paths and network drives.\nOCR Issues: Make sure Tesseract is installed with chi_sim support.\nAPI Failures: Confirm api_server.py is running and keys are set.\nUI Bugs: Check customtkinter compatibility.\n\nOpen an \nissue on GitHub\n with logs if needed.\n\n📜 License\n\nMIT License – see \nLICENSE\n for details.\n\n🙌 Acknowledgments\nCustomTkinter: Modern GUI framework\nSentenceTransformers: Chinese text embeddings\nTransformers: ChineseCLIP integration\nTesseract OCR: Robust OCR engine\n\nBuilt with ❤️ for Chinese-speaking users, AI File Finder is your ultimate file management companion!\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAI File Finder📂\n\nInstall Setup File\n\n✨ Features\n\n📸 Screenshots\n\n🚀 Installation\n\nPrerequisites\n\nSetup Steps\n\nConfigure AI Models ⚙️\n\nLaunch the App\n\n🖱️ Usage\n\nView all\nCode\nYou might be interested\nTalk2PDF - Chat with your Documents\nN\nJul 26, 202518 reads\nAi-pdf-assistantpdf-reader+3\n🤖 IFRI FAQ Chatbot - Intelligent Assistant with RAG Agent 📚💡\nF\nMar 31, 202518 reads\nAgentAI+3\nMulti-Model RAG with Gemini, LangChain, and Google AI Studio\nMar 15, 202516 reads\nMulti-PDF-RAG: Free and Local Intelligent Interaction with Complex, Tabular, and Scanned Documents\nA\nFeb 27, 202514 reads\nAIChromaDB+12",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "ai-multi-agent-hiring-orchestrator-m6Fw76jO7ROe",
    "username": "Addisu Taye Dadi",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nOct 03, 2025\n●\n11 reads\n●\nMIT License\nAI Multi-Agent Hiring Orchestrator\nAddisu Taye\nHR Recruitment\nAddisu Taye Dadi\nLike\nBookmark\nShare\n\nAbstract\nIntroduction\n\nImagine a recruiter overwhelmed with hundreds of resumes for a single job posting. Hours are spent scanning CVs, checking skills, and shortlisting candidates—only to risk missing out on the perfect hire.\n\nNow, picture a team of AI agents working together behind the scenes: one agent cleans and parses resumes, another matches them against job descriptions, while a third highlights strengths and gaps. Finally, all the insights flow into a simple dashboard where recruiters see ranked candidates at a glance.\n\nThis is what the Multi-Agent AI Recruitment System does—turning a messy hiring process into a smooth, intelligent, and automated workflow.\n\n✅Here are four points on how this recruitment agent is useful:\n\nSpeeds Hiring-Automates resume parsing and matching to cut screening time from hours to minutes.\nImproves Match Quality -Uses AI embeddings and scoring to align candidates with job requirements accurately.\nReduces Bias-Redacts personal information to ensure fairer candidate evaluation.\nProvides Clear Insights-Highlights strengths, gaps, and recommendations for smarter decisions.\nMethodology\n\nThe Multi-Agent AI Recruitment System works as a pipeline of specialized agents. Each agent performs a specific role using the best-suited tools and models. The output of one agent becomes the input for the next, ensuring a smooth and automated process.\n\n\nFigure 1: Orchestration of the agents\n\nAgents\n1. Resume parsing\n\nAs shown in Figure 1 above, the first step is resume parsing. This agent cleans the resume by removing personal information using custom PII detection and redaction tools.\n\nTools and Models:\n\nPython Regex (re) for pattern matching.\nNamed Entity Recognition model: dslim/bert-base-NER via HuggingFace Transformers.\nCustom Personally Identifiable Information (PII) detection and redaction tools (scan_pii, redact_pii).\n\nOutput\n\nA structured dictionary containing cleaned resume text, extracted skills, experience years, education, and processing messages.\n{\n  \"resume_text\": \"<cleaned resume text>\",\n  \"extracted_skills\": [\"Python\", \"Data Analysis\", \"SQL\"],\n  \"experience_years\": 5,\n  \"education\": \"Bachelor\",\n  \"messages\": [\"PII detected and redacted\", \"Parsed: 5 skills, 5 yrs exp\"]\n}\n\n2. Embedding and matching\n\nNext is the embedding and matching step. Cosine similarity is computed using PyTorch to produce a match score expressed as a percentage. This score reflects how well the candidate fits the job requirements.\n\nTools and Models:\n\nsentence-transformers/all-MiniLM-L6-v2 for embedding generation.\nPyTorch (torch.cosine_similarity) for similarity computation.\n\nOutput\n\nA dictionary containing the match score and processing messages.\n{\n  \"match_score\": 85.4,\n  \"messages\": [\"Match score: 85.4%\"]\n}\n\n3. Insight generation\n\nThe third step is insight generation. This agent decides whether a candidate should be shortlisted by checking if the match score exceeds a threshold (70%).\n\nTools and Models:\n\nPython string and list operations for logic and keyword matching.\n\nOutput\n\nA dictionary with strengths, missing skills, shortlist status, and processing messages.\n{\n  \"strengths\": [\"Python\", \"Data Analysis\", \"SQL\"],\n  \"missing_skills\": [\"Banking experience\"],\n  \"shortlisted\": true,\n  \"messages\": [\"Shortlisted: True\"]\n}\n4. Orchestration connects all agents into a sequence\n\nFinally, the workflow orchestration connects all agents into a sequence. This is done using LangGraph’s StateGraph, which manages state and message flow between agents. The process starts at the resume parser, moves to the embedder, and finishes with the insight generator. The final output is a comprehensive candidate evaluation that includes all extracted and computed information, along with a set of messages documenting the process.\n\nTools and Models:\n\nStateGraph from LangGraph for workflow orchestration.\nPython TypedDict for structured state representation (RecruitmentState)\n\nFinal Output:\n\nA comprehensive state dictionary containing all processed candidate information and evaluation results.\n{\n  \"candidate_name\": \"John Doe\",\n  \"candidate_email\": \"johndoe@example.com\",\n  \"resume_text\": \"<cleaned resume text>\",\n  \"job_description\": \"...\",\n  \"job_requirements\": \"...\",\n  \"extracted_skills\": [\"Python\", \"Data Analysis\"],\n  \"experience_years\": 5,\n  \"education\": \"Bachelor\",\n  \"match_score\": 85.4,\n  \"strengths\": [\"Python\", \"Data Analysis\"],\n  \"missing_skills\": [\"Banking experience\"],\n  \"shortlisted\": true,\n  \"messages\": [\"PII detected and redacted\", \"Parsed: 5 skills, 5 yrs exp\", \"Match score: 85.4%\", \"Shortlisted: True\"]\n}\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "ai-nexus-herald-a-multi-agent-ai-newsletter-generator-5KZCguswBusj",
    "username": "aAmina Javaid",
    "license": "MIT License",
    "title": "AI Nexus Herald - A Multi-Agent AI Newsletter Generator",
    "publication_description": "Back to publications\nJul 14, 2025\n●\n54 reads\n●\nMIT License\nAI Nexus Herald - A Multi-Agent AI Newsletter Generator\nAAIDC2025\nAgent Orchestration\nAgentic Architecture\nAgentic Workflow\nAgenticAI\nAI\nAI Product Development\nAutonomous Agents\nGroq API\nLangChain\nLanggraph\nLLMs\nMulti-Agent Systems\nReAct Agent\nTool-Use Agent\nA\nAmina Javaid\nLike\nBookmark\nShare\n\nAbstract\n\nAI Nexus Herald is a multi-agent system built using LangGraph that creates an AI generated newsletter focused on trending topics in AI. It leverages the full potential of agent orchestration workflow for designing and building the system. The system coordinates specialized agents for topic discovery, deep research, and summarization. It integrates custom tools and LLMs to automate high-quality newsletter generation with minimal human intervention.\n\nIntroduction\n\nKeeping pace with rapid AI advancements is challenging for anyone including students, professionals, and AI enthusiasts. AI Nexus Herald addresses this by automating the end-to-end process of discovering and researching key AI updates, organizing them into a well-formed newsletter. Leveraging vertical agents and structured orchestration, it ensures accurate, timely, and readable content delivery.\n\nBuilding multi-agent systems is quite a challenging task. There are many agentic frameworks and SDKs available like Crew AI, Agno, Google Agent Development SDK, OpenAI Agent Development SDK and more. Each framework has its own pros and cons in terms of providing various agentic architectures for orchestration workflows. I have chosen LangGraph for this project which is a stateful orchestration framework that brings added control to agent workflows. It's simple and intuitive to understand and build. Moreover, it really helps make your project modular and scalable from the very beginning. The main components of LangGraph include nodes and edges which make it very convenient to build workflows in the form of graphs. It allows for sequential as well as conditional execution workflows.\n\nAI newsletter generation has been an exciting project from the very beginning which has taught me many lessons to improve my understanding of agentic workflows. In this publication, I'll be sharing how I have designed and built this system along with some useful and practical insights which I have gained during this process.\n\nPrerequisites\n\nTo fully understand and replicate the implementation of AI Nexus Herald, the following tools, libraries, and knowledge are required. These are divided into must-have and optional prerequisites:\n\nMust-Have Prerequisites\nCategory\tRequirement\nProgramming\tIntermediate proficiency in Python\nFrameworks\tFamiliarity with LangChain, LangGraph, and FastAPI\nLLMs\tWorking knowledge of Large Language Models (LLMs) and API-based usage (e.g., OpenAI, Groq)\nAPIs\tUnderstanding of RESTful API integration and authentication\nTools\tExperience with virtual environments, package managers (pip, anaconda)\nFrontend\tBasic familiarity with Streamlit for building minimal, lightweight UI\nYAML/JSON\tAbility to read and write YAML and JSON for config and prompt files\nOptional Prerequisites\nCategory\tBenefit\nDocker\tEnables containerization and easier deployment across environments\nLLM Prompt Engineering\tHelps in refining agent prompts for deterministic outputs\nLangChain Tools Development\tUseful for custom tool integration\nDeployment Platforms\tKnowledge of Fly.io, Railway, or AWS for production deployment\nAI Nexus Herald - Implementation\nHigh-Level Architecture\n\nAI Nexus Herald stands at Level-4 agentic architecture (LLMs + Tools + Reasoning + Multiple Agents).\n\nThis multi-agent system has four agents and three tools. The agents interact through an Orchestrator that handles the whole agentic workflow. First it calls the Topic Finder Agent to extract trending AI topics from RSS feeds using the RSS Title Extraction Tool and updates its state. Then it passes on those topics to the Deep Research Agent that extracts news related to those topics from the RSS feeds using the RSS News Extraction Tool. Finally, it passes all the news content to the Newsletter Writer Agent that crafts a comprehensive newsletter in markdown format and saves it to a directory using the Newsletter Saving Tool.\n\nHere is a brief description of all agents and tools in AI Nexus Herald.\n\nAgents\nTopic Finder Agent - Searches for top trending AI topics\nDeep Research Agent - Searches for news related to top trending AI topics\nNewsletter Writer Agent - Generates a newsletter using the searched news information\nOrchestrator - Handles the whole orchestration workflow of all agents\nTools\nRSS Titles Extraction Tool - Extracts titles from RSS feeds\nRSS News Extraction Tool - Extracts news articles related to selected topics from RSS feeds\nNewsletter Saving Tool - Saves the newsletter in a directory\nSystem Design\n\nHere is a high-level architectural diagram of this multi-agent workflow.\n\nOrchestrator Graph\n\nAgent Graphs\nTopic Finder Graph\n\nDeep Researcher Graph\n\nNewsletter Writer Graph\n\nTech Stack\n📊 Streamlit\n⚡ FastAPI\n🧬 LangGraph\n🧠 GroqAPI - The latest moonshotai/kimi-k2-instruct model\nSetup Instructions\n1. Clone the repo\ngit clone https://github.com/aminajavaid30/ai_nexus_herald.git\ncd ai_nexus_herald\n2. Install dependencies\npip install -r requirements.txt\n3. Add your API keys - Create a .env file with:\nGROQ_API_KEY=...\nGMAIL_PASSWORD=...\n4. Initialize the backend\n\nRun the following command:\n\nuvicorn src.backend.main:app --reload\n5. Run the app\n\nNavigate to the frontend folder and run the following command:\n\nstreamlit run Home.py\nFrontend\n\nThis is the Home page for AI Nexus Herald.\n\nThis is the Newsletter page of the application displaying the AI generated newsletter.\n\nStreamlit UI\nHome.py\n\nThis is the main entry point of the system built using Streamlit. It has a minimal interface with a sidebar and a button to generate the weekly newsletter.\n\nimport streamlit as st\nimport requests\n\nBACKEND_URL = \"http://localhost:8000/generate\"  # FastAPI endpoint\n\n# Page Configuration\nst.set_page_config(\n    page_icon=\"📰\", \n    page_title=\"AI Nexus Herald\", \n    initial_sidebar_state=\"auto\",\n    layout=\"wide\")\n\n# Load external CSS\nwith open(\"style.css\") as f:\n    st.markdown(f\"<style>{f.read()}</style>\", unsafe_allow_html=True)\n\n# Sidebar\nst.sidebar.header(\"📰 AI Nexus Herald\")\nst.sidebar.markdown(\"\"\"\n    **About**  \n    This is an AI generated newsletter that brings together top trending and latest news related to artificial intelligence.   \n\"\"\")\nst.sidebar.info(\"\"\"\n    **Features**  \n    - Latest Trending AI News\n    - Deep Research Capability\n\"\"\")\n\n# Main title with an icon\nst.markdown(\n    \"\"\"\n    <div class=\"custom-header\"'>\n        <span>📰 AI Nexus Herald</span><br>\n        <span>Top Trending AI News Publication</span>\n    </div>\n    \"\"\",\n    unsafe_allow_html=True\n)\n\n# Horizontal line\nst.markdown(\"<hr class='custom-hr'>\", unsafe_allow_html=True)\n\n# Initialize Welcome Message\nif \"welcome_message\" not in st.session_state:\n    st.session_state.welcome_message = True\n\n# Initialize Generated Newsletter State\nif \"generate_newsletter\" not in st.session_state:\n    st.session_state.generate_newsletter = False\n\n# Initialize Newsletter Content State\nif \"newsletter_content\" not in st.session_state:\n    st.session_state.newsletter_content = None\n\n\ncol1, col2, col3 = st.columns([1, 3, 1])\n\nwith col2:\n    # Display Welcome Message\n    if st.session_state.welcome_message == True:\n        st.success(\"\"\"\n            **Welcome to AI Nexus Herald!**\n            - We'll generate the latest AI news for you.\n            - Click the \"Generate Newsletter\" button to get started.\n        \"\"\")\n\n    st.markdown(\"## Generate Your Weekly AI Newsletter\")\n    st.markdown(\"\"\"\n                🌐 **Overwhelmed by the fast pace of AI?**\n\n                Sit back and relax—we'll bring you this week's top AI breakthroughs, trends, and research.\n\n                **Click the button below to generate your personalized AI newsletter.**\n                \"\"\")\n    \n    # Center the button in the layout\n    st.markdown('<div class=\"center-button\">', unsafe_allow_html=True)\n    generate_clicked = st.button(\"Generate Newsletter\")\n    st.markdown('</div>', unsafe_allow_html=True)\n\n    # Generate Newsletter\n    if generate_clicked:\n        st.session_state.welcome_message = False\n        st.session_state.generate_newsletter = True\n        st.rerun()  # Refresh to clear the welcome message\n\n    if st.session_state.generate_newsletter == True:\n        with st.spinner(\"Generating Newsletter...\"):\n            try:\n                response = requests.post(BACKEND_URL)\n                if response is not None:\n                    data = response.json()\n                    content = data[\"response\"]\n                    st.session_state.newsletter_content = content\n                    st.session_state.generate_newsletter = False\n                    st.switch_page(\"pages/1 - Newsletter.py\")\n            except Exception:\n                result = \"❌ Error: Something went wrong while processing your request. Please try again.\"\n                st.error(result)\nNewsletter.py\n\nHere is the Newsletter page which displays the AI generated newsletter in markdown format.\n\nimport streamlit as st\n\n# Page Configuration\nst.set_page_config(\n    page_icon=\"📰\", \n    page_title=\"AI Nexus Herald\", \n    initial_sidebar_state=\"auto\",\n    layout=\"wide\")\n\n# Load external CSS\nwith open(\"style.css\") as f:\n    st.markdown(f\"<style>{f.read()}</style>\", unsafe_allow_html=True)\n\n# Sidebar\nst.sidebar.header(\"📰 AI Nexus Herald\")\nst.sidebar.markdown(\"\"\"\n    **About**  \n    This is an AI generated newsletter that brings together top trending and latest news related to artificial intelligence.   \n\"\"\")\nst.sidebar.info(\"\"\"\n    **Features**  \n    - Latest Trending AI News\n    - Deep Research Capability\n\"\"\")\n\n# Main title with an icon\nst.markdown(\n    \"\"\"\n    <div class=\"custom-header\"'>\n        <span>📰 AI Nexus Herald</span><br>\n        <span>Top Trending AI News Publication</span>\n    </div>\n    \"\"\",\n    unsafe_allow_html=True\n)\n\n# Horizontal line\nst.markdown(\"<hr class='custom-hr'>\", unsafe_allow_html=True)\n\ncol1, col2, col3 = st.columns([1, 3, 1])\n\nwith col2:\n    st.markdown(\"## Here's Your AI Newsletter for this Week. Happy Reading!\")\n    newsletter = st.session_state.get(\"newsletter_content\", \"No newsletter generated yet.\")\n    if newsletter:\n        st.markdown(newsletter)\n    else:\n        st.warning(\"No newsletter generated yet.\")\nBackend\nmain.py\n\nThe backend is built using FastAPI endpoints. This is the main backend entry point. It is accessed through http requests from the frontend. The generate endpoint invokes the LangGraph orchestrator which then handles all the agentic workflow through its internal orchestration and coordination with multiple agents. The newsletter is saved in a directory after generation.\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom fastapi.middleware.cors import CORSMiddleware\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom src.backend.agents.orchestrator import Orchestrator, OrchestratorState\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\napp = FastAPI()\n\n# CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nclass QueryResponse(BaseModel):\n    response: str\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Welcome to AI Nexus Herald!\"}\n\n@app.post(\"/generate\")\nasync def generate_newsletter():\n    try:\n        groq_api_key = os.getenv(\"GROQ_API_KEY\")\n        orchestrator = Orchestrator(groq_api_key)\n        graph = orchestrator.build_orchestrator_graph()\n\n        initial_state = OrchestratorState()\n        \n        try:\n            final_state = graph.invoke(initial_state) \n        except Exception as e:\n            print(f\"Error occurred: {str(e)}\")\n            raise e\n        \n        newsletter = final_state[\"newsletter\"]\n\n        return QueryResponse(response=newsletter)\n    except Exception as e:\n        return QueryResponse(response=f\"Error occurred: {str(e)}\")\nAgents\norchestrator.py\n\nThis is the main agent that handles the complete orchestration workflow of all the agents in the AI Nexus Herald ecosystem. The workflow first calls the Topic Finder Agent to extract titles and select top 5 trending AI topics through an LLM call. Then it calls the Deep Research Agent multiple times to extract news related to each AI topic. The Deep Research Agent then transfers the flow to a Newsletter Writer Agent which summarizes and generates a newsletter in markdown format updating the Orchestrator state.\n\nimport os\nimport time\nfrom pydantic import BaseModel\nfrom typing import Annotated\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.graph.state import CompiledStateGraph\nfrom operator import add\nfrom IPython.display import Image\n\nfrom src.backend.prompt_builder import build_prompt_from_config\nfrom src.backend.agents.topic_finder import TopicFinder, TopicState\nfrom src.backend.agents.deep_researcher import DeepResearcher, ResearchState, Article\nfrom src.backend.agents.newsletter_writer import NewsletterWriter, NewsletterState, News\nfrom src.backend.logger import logger\n\nload_dotenv()\n\nclass OrchestratorState(BaseModel):\n    topics: list = []\n    news_articles: Annotated[list[Article], add] = []\n    news: Annotated[list[News], add] = []\n    newsletter: str = \"\" # markdown\n\nclass Orchestrator:\n    def __init__(self, groq_api_key: str):\n        self.topic_finder = TopicFinder(groq_api_key)\n        self.deep_researcher = DeepResearcher(groq_api_key)\n        self.newsletter_writer = NewsletterWriter(groq_api_key)\n\n        logger.info(\"[Orchestrator] Agent initialized\")\n\n    def call_topic_finder(self, state: OrchestratorState) -> dict:\n        \"\"\"\n        Invoke TopicFinder graph and propagate results into orchestrator state\n        \"\"\"\n        logger.info(\"[Orchestrator] Invoking TopicFinder graph...\")\n        tf_graph = self.topic_finder.build_topic_finder_graph()\n\n        # Add RSS feed URLs to the system prompt\n        rss_feed_urls = \"\\n\\nRSS Feed URLs:\\n\"\n        for url in self.topic_finder.rss_feed_urls:\n            rss_feed_urls += f\"- {url}\\n\"\n\n        system_prompt = build_prompt_from_config(config=self.topic_finder.topic_finder_prompt, input_data=rss_feed_urls)\n\n        initial_state = TopicState(messages=[SystemMessage(content=system_prompt)], topics=[])\n\n        result_state = tf_graph.invoke(initial_state, config={\"recursion_limit\": 100})\n\n        state.topics = result_state[\"topics\"]\n        return {\"topics\": state.topics}\n    \n    def call_deep_researcher(self, state: OrchestratorState) -> dict:\n        \"\"\"\n        Invoke DeepResearcher graph and propagate results into orchestrator state\n        \"\"\"\n        logger.info(\"[Orchestrator] Invoking DeepResearcher graph...\")\n        dr_graph = self.deep_researcher.build_deep_researcher_graph()\n\n        # Add RSS feed URLs to the system prompt\n        rss_feed_urls = \"\\n\\nRSS Feed URLs:\\n\"\n        for url in self.deep_researcher.rss_feed_urls:\n            rss_feed_urls += f\"- {url}\\n\"\n\n        self.deep_researcher.topic = state.topics[0]\n        # Add topic to the system prompt    \n        topic = f\"\\n\\nTopic:\\n{self.deep_researcher.topic}\"\n\n        system_prompt = build_prompt_from_config(config=self.deep_researcher.deep_researcher_prompt, input_data=rss_feed_urls + topic)\n\n        initial_state = ResearchState(messages=[SystemMessage(content=system_prompt)], topic=self.deep_researcher.topic, news_articles=[])\n\n        try:\n            time.sleep(2)\n            result_state = dr_graph.invoke(initial_state, config={\"recursion_limit\": 100})\n        except Exception as e: # Rate Limit Error\n            time.sleep(2)\n            result_state = dr_graph.invoke(initial_state, config={\"recursion_limit\": 100})\n        time.sleep(2)\n\n        state.news_articles = result_state[\"news_articles\"]\n\n        # Remove the first topic from the list\n        if len(state.topics) > 1:\n            state.topics = state.topics[1:]\n        elif len(state.topics) == 1:\n            state.topics = []\n\n        # Build news object\n        state.news = [News(topic=self.deep_researcher.topic, news_articles=state.news_articles)]\n\n        return {\"topics\": state.topics, \"news_articles\": state.news_articles, \"news\": state.news}\n    \n    def should_research_continue(self, state: OrchestratorState):\n        \"\"\"Decides whether to call deep researcher or provide final answer.\"\"\"\n        remaining_topics = len(state.topics)\n \n        if remaining_topics > 0:\n            return \"yes\"  # Agent wants to perform deep research if there are more topics\n        return \"no\"  # Agent is ready to respond\n    \n    def call_newsletter_writer(self, state: OrchestratorState) -> dict:\n        \"\"\"\n        Invoke NewsletterWriter graph and propagate results into orchestrator state\n        \"\"\"\n        logger.info(\"[Orchestrator] Invoking NewsletterWriter graph...\")\n        nw_graph = self.newsletter_writer.build_newsletter_writer_graph()\n\n        # Add news (topics and news articles) to the system prompt\n        news = \"\\n\\nNews:\\n\"\n        for n in state.news:\n            for topic, articles in n:\n                news += f\"Topic: {topic}\\n\"\n                news += \"Articles:\\n\"\n                for article in articles:\n                    news += str(article)\n\n        system_prompt = build_prompt_from_config(config=self.newsletter_writer.newsletter_writer_prompt, input_data=news)\n\n        initial_state = NewsletterState(messages=[SystemMessage(content=system_prompt)], newsletter=\"\")\n\n        result_state = nw_graph.invoke(initial_state, config={\"recursion_limit\": 100})\n\n        state.newsletter = result_state[\"newsletter\"]\n        return {\"newsletter\": state.newsletter}\n\n    def build_orchestrator_graph(self) -> CompiledStateGraph:\n        \"\"\"Builds the orchestrator graph.\"\"\"\n        logger.info(\"[Orchestrator] Building graph...\")\n        workflow = StateGraph(OrchestratorState)\n\n        workflow.add_node(\"topic_finder_agent\", self.call_topic_finder)\n        workflow.add_node(\"deep_research_agent\", self.call_deep_researcher)\n        workflow.add_node(\"newsletter_writer_agent\", self.call_newsletter_writer)\n\n        workflow.set_entry_point(\"topic_finder_agent\")\n        workflow.add_edge(\"topic_finder_agent\", \"deep_research_agent\")\n\n        # Add the flow logic\n        workflow.add_conditional_edges(\"deep_research_agent\", self.should_research_continue, {\"yes\": \"deep_research_agent\", \"no\": \"newsletter_writer_agent\"})\n        workflow.add_edge(\"newsletter_writer_agent\", END)\n\n        return workflow.compile()\ntopic_finder.py\n\nThis is the Topic Finder Agent that searches for the latest and top trending AI topics from RSS feeds in a configuration file called rss_config.yaml. It parses the RSS feeds using RSS Title Extraction Tool, extracts all the titles and then selects top 5 trending titles based on their mentions through an LLM call.\n\nimport os\nimport json\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom typing import Annotated\nfrom langchain_groq import ChatGroq\nfrom langchain_core.messages import SystemMessage, ToolMessage\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.graph.message import add_messages\nfrom operator import add\nfrom IPython.display import Image\n\nfrom src.backend.tools import extract_titles_from_rss\nfrom src.backend.utils import load_yaml_config\nfrom src.backend.prompt_builder import build_prompt_from_config\nfrom src.backend.paths import APP_CONFIG_FPATH, PROMPT_CONFIG_FPATH, RSS_CONFIG_FPATH\nfrom src.backend.logger import logger\n\nload_dotenv()\n\nclass TopicState(BaseModel):\n    topics: Annotated[list[str], add] = []\n    messages: Annotated[list, add_messages]\n\nclass TopicFinder:\n    def __init__(self, groq_api_key: str):\n        # Load application configurations\n        app_config = load_yaml_config(APP_CONFIG_FPATH)\n        self.llm_model = app_config[\"llm\"]\n\n        # Load prompt configurations\n        prompt_config = load_yaml_config(PROMPT_CONFIG_FPATH)\n        self.topic_finder_prompt = prompt_config[\"topic_finder_agent_prompt\"]\n\n        rss_config = load_yaml_config(RSS_CONFIG_FPATH)\n        rss_feeds = rss_config[\"rss_feeds\"]\n        # Extract RSS feed URLs\n        self.rss_feed_urls = [feed[\"url\"] for feed in rss_feeds.values()]\n\n        self.llm = ChatGroq(api_key=groq_api_key, model_name=self.llm_model)\n\n        logger.info(\"[TopicFinder] Agent initialized\")\n\n    def get_tools(self):\n        \"\"\"\n        Create and return a list of tools that the agent can use.\n        Currently, it includes an RSS feed extraction tool for finding trending topics in AI.\n        \n        Returns:\n            list: A list of tools available to the agent.\n        \"\"\"\n        return [\n            # self.extract_titles_from_rss\n            extract_titles_from_rss\n        ]\n    \n    # The LLM node - where your agent thinks and decides\n    def search_topics(self, state: TopicState) -> dict:\n        \"\"\"\n        Search trending topics in AI.\n        This function uses RSS title extraction tool to find trending topics in AI and updates the state with the results.\n\n        Args:\n            state (TopicState): The current state of the topic finder, which will be updated with the found topics.\n\n        Returns:\n            dict: A dictionary containing the updated state with the found topics.\n        \"\"\"\n\n        # Use the LLM to find topics using the response from rss title extraction tool\n        logger.info(\"[TopicFinder] Calling LLM...\")\n\n        response = self.llm.invoke(state.messages)\n\n        # If tool instructions were included, send directly to tool node\n        if getattr(response, \"tool_calls\", None):\n            return {\"messages\": [response]}\n\n        # Final JSON processing\n        try:\n            # Convert response to a list of Topic objects\n            data = json.loads(response.content)\n            state.topics = [list[str](item.values())[0] for item in data[\"topics\"]]\n        except json.JSONDecodeError as e:\n            logger.error(\"[TopicFinder] Failed to parse JSON:\", e)\n            raise e\n\n        return {\"messages\": [response], \"topics\": state.topics}\n    \n    # The tools node - where the agent takes action\n    def tools_node(self, state: TopicState):\n        \"\"\"The agent's hands - executes the chosen tools.\"\"\"\n        tools = self.get_tools()\n        tool_registry = {tool.name: tool for tool in tools}\n        \n        last_message = state.messages[-1]\n        tool_messages = []\n\n        # Execute each tool the agent requested\n        for tool_call in last_message.tool_calls:\n            tool = tool_registry[tool_call[\"name\"]]\n            result = tool.invoke(tool_call[\"args\"])\n\n            # Print the tool call\n            logger.info(f\"[TopicFinder] Executing tool: {tool.name}\")\n            \n            # Send the result back to the agent\n            tool_messages.append(ToolMessage(\n                content=str(result),\n                tool_call_id=tool_call[\"id\"]\n            ))\n        \n        return {\"messages\": tool_messages}\n    \n    # Decision function - should we use tools or finish?\n    def should_continue(self, state: TopicState):\n        \"\"\"Decides whether to use tools or provide final answer.\"\"\"\n        last_message = state.messages[-1]\n        \n        if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n            return \"tools\"  # Agent wants to use tools\n        return END  # Agent is ready to respond\n\n    # Build graph\n    def build_topic_finder_graph(self) -> CompiledStateGraph:\n        logger.info(\"[TopicFinder] Building graph...\")\n        # Bind tools once\n        tools = self.get_tools()\n        self.llm = self.llm.bind_tools(tools)\n        # tool_node = ToolNode(tools)\n\n        workflow = StateGraph(TopicState)\n\n        # Register nodes\n        workflow.add_node(\"search_topics\", self.search_topics)\n        workflow.add_node(\"tools\", self.tools_node)\n        # workflow.add_node(\"tools\", tool_node)\n        \n        # Set entry point\n        workflow.set_entry_point(\"search_topics\")\n\n        # Add the flow logic\n        workflow.add_conditional_edges(\"search_topics\", self.should_continue, {\"tools\": \"tools\", END: END})\n        workflow.add_edge(\"tools\", \"search_topics\")  # After using tools, go back to thinking\n\n        return workflow.compile()\ndeep_researcher.py\n\nThis is the Deep Research Agent that searches for news relevant to the top trending AI topics from the provided RSS feeds. This agent runs in a conditional loop to extract news for all topics one by one. It selects the news based on semantic similarity using sentence transformers. The most relevant news to the topic are sorted, selected, and returned to the Orchestrator.\n\nimport os\nimport json\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom typing import Annotated\nfrom langchain_groq import ChatGroq\nfrom langchain_core.messages import SystemMessage, ToolMessage\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.graph.message import add_messages\nfrom operator import add\nfrom IPython.display import Image\n\nfrom src.backend.tools import extract_news_from_rss\nfrom src.backend.utils import load_yaml_config\nfrom src.backend.prompt_builder import build_prompt_from_config\nfrom src.backend.paths import APP_CONFIG_FPATH, PROMPT_CONFIG_FPATH, RSS_CONFIG_FPATH\nfrom src.backend.logger import logger\n\nload_dotenv()\n\nclass Article(BaseModel):\n    title: str\n    link: str\n    summary: str\n    content: str\n\nclass ResearchState(BaseModel):\n    news_articles: Annotated[list[Article], add] = []\n    topic: str\n    messages: Annotated[list, add_messages]\n\nclass DeepResearcher:\n    def __init__(self, groq_api_key: str):\n        # Load application configurations\n        app_config = load_yaml_config(APP_CONFIG_FPATH)\n        self.llm_model = app_config[\"llm\"]\n\n        # Load prompt configurations\n        prompt_config = load_yaml_config(PROMPT_CONFIG_FPATH)\n        self.deep_researcher_prompt = prompt_config[\"deep_researcher_agent_prompt\"]\n\n        rss_config = load_yaml_config(RSS_CONFIG_FPATH)\n        rss_feeds = rss_config[\"rss_feeds\"]\n        # Extract RSS feed URLs\n        self.rss_feed_urls = [feed[\"url\"] for feed in rss_feeds.values()]\n\n        self.llm = ChatGroq(api_key=groq_api_key, model_name=self.llm_model)\n\n        logger.info(\"[DeepResearcher] Agent initialized\")\n\n    def get_tools(self):\n        \"\"\"\n        Create and return a list of tools that the agent can use.\n        Currently, it includes an RSS feed extraction tool for finding trending topics in AI.\n        \n        Returns:\n            list: A list of tools available to the agent.\n        \"\"\"\n        return [\n            # self.extract_titles_from_rss\n            extract_news_from_rss\n        ]\n    \n    # The LLM node - where your agent thinks and decides\n    def search_news(self, state: ResearchState) -> dict:\n        \"\"\"\n        Search news articles for trending topics.\n        This function uses RSS news extraction tool to find news articles related to trending topics in AI and updates the state with the results.\n\n        Args:\n            state (ResearchState): The current state of the deep researcher, which will be updated with the found news articles.\n\n        Returns:\n            dict: A dictionary containing the updated state with the found news articles.\n        \"\"\"\n\n        # Use the LLM to find news articles using the response from rss news extraction tool\n        logger.info(\"[DeepResearcher] Calling LLM...\")\n        response = self.llm.invoke(state.messages)\n        \n        # If tool instructions were included, send directly to tool node\n        if getattr(response, \"tool_calls\", None):\n            return {\"messages\": [response]}\n        \n        # Return if the response is empty\n        if not response.content or response.content.strip() == \"\":\n            return {\"messages\": [response]}\n\n        # Final JSON processing\n        try:\n            # Convert response to a list of Topic objects\n            data = json.loads(response.content)\n            state.news_articles = [list[Article](item.values())[0] for item in data[\"articles\"]]\n        except json.JSONDecodeError as e:\n            logger.error(\"[DeepResearcher] Failed to parse JSON:\", e)\n            raise e\n\n        return {\"messages\": [response], \"news_articles\": state.news_articles}\n    \n    # The tools node - where the agent takes action\n    def tools_node(self, state: ResearchState):\n        \"\"\"The agent's hands - executes the chosen tools.\"\"\"\n        tools = self.get_tools()\n        tool_registry = {tool.name: tool for tool in tools}\n        \n        last_message = state.messages[-1]\n        tool_messages = []\n\n        # Execute each tool the agent requested\n        for tool_call in last_message.tool_calls:\n            tool = tool_registry[tool_call[\"name\"]]\n            result = tool.invoke(tool_call[\"args\"])\n\n            # Print the tool call\n            logger.info(f\"[DeepResearcher] Executing tool: {tool.name} for topic: {state.topic}\")\n            \n            # Send the result back to the agent\n            tool_messages.append(ToolMessage(\n                content=str(result),\n                tool_call_id=tool_call[\"id\"]\n            ))\n        \n        return {\"messages\": tool_messages}\n    \n    # Decision function - should we use tools or finish?\n    def should_continue(self, state: ResearchState):\n        \"\"\"Decides whether to use tools or provide final answer.\"\"\"\n        last_message = state.messages[-1]\n        \n        if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n            return \"tools\"  # Agent wants to use tools\n        return END  # Agent is ready to respond\n\n    # Build graph\n    def build_deep_researcher_graph(self) -> CompiledStateGraph:\n        logger.info(\"[DeepResearcher] Building graph...\")\n        # Bind tools once\n        tools = self.get_tools()\n        self.llm = self.llm.bind_tools(tools)\n        # tool_node = ToolNode(tools)\n\n        workflow = StateGraph(ResearchState)\n\n        # Register nodes\n        workflow.add_node(\"search_news\", self.search_news)\n        workflow.add_node(\"tools\", self.tools_node)\n        # workflow.add_node(\"tools\", tool_node)\n        \n        # Set entry point\n        workflow.set_entry_point(\"search_news\")\n\n        # Add the flow logic\n        workflow.add_conditional_edges(\"search_news\", self.should_continue, {\"tools\": \"tools\", END: END})\n        workflow.add_edge(\"tools\", \"search_news\")  # After using tools, go back to thinking\n\n        return workflow.compile()\nnewsletter_writer.py\n\nThis is the Newsletter Writer Agent that generates a structured newsletter in markdown format using the information extracted by the above two agents in the workflow. It summarizes the news articles and generates a formatted AI newsletter.\n\nimport os\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom typing import Annotated\nfrom langchain_groq import ChatGroq\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.graph.message import add_messages\nfrom IPython.display import Image\n\nfrom src.backend.agents.deep_researcher import Article\nfrom src.backend.utils import load_yaml_config\nfrom src.backend.prompt_builder import build_prompt_from_config\nfrom src.backend.paths import APP_CONFIG_FPATH, PROMPT_CONFIG_FPATH\nfrom src.backend.tools import save_newsletter\nfrom src.backend.logger import logger\n\nload_dotenv()\n\nclass News(BaseModel):\n    topic: str\n    news_articles: list[Article]\n\nclass NewsletterState(BaseModel):\n    news: list[News] = []\n    newsletter: str # Markdown string\n    messages: Annotated[list, add_messages]\n\nclass NewsletterWriter:\n    def __init__(self, groq_api_key: str):\n        # Load application configurations\n        app_config = load_yaml_config(APP_CONFIG_FPATH)\n        self.llm_model = app_config[\"llm\"]\n\n        # Load prompt configurations\n        prompt_config = load_yaml_config(PROMPT_CONFIG_FPATH)\n        self.newsletter_writer_prompt = prompt_config[\"newsletter_writer_agent_prompt\"]\n\n        self.llm = ChatGroq(api_key=groq_api_key, model_name=self.llm_model)\n\n        logger.info(\"[NewsletterWriter] Agent initialized\")\n    \n    # The LLM node - where your agent thinks and decides\n    def generate_newsletter(self, state: NewsletterState) -> dict:\n        \"\"\"\n        Generate a newsletter markdown.\n        This function uses news articles related to trending topics in AI to generate a newsletter and updates the state with the results.\n\n        Args:\n            state (NewsletterState): The current state of the newsletter writer, which will be updated with the generated newsletter.\n\n        Returns:\n            dict: A dictionary containing the updated state with the generated newsletter.\n        \"\"\"\n\n        # Use the LLM to find news articles using the response from rss news extraction tool\n        logger.info(\"[NewsletterWriter] Calling LLM...\")\n        logger.info(\"[NewsletterWriter] Generating newsletter...\")\n        newsletter = self.llm.invoke(state.messages)\n\n        state.newsletter = newsletter.content\n\n        return {\"newsletter\": state.newsletter}\n    \n    def save_newsletter(self, state: NewsletterState) -> None:\n        logger.info(\"[NewsletterWriter] Saving newsletter...\")\n        save_newsletter.invoke(state.newsletter)\n    \n    # Build graph\n    def build_newsletter_writer_graph(self) -> CompiledStateGraph:\n        logger.info(\"[NewsletterWriter] Building graph...\")\n        workflow = StateGraph(NewsletterState)\n\n        # Register nodes\n        workflow.add_node(\"generate_newsletter\", self.generate_newsletter)\n        workflow.add_node(\"save_newsletter\", self.save_newsletter)\n        \n        # Set entry point\n        workflow.set_entry_point(\"generate_newsletter\")\n\n        # Add the flow logic\n        workflow.add_edge(\"generate_newsletter\", \"save_newsletter\")\n        workflow.add_edge(\"save_newsletter\", END)\n\n        return workflow.compile()\nTools\ntools.py\n\nThese are the custom built tools used by different agents in the workflow. There are three tools in this file:\n\nRSS Title Extraction Tool - Used by the Topic Finder Agent\nRSS News Extraction Tool - Used by the Deep Research Agent\nNewsletter Saving Tool - Used by the main application.\nfrom langchain_core.tools import tool\nimport feedparser\nimport datetime\nimport os\nfrom src.backend.paths import OUTPUTS_DIR\n\nfrom sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")  # lightweight and fast\n\n@tool(\"extract_titles_from_rss\", return_direct=True)\ndef extract_titles_from_rss(urls: list[str]) -> list[str]:\n    \"\"\"Extracts titles from RSS feeds.\"\"\"\n    titles = []\n    for url in urls:\n        feed = feedparser.parse(url)\n        for entry in feed.entries:\n            if 'title' in entry:\n                titles.append(entry.title)\n    return titles\n\n@tool(\"extract_news_from_rss\", return_direct=True)\ndef extract_news_from_rss(feed_urls: list[str], topic: str, threshold: float = 0.5):\n    \"\"\"Extracts news articles from RSS feeds relevant to a single topic using embeddings.\"\"\"\n    topic_articles = []\n\n    topic_embedding = model.encode(topic, convert_to_tensor=True)\n\n    for url in feed_urls:\n        feed = feedparser.parse(url)\n        for entry in feed.entries:\n            title = entry.get('title', '')\n            link = entry.get('link', '')\n            summary = entry.get('summary', '') or entry.get('description', '')\n\n            raw_content = entry.get('content')\n            if isinstance(raw_content, list) and raw_content:\n                content = raw_content[0].get('value', '')\n            elif isinstance(raw_content, str):\n                content = raw_content\n            else:\n                content = ''\n\n            article_text = title + \" \" + summary + \" \" + content\n            article_embedding = model.encode(article_text, convert_to_tensor=True)\n\n            score = util.cos_sim(article_embedding, topic_embedding).item()\n\n            # Replace double quotes inside title, summary, and content with single quotes\n            title = title.replace('\"', \"'\")\n            summary = summary.replace('\"', \"'\")\n            content = content.replace('\"', \"'\")\n            \n            if score >= threshold:\n                topic_articles.append({\n                    \"title\": title,\n                    \"link\": link,\n                    \"summary\": summary,\n                    \"content\": content,\n                    \"similarity\": score\n                })\n\n    # Sort articles by similarity score\n    topic_articles.sort(key=lambda x: x[\"similarity\"], reverse=True)\n\n    # Select top 1 article based on similarity score - due to LLM rate limits\n    if len(topic_articles) > 1:\n        topic_articles = topic_articles[:1]\n\n    return topic_articles\n\n@tool(\"save_newsletter\", return_direct=True)\ndef save_newsletter(newsletter: str):\n    \"\"\"Saves a newsletter to a file.\"\"\"\n\n    # get current date and time\n    now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    news_dir = os.path.join(OUTPUTS_DIR, \"newsletters\")\n    os.makedirs(news_dir, exist_ok=True)\n\n    filename = f\"{news_dir}/newsletter_{now}.md\"\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(newsletter)\nSystem Prompts\nprompt_config.yaml\n\nThis is the prompt configuration file that includes system prompts for individual agents. Each prompt is carefully crafted using role, style or tone, instructions, output constraints, and output format guidelines.\n\n# System Prompt Configurations for AI Nexus Herald\ntopic_finder_agent_prompt:\n  description: \"Topic Finder Agent Prompt\"\n  role: |\n    You are an expert AI research assistant.\n    Your job is to find trending AI news topics using available tools.\n  style_or_tone: |\n    Informative, concise, and engaging.\n  instruction: |\n    Your task is to search for the latest trending topics in AI news and provide a list of 5 topics.\n    Call the RSS title extraction tool exactly once to extract the list of all available titles from RSS feeds using the provided URLs.\n    Given the list of titles, suggest 5 topics based on maximum number of mentions in the titles.\n    Each topic should be a brief phrase that captures the essence of the trend.\n  output_constraints: |\n    - Provide 5 unique topics.\n    - Each topic should be no more than 10 words.\n    - Ensure topics are relevant to current AI trends.\n    - Do NOT make up any topics.\n    - Return the topics in a structured JSON format.\n    - Do not include any additional text, backticks, or special characters around the JSON object.\n    - Do NOT include newlines or backslashes or illegal trailing commas in the JSON object.\n    - Important: Your response MUST NOT include anything else other than the JSON object.\n  output_format: |\n    - JSON format\n    - Example:\n    {\n      \"topics\": [\n        {\"1\": \"Topic One\"},\n        {\"2\": \"Topic Two\"},\n        {\"3\": \"Topic Three\"},\n        {\"4\": \"Topic Four\"},\n        {\"5\": \"Topic Five\"}\n      ]\n    }\n    \n\ndeep_researcher_agent_prompt:\n  description: \"Deep Researcher Agent Prompt\"\n  role: |\n    You are an expert AI researcher.\n    Your job is to find AI news articles related to given topics using available tool.\n  style_or_tone: |\n    Informative, concise, and engaging.    \n  instruction: |\n    Your task is to find AI news articles related to a given topic using the available tool.\n    Important: You MUST use the tool to find news articles related to the given topic from RSS feeds.\n    Call the RSS news extraction tool exactly once to extract news articles related to the given topic from RSS feeds.\n    Use threshold of 0.5 for similarity score in the tool call.\n    If the tool returns an empty list, return a list with a dummy dictionary.\n  output_constraints: |\n    Ensure news articles are relevant to given topics.\n    Do NOT make up any news articles.\n    Return the news articles returned by the tool in a structured JSON format.\n    Do NOT include any additional text, backticks, or special characters around the JSON object.\n    Do NOT include newlines or backslashes or illegal trailing commas in the JSON object.\n    Important: Your response MUST NOT include anything else other than the JSON object. \n  output_format: |\n    - Strict JSON format\n    - Remove any double quotes inside title, summary, and content\n    - Example of tool output:\n    {\n      \"articles\": [\n        {\"1\": {\"title\": \"Article One\", \"link\": \"https://example.com/article-one\", \"summary\": \"Summary One\", \"content\": \"Content One\"}},\n        {\"2\": {\"title\": \"Article Two\", \"link\": \"https://example.com/article-two\", \"summary\": \"Summary Two\", \"content\": \"Content Two\"}}\n      ]\n    }\n    - Example of dummy dictionary:\n    {\n      \"articles\": [\n        {\"1\": {\"title\": \"Dummy Title\", \"link\": \"https://example.com/dummy-article\", \"summary\": \"Dummy Summary\", \"content\": \"Dummy Content\"}}\n      ]\n    }\n\nnewsletter_writer_agent_prompt:\n  description: \"Newsletter Writer Agent Prompt\"\n  role: |\n    You are an expert AI newsletter writer.\n    Your job is to write a newsletter based on given AI news articles for topics.\n  style_or_tone: |\n    Informative, concise, and engaging.\n  instruction: |\n    Your task is to write a newsletter based on given AI news articles for topics.\n    Summarize each article in a maximum of 100 words using the summary and content provided.\n    Write a newsletter in structured markdown format with a maximum of 1000 words.\n    Include links to each article in the newsletter.\n  output_constraints: |\n    - Return only the newsletter as your response.\n    - Do not include any extra commentary or formatting in the newsletter.\n    - Format the newsletter in a structured markdown format taking care of spacing, indentation, and line breaks.\n    - Include appropriate emojis in the newsletter infront of each title.\n    - Do not add any extra characters inside or around the newsletter.\n    - Verify that the links to each article are correct.\n  output_format: |\n    - Structured Markdown format\n    - Example:\n    ## Topic One\n    ### [Article One](https://example.com/article-one)\n      Summarize the article here using the summary and content provided.\n    ###[Article Two](https://example.com/article-two)\n      Summarize the article here using the summary and content provided.\n    ## Topic Two\n    ### [Article Three](https://example.com/article-three)\n      Summarize the article here using the summary and content provided.\n    ### [Article Four](https://example.com/article-four)\n      Summarize the article here using the summary and content provided.\nEvaluation & Metrics\n\nEvaluating the performance of AI Nexus Herald is critical to ensuring the quality, relevance, and reliability of its generated newsletters. It is proposed to carry out evaluation using RAGAS, DeepEval, or similar frameworks using evaluation methodologies such as Rule-based Evaluation, LLM-as-a-Judge, and Golden Dataset. Following metrics can be taken into consideration while evaluating AI Nexus Herald:\n\nTask success rate\nResponse correctness\nRelevance\nLatency\nConsistency\n\nA proper evaluation plan must be created and implemented in order to make sure the system is reliable, secure, and robust.\n\nDemo\n\nHere is a demo of the multi-agent system showing the process of newsletter generation.\n\nChallenges\n1. Using Web Search Tools (API Rate Limits)\nChallenge - Reliance on public search APIs like DuckDuckGo or Exa led to frequent rate limit issues, which constrained the system's ability to retrieve consistent real-time information across multiple topics.\nSolution - Switched from real-time web search and high-volume APIs to curated RSS feeds, reducing dependency on rate-limited endpoints while maintaining up-to-date, topic-relevant information.\n2. Rate Limit Errors\nChallenge - Rate-limited endpoints often caused partial or failed research workflows, requiring retries and the addition of fallback mechanisms to ensure continuity.\nSolution - Restricted the RSS news extraction tool to extract two news articles per topic to avoid LLM rate limit errors.\n3. JSON Parse Errors\nChallenge - Inconsistent or malformed JSON responses from LLM outputs occasionally broke the workflow, necessitating robust error handling and system prompt configurations.\nSolution - Tweaked the system prompts and added intermediate validation layers to clean malformed JSON before passing data between agents or tools, improving fault tolerance in the workflow. I used various llama models available through the Groq API but they had a big problem with generating a JSON response, entered the latest model moonshotai/kimi-k2-instruct and it solved the problem like it never existed before and exceptionally improved the inference time as well.\n4. Different LLMs Producing Different Outputs\nChallenge - Switching between LLM models (e.g., LLaMA, Mistral) resulted in varied formatting, hallucination rates, and reliability, affecting downstream processing and requiring prompt tuning per model.\nSolution - Standardized response parsing using prompt-engineered format constraints to normalize outputs across various LLM models.\n5. Designing the Deep Researcher Mechanism\nChallenge - Implementing parallel research for multiple sub-queries demanded careful orchestration to avoid redundant API calls, ensure diversity in results, and manage execution time efficiently.\nSolution - Used conditional looping mechanism to get results for each topic using conditional edges in LangGraph. Executing parallel workflows is still a challenge.\n6. Workflow Breaks from Improper LLM Responses\nChallenge - Unpredictable or poorly structured responses from LLMs sometimes broke the LangGraph pipeline, pushing the need for structured outputs (e.g., JSON mode) and intermediate validation logic.\nSolution - This is still a challenge to be looked into and the most feasible method seems to be using structured outputs with LLM calls.\nKey Takeaways\n\n1. Vertical AI Agents\nDesigning agents with domain-specific vertical roles (e.g., Topic Finder, Deep Researcher, Newsletter Writer) enables clearer specialization, better prompt targeting, and modular debugging, which significantly improves system clarity and maintainability.\n\n2. Each Agent Must Have Its Unique Context\nIsolating each agent’s contextual scope via role-specific system prompts ensures focused behavior and avoids context confusion, especially when multiple agents process similar information concurrently.\n\n3. LLM Usage, Context Window, and Rate Limits\nEfficient use of LLMs requires understanding their context window limitations, batching strategies, and respecting API rate limits through fallback models, and output compression to stay within token boundaries.\n\n4. LLM Hallucinations\nHallucinated facts during summarization or research need to be addressed using multi-source cross-verification and structured prompting, reinforcing the importance of fact validation in autonomous research workflows.\n\n5. System Prompt\nCarefully engineered system prompts act as anchor points for agent behavior, defining role, tone, output format, and constraints, critical for achieving deterministic and interpretable results across agents.\n\n6. Context Engineering\nSuccessful orchestration heavily relies on context engineering, selectively crafting and injecting only the most relevant and concise information into the agent’s working memory to avoid prompt overload and ensure clarity of intent.\n\nLimitations\n\nDespite its modular design and effective multi-agent coordination, AI Nexus Herald faces several limitations.\n\nThe system still depends on external APIs for topic discovery and web search, making it vulnerable to rate limits, performance issues, and data inconsistencies.\nWhile prompt engineering and output validation reduce LLM hallucinations, they do not eliminate them entirely, especially when dealing with emerging or unverified information. Additionally, the lack of a formal evaluation framework limits the ability to benchmark the quality and factual accuracy of generated newsletters.\nFinally, the current implementation lacks user-level customization and multilingual support, which restricts its accessibility and personalization capabilities for a broader audience.\nDeployment Considerations\n\nDeploying AI Nexus Herald in a production environment requires careful planning around compute, memory, and integration dependencies.\n\nThe system leverages LLMs (e.g., LLaMA, Mistral via Groq API), which necessitate low-latency, high-availability API access and stable outbound connectivity.\nIt is essential to incorporate proper authentication and security mechanisms for a robust multi-agent system because integration with external APIs (e.g., web search, email delivery) requires secure API key management and retry mechanisms.\nMonitoring via LangSmith and error logging must be considered for scalability and robustness.\n\nThis project is a minimal prototype, so it is sufficient to create a GitHub repository with a proper README.md including setup instructions so that anyone may take it as a starting point and build further upon it. However, for a small scale deployment, building and deploying through Docker containerization is recommended.\n\nLicensing\n\nAI Nexus Herald is released under the MIT License, allowing free use, modification, and distribution of the software with proper attribution. Users are encouraged to build upon and adapt the system for their own applications.\n\nMaintenance & Support\nBasic Information\nCurrent Status: Stable prototype (MVP)\nRelease Date: 14 July 2025\nSupported Features: Multi-agent newsletter generation, RSS-based topic discovery, summarization, LangGraph-based orchestration.\nIssue Reporting Process\n\nIf you encounter bugs, unexpected behavior, or have suggestions:\n\nOpen an issue on the project’s GitHub repository under Issues.\nUse the bug, enhancement, or question labels to categorize your report.\nInclude relevant logs, screenshots, and steps to reproduce.\nFuture Enhancements\n\nDespite all the challenges this project brought, it can be a great starting point for future enhancements taking it to the next level.\n\nAdd real-time web search for topic discovery and research.\nEnforce structured outputs using JSON schemas.\nRun deep research agents in parallel for efficiency.\nMake the entire pipeline fully autonomous.\nStore generated newsletters in a database.\nAllow users to request newsletters on custom topics.\nAdd publishing capability to send the newsletter to a list of subscribers through email.\nCarry out proper system evaluation using frameworks like RAGAS and DeepEval and benchmark against proposed metrics.\nConclusion\n\nAutonomous multi-agent systems are the future. This project demonstrates the power of these agentic systems in real-world applications. By combining modular design, tool integration, and prompt engineering, AI Nexus Herald provides a scalable foundation for personalized, high-frequency content delivery in the AI domain. LangGraph as an orchestration framework is a solid choice for such systems. Future work aims to enhance autonomy, accuracy, and customization in order to build a robust multi-agent system.\n\nReferences\nAwesome ML AI RSS Feed\nLangGraph Documentation\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nPrerequisites\n\nMust-Have Prerequisites\n\nOptional Prerequisites\n\nAI Nexus Herald - Implementation\n\nHigh-Level Architecture\n\nAgents\n\nTools\n\nSystem Design\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "ai-research-assistant-using-rag-OBcHrpCicM8n",
    "username": "Etheal Sintayehu",
    "license": "MIT License",
    "title": "AI Research Assistant using RAG",
    "publication_description": "Back to publications\nOct 11, 2025\n●\n10 reads\n●\nMIT License\nAI Research Assistant using RAG\nAAIDC2025\nchat with research papers\nmulti-agent research papers\nresearch assistance\nEtheal Sintayehu\nLike\nBookmark\nShare\nAI Research Assistant: A RAG-Powered Solution to Information Overload for Researchers and Developers\n\n1. Introduction\n\nIn the rapidly expanding field of Artificial Intelligence, researchers and practitioners face an overwhelming volume of academic literature. Staying up-to-date is a significant challenge. This publication introduces a custom-built AI Research Assistant—a tool that leverages Retrieval-Augmented Generation (RAG) to provide concise, accurate, and source-cited answers from a personal library of research papers.\n\nThis project serves two primary purposes:\nFor Researchers & Students:\n\nImagine being able to \"talk\" to your entire collection of research papers. Instead of manually searching through dozens of PDFs, you can ask a direct question like, \"What are the key challenges in multi-agent reinforcement learning?\" and receive a synthesized answer with citations pointing to the exact source papers. This dramatically accelerates research and study.\n\nFor Developers & Engineers:\n\nThis project serves as a comprehensive, real-world blueprint for building a production-grade RAG application. It follows professional software engineering practices, including a clean architecture, configuration management, and a robust, intelligent user interface. It answers the question: \"How do I build a RAG system that is reliable, maintainable, and user-friendly?\"\n\nThis document details the system's architecture, implementation flow, and future potential, demonstrating a tool that is not only powerful but also trustworthy and easy to use.\n\n2. System Architecture\n\nThe assistant is built on a Retrieval-Augmented Generation (RAG) architecture, which enhances a Large Language Model (LLM) with an external, searchable knowledge base. This prevents the LLM from relying solely on its pre-trained (and potentially outdated) knowledge, resulting in more accurate and contextually relevant answers. The system is composed of two distinct pipelines.\n\n2.1 Ingestion Pipeline: Building the Knowledge Base\n\nThis offline process prepares the research papers to be searched. It indexes the knowledge so that relevant information can be found quickly.\n\nSourcing: The process begins by automatically downloading relevant research papers from ArXiv.\nMetadata Generation: A metadata.json file is created to map each unique PDF filename to the paper's full title, ensuring user-friendly citations.\nChunking & Embedding: Documents are split into small, overlapping text chunks, which are then converted into numerical vectors (embeddings) using a sentence-transformer model.\nStorage: The embeddings are stored in a FAISS vector store, which acts as the assistant's searchable long-term memory.\n2.2 Query Pipeline: Answering Questions\n\nThis real-time process occurs when a user asks a question. It is designed for both intelligence and efficiency.\n\nIntelligent Routing: The user's query is first analyzed by a Router Chain. This uses the LLM to classify the user's intent: is it a meta-question about the system's resources, or is it a research question for the RAG pipeline?\nRetrieval: For research questions, the system finds the most relevant text chunks from the FAISS knowledge base.\nAugmentation & Generation: The retrieved chunks are combined with the user's question into a detailed prompt. This augmented prompt is then sent to Google's gemini-1.0-pro model to generate a synthesized, in-text cited answer.\n\nhttps://youtu.be/5jO-ANWdUew\n\n3. Implementation Details: The Flow of a Query\n\nTo understand how the assistant works, let's trace the journey of a single question from user input to final answer.\n\nUser Input: The user types a question into the command-line or Gradio web interface.\nIntent Classification: The question is sent to the RAGPipeline's Router. The router makes its first, lightweight call to the Gemini LLM with a specific prompt, asking it to classify the question as either LIST_RESOURCES or RESEARCH_QUESTION.\nBranching: The RunnableBranch logic directs the flow based on the classification.\nIf LIST_RESOURCES: The system takes a shortcut. It reads the metadata.json file and instantly returns a formatted list of all available paper titles. No retrieval or second LLM call occurs.\nIf RESEARCH_QUESTION (Default Path): The main RAG process begins.\nVector Search (Retrieval): The user's question is passed to the FAISS retriever. The retriever converts the question into an embedding and performs a similarity search, finding the top 3 most relevant text chunks from the indexed documents.\nContext Augmentation: The retrieved document chunks are passed to a helper function. This function looks up each chunk's source filename in the metadata.map to find the full paper title. It then formats this information into a single, clean context block, with each piece of evidence clearly labeled with its source (e.g., Source: [Title of Paper]).\nPrompt Injection: This rich context block and the original user question are injected into the main RAG prompt template. This template contains critical instructions for the LLM on how to answer and cite sources.\nAnswer Generation: The final, augmented prompt is sent to the Gemini LLM in a second API call. The LLM synthesizes the information from the context and generates a human-readable answer with in-line citations, as instructed.\nOutput Display: The final answer and the list of unique source titles are returned to the user interface (CLI or Gradio) for a clean, verifiable presentation.\n4. Getting Started: Can I Use It?\n\nYes! The project is open-source under the MIT License, and you can get it running with a few simple steps.\n\n4.1 Prerequisites\nPython 3.10+\nA Google Gemini API Key\n## Definitive Getting Started Guide\n\nThis guide includes the crucial steps for creating a virtual environment and ensuring all dependencies are installed correctly.\n\nStep 1: Clone the Repository\n\nFirst, clone the project from your GitHub repository to your local machine. You need to replace <your-repository-url> with the actual URL from GitHub.\n\ngit clone <your-repository-url>\ncd Ai-Research-RAG-system\nStep 2: Create and Activate a Virtual Environment\n\nThis is a critical best practice. It creates an isolated environment for your project's dependencies.\n\n# For Windows\npython -m venv venv\n.\\venv\\Scripts\\activate\n\n# For macOS/Linux\npython3 -m venv venv\nsource venv/bin/activate\n\nYou should see (venv) appear at the beginning of your terminal prompt.\n\nStep 3: Install All Required Packages\n\nInstead of relying on a requirements.txt file that may not exist yet, let's install all the packages we've used directly. This is the most likely step that was failing.\n\npip install gradio langchain langchain-google-genai faiss-cpu sentence-transformers pypdf python-dotenv arxiv langchain-huggingface\n(Optional) Step 4: Create the requirements.txt File\n\nNow that all the packages are installed, you can create the requirements.txt file for future use with this one command:\n\npip freeze > requirements.txt\nStep 5: Set Up Your API Key\n\nCreate a new file named .env in the project root and add your Google Gemini API key to it.\n\nGOOGLE_API_KEY=\"your_api_key_here\"\n\nStep 6: Run the Application\n\nYou are now fully set up. Run the scripts in order.\n\nDownload the papers:\npython scripts/download_papers.py\nBuild the knowledge base:\npython ingest.py\nStart the assistant:\n# For the Gradio web interface\npython app.py\n\n# Or for the command-line interface\npython main.py\n\nIf you follow these more detailed steps and still encounter an issue, please copy and paste the full error message you see in the terminal. We will solve it immediately.\n\n5. Limitations and Future Development\n\nWhile robust, the assistant has several limitations that offer clear opportunities for future enhancements.\n\n5.1 Limitations\nStatic Knowledge Base: The assistant's knowledge is frozen at the time of the last data ingestion. It cannot access new papers or real-time information.\nRetrieval Dependency: The quality of every answer is entirely dependent on the retriever finding the correct context. If the user's phrasing doesn't semantically match the text, the retriever may fail to find the right information.\nNo Conversational Memory: The assistant treats every question as new. It cannot remember previous questions or answers to handle follow-up queries.\n5.2 Future Enhancements\nConversational Memory: Integrate a memory component (e.g., ConversationBufferMemory) to allow for multi-turn, contextual conversations.\nHybrid Search: Upgrade the retriever to use a hybrid approach, combining semantic (vector) search with traditional keyword search (e.g., BM25) to improve retrieval accuracy.\nRe-ranking Step: Add a cross-encoder model after the initial retrieval step to re-rank the retrieved documents for relevance before passing them to the LLM, further increasing accuracy.\nAutomated Data Refresh: Implement a scheduled workflow (e.g., a cron job) that periodically runs the download and ingest scripts to keep the knowledge base up-to-date with the latest research from ArXiv.\n6. Conclusion\n\nThis project successfully demonstrates the creation of a complete, end-to-end AI Research Assistant. By integrating an intelligent routing system with a robust RAG pipeline, the final application is not only functional but also efficient and user-friendly. It stands as both a practical tool for researchers and a professional blueprint for developers, showcasing how modern AI techniques can be applied to solve the real-world problem of information overload. The commitment to a clean architecture, detailed documentation, and user-centric features ensures that this project is a valuable asset for anyone looking to build or understand production-grade RAG systems.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAI Research Assistant: A RAG-Powered Solution to Information Overload for Researchers and Developers\n\nIntroduction\n\nThis project serves two primary purposes:\n\nFor Developers & Engineers:\n\n2. System Architecture\n\n2.1 Ingestion Pipeline: Building the Knowledge Base\n\n2.2 Query Pipeline: Answering Questions\n\n3. Implementation Details: The Flow of a Query\n\n4. Getting Started: Can I Use It?\n\n4.1 Prerequisites\n\nView all\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "ai-video-generator-a-multi-agent-system-for-automated-video-content-creation-G7yDBxE9Rec4",
    "username": "e@ezedinfedlu",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nAug 10, 2025\n●\n513 reads\n●\nMIT License\nAI Video Generator: A Multi-Agent System for Automated Video Content Creation\nE\n@ezedinfedlu\nLike\nBookmark\nShare\nAI Video Generator: A Multi-Agent System for Automated Video Content Creation\n\n\nTransforming ideas into compelling video content through intelligent automation\n\nAbstract\n\nThis publication presents a comprehensive multi-agent system designed for automated video generation from textual descriptions. The system employs four specialized AI agents with integrated database logging and human-in-the-loop feedback mechanisms to produce high-quality video content with synchronized audio narration.\n\n1. Introduction and Context\n1.1 The Challenge of Modern Video Creation\n\nIn today's digital landscape, video content has become the dominant medium for communication, education, and entertainment. However, traditional video content creation remains a complex and resource-intensive process that demands significant manual effort, specialized technical expertise, and substantial time investment. Content creators consistently encounter numerous obstacles throughout their production workflows.\n\nThe journey begins with storyboard development and scene planning, where creators must transform abstract ideas into concrete visual narratives. This process requires not only creative vision but also technical understanding of how individual scenes will connect to form a cohesive story. Following the planning phase, creators face the intricate challenge of audio-visual synchronization, ensuring that narration, background music, and visual elements work harmoniously together to deliver their intended message.\n\nQuality assurance presents another significant hurdle, as creators must iterate through multiple versions of their content, identifying areas for improvement and refining their work until it meets professional standards. Throughout this entire process, progress tracking and workflow management become increasingly complex, especially when working with teams or managing multiple projects simultaneously.\n\n1.2 Our Innovative Solution Approach\n\nRecognizing these persistent challenges in video content creation, we have developed a sophisticated multi-agent system that fundamentally transforms how videos are produced from textual descriptions. Our approach centers on the implementation of specialized AI agents, each designed to excel at distinct aspects of video creation while working collaboratively toward a unified goal.\n\nThe system incorporates comprehensive database logging capabilities that create detailed audit trails, enabling creators to track their progress, understand decision-making processes, and maintain complete visibility into their production workflows. Rather than replacing human creativity, our solution integrates strategic human feedback loops that enhance quality while preserving creative control, allowing creators to guide and refine the automated processes according to their vision.\n\nThis innovative framework enables truly automated yet controllable video generation workflows, where creators can input their ideas and receive professional-quality video content while maintaining the ability to intervene, adjust, and refine the output at crucial decision points throughout the production process.\n\n2. System Architecture\n\n🎬 Demo Video\n\nWatch the AI Video Generator in action:\n\nAt the heart of our solution lies a sophisticated multi-agent architecture that orchestrates five specialized AI agents, each contributing unique capabilities to the video creation process. The Scene Generator Agent serves as the creative foundation, intelligently transforming user input into structured video scenes that form the narrative backbone of the final product. Working alongside this creative engine, the Database Logger Agent ensures comprehensive persistent storage and meticulous progress tracking throughout the entire production pipeline.\n\nQuality assurance comes through the Scene Critic Agent, which provides automated assessment and improvement suggestions, acting as an intelligent reviewer that identifies opportunities for enhancement before content moves to production. The Audio Agent brings the visual narrative to life by generating synchronized narration and audio content that perfectly complements the visual elements. Finally, the Video Agent serves as the master conductor, synthesizing all visual and audio elements into the final video output that represents the culmination of the collaborative agent workflow.\n\n2.1 Intelligent Database Integration\n\nOur system's robustness stems from its sophisticated database integration layer, which includes custom tools specifically designed for comprehensive workflow logging and optimization. The save_scene_progress function ensures that both raw and improved scenes are persistently stored in the database, creating a complete historical record of the creative evolution process. Meanwhile, the log_progress_event tool captures detailed progress events for each workflow step, providing granular visibility into the system's operation and enabling precise monitoring of production timelines.\n\nThe system's intelligence is further enhanced through the get_video_context function, which retrieves contextual information that enables more sophisticated and relevant scene generation. Additionally, the search_similar_videos capability identifies related content for creative inspiration, helping the system learn from existing successful patterns while maintaining originality in new productions.\n\nThis comprehensive approach ensures that all progress is automatically tracked in the database, providing complete audit trails that not only enable recovery from failures but also facilitate continuous improvement of the system's capabilities through analysis of successful production patterns.\n\nProduction-Readiness Enhancements\n\nTo make the platform production-ready, we hardened configuration, improved API reliability, and engineered resilience across the stack. Configuration is centralized in backend/config.py and initialized through an application factory that promotes clean startup in both development and test environments. All critical directories—output_dir, temp_dir, and data_dir—resolve to absolute, sandboxed paths under the backend root to prevent path drift across hosts and containers. The database connection is intentionally decoupled from file storage locations, defaulting to SQLite paths suitable for containers or local runs, while CORS policies are constrained by environment and configuration to only allow trusted origins. For development and CI, a mock_mode switch disables external API calls and substitutes deterministic behaviors, enabling fast, repeatable test runs.\n\nOn the API surface, request payloads are validated using Pydantic models that enforce strict bounds on titles, descriptions, and user input lengths. File downloads are protected by canonical path checks to ensure that only files within the configured output_dir can be served; the same endpoint supports inline playback for a better user experience. A health endpoint verifies database connectivity and filesystem writability, while a lightweight metrics endpoint exposes high-level counts by video status, providing a quick operational snapshot without external dependencies.\n\nResilience is addressed through explicit retry strategies with exponential backoff for external LLM calls, graceful fallbacks in mock mode for audio and video processing, and deterministic filenames based on time.time_ns() to avoid collisions. Duration reporting prioritizes reading the actual rendered video file; when unavailable, the system estimates from scene timings, keeping responses informative even when media tools are absent. The orchestration layer, built with LangGraph, incorporates a critique loop and approval gate so that only improved scenes progress downstream. Throughout processing, progress is persisted to the database and streamed via SSE, enabling real-time monitoring and robust recovery.\n\nSafety, Security, and Guardrails\n\nSafety is implemented in layers. At the prompt level, each agent is guided by safety system messages and explicit output constraints defined in backend/config/prompt_config.yaml. These instructions resist prompt injection and constrain outputs to policy-compliant formats. At request time, an optional moderation step calls _llm_moderation_flagged() which integrates the omni-moderation-latest model and applies a configurable threshold. This check can be disabled in tests or mock runs for determinism; even when disabled, downstream agents still sanitize outputs by removing URLs, emails, code fences, and by normalizing whitespace.\n\nOn the platform side, SQLAlchemy parameterization guards against injection, CORS policies restrict cross-origin access, and file serving enforces strict containment checks to mitigate path traversal risks. Secrets and environment-dependent values are provided via .env so that sensitive configuration never needs to be embedded directly in the codebase.\n\nTesting Strategy and Coverage\n\nTesting emphasizes breadth, determinism, and operational realism. Unit tests verify agent utilities, validation logic, and database helpers. Integration tests exercise the API surface, including boundary validations, secure download behavior, moderation pathways, database operations, and the Server-Sent Events stream for progress updates. End-to-end tests execute the full multi-agent workflow in mock mode, ensuring that orchestration, persistence, and status reporting function coherently without external dependencies.\n\nExternal services such as OpenAI and MoviePy are mocked where appropriate to remove flakiness and ensure reproducible results. For moderation, tests directly stub the _llm_moderation_flagged function to drive both allow and block outcomes deterministically. The suite runs under pytest with pytest-cov, and coverage is scoped to application modules by configuration. Recent runs report approximately 75% line coverage across backend modules, reflecting a comprehensive safety net for critical code paths.\n\nUser Interface and Experience\n\nThe interface is designed to feel fast, informative, and polished. The library presents generated videos as interactive cards that display muted, looping previews with a prominent, centered play affordance. Clicking a card opens a modal player for focused viewing, while the creation flow includes an inline player in the final step that streams from the backend’s inline download endpoint. Throughout the process, a progress indicator subscribes to SSE updates to reflect the system’s current stage, offering responsive feedback without manual refreshes.\n\nScreenshots:\n\nLibrary view with clickable cards, live previews, and quick access to recent outputs.\n\nGuided creation flow showing step-by-step progress and inline playback of the generated video.\n\nModal player with focused viewing experience and clean controls optimized for review.\n\nFailure Handling, Monitoring, and Operations\n\nOperational reliability is anchored by layered failure handling and simple, actionable observability. Transient failures against external services are retried with exponential backoff, while mock fallbacks ensure the system remains usable during outages or in constrained environments. Because progress events and workflow artifacts are persisted, operators can reconstruct state, diagnose issues, and resume work without guesswork.\n\nHealth checks validate database connectivity and directory writability to catch misconfigurations early, and the metrics endpoint summarizes video statuses for rapid triage and simple dashboards. Logs include structured progress messages and moderation outcomes, creating a clear operational narrative suitable for analysis and alerting. Absolute path resolution eliminates surprises between host and container filesystems, and startup ensures required directories are present and writable. CORS and environment-based configuration round out a secure default posture with minimal setup overhead.\n\nDeployment\n\nThe project ships with Docker Compose definitions for a straightforward deployment of both backend (Flask) and frontend (Next.js). Image builds leverage caching for faster iterations, and volumes persist outputs and instance data across restarts. Environment variables—OPENAI_API_KEY, DATABASE_URL, and FLASK_ENV—are managed via a backend .env, keeping secrets out of source control and making environment promotion predictable. The API is stateless and suitable for horizontal scaling behind a load balancer; persistent state is isolated in the database and object storage (outputs/), which can be mapped to cloud services or shared volumes in production.\n\n3. Human-in-the-Loop Integration\n3.1 Seamless User Collaboration Throughout the Creative Process\n\nOur system recognizes that the most compelling video content emerges from the synergy between artificial intelligence capabilities and human creativity. Rather than replacing human input, we have designed multiple strategic interaction points that amplify creative vision while leveraging AI efficiency.\n\nThe creative journey begins at the Initial Input Stage, where users provide natural language descriptions of their desired video content, transforming abstract ideas into concrete project specifications. Users can enhance their input by specifying optional technical parameters such as duration, style, and tone, giving them precise control over the final output characteristics. The system also accommodates reference materials, allowing users to upload supporting documents or media that provide additional context and inspiration for the AI agents.\n\nDuring the Intermediate Review Points, the collaborative process deepens through scene review capabilities that enable users to examine and modify generated scenes before audio creation begins. This critical checkpoint prevents downstream issues and ensures content alignment with the creator's vision. The integrated rating system captures quality feedback and relevance assessments, while iterative refinement cycles allow for multiple revision rounds based on user feedback, ensuring that each element meets the creator's standards.\n\nThe Final Approval Stage provides comprehensive control over the finished product through preview generation, allowing users to experience the complete video before final rendering commits significant computational resources. Users can submit specific modification requests for individual scenes, and the flexible export system accommodates user-defined output formats and quality settings to match their distribution requirements.\n\n3.2 Intelligent Feedback and Monitoring Systems\n\nOur feedback mechanisms create a transparent and responsive environment where users maintain visibility and control throughout the production process. Real-time progress monitoring through an intuitive web interface displays the current processing stage, while detailed progress indicators provide granular insight into each agent's work progress. The system proactively communicates through immediate error notifications, ensuring users stay informed about any processing issues that require attention.\n\nThe quality assurance framework operates through sophisticated loops that combine automated intelligence with human oversight. The Scene Critic Agent continuously provides improvement suggestions based on content analysis, while human override capabilities ensure users can accept, reject, or modify agent recommendations according to their creative judgment. This collaborative approach enables continuous learning, as the system analyzes user preferences and feedback patterns to improve future recommendations and outputs.\n\nCollaborative enhancement features support complex production workflows through comprehensive version control that preserves multiple iterations alongside user annotations. The integrated comment system enables timestamped feedback on specific scenes or elements, facilitating detailed communication between team members. For organizations requiring formal review processes, multi-stage approval workflows accommodate complex project requirements while maintaining the system's efficiency and user-friendly operation.\n\n4. Installation and Usage Instructions\n4.1 System Requirements\nDocker Engine 20.10+\nDocker Compose 2.0+\n8GB RAM minimum (16GB recommended)\nOpenAI API key with GPT-4 access\n4.2 Installation Process\n4.2.1 Repository Setup\n# Clone the repository\ngit clone https://github.com/ezedinff/AAIDC.git\ncd AAIDC/project-2\n\n# Create required directories\nmkdir -p data outputs temp backend/instance\n4.2.2 Environment Configuration\n# Create environment file\ncp backend/env.example backend/.env\n\n# Configure API credentials\necho \"OPENAI_API_KEY=your_openai_api_key_here\" >> backend/.env\necho \"FLASK_ENV=production\" >> backend/.env\necho \"DATABASE_URL=sqlite:///instance/video_generator.db\" >> backend/.env\n4.2.3 Docker Deployment\n# Build and start all services\ndocker-compose up --build -d\n\n# Verify services are running\ndocker-compose ps\n4.3 Usage Instructions\n4.3.1 Web Interface Access\n# Frontend application\nhttp://localhost:3000\n\n# Backend API documentation\nhttp://localhost:5000/api/docs\n4.3.2 API Usage Examples\n\nCreate New Video Project:\n\ncurl -X POST http://localhost:5000/api/videos \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"title\": \"Educational Content\",\n    \"description\": \"Tutorial video about machine learning\",\n    \"user_input\": \"Create a 3-minute video explaining neural networks\",\n    \"style\": \"educational\",\n    \"duration\": 180\n  }'\n\nMonitor Progress:\n\n# Get real-time progress updates\ncurl http://localhost:5000/api/videos/{video_id}/progress\n\n# Subscribe to Server-Sent Events\ncurl -N http://localhost:5000/api/videos/{video_id}/events\n\nProvide Feedback:\n\ncurl -X POST http://localhost:5000/api/videos/{video_id}/feedback \\\n  -H \"Content-Type\": \"application/json\" \\\n  -d '{\n    \"scene_id\": \"scene_1\",\n    \"rating\": 4,\n    \"comments\": \"Excellent content, minor timing adjustment needed\",\n    \"suggestions\": [\"Slow down narration\", \"Add visual examples\"]\n  }'\n4.4 Advanced Configuration\n4.4.1 Agent Customization\n# config/config.yaml\nagents:\n  scene_generator:\n    model: \"gpt-4\"\n    temperature: 0.7\n    max_scenes: 10\n  \n  scene_critic:\n    model: \"gpt-4\"\n    evaluation_criteria: [\"relevance\", \"clarity\", \"engagement\"]\n    \n  audio_agent:\n    voice_model: \"tts-1\"\n    voice: \"alloy\"\n    speed: 1.0\n4.4.2 Database Configuration\n# config/config.yaml\ndatabase:\n  logging_level: \"detailed\"\n  retention_days: 30\n  backup_interval: \"daily\"\n  audit_trail: true\n5. Technical Support and Troubleshooting\n5.1 Comprehensive Problem Resolution Guide\n\nUnderstanding that complex AI systems can present various challenges during deployment and operation, we have developed comprehensive troubleshooting guidance based on common user experiences and system behavior patterns.\n\nInstallation challenges typically manifest in three primary areas that users should address systematically. Docker build failures often indicate version compatibility issues, requiring verification that your Docker installation meets the minimum version requirements specified in our documentation. Port conflicts represent another frequent obstacle, particularly when ports 3000 and 5000 are already occupied by other services on your system. Additionally, permission issues can prevent proper system initialization, making it essential to ensure that all directories have appropriate read and write permissions for the Docker containers.\n\nRuntime complications emerge primarily from resource constraints and external service interactions. API rate limits from OpenAI services can interrupt video generation workflows, making it crucial to implement exponential backoff strategies for API calls to maintain system stability. Memory usage monitoring becomes essential for sustained operation, as Docker containers must have adequate resources allocated to handle the computational demands of multi-agent video generation. Database locks can also occur during concurrent access scenarios, requiring careful monitoring of database connection patterns and potential conflicts.\n\nQuality-related issues in generated content often trace back to input specifications and system configuration. Poor scene generation typically results from unclear or insufficiently specific input descriptions, emphasizing the importance of providing detailed, well-structured content briefs. Audio synchronization problems usually stem from parameter misalignment in the audio generation configuration, while rendering failures commonly indicate output directory permission restrictions or insufficient disk space for the final video files.\n\n5.2 Multi-Tiered Support Ecosystem\n\nOur support infrastructure provides multiple pathways for assistance, ensuring that users can access appropriate help regardless of their technical expertise level or project requirements.\n\nComprehensive documentation forms the foundation of our support ecosystem, with detailed API reference materials available directly through the /api/docs endpoint for immediate access during development work. Configuration guidance is thoroughly documented in the config/README.md file, providing step-by-step instructions for system customization. Development teams can access complete setup instructions through the CONTRIBUTING.md documentation, which covers everything from initial environment preparation to advanced customization techniques.\n\nCommunity-driven support channels foster collaborative problem-solving and knowledge sharing among users. The GitHub Issues platform serves as the primary venue for submitting bug reports and feature requests, where community members and maintainers collaborate to resolve challenges and enhance system capabilities. Our GitHub Discussions forum provides a welcoming space for questions, best practice sharing, and community-driven troubleshooting sessions. The comprehensive wiki documentation offers detailed explanations, tutorials, and advanced configuration examples contributed by both maintainers and experienced community members.\n\nProfessional support services address enterprise requirements and specialized implementation needs. Commercial licensing options provide enhanced support and service level agreements for enterprise deployments requiring dedicated assistance. Custom development services enable specialized agent development and system modifications tailored to specific organizational requirements. Training services offer implementation and optimization consulting, helping teams maximize their investment in the video generation platform while developing internal expertise for ongoing operations and maintenance.\n\n6. Licensing and Legal Information\n6.1 Software License\n\nThis project is licensed under the MIT License. See the \nLICENSE\n file for the complete license text.\n\n6.2 Third-Party Dependencies and Compliance\n\nOur system builds upon a carefully selected foundation of established open-source technologies and commercial services, each governed by their respective licensing terms. The OpenAI API integration operates under OpenAI's Terms of Service, ensuring compliance with their usage policies and rate limiting requirements. Docker containerization technology operates under the Apache License 2.0, providing robust deployment capabilities with clear licensing obligations. The Next.js frontend framework and Flask backend framework both operate under permissive licenses (MIT and BSD-3-Clause respectively), enabling flexible usage and modification while maintaining compliance with their attribution requirements.\n\n6.3 Data Privacy and Security Framework\n\nData privacy and security represent fundamental priorities in our system design, with comprehensive policies governing how user information and generated content are handled throughout the video creation process. All user inputs and generated content remain the exclusive property of the user, ensuring that creators maintain complete ownership and control over their intellectual property. API interactions with external services are governed by the respective service provider terms, with clear documentation of what data is transmitted and how it is processed.\n\nCore video generation processing occurs locally within your deployment environment, minimizing external data exposure and providing enhanced security for sensitive content creation workflows. The system implements configurable retention policies for generated content, allowing organizations to establish data lifecycle management practices that align with their compliance requirements and storage preferences.\n\n6.4 Commercial Usage and Enterprise Considerations\n\nThe open-source nature of our platform enables both non-commercial and commercial usage without licensing fees, making it accessible to individual creators, educational institutions, and commercial organizations alike. Attribution requirements for derivative works ensure proper recognition of the original contributors while enabling customization and extension for specific use cases.\n\nEnterprise support services are available under separate commercial agreements for organizations requiring dedicated assistance, service level agreements, or custom development work. Standard MIT license limitations apply to the core platform, providing clarity on liability considerations while maintaining the flexibility that makes open-source adoption attractive for commercial deployments.\n\n7. Broader Significance and Future Implications\n7.1 Transforming the Landscape of Content Creation\n\nOur multi-agent video generation system represents a paradigm shift in how we approach automated content creation, with implications that extend far beyond traditional video production workflows. This work demonstrates that complex creative processes can be effectively decomposed into specialized, collaborative AI agents while maintaining quality standards and creative control that meet professional requirements.\n\nThe significance of this approach lies in its potential to democratize high-quality video content creation, making sophisticated production capabilities accessible to educators, small businesses, content creators, and researchers who previously lacked the resources or technical expertise for professional video production. By reducing the barrier to entry for video content creation, this system enables new forms of educational content, training materials, and creative expression that were previously constrained by resource limitations.\n\n7.2 Advancing Multi-Agent AI Systems Research\n\nFrom a research perspective, this work contributes valuable insights to the growing field of multi-agent AI systems, particularly in creative and content generation domains. The integration of specialized agents with distinct responsibilities—scene generation, quality assessment, audio synthesis, and video compilation—demonstrates effective task decomposition strategies that could inform future multi-agent architectures across various creative and technical domains.\n\nThe human-in-the-loop integration patterns we've developed provide a blueprint for maintaining human agency and creative control in AI-driven workflows, addressing critical concerns about automation in creative industries. This balanced approach between automation efficiency and human oversight offers a model for implementing AI assistance in other creative fields such as writing, graphic design, and interactive media development.\n\n7.3 Implications for Educational Technology and Knowledge Transfer\n\nThe automated video generation capabilities present transformative opportunities for educational technology and knowledge transfer initiatives. Academic institutions can leverage this system to rapidly produce course materials, enabling faculty to focus on curriculum development rather than technical video production. The system's ability to transform textual content into engaging visual narratives has particular relevance for online education, where video content significantly improves learning outcomes and student engagement.\n\nCorporate training and knowledge management applications represent another significant opportunity, where organizations can efficiently convert documentation, procedures, and training materials into accessible video formats. This capability becomes increasingly valuable as remote work patterns create demand for scalable, engaging training content that can be produced efficiently and updated regularly.\n\n7.4 Future Research Directions and Applications\n\nThis work opens several promising avenues for future research and development. Adaptive Learning Integration represents one compelling direction, where the system could learn from user feedback patterns to automatically improve content generation quality and better align with individual creator preferences and audience requirements.\n\nCross-Modal Content Generation presents opportunities to extend the multi-agent approach beyond video, incorporating additional media types such as interactive presentations, augmented reality experiences, and adaptive content that responds to viewer engagement patterns. The modular architecture we've developed provides a foundation for integrating additional content generation capabilities.\n\nIndustry-Specific Customization offers practical research directions, where specialized agent configurations could be developed for specific domains such as medical education, technical documentation, marketing content, or scientific communication. Each domain brings unique requirements for accuracy, style, and presentation that could benefit from targeted agent specialization.\n\nCollaborative Content Creation systems could build upon our human-in-the-loop patterns to enable multiple users to collaboratively guide and refine automated content generation, supporting team-based creative workflows and distributed content development processes.\n\n7.5 Societal and Economic Impact Considerations\n\nThe broader deployment of automated video generation technology carries significant implications for creative industries and content creation labor markets. While our system is designed to augment rather than replace human creativity, the efficiency gains it provides could reshape content production economics and workflow patterns across multiple industries.\n\nUnderstanding and addressing these implications requires ongoing dialogue between technologists, content creators, and industry stakeholders to ensure that automation benefits enhance rather than disrupt existing creative ecosystems. The open-source nature of our platform enables community-driven development and adaptation, fostering innovation while maintaining accessibility for diverse users and use cases.\n\nOur work contributes to the broader conversation about responsible AI development in creative domains, demonstrating approaches that preserve human agency while leveraging AI capabilities to enhance creative productivity and accessibility.\n\n8. Conclusion\n\nThis multi-agent video generation system demonstrates the effective integration of specialized AI agents with human-in-the-loop feedback mechanisms. The comprehensive database logging, real-time progress monitoring, and quality assurance loops provide a robust foundation for automated yet controllable video content creation.\n\nThe system's modular architecture enables easy customization and extension, while the detailed installation and usage instructions ensure accessibility for both researchers and practitioners in the field of automated content generation. More importantly, this work establishes a framework for responsible AI integration in creative workflows that preserves human agency while democratizing access to professional-quality content creation capabilities.\n\nCitation: If you use this system in academic research, please cite as:\n\nAAIDC Project Contributors. (2024). AI Video Generator: A Multi-Agent System for Automated Video Content Creation. GitHub. https://github.com/ezedinff/AAIDC/tree/main/project-2\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "askpert-expert-at-your-fingertips-gPOH7nTVMekt",
    "username": "uUnwita",
    "license": "Iteration 16: Conducted a Top K evaluation to determine the optimal K value for licensed models.",
    "title": "",
    "publication_description": "Back to publications\nOct 31, 2024\n●\n75 reads\nAskPert - Expert at your fingertips\nComputer Aided Engineering\nKnowledge retention\nMultimodal model\nRAG\nU\nUnwita\nLike\nBookmark\nShare\n\nIntroduction\n\nKnowledge retention is a challenge for organizations across various sectors. The loss of intellectual capital when employees leave without an effective knowledge transfer leads to inefficient operations and disrupts business continuity.\n\nThe traditional model of knowledge management has created data silos within the organizations and leads to:\n\nLack of collaboration between multi-disciplinary teams\nInadequate mentoring for less experienced employees\nNon-productive time searching for information\nHigh employee training costs\nData integration challenges for wholistic insights\n\nA Retrieval Augmented Generation (RAG) based application has been developed in collaboration with a Computer Aided Engineering (CAE) services company to utilize data from that domain. This publication gives an overview of the approach, testing methodology, results and insights based on extensive testing and integration of the RAG components. The application has been validated by domain experts and is able to give domain specific contextual and relevant responses with good accuracy.\n\nMethodology\n\nThe knowledge base encompasses CAE domain specific proprietary documents, technical manuals, online lecture transcripts, official websites, expert interviews, channel conversations and Frequently Asked Questions (FAQs). The data is stored in a vector database, where it is indexed and made searchable for real-time retrieval during query operations. The system relies on a three-step process:\n\nQuery Creation: User initiates a query using the Large Language Model.\nData Retrieval: The system retrieves relevant documents from the vector database using semantic similarity search powered by embeddings.\nResponse Generation: The retrieved data is combined with the original user query to generate a contextual and relevant response.\nInfrastructure Requirements:\n\nThe following hardware specifications were utilized for the development:\n\nOS – Windows Server 2022 - 64-bit\nUbuntu 22.04.5 LTS (GNU/Linux 5.15.153.1-microsoft-standard-WSL2 x86_64) \nProcessor – Intel(R) @ Xeon(R) Silver 4114 CPU @ 2.20 GHz 2.19 GHz (2 processors)\nRAM – 128 GB\nDisk Capacity – 1 TB\nGPU - RTX A4000 – 16 GB\n\nAdditional development cost includes the API key pricing for OpenAI and Claude.\n\nRAG Components\nVector Database\n\nThe vector database serves as the core component for managing unstructured data, transforming domain-specific documents into vector embeddings. This enables advanced semantic similarity search, ensuring relevant documents are retrieved efficiently. Milvus and Pinecone were tested due to their scalability and efficiency in managing high-dimensional data vectors. Additionally, they integrate seamlessly with existing Machine Learning pipelines, streamlining deployment in production environments.\n\nEmbedding Model\n\nEmbedding models are central to transforming documents into dense, multi-dimensional vector representations. These embeddings capture the semantic meaning of the documents, allowing the system to perform effective searches based on user queries. Embedding models like BERT and GTE are used to map text data into a searchable vector space. Snowflake and GTE embedding models were tested extensively and GTE was chosen for pipeline development.\n\nLarge Language Model (LLM)\n\nThe LLMs such as GPT, Mistral, or Vicuna, are used to generate responses. The model processes both the query and the retrieved documents, integrating domain-specific knowledge with the user’s query to generate a reasonably accurate response. The combination of these components allows for a dynamic system where real-time information retrieval is integrated with pre-trained knowledge.\n\nPipelines Evaluated:\n\nThe following pipelines were evaluated to arrive at the right combination for CAE specific information:\n\nLangchain - Milvus – Mistral 7B\nLangchain - Milvus – OpenAI GPT 3.5 Turbo\nLangchain - Milvus – OpenAI GPT 4o\nLangchain - Milvus – OpenAI GPT 4o mini\nLangchain - Milvus – Claude 3.5 Sonnet\nLangchain - Milvus – LlaMA 3.1 - 8B\nLangchain - Milvus – LlaMA 3.1 - 70B\nLangchain - Milvus – LlaVA 7B (Mistral)\nLangchain - Milvus – LlaVA 7B (Vicuna)\nLangchain – Milvus – LlaVA 13B (Vicuna)\nLangchain – Milvus – LlaVA 34B\nLangchain – Milvus – LlaVA-Llama 3\nData Loading System\n\nThe data loading system handles the ingestion and processing of documents into the vector database. It involves a step-by-step process that transforms raw data into searchable vectors:\n\nText and Table processing\nDocuments are parsed, and text is chunked for embedding. Chunking is a pre-processing technique that involves dividing texts into smaller segments known as \"chunks.\" Each chunk is processed through the embedding model to generate vector representations.\nVector representations: The mathematical encoding, including words, sentences, photos, or any other sort of data, into numerical vectors in a multi-dimensional space is known as a vector representation.\n\nA sentence can be transformed into a vector using an embedding model. The code along with its output that demonstrates this conversion into a vector is shown below.\n\n\nThe vector representations of each chunk will be stored along with metadata in the vector database.\n\nImage processing\n\nImages are extracted and sent to vision LLMs for generating descriptions. These descriptions are converted into vectors and stored in the database alongside the images. This structured approach ensures that all relevant data is searchable, allowing the AI assistant to retrieve and process information from diverse sources.\n\nThe workflow is shown in the figure below.\n\n\nRetrieving System and Generation System\n\nThe LLM is used initially to process user queries, rephrasing them for best retrieval. An embedding model is used to convert the rephrased question into a vector representation, which makes it easier to find relevant text chunks in the Milvus vector database. The question can now be represented in a high-dimensional space, where each dimension represents distinct semantic aspects of the text.\n\nCosine similarity is used as the main comparison metric to find relevant chunks. An effective way to assess the degree of similarity between the vectors of text chunks and the vector representations of the question is to use cosine similarity, which calculates the cosine of the angle between the two. The associated text chunk and the query have a closer association when the cosine similarity score is higher, indicating that the chunk is probably relevant to the query. The cosine similarity between two closely related sentences is displayed below.\n\nAt the same time the system looks for any image IDs added in the meta data that would necessitate fetching related images from MongoDB. To generate a comprehensive final response, the system combines the rephrased query with a prompt template that guides the LLM to formulate an answer using the retrieved content, including both text chunks and relevant images.\n\nThe workflow is shown in the figure below.\n\n\nPipeline Evaluation\n\nThe system was evaluated through a series of structured experiments. The primary goal was to determine the best combination of vector databases, embedding models, and large language models (LLMs) for providing contextual, accurate and relevant responses. The experiments focused on tuning the following parameters to optimize the pipeline:\n\nTop K Value:\n\nThe Top K parameter controls the number of most relevant documents retrieved from the vector database for any given query. We experimented with different Top K values (e.g., 5, 10, 50) to balance retrieval relevance and computational efficiency.\n\nSimilarity Threshold:\n\nThe similarity threshold determines how closely the retrieved documents match the query in semantic space. This was evaluated by adjusting cosine similarity thresholds (e.g., 30%, 60%) to find the optimal balance between retrieval accuracy and the quality of the generated response.\n\nIndex Type:\n\nDifferent vector database index types were tested (IVF-FLAT, HNSW) to compare retrieval speeds and accuracy. IVF-FLAT provided a good trade-off between speed and similarity score, while HNSW (Hierarchical Navigable Small World) offered higher accuracy at the cost of speed.\n\nOther iterations tested are listed below:\n\nIteration 1: Evaluated retrieval accuracy between Mistral and GPT 3.5 Turbo using different index types.\nIteration 2: Compared performance of GPT 4o vs Claude 3.5 Sonnet for index optimization.\nIteration 3: Assessed the performance of LLaMA 3.1 across various retrieval and generation tasks.\nIteration 4: Tested the default LangChain framework to establish a baseline for pipeline efficiency.\nIteration 5: Conducted an initial evaluation of Pinecone as an alternative vector database solution.\nIteration 6: Performed text-based evaluations on Vision models to assess their understanding of image-related tasks.\nIteration 7: Tested the performance of new loaders for improving data ingestion efficiency.\nIteration 8: Re-evaluated Pinecone performance specifically on a Windows Server environment.\nIteration 9: Tuned the similarity threshold to optimize retrieval relevance and response accuracy.\nIteration 10: Evaluated different chunk sizes and overlaps to enhance retrieval accuracy without increasing latency.\nIteration 11: Conducted Vision description tests to assess the generation quality of domain-specific image descriptions.\nIteration 12: Performed a general mathematics evaluation to assess the numerical reasoning capability of the LLMs.\nIteration 13: Focused on domain-specific mathematical queries to test specialized mathematical problem-solving.\nIteration 14: Evaluated Vision prompts to test model responses based on image analysis queries.\nIteration 15: Assessed the accuracy of model responses in table-related queries.\nIteration 16: Conducted a Top K evaluation to determine the optimal K value for licensed models.\nIteration 17: Tested GPT 4o mini across tasks involving tables, mathematics, and text generation.\nIteration 18: Evaluated GPT 4o mini for vision-based tasks to assess its capability in image-related queries.\nIteration 19: Performed a comprehensive multimodal evaluation to test the pipeline's performance across text, tables, and images.\nEvaluation Metrics\n\nTo evaluate the effectiveness of the various configurations, we used the following metrics:\n\nResponse Accuracy:\n\nThe accuracy of responses generated by the LLMs was measured based on how well they addressed the user's query, incorporating the correct information retrieved from the vector database.\n\nRelevance Score:\n\nEach retrieved document’s relevance to the query was rated on a scale from 0 to 5, with 5 representing a perfect match and 0 indicating irrelevance. These relevance scores were aggregated to assess overall retrieval performance.\n\nLatency:\n\nThe response time from query to answer was measured to ensure the system could provide real-time responses. Latency was particularly important when testing different Top K values and indexing methods.\n\nMemory and Computational Efficiency:\n\nThe system’s memory usage and computational load were monitored to assess the trade-offs between higher retrieval accuracy and processing efficiency, particularly when using more complex models such as LLaMA 70B.\n\nRAGAS Metrics\n\nIn addition, RAGAS metrics were incorporated to provide deeper insights into the quality of the responses:\n\nContextual Precision:\n\nMeasures how accurately the information retrieved from the vector database matches the query’s intent. High contextual precision indicates that the retrieved information is directly relevant to the user's question.\n\nContextual Recall:\n\nEvaluates how comprehensively the retrieved information addresses all aspects of the query. High contextual recall indicates that the system is not only retrieving relevant chunks but is also covering the full scope of the query.\n\nAnswer Relevancy:\n\nFocuses on the relevance of the final generated answer in relation to both the query and the retrieved documents. This metric assesses how well the LLM integrates the retrieved information to form a coherent and contextually appropriate response.\n\nFaithfulness:\n\nMeasures the factual consistency between the retrieved documents and the generated answer. High faithfulness ensures that the LLM does not introduce hallucinations or incorrect information in the final response, particularly critical in domain-specific applications like CAE.\n\nExperiment Results\nGiskard Evaluation\n\nGiskard RAG (Retriever-Augmented Generation) evaluation is a framework designed to evaluate the performance of RAG pipelines, especially in the context of NLP systems like question-answers or summarization tasks. The idea behind RAG is to combine the strengths of two components: a retriever, which fetches relevant information from a knowledge base, and a generator, which formulates natural language responses based on the retrieved information.\n\nMetric Analysis for Question type – Correctness\nSimple questions: Simple questions generated from an excerpt of the knowledge base\nComplex questions: Questions made more complex by paraphrasing\nDistracting questions: Questions made to confuse the retrieval part of the RAG with a distracting element from the knowledge base but irrelevant to the question\nSituational questions: Questions including user context to evaluate the ability of the generation to produce relevant answer according to the context\nDouble questions: Questions with two distinct parts to evaluate the capabilities of the query rewriter of the RAG\nConversational questions: Questions made as part of a conversation, first message describes the context of the question that is asked in the last message, also tests the rewriter\n\nMetric Analysis for RAG Components – Correctness\nTarget RAG components\n\nGenerator\n\nThe LLM used inside the RAG to generate the answers\n\nRetriever\n\nFetch relevant documents from the knowledge base according to a user query\n\nRewriter\n\nRewrite the user query to make it more relevant to the knowledge base or to account for chat history\n\nRouter\n\nFilter the query of the user based on his intentions (intentions detection)\n\nKnowledge Base\n\nThe set of documents given to the RAG to generate the answers\n\nBased on the text-based Giskard evaluation metrics, for closed-source models, OpenAI 4o mini outperforms Claude 3.5 Sonnet and GPT 4o, and for open-source models, Llama 3.1 8B outperforms Llava -Mistral. The results are shown below.\n\n\nDeepEval Evaluation\n\nA dataset of 20 questions was generated from the files and unit test was conducted in the pipelines. Bias is one of the metrics which was introduced during the pytest. Along with the existing RAGAS metrics —contextual precision, contextual recall, answer relevancy, and faithfulness, Bias has now been incorporated as an additional evaluation criterion for the unit tests. A threshold of 0.5 was set for all metrics, meaning any test case scoring below 0.5 on even a single metric would result in a failed evaluation.\n\nNote: The bias metric determines whether your LLM output contains gender, racial, or political bias. Therefore, Bias should fall below 0.5 with 0% to pass the test.\n\nUnit Test – LLaMA3.1 – 20 Test Cases\n\nIt is observed that the LLaMA3.1 has passed 13 out of 20 test cases in the unit testing.\n\nConsidering that a test case is deemed successful only if all metrics exceed the 50% threshold, the following metrics analysis table was created to provide a better understanding of the unit testing results.\n\nThe table above presents the unit testing results, with metric definitions provided in the previous section. This table summarizes the outcomes for 20-unit tests conducted on the pipeline. In each test case, the bias metric consistently scored 0, leading to a count of 20 in the 0.00 to 0.30 range. For bias, a score of 0% is considered a pass, as it indicates no presence of gender, racial, or political bias in the output, which is ideal. This result shows the LLM's responses are neutral and free from potentially problematic or offensive content. All three remaining metrics scored above 70%, and the overall unit test achieved a score of 65%, passing 13 out of 20 test cases. For a test case to pass, each metric must individually score above 50%.\n\nUnit test - GPT – 4o - 20 Test Cases:\n\nThe GPT-4o has identical results to LLaMA3.1 in the unit testing where it has passed in 13 out 20 tests.\n\nThe table above presents the unit testing results, with metric definitions provided in the previous section. This table summarizes the outcomes for 20-unit tests conducted on the pipeline. In each test case, the bias metric consistently scored 0, leading to a count of 20 in the 0.00 to 0.30 range. For bias, a score of 0% is considered a pass, as it indicates no presence of gender, racial, or political bias in the output, which is ideal. This result shows the LLM's responses are neutral and free from potentially problematic or offensive content. All four remaining metrics scored above 70%, and the overall unit test achieved a score of 65%, passing 13 out of 20 test cases. For a test case to pass, each metric must individually score above 50%.\n\nVector Database Performance\n\nMilvus demonstrated high efficiency in retrieving documents at scale, with IVF-FLAT providing the best balance between retrieval time and accuracy. HNSW offered slightly better accuracy but introduced significant latency at larger scales, making it less practical for real-time applications.\n\nLLM Performance:\nGPT-4o: Excelled in handling complex, multi-step queries and provided the most accurate responses, particularly when paired with larger Top K values.\nClaude 3.5 Sonnet: Performed well with simpler queries and required fewer computational resources but was not concise and precise with answers regarding CAE domain specific content.\nLLaMA 3.1 8B: Outperformed other open-source models in terms of response relevance but required more tuning in similarity threshold selection.\nGPT 4o mini: The giskard examination showed that GPT 4o mini outperformed than 4o, but another important finding is that the vision description irrelevancy is more than 4o.\nVision Models (LLaVA- Mistral & LLaVA - 34B): Both models performed well on image-based queries, with LLaVA 34B showing slightly better accuracy in domain-specific image description tasks.\nDiscussion\n\nIn the initial development phase, due diligence was done to test numerous open-source and licensed large language models (LLMs). The availability of more advanced LLMs in a short timeframe enabled us to test increasingly powerful options. Initial iterations included models like Falcon, LLaMA-2, and Mistral 7B. The primary motivation for adopting a Retrieval Augmented Generation (RAG) approach was the rapid advancements in Generative AI, promising continual access to faster, more capable models suited to complex tasks.\n\nThe knowledge base contains approximately 2,500 files related to the CAE domain, enabling the LLM to deliver content-rich answers to user queries.\n\nKey components were evaluated based on variety of criteria, including retrieval accuracy, response quality and system efficiency. While the detailed results of the final configuration remain proprietary, the exploration phase provided valuable insights. The system demonstrated strong performance in handling a variety of input types including text, images, and mathematical queries. Evaluation metrics such as contextual precision, recall and answer relevancy were used to measure the system’s performance. Across all the iterations, the system consistently met or at times exceeded the predefined threshold (0.5), ensuring a reliable retrieval of relevant documents and quality of response generation.\n\nBy incorporating the multimodal capabilities, the pipeline effectively supports diverse use cases within the CAE domain. A user-friendly GUI enables seamless switching between input types, making domain knowledge easily accessible. A few snapshots of the application is shown below.\n\n\n\n\nConclusion\n\nTo address the challenge of knowledge retention in organisations, an AI-driven assistant with multimodal data handling capabilities was developed by integrating RAG techniques for CAE domain. The system demonstrates strong performance in handling text, images, and mathematical queries, with GPT-4o-mini, GPT-4o, Claude 3.5 Sonnet, and LLaMA 3.1 8B emerging as top performers.\n\nFor the given experimental environment set up, the model performed with good accuracy hence validating the approach for knowledge retention. Currently in progress is the deployment of the model in a real-time environment, where knowledge base is constantly updated, and multiple users will be accessing the application.\n\nThe model will be improved to make sure the company never “forgets” critical information and keeps the employees informed, hence maintaining competitive advantage and improved financial metrics.\n\nReferences:\nhttps://huggingface.co/\nhttps://milvus.io/\nhttps://www.langchain.com/\nhttps://openai.com/index/\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://python.langchain.com/docs/integrations/llms/ollama/\nhttps://www.pinecone.io/\nhttps://github.com/Giskard-AI/giskard\nhttps://github.com/confident-ai/deepeval\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "automating-clinical-document-classification-ai-solutions-for-enhanced-healthcare-decision-support-mkisjSwuptcu",
    "username": "mMyriam Joseph",
    "license": null,
    "title": "Automating Clinical Document Classification: AI Solutions for Enhanced Healthcare Decision Support",
    "publication_description": "Back to publications\nOct 15, 2024\n●\n75 reads\nWinner of\nDistinguished Applied Solution Showcase\nat the\nNLP Projects Expo 2024\nAutomating Clinical Document Classification: AI Solutions for Enhanced Healthcare Decision Support\nAutomated Systems\nBERT Transformer\nClincal BERT\nClinical Decision Support\nComparative Study\nDeep Learning\nDigital Healthcare\nFine-tuning\nMachine Learn\u0002ing\nNarrative Clinical Documents\nNLP application\nPrompt engineering\nSequence Models\nText Classification\nTransfer Learning\nTransformer\nM\nMyriam Joseph\nLike\nBookmark\nShare\n\nAbstract\n\nManual labeling of narrative clinical documents not only burdens healthcare workers but also increases the risk of classification errors and delays. Integrating artificial intelligence (AI) in the medical field enables the automation of this classification process. The research aims to identify the optimal combination of feature engineering technique and classification model for accurate narrative document classification. A robust preprocessing pipeline is also introduced to enhance data quality and model performance. The analysis includes three categories of feature engineering techniques: feature representation, word embeddings, and contextual embeddings, as well as three types of classification models: traditional machine learning algorithms, sequence models, and BERT transformers. The results indicate that a voting classifier with TF-IDF representation outperforms other combinations, achieving an accuracy of 89% and an F1-score of 88.6%. These findings demonstrate the potential for AI to streamline clinical documentation processes and prove that in\u0002tegrating such tools enhances healthcare providers’ performance.\n\nIntroduction\n\nHealthcare workers spend approximately 27% of their working hours on direct patient care and 49% on desk work, including labeling clinical documents. Despite this significant investment of time, clinical documents are often mislabeled, misplaced, and lost, which leads to delays in medical work. In 2019, the World Health Organization (WHO) reported that around 2.6 million deaths in low- and middle-income countries were due to medical errors, with approximately 58% of reported medication errors linked to usability issues in clinical documents.\n\nTo address these challenges, artificial intelligence (AI) can assist the healthcare system by helping medical staff work more effectively and reducing their workload in tasks like automated document classification. Since almost 80% of clinical data are locked in an unstructured format, Natural Language Processing (NLP), a sub-field of AI, is designed to process and analyze this unstructured data.\n\nThere is still limited research focused on classifying narrative clinical documents. These documents, which contain free-text descriptions of patients' conditions, are crucial for clinical decision-making. Their unstructured format makes them difficult to automatically process without effective pre-processing techniques.\n\nOur research objective is to identify the optimal combination of feature engineering techniques and classification models that enhances accuracy and efficiency in classifying narrative clinical documents, thereby reducing the manual workload of healthcare workers.\n\nOur key contributions to the medical field include:\n\nA comprehensive comparison of feature engineering techniques—feature representation, word embeddings, and contextual embeddings.\nA detailed evaluation of classification models, including traditional machine learning, sequence models, and transformers.\nThe introduction of a robust pre-processing pipeline specifically tailored to the challenges of unstructured narrative clinical data, significantly improving the input quality for classification models.\nMethodology\n\nThe methodology involves a systematic approach to trans\u0002forming clinical documents into predicted labels through a series of well-defined steps: text pre-processing, feature en\u0002gineering, classification model, and validation & evaluation.\n\nDataset\n\nDue to privacy concerns, obtaining clinical datasets while preserving patient privacy is challenging. That is why previous research that relies on real documents tends to refrain from sharing them. Therefore, the dataset is an open-source dataset from Kaggle website. The dataset comprises 500 clinical documents in plain text format, with an average length of 2300 words. The documents lack a defined or specific format, and the variance in document length is significant, necessitating advanced techniques for analysis. These documents were originally in paper format and were digitized using OCR (Optical character recognition) technique. The dataset is divided into 5 medical categories, including neurology, Radiology, Discharge Summary, General Medicine, and Gastroenterology, with approximately 100 documents per category.\n\nDue to the small dataset, data augmentation is necessary to increase its size and enhance model performance, robustness, and generalization. ChatGPT prompts were used to generate free-text documents, inspired by the layout and writing style of documents in the dataset. Related procedures for each class are referenced through these links \nneurology\n, \nradiology\n, and \ngastroenterology\n to give ChatGPT the procedure’s name. 250 documents were generated with 50 documents in each class, ensuring a balanced representation across all categories.\n\n1) Text Pre-processing\n\nIt aims to enhance the quality of the input data by addressing various linguistic irregularities present in unstructured docu\u0002ments to ensure that the subsequent stages operate on standard\u0002ized data, hence improving the classification performance. The 5 following techniques are followed sequentially to address the data quality and consistency issues.\n\nThe pre-processing steps include:\n\nAcronym Expansion: is achieved by creating a dictionary of commonly used acronyms within each class and mapping them to their expanded forms in lowercase. The dictionary contains 195 acronyms with their relevant expansion. Since the usage of abbreviations is very common in clinical text, this step facilitates model comprehension of unseen expressions, enhancing the interpretation in clinical document processing.\n\nRegular Expression (RE) Matching: includes the usage of the RE library to cleanse the text from various forms of noise that could potentially mislead the model during analysis. To ensure the anonymity of individuals mentioned in the documents, patients’, doctors’, and hospitals’ names and titles were removed. Irrelevant elements that do not contribute to the semantic content such as digits, dates, extra spaces, newline characters, single characters, and special characters were eliminated.\n\nTokenization: is the process of breaking down a text into units (tokens). Unlike simple splitting which helps handle the ambiguity between words and special cases. This step is essential for initiating any NLP task and ensuring precise text analysis.\n\nLemmatization: is a technique that accurately reduces words to their root form. This process maps verbs to their infinitive forms and plural nouns to singular. Its benefits include reducing dictionary size and eliminating unrelated words, ensuring that related words are consolidated into one, thereby facilitating better classification.\n\nStop-word Removal: involves eliminating commonly used words that do not carry meaningful information, such as articles, prepositions, and conjunctions. The main aim is to reduce text data dimensionality and improve computational efficiency. A total of 233 words were added to the predefined lists of stop-words from both Spacy and NLTK libraries for elimination. It includes frequent adjectives, medical expres\u0002sions, and adverbs used in clinical contexts.\n\nBelow is a comparison of a discharge sum\u0002mary document before and after applying pre-processing steps. The average document size was significantly reduced from 2300 to 750 words, indicating successful data refinement.\n\nDischarge Summary Document Before Pre-processing\n\nDischarge Summary Document Atfer Pre-processing\n\n2) Feature Engineering\n\nFeature engineering focuses on transforming raw text into a numerical format suitable for model learning. The choice of technique was aligned with the classifier type to optimize overall model performance.\n\nThe techniques used are:\n\nFeature Representation: Bag of Words, TF-IDF, and Doc2Vec.\nWord Embedding: Static embeddings like GloVe and FastText, along with dynamic embeddings.\nContextual Embedding: Using ClinicalBERT embeddings to capture word context from surrounding text.\n3) Classification Models\n\nWe classified algorithms into three categories:\n\nA) Traditional Machine Learning Models: Random Forest (RF), Support Vector Machine (SVM), Naïve Bayes (NB), K-nearest neighbors (KNN) , and Multilayer Perceptron (MLP).\nB) Sequence Models: Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRU).\nC) Transformer Models: Transfer learning with BERT variants such as BERT Base, BioClinicalBERT, PubMedBERT, SciBERT, and ClinicalBERT.\n\nOur goal was to identify the top-performing model within each category. The table below provides a summary of the classification models across these categories, along with the feature engineering techniques applied.\n\nCategory\tFeature Engineering\tClassification Models\nCategory 1\tBag of Words, TF-IDF, Doc2Vec\tRF, SVM, NB, KNN, and MLP\nCategory 2\tWord Embedding:\nStatic Embedding (GloVe & FastText)\nDynamic Embedding\tRNN, LSTM, and GRU\nCategory 3\tContextual Embedding\tBERT Base, BioClinicalBERT, PubMedBERT, SciBERT, ClinicalBERT\n4) Validation & Evaluation\n\nThe dataset was split into 80% for training and 20% for testing. Hyperparameter optimization techniques, including grid search for ML models, Keras Tuner for sequence models, and k-fold cross-validation, were used to optimize the performance of all models.\n\nEvaluation metrics include:\n\nOur main focus will be on accuracy, as the balanced dataset ensures that accuracy is a reliable indicator of overall performance across classes. We will also focus on the F1-score to ensure that the model performs well overall, accurately identifies each class and avoids bias by balancing precision and recall across classes.\n\nResults\n\nWe used Python programming language to implement the classification models. The NLP and deep learning libraries and frameworks employed in the implementation are Natural Language Toolkit (NLTK), SpaCy, Regex (re-module), Scikit-learn, Gensim, TensorFlow, PyTorch, Transformers from Hugging Face website.\n\nA) Traditional ML classifiers\n\nTable below illustrates the optimized performance of each algorithm using TF-IDF and Doc2Vec. Since we have limited data, TF-IDF generally achieved better results than Doc2Vec. Using the TF-IDF technique, KNN demonstrates the highest accuracy and precision values, while SVM achieves the highest recall and F1-score values.\n\nModel\tTF-IDF Accuracy\tTF-IDF Precision\tTF-IDF Recall\tTF-IDF F1-Score\tDoc2Vec Accuracy\tDoc2Vec Precision\tDoc2Vec Recall\tDoc2Vec F1-Score\nRF\t0.80\t0.81\t0.81\t0.80\t0.86\t0.86\t0.859\t0.858\nNB\t0.85\t0.85\t0.84\t0.844\t0.77\t0.77\t0.77\t0.77\nKNN\t0.89\t0.885\t0.87\t0.876\t0.76\t0.77\t0.76\t0.75\nSVM\t0.88\t0.884\t0.88\t0.88\t0.84\t0.84\t0.85\t0.84\nMLP\t0.85\t0.85\t0.846\t0.848\t0.81\t0.81\t0.81\t0.81\nVoting Classifier\t0.89\t0.893\t0.885\t0.886\t0.858\t0.858\t0.855\t0.854\n\nTherefore, we decided to combine both of them to complement each other. As a result, the voting classifier achieved the best performance.\n\nB) Sequence Models\n\nThe architecture of each model consists of an embedding layer followed by sequence layers with defined dropout and recurrent dropout, and a dense layer containing 5 neurons (representing the number of classes) with a softmax activation function to classify the document. Table below illustrates the performance of each model based on predefined metrics. GRU has the highest performance among other sequence models since it can capture long-term dependencies better than RNN and converges faster than LSTM before the occurrence of overfitting.\n\nC) Transfer Learning using BERT Transformers\n\nTo fine-tune the BERT models, we used the PyTorch library to perform additional data pre-processing steps to prepare the data in a format suitable for the transformer. This pipeline included tokenization, padding, and truncation to ensure uniform data formatting. During training, a dense layer was added on top of the pre-trained BERT model while freezing its layers to classify the document.\nTable below shows the performance of each model, and it is evident that the ClinicalBERT transformer outperforms all other models since it is trained on discharge summary documents that have the same characteristics as our dataset.\n\nType\tModel\tAccuracy\tPrecision\tRecall\tF1-score\nSequence Models\tRNN\t0.61\t0.55\t0.50\t0.52\n\tLSTM\t0.80\t0.80\t0.78\t0.79\n\tGRU\t0.88\t0.88\t0.88\t0.88\nTransformers\tBERT Base\t0.70\t0.75\t0.71\t0.70\n\tBioClinicalBERT\t0.80\t0.80\t0.80\t0.80\n\tPubMedBERT\t0.81\t0.83\t0.82\t0.81\n\tSciBERT\t0.81\t0.84\t0.82\t0.82\n\tClinicalBERT\t0.85\t0.85\t0.84\t0.84\n\nThe voting classifier achieved the best performance across three categories, achieving an accuracy of 0.89 and an F1-score of 0.886. The GRU model outputs close results with an accuracy of 0.88 and an F1-score of 0.878. The least achieving model was ClinicalBERT with an accuracy of 0.85 and an F1-score of 0.84. The reason deep learning and transformers did not perform better than machine learning is that they require a large amount of data for training or tuning their parameters. However, they produced comparable results, which proves that increasing the dataset size will definitely improve their performance. By comparing the best results before and after applying all the model steps, it is clear that the accuracy has improved by 35% and the F1-score by 32%. This highlights the critical role of the introduced pre-processing pipeline in improving model performance.\n\nEffect of Pre-processing\n\n# Web Application Demo\n\nWe have integrated the high-performing model into the ‘CliniDocPredictor’ web application demo, which provides three core features: document upload, content display, and label prediction. The frontend is built using React.js, while the backend is powered by Django. The primary goal of the demo is to showcase the model’s capabilities, particularly on new documents generated that have not been seen by the model either in training or testing. A recording of the application in action can be viewed in the following YouTube video.\n\nConclusion\n\nThe study evaluated and compared various classification models and feature engineering techniques to identify the best combination for accurately labeling narrative clinical documents. To address the small dataset size, data augmentation increased the dataset by 50%, and a robust pre-processing pipeline improved model performance. The top-performing model was a voting classifier with the TF-IDF feature engineering technique, achieving an accuracy of 89% and an F1-score of 88.6%. This work serves as a proof of concept and could be a viable solution for adoption on a larger scale to enhance healthcare decision-making. By documenting all relevant information about patients, this system has the potential to reduce the workload on healthcare providers and, most importantly, save patients' lives.\n\nLimitations\n\nAfter conducting our analysis, it is essential to address the research limitations. Firstly, the dataset size is a significant constraint. Access to a larger dataset is needed to improve the model’s generalizability. Additionally, incorporating more categories to include all departments in the hospital or healthcare unit, as well as documents outside the medical field, would allow the model to differentiate between medical-related documents and unrelated content, classifying the latter as 'other.' Lastly, the limited computational resources provided by the free version of Colab affected the fine-tuning of BERT models, which require substantial computational power.\n\nFuture Work\n\nA practical application is to integrate the model with a system database to automate the data entry of all clinical documents. The provided model will take unlabeled documents as input and decide to label them with their corresponding class and feed them to the hospital system. As a result, by entering the patient identification number, we can access the patient history with all relevant clinical documents. The figure below explains the pipeline of this solution and how each part will interact with the others.\n\nAdditionally, researchers will benefit from well-organized documents to achieve more effective research outcomes. Finally, with minor adjustments to pre-processing techniques and fine-tuning of the relevant BERT models, the approach can be applied to various domains, not just the medical field.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nMethodology\n\nDataset\n\nText Pre-processing\nFeature Engineering\nClassification Models\nValidation & Evaluation\n\nResults\n\nA) Traditional ML classifiers\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nFiles\nMedia1.mp4\nYou might be interested\nProduct Category Classification Prediction using AI/ML models\nApr 23, 20258 reads\nArtificial IntelligenceBert-Embeddings+39\nPrompt Injection Attacks on LLM: Medical Diagnosis by Symptom Elaboration\nS\nJ\nMar 31, 202515 reads\nClinical NoteDiseases+4\nClassical Sentiment Analysis\nApr 26, 202515 reads\nlogistic-regressionsentiment-analysis\nRAG4HealthQA – Healthcare Q&A Assistant\nOct 01, 20255 reads\nAIEvidenceAIForGood+29",
    "awards": [
      "distinguished applied solution showcase"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "backstage-ai-agent-5uIxbmkv0qve",
    "username": "f@forando",
    "license": "No License",
    "title": "Backstage AI Agent",
    "publication_description": "Back to publications\nFeb 27, 2025\n●\n107 reads\n●\nNo License\nBackstage AI Agent\nF\n@forando\nLike\nBookmark\nShare\nBackstage AI Agent\n\nBackstage\n is a popular developer portal that helps to streamline software development, manage infrastructure, tools, and services in a unified interface.\n\nIn this article we will demonstrate how to build an AI Agent that helps you to navigate inside Backstage documentation as well as perform a set of predefined actions on its behalf.\n\nAI agents are now a new standard on how to build and manage enterprise infrastructure. They assist you in information discovery, configuration and maintenance of a product.\n\nAWS Bedrock\n provides an option to configure such an \nagent\n.\n\nThe following diagram shows the solution on how to implement a Bedrock AI Agent for the Backstage.\nThe solution assumes that Backstage Techdocs are deployed following \nRecommended deployment\n guide and AWS S3 used as a storage.\n\nFor each document entry in Backstage s3 bucket there is a correspondent search_index.json that contains all document data (see an example in the attachment).\nThe data in search_index.json is split in chunks (one per subheading). Content Creator function reads the\nfile and assembles those chunks together. It also adds backstage links that point to the correspondent subheadings (see: generated_document.md in the attachment).\nThe resulted document is stored in Bedrock S3 bucket.\nBedrock has an option to build a Knowledge Base from the documents (which is essentially \nRAG\n). The build process works as follows:\nThe Knowledge Base agent takes a document form the s3 bucket and pipes it thorough amazon.titan-embed-text-v1 model to generate a vector representation of the document text (aka embeddings).\nThe Knowledge Base agent then stores this vector together with the document text in OpenSearch as \ninverted index\n for the later use.\nThe Bedrock Agent is a middle man that processes user requests. The flow looks as follows:\nThe user sends a query to the Bedrock Agent.\nThe Bedrock Agent pipes the query through amazon.titan-embed-text-v1 model to generate a vector representation of the query.\nThe Bedrock Agent then sends this vector to OpenSearch to find the most similar documents.\nThe found documents and the query are piped through anthropic.claude-3-5-sonnet-20240620-v1:0 to get an answer.\nThe Bedrock Agent has a set of preconfigured actions that it can perform by invoking Action Performer lambda function (described next).\nAgent Configuration\n\nAs mentioned above, an Agent is an add-on on top of RAG. It can perform some actions on your behalf.\n\nThe Bedrock Agent has an instruction setting that is used to tell it what role it has to impersonate and what behaviour you expect it to have.\n\nHere is how we configured the setting:\n\nThe Bedrock Agent has an option to configure so-called \naction groups\n. An action group in a combination of:\n\ninstruction for the agent on how and when to perform the action\none or more lambda functions that agent can invoke to perform the action\n\nhere is an example of such configuration:\n\n\nAs you can see here we are configuring an action group to check validity of Backstage links.\nRemember that we generated our markdown documents with those links embedded. Unfortunately, the model hallucinates quite often when it comes to those links and may give you a broken one. That's why we instructing the agent to check the link validity before returning it to the user.\n\nHere is how conversation might look like:\n\nYou can imagine to configure a fleet of those action groups, each one responsible for a particular domain in your organisation.\nHere is an example of one:\n\nWith this action group the agent can recognise from the question that an action can be immediately performed in order to assist a user in the discussed topic.\n\nHere is how it looks:\n\nConclusion\n\nAgents can significantly facilitate user experience and help them to quickly find organisation binding answers from its knowledge base. They can assist users in their daily work and help them to streamline their workflows.\n\nSource Code\n\nThe source code can be found here: \nbackstage-agent\n.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nBackstage AI Agent\n\nAgent Configuration\n\nConclusion\n\nSource Code\n\nCode\nFiles\nsearch_index.json\ngenerated_document.md",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "balancing-the-scales-a-comprehensive-study-on-tackling-class-imbalance-in-binary-classification-tum5RnE4A5W8",
    "username": "Ready Tensor",
    "license": null,
    "title": "Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification",
    "publication_description": "Back to publications\nSep 12, 2024\n●\n298 reads\n●\nCreative Commons Attribution-ShareAlike (CC BY-SA)\nBalancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification\nHub:\nReady Tensor Research\naccuracy\nauc-score\nbinary classification\nclass imbalance\nclass weights\ncomparative study\ndecision threshold\nf1-score\nf2-score\nimbalanced data\npr-auc\nprecision\nrecall\nsmote\nReady Tensor\nMohamed Abdelhamid\nLike\nBookmark\nShare\n\nTL;DR\nThis study evaluates three strategies for handling imbalanced datasets in binary classification—SMOTE, class weights, and decision threshold calibration—across 15 classifiers and 30 datasets. Results from 9,000 experiments show all methods generally outperform the baseline, with decision threshold calibration emerging as the most consistent performer. However, significant variability across datasets emphasizes the importance of testing multiple approaches for specific problems.\n\nAbstract\n\nClass imbalance in binary classification tasks remains a significant challenge in machine learning, often resulting in poor performance on minority classes. This study comprehensively evaluates three widely-used strategies for handling class imbalance: Synthetic Minority Over-sampling Technique (SMOTE), Class Weights tuning, and Decision Threshold Calibration. We compare these methods against a baseline scenario across 15 diverse machine learning models and 30 datasets from various domains, conducting a total of 9,000 experiments. Performance was primarily assessed using the F1-score, with additional 9 metrics including F2-score, precision, recall, Brier-score, PR-AUC, and AUC. Our results indicate that all three strategies generally outperform the baseline, with Decision Threshold Calibration emerging as the most consistently effective technique. However, we observed substantial variability in the best-performing method across datasets, highlighting the importance of testing multiple approaches for specific problems. This study provides valuable insights for practitioners dealing with imbalanced datasets and emphasizes the need for dataset-specific analysis in evaluating class imbalance handling techniques.\n\nIntroduction\n\nBinary classification tasks frequently encounter imbalanced datasets, where one class significantly outnumbers the other. This imbalance can severely impact model performance, often resulting in classifiers that excel at identifying the majority class but perform poorly on the critical minority class. In fields such as fraud detection, disease diagnosis, and rare event prediction, this bias can have serious consequences.\n\nOne of the most influential techniques developed to address this challenge is the Synthetic Minority Over-sampling Technique (SMOTE), a method proposed by Chawla et al. (2002) that generates synthetic examples of the minority class. Since its introduction, the SMOTE paper has become one of the most cited papers in the field of imbalanced learning, with over 30,000 citations. SMOTE's popularity has spurred the creation of many other oversampling techniques and numerous SMOTE variants. For example, Kovács (2019) documents 85 SMOTE-variants implemented in Python, including:\n\nBorderline-SMOTE (Han et al., 2005)\nSafe-Level-SMOTE (Bunkhumpornpat et al., 2009)\nSMOTE + Tomek and SMOTE + ENN (Batista et al., 2004)\n\nDespite its widespread use, recent studies have raised some criticisms of SMOTE. For instance, Blagus and Lusa (2013) indicate limitations in handling high-dimensional data, while Elor and Averbuch-Elor (2022) and Hulse et al. (2007) suggest the presence of better alternatives for handling class imbalance. This highlights that while SMOTE is a powerful tool, it is not without limitations.\n\nIn this study, we aim to provide a more balanced view of techniques for handling class imbalance by evaluating not only SMOTE but also other widely-used strategies, such as Class Weights and Decision Threshold Calibration. These three treatment scenarios target class imbalance at different stages of the machine learning pipeline:\n\nSMOTE: Generating synthetic examples of the minority class during data preprocessing.\nClass Weights: Adjusting the importance of classes during model training.\nDecision Threshold Calibration: Adjusting the classification threshold post-training.\n\nWe compare these strategies with the Baseline approach (standard model training without addressing imbalance) to assess their effectiveness in improving model performance on imbalanced datasets. Our goal is to provide insights into which treatment methods offer the most significant improvements in performance metrics such as F1-score, F2-score, accuracy, precision, recall, MCC, Brier score, Matthews Correlation Coefficient (MCC), PR-AUC, and AUC. We also aim to evaluate these techniques across a wide range of datasets and models to provide a more generalizable understanding of their effectiveness.\n\nTo ensure a comprehensive evaluation, this study encompasses:\n\n30 datasets from various domains, with sample sizes ranging from ~500 to 20,000 and rare class percentages between 1% and 20%.\n15 classifier models, including tree-based methods, boosting algorithms, neural networks, and traditional classifiers.\nEvaluation using 5-fold cross-validation.\n\nIn total, we conduct 9,000 experiments involving the 4 scenarios, 15 models, 30 datasets, and validation folds. This extensive approach allows us to compare these methods and their impact on model performance across a wide range of scenarios and algorithmic approaches. It provides a robust foundation for understanding the effectiveness of different imbalance handling strategies in binary classification tasks.\n\nMethodology\nDatasets\n\nWe selected 30 datasets based on the following criteria:\n\nBinary classification problems\nImbalanced class distribution (minority class < 20%)\nSample size ≤ 20,000\nFeature count ≤ 100\nReal-world data from diverse domains\nPublicly available\n\nThe characteristics of the selected datasets are summarized in the chart below:\n\nThe dataset selection criteria were carefully chosen to ensure a comprehensive and practical study:\n\nThe 20% minority class threshold for class imbalance, while somewhat arbitrary, represents a reasonable cut-off point that is indicative of significant imbalance.\nThe limitations on sample size (≤ 20,000) and feature count (≤ 100) were set to accommodate a wide range of real-world datasets while ensuring manageable computational resources for an experiment of our scale. This balance allows us to include diverse, practically relevant datasets without compromising the breadth of our study.\nThe focus on diverse domains ensures that our models are tested across a wide range of industries and data characteristics, enhancing the generalizability of our findings.\nDataset Repository\n\nYou can find the study datasets and information about their sources and specific characteristics in the following repository:\n\nImbalanced Classification Study Datasets\n\nThis repository is also linked in the Datasets section of this publication.\n\nModels\n\nOur study employed a diverse set of 15 classifier models, encompassing a wide spectrum of algorithmic approaches and complexities. This selection ranges from simple baselines to advanced ensemble methods and neural networks, including tree-based models and various boosting algorithms. The diversity in our model selection allows us to assess how different imbalanced data handling techniques perform across various model types and complexities.\n\nThe following chart lists the models used in our experiments:\n\nA key consideration in our model selection process was ensuring that all four scenarios (Baseline, SMOTE, Class Weights, and Decision Threshold Calibration) could be applied consistently to each model. This criterion influenced our choices, leading to the exclusion of certain algorithms such as k-Nearest Neighbors (KNN) and Naive Bayes Classifiers, which do not inherently support the application of class weights. This careful selection process allowed us to maintain consistency across all scenarios while still representing a broad spectrum of machine learning approaches.\n\nImplementation Details\nEach model is implemented in a separate repository to accommodate differing dependencies, but all are designed to work with any dataset in a generalized manner. These repositories include:\nTraining and testing code\nDocker containerization for environment-independent usage\nHyperparameter tuning code, where applicable\n\nTo ensure a fair comparison, we used the same preprocessing pipeline for all 15 models and scenarios. This pipeline includes steps such as one-hot encoding, standard scaling, and missing data imputation. The only difference in preprocessing occurs in the SMOTE scenario, where synthetic minority class examples are generated. Otherwise, the preprocessing steps are identical across all models and scenarios, ensuring that the only difference is the algorithm and the specific imbalance handling technique applied.\n\nAdditionally, each model's hyperparameters were kept constant across the Baseline, SMOTE, Class Weights, and Decision Threshold scenarios to ensure fair comparisons.\n\nThe imbalanced data handling scenarios are implemented in a branch named imbalance. A configuration file, model_config.json, allows users to specify which scenario to run: baseline, smote, class_weights, or decision_threshold.\n\nModel Repositories\n\nAll model implementations are available in our public repositories, linked in the Models section of this publication.\n\nEvaluation Metrics\n\nTo comprehensively evaluate the performance of the models across different imbalanced data handling techniques, we tracked the following 10 metrics:\n\nOur primary focus is on the F1-score, a label metric that uses predicted classes rather than underlying probabilities. The F1-score provides a balanced measure of precision and recall, making it particularly useful for assessing performance on imbalanced datasets.\n\nWhile real-world applications often employ domain-specific cost matrices to create custom metrics, our study spans 30 diverse datasets. The F1-score allows us to evaluate all four scenarios, including decision threshold tuning, consistently across this varied set of problems.\n\nAlthough our analysis emphasizes the F1-score, we report results for all 10 metrics. Readers can find comprehensive information on model performance across all metrics and scenarios in the detailed results repository linked in the Datasets section of this publication.\n\nExperimental Procedure\n\nOur experimental procedure was designed to ensure a robust and comprehensive evaluation of the four imbalance handling scenarios across diverse datasets and models. The process consisted of the following steps:\n\nDataset Splitting\n\nWe employed a form of nested cross-validation for each dataset to ensure robust model evaluation and proper hyperparameter tuning:\n\nOuter Loop: 5-fold cross-validation\n\nEach dataset was split into five folds\nResults were reported for all five test splits, providing mean and standard deviation values across the folds\n\nInner Validation: 90/10 train-validation split\n\nFor scenarios requiring hyperparameter tuning (SMOTE, Class Weights, and Decision Threshold Calibration), the training split from the outer loop was further divided into a 90% train and 10% validation split\nThe validation split was used exclusively for tuning hyperparameters\n\nThis nested structure ensures that the test set from the outer loop remains completely unseen during both training and hyperparameter tuning, providing an unbiased estimate of model performance. The outer test set was reserved for final evaluation, while the inner validation set was used solely for hyperparameter optimization in the relevant scenarios.\n\nScenario Descriptions\nWe evaluated four distinct scenarios for handling class imbalance:\n\nBaseline: This scenario involves standard model training without any specific treatment for class imbalance. It serves as a control for comparing the effectiveness of the other strategies.\n\nSMOTE (Synthetic Minority Over-sampling Technique): In this scenario, we apply SMOTE to the training data to generate synthetic examples of the minority class.\n\nClass Weights: This approach involves adjusting the importance of classes during model training, focusing on the minority class weight while keeping the majority class weight at 1.\n\nDecision Threshold Calibration: In this scenario, we adjust the classification threshold post-training to optimize the model's performance on imbalanced data.\n\nEach scenario implements only one treatment method in isolation. We do not combine treatments across scenarios. Specifically:\n\nFor scenarios 1, 2, and 3, we apply the default decision threshold of 0.5.\nFor scenarios 1, 2, and 4, the class weights are set to 1.0 for both positive and negative classes.\nSMOTE is applied only in scenario 2, class weight adjustment only in scenario 3, and decision threshold calibration only in scenario 4.\n\nThis approach allows us to assess the individual impact of each treatment method on handling class imbalance.\n\nHyperparameter Tuning\n\nFor scenarios requiring hyperparameter tuning (SMOTE, Class Weights, and Decision Threshold), we employed a simple grid search strategy to maximize the F1-score measured on the single validation split (10% of the training data) for each fold.\n\nThe grid search details for the three treatment scenarios were as follows:\n\nSMOTE\nWe tuned the number of neighbors hyperparameter, performing a simple grid search over `k` values of 1, 3, 5, 7, and 9.\n\n\nClass Weights\nIn this scenario, we adjusted the class weights to handle class imbalance during model training. The tuning process involved adjusting the weight for the minority class relative to the majority class. If both classes were given equal weights (e.g., 1 and 1), no class imbalance handling was applied—this corresponds to the baseline scenario. For the balanced scenario, we set the minority class weight proportional to the class imbalance (e.g., if the majority/minority class ratio was 5:1, the weight for the minority class would be 5). We conducted grid search on the following factors: 0 (baseline case), 0.25, 0.5, 0.75, 1.0 (balanced), and 1.25 (over-correction). The optimal weight was selected based on the F1-score on the validation split.\n\n\nDecision Threshold Calibration\nWe tuned the threshold parameter from 0.05 to 0.5 with a step size of 0.05, allowing for a wide range of potential decision boundaries.\n\nThere are no scenario-specific hyperparameters to tune for the Baseline scenario. As a result, no train/validation split was needed, and the entire training set was used for model training.\n\nOverall Scope of Experiments\nOverall, this study contains 9,000 experiments driven by the following factors:\n30 datasets\n15 models\n4 scenarios\n5-fold cross-validation\n\nFor each experiment, we recorded the 10 performance metrics across the five test splits. In the following sections, we present the results of these extensive experiments.\n\nResults\n\nThis section presents a comprehensive analysis of our experiments comparing four strategies for handling class imbalance in binary classification tasks. We begin with an overall comparison of the four scenarios (Baseline, SMOTE, Class Weights, and Decision Threshold Calibration) across all ten evaluation metrics. Following this, we focus on the F1-score metric to examine performance across the 15 classifier models and 30 datasets used in our study.\n\nOur analysis is structured as follows:\n\nOverall performance comparison by scenario and metric\nModel-specific performance on F1-score\nDataset-specific performance on F1-score\nStatistical analysis, including repeated measures tests and post-hoc pairwise comparisons\n\nFor the overall, model-specific, and dataset-specific analyses, we report mean performance and standard deviations across the five test splits from our cross-validation procedure. The final section presents the results of our statistical tests, offering a rigorous comparison of the four scenarios' effectiveness in handling class imbalance.\n\nOverall Comparison\n\nFigure 1 presents the mean performance and standard deviation for all 10 evaluation metrics across the four scenarios: Baseline, SMOTE, Class Weights, and Decision Threshold Calibration.\n\nFigure 1: Mean performance and standard deviation of evaluation metrics across all scenarios. Best values per metric are highlighted in blue.\n\nThe results represent aggregated performance across all 15 models and 30 datasets, providing a comprehensive overview of the effectiveness of each scenario in handling class imbalance.\n\nF1-Score Performance\n\nThe results show that all three class imbalance handling techniques outperform the Baseline scenario in terms of F1-score:\n\nDecision Threshold Calibration achieved the highest mean F1-score (0.617 ± 0.005)\nSMOTE followed closely (0.605 ± 0.006)\nClass Weights showed improvement over Baseline (0.594 ± 0.006)\nBaseline had the lowest F1-score (0.556 ± 0.006)\n\nThis suggests that addressing class imbalance, regardless of the method, generally improves model performance as measured by the F1-score.\n\nOther Metrics\n\nWhile our analysis primarily focuses on the F1-score, it's worth noting observations from the other metrics:\n\nF2-score and Recall: Decision Threshold Calibration and SMOTE showed the highest performance, indicating these methods are particularly effective at improving the model's ability to identify the minority class.\nPrecision: The Baseline scenario achieved the highest precision, suggesting a more conservative approach in predicting the minority class.\nMCC (Matthews Correlation Coefficient): SMOTE and Decision Threshold Calibration tied for the best performance, indicating a good balance between true and false positives and negatives.\nPR-AUC and AUC: These metrics showed relatively small differences across scenarios. Notably, SMOTE and Class Weights did not deteriorate performance on these metrics compared to the Baseline. As expected, Decision Threshold Calibration, being a post-model adjustment, does not materially impact these probability-based metrics (as well as Brier-Score).\nAccuracy: The Baseline scenario achieved the highest accuracy, which is common in imbalanced datasets where high accuracy can be achieved despite poor minority class detection.\nLog-Loss: The Baseline scenario performed best, suggesting it produces the most well-calibrated probabilities. SMOTE showed the highest log-loss, indicating potential issues with probability calibration.\nBrier-Score: As expected, the Baseline and Decision Threshold scenarios show identical performance, as Decision Threshold Calibration is a post-prediction adjustment and doesn't affect the underlying probabilities used in the Brier Score calculation. Notably, SMOTE performed significantly worse on this metric, indicating it produces poorly calibrated probabilities compared to the other scenarios.\n\nBased on these observations, Decision Threshold Calibration demonstrates strong performance across several key metrics, particularly those focused on minority class prediction (F1-score, F2-score, and Recall). It achieves this without compromising the calibration of probabilities of the baseline model, as evidenced by the identical Brier Score. In contrast, while SMOTE improves minority class detection, it leads to the least well-calibrated probabilities, as shown by its poor Brier Score. This suggests that Decision Threshold Calibration could be particularly effective in scenarios where accurate identification of the minority class is crucial, while still maintaining the probability calibration of the original model.\n\nFor the rest of this article, we will focus on the F1-score due to its balanced representation of precision and recall, which is particularly important in imbalanced classification tasks.\n\nResults by Model\n\nFigure 2 presents the mean F1-scores and standard deviations for each of the 15 models across the four scenarios. Each model's scores are averaged across the 30 datasets.\n\n\nFigure 2: Mean F1-scores and standard deviations for each model across the four scenarios. Highest values per model are highlighted in blue.\n\nKey observations from these results include:\n\nScenario Comparison: For each model, we compared the performance of the four scenarios (Baseline, SMOTE, Class Weights, and Decision Threshold Calibration). This within-model comparison is more relevant than comparing different models to each other, given the diverse nature of the classifier techniques.\n\n\nDecision Threshold Performance: The Decision Threshold Calibration scenario achieved the highest mean F1-score in 10 out of 15 models. Notably, even when it wasn't the top performer, it consistently remained very close to the best scenario for that model.\n\n\nOther Scenarios: Within individual models, Class Weights performed best in 3 cases, while SMOTE and Baseline each led in 1 case.\n\n\nConsistent Improvement: All three imbalance handling techniques generally showed improvement over the Baseline scenario across most models, with 1 exception.\n\n\nThese results indicate Decision Threshold Calibration was most frequently the top performer across the 15 models. This suggests that post-model adjustments to the decision threshold is a robust strategy for improving model performance across different classifier techniques. However, the strong performance of other techniques in some cases underscores the importance of testing multiple approaches when dealing with imbalanced datasets in practice.\n\nResults by Dataset\n\nFigure 3 presents the mean F1-scores and standard deviations for each of the 30 datasets across the four scenarios.\n\nFigure 3: Mean F1-scores and standard deviations for each dataset across the four scenarios. Highest values per dataset are highlighted in blue.\n\nThese results are aggregated across the 15 models for each dataset. While this provides insights into overall trends, in practice, one would typically seek to identify the best model-scenario combination for a given dataset under consideration.\n\nKey observations from these results include:\n\nVariability: There is substantial variability in which scenario performs best across different datasets, highlighting that there is no one-size-fits-all solution for handling class imbalance.\n\nScenario Performance:\n\nDecision Threshold Calibration was best for 12 out of 30 datasets (40%)\nSMOTE was best for 9 datasets (30%)\nClass Weights was best for 7 datasets (23.3%)\nBaseline was best for 3 datasets (10%)\nThere was one tie between SMOTE and Class Weights\n\nImprovement Magnitude: The degree of improvement over the Baseline varies greatly across datasets, from no improvement to substantial gains (e.g., satellite vs abalone_binarized).\n\nBenefit of Imbalance Handling: While no single technique consistently outperformed others across all datasets, the three imbalance handling strategies generally showed improvement over the Baseline for most datasets.\n\nThese results underscore the importance of testing multiple imbalance handling techniques for each specific dataset and task, rather than relying on a single approach. The variability observed suggests that the effectiveness of each method may depend on the unique characteristics of each dataset.\n\nOne notable observation is the contrast between these dataset-level results and the earlier model-level results. While the model-level analysis suggested Decision Threshold Calibration as a generally robust approach, the dataset-level results show much more variability. This apparent discrepancy highlights the complexity of handling class imbalance and suggests that the effectiveness of different techniques may be more dependent on dataset characteristics than on model type.\n\nStatistical Analysis\n\nTo rigorously compare the performance of the four scenarios, we conducted statistical tests on the F1-scores aggregated by dataset (averaging across the 15 models for each dataset).\n\nRepeated Measures ANOVA\n\nWe performed a repeated measures ANOVA to test for significant differences among the four scenarios. For this test, we have 30 datasets, each with four scenario F1-scores, resulting in 120 data points. The null hypothesis is that there are no significant differences among the mean F1-scores of the four scenarios. We use Repeated Measures ANOVA to account because we have multiple measurements (scenarios) for each dataset.\n\nResult: The test yielded a p-value of 2.01e-07, which is well below our alpha level of 0.05.\nInterpretation: This result indicates statistically significant differences among the mean F1-scores of the four scenarios.\nPost-hoc Pairwise Comparisons\n\nFollowing the significant ANOVA result, we conducted post-hoc pairwise comparisons using a Bonferroni correction to adjust for multiple comparisons. With 6 comparisons, our adjusted alpha level is 0.05/6 = 0.0083.\n\nThe p-values for the pairwise comparisons are presented in Table 1.\n\nTable 1: P-values for pairwise comparisons (Bonferroni-corrected)\n\nScenario\tClass Weights\tDecision Threshold\tSMOTE\nBaseline\t7.77e-05\t2.26e-04\t1.70e-03\nClass Weights\t-\t2.06e-03\t1.29e-01\nDecision Threshold\t-\t-\t2.83e-02\n\nKey findings from the pairwise comparisons:\n\nThe Baseline scenario is significantly different from all other scenarios (p < 0.0083 for all comparisons).\nClass Weights is significantly different from Baseline and Decision Threshold, but not from SMOTE.\nThere is no significant difference between SMOTE and Decision Threshold, or between SMOTE and Class Weights at the adjusted alpha level.\n\nThese results suggest that while all three imbalance handling techniques (SMOTE, Class Weights, and Decision Threshold) significantly improve upon the Baseline, the differences among these techniques are less pronounced. The Decision Threshold approach shows a significant improvement over Baseline and Class Weights, but not over SMOTE, indicating that both Decision Threshold and SMOTE may be equally effective strategies for handling class imbalance in many cases.\n\nDiscussion of Results\nKey Findings and Implications\n\nOur comprehensive study on handling class imbalance in binary classification tasks yielded several important insights:\n\nAddressing Class Imbalance: Our results strongly suggest that handling class imbalance is crucial for improving model performance. Across most datasets and models, at least one of the imbalance handling techniques outperformed the baseline scenario, often by a significant margin.\n\n\n\nEffectiveness of SMOTE: SMOTE demonstrated considerable effectiveness in minority class detection, showing significant improvements over the baseline in many cases. It was the best-performing method for 30% of the datasets, indicating its value as a class imbalance handling technique. However, it's important to note that while SMOTE improved minority class detection, it also showed the worst performance in terms of probability calibration, as evidenced by its high Log-Loss and Brier Score. This suggests that while SMOTE can be effective for improving classification performance, it may lead to less reliable probability estimates. Therefore, its use should be carefully considered in applications where well-calibrated probabilities are crucial.\n\n\n\nOptimal Method: Decision Threshold Calibration emerged as the most consistently effective technique, performing best for 40% of datasets and showing robust performance across different model types. It's also worth noting that among the three methods studied, Decision Threshold Calibration is the least computationally expensive. Given its robust performance and efficiency, it could be considered a strong default choice for practitioners dealing with imbalanced datasets.\n\n\n\nVariability Across Datasets: Despite the overall strong performance of Decision Threshold Calibration, we observed substantial variability in the best-performing method across datasets. This underscores the importance of testing multiple approaches for each specific problem.\n\n\n\nImportance of Dataset-Level Analysis: Unlike many comparative studies on class imbalance that report results at the model level aggregated across datasets, our study emphasizes the importance of dataset-level analysis. We found that the best method can vary significantly depending on the dataset characteristics. This observation highlights the necessity of analyzing and reporting findings at the dataset level to provide a more nuanced and practical understanding of imbalance handling techniques.\n\nStudy Limitations and Future Work\n\nWhile our study provides valuable insights, it's important to acknowledge its limitations:\n\nFixed Hyperparameters: We used previously determined model hyperparameters. Future work could explore the impact of optimizing these hyperparameters specifically for imbalanced datasets. For instance, adjusting the maximum depth in tree models might allow for better modeling of rare classes.\n\n\n\nStatistical Analysis: Our analysis relied on repeated measures ANOVA and post-hoc tests. A more sophisticated approach, such as a mixed-effects model accounting for both dataset and model variability simultaneously, could provide additional insights and is an area for future research.\n\n\n\nDataset Characteristics: While we observed variability in performance across datasets, we didn't deeply analyze how specific dataset characteristics (e.g., sample size, number of features, degree of imbalance) might influence the effectiveness of different methods. Future work could focus on identifying patterns in dataset characteristics that predict which imbalance handling technique is likely to perform best.\n\n\n\nLimited Scope of Techniques: Our study focused on three common techniques for handling imbalance. Future research could expand this to include other methods or combinations of methods.\n\n\n\nPerformance Metric Focus: While we reported multiple metrics, our analysis primarily focused on F1-score. Different applications might prioritize other metrics, and the relative performance of these techniques could vary depending on the chosen metric.\n\nThese limitations provide opportunities for future research to further refine our understanding of handling class imbalance in binary classification tasks. Despite these limitations, our study offers valuable guidance for practitioners and researchers dealing with imbalanced datasets, emphasizing the importance of addressing class imbalance and providing insights into the relative strengths of different approaches.\n\nConclusion\n\nOur study provides a comprehensive evaluation of three widely used strategies—SMOTE, Class Weights, and Decision Threshold Calibration—for handling imbalanced datasets in binary classification tasks. Compared to a baseline scenario where no intervention was applied, all three methods demonstrated substantial improvements in key metrics related to minority class detection, particularly the F1-score, across a wide range of datasets and machine learning models.\n\nThe results show that addressing class imbalance is crucial for improving model performance. Decision Threshold Calibration emerged as the most consistent and effective technique, offering significant performance gains across various datasets and models. SMOTE also performed well, and Class Weights tuning proved to be a reasonable method for handling class imbalance, showing moderate improvements over the baseline.\n\nHowever, the variability in performance across datasets highlights that no single method is universally superior. Therefore, practitioners should consider testing multiple approaches and tuning them based on their specific dataset characteristics.\n\nWhile our study offers valuable insights, certain areas could be explored in future research. We fixed the hyperparameters across scenarios to ensure fair comparisons, holding all factors constant except for the treatment. Future research could investigate optimizing hyperparameters specifically for imbalanced datasets. Additionally, further work could explore how specific dataset characteristics influence the effectiveness of different techniques. Expanding the scope to include other imbalance handling methods or combinations of methods would also provide deeper insights. While our primary analysis focused on the F1-score, results for other metrics are available, allowing for further exploration and custom analyses based on different performance criteria.\n\nIn conclusion, our findings emphasize the importance of addressing class imbalance and offer guidance on choosing appropriate techniques based on dataset and model characteristics. Decision Threshold Calibration, with its strong and consistent performance, can serve as a valuable starting point for practitioners dealing with imbalanced datasets, but flexibility and experimentation remain key to achieving the best results.\n\nAdditional Resources\n\nTo support the reproducibility of our study and provide further value to researchers and practitioners, we have made several resources publicly available:\n\nModel Repositories: Implementations of all 15 models used in this study are available in separate repositories. These can be accessed through links provided in the \"Models\" section of this publication.\n\n\n\nDataset Repository: The 30 datasets used in our study are available in a GitHub repository titled \"30 Imbalanced Classification Study Datasets\". This repository includes detailed information about each dataset's characteristics and sources.\n\nGitHub link: \nhttps://github.com/readytensor/rt-datasets-binary-class-imbalance\n\nResults Repository: A comprehensive collection of our study results is available in a GitHub repository titled \"Imbalanced Classification Results Analysis\". This includes detailed performance metrics and analysis scripts.\n\nGitHub link: \nhttps://github.com/readytensor/rt-binary-class-imbalance-results\n\nHyperparameters: The hyperparameters used in the experiment are listed in the hyperparmeters.csv file in the \"Resources\" section.\n\nAll project work is open-source, encouraging further exploration and extension of our research. We welcome inquiries and feedback from the community. For any questions or discussions related to this study, please contact the authors at \ncontact@readytensor.com\n.\n\nWe encourage researchers and practitioners to utilize these resources to validate our findings, conduct further analyses, or extend this work in new directions.\n\nReferences\nBatista, G.E., Prati, R.C., & Monard, M.C. (2004). A study of the behavior of several methods for balancing machine learning training data. ACM SIGKDD Explorations Newsletter, 6(1), 20-29.\nBlagus, R., & Lusa, L. (2013). SMOTE for high-dimensional class-imbalanced data. BMC Bioinformatics, 14(1), 106.\nBunkhumpornpat, C., Sinapiromsaran, K., & Lursinsap, C. (2009). Safe-level-SMOTE: Safe-level-synthetic minority over-sampling technique for handling the class imbalanced problem. In Advances in Knowledge Discovery and Data Mining: 13th Pacific-Asia Conference, PAKDD 2009 Bangkok, Thailand, April 27-30, 2009 Proceedings (pp. 475-482). Springer Berlin Heidelberg.\nChawla, N.V., Bowyer, K.W., Hall, L.O., & Kegelmeyer, W.P. (2002). SMOTE: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16, 321-357.\nElor, Y., & Averbuch-Elor, H. (2022). To SMOTE, or not to SMOTE? arXiv preprint arXiv:2201.08528.\nHan, H., Wang, W.Y., & Mao, B.H. (2005, August). Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning. In International Conference on Intelligent Computing (pp. 878-887). Berlin, Heidelberg: Springer Berlin Heidelberg.\nKovács, G. (2019). SMOTE-variants: A Python implementation of 85 minority oversampling techniques. Neurocomputing, 366, 352-354.\nVan Hulse, J., Khoshgoftaar, T.M., & Napolitano, A. (2007, June). Experimental perspectives on learning from imbalanced data. In Proceedings of the 24th International Conference on Machine Learning (pp. 935-942).\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nMethodology\n\nDatasets\n\nModels\n\nEvaluation Metrics\n\nExperimental Procedure\n\nResults\n\nOverall Comparison\n\nResults by Model\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nFiles\nimbalanced-models-parameters.csv\nhyperparameters.csv\nYou might be interested\nSurface to Volume Penalized Decision Boundary For Binary Imbalanced Classification\nR\nAug 09, 202413 reads\nClassificationDecision Tree+1\nFocus on Vision: A Statistical Study of Eye\nU\nApr 28, 202511 reads\nClassificationDeep Learning+8\nClassification of tumour data and metastasis\nM\nDec 29, 202414 reads\nData ClasificationMachine Learning+1\nComprehensive Guide to Evaluation Metrics for Computer Vision Task\nT\nJan 29, 202511 reads\nComputer VisionModel Evaluation",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "best-practices-for-ai-project-code-repositories-0llldKKtn8Xb",
    "username": "Ready Tensor",
    "license": "This article presents a comprehensive framework for creating and structuring AI/ML project repositories that maximize accessibility, reproducibility, and community benefit. We introduce a three-tiered evaluation system, namely, Essential, Professional, and Elite, to help practitioners assess and improve their code repositories at appropriate levels of rigor. The framework encompasses five critical categories: Documentation, Repository Structure, Environment and Dependencies, License and Legal considerations, and Code Quality. Drawing from industry standards and best practices, we provide concrete criteria, common pitfalls, and practical examples that enable AI practitioners, researchers, and students to create repositories that serve as valuable resources for both their creators and the wider community. By implementing these practices, contributors can enhance their professional portfolios while simultaneously advancing open science principles in the AI landscape.",
    "title": "The Open Source Repository Guide: Best Practices for Sharing Your AI/ML and Data Science Projects",
    "publication_description": "Back to publications\nFeb 05, 2025\n●\n508 reads\n●\nCreative Commons Attribution (CC BY)\nThe Open Source Repository Guide: Best Practices for Sharing Your AI/ML and Data Science Projects\nHub:\nReady Tensor Academy\nAIProjectStructure\nAIRepositories\nBestPractices\nCodeOrganization\nCodeQuality\nDataScience\nDocumentationPractices\nGitHub\nGitHubBestPractices\nMachineLearning\nOpenSourceAI\nReproducibility\nReproducibleAI\nReusability\nReady Tensor\nMohamed Abdelhamid\nA\nAryan Sunil Patil\nKidist Demessie\nLike\nBookmark\nShare\n\nImage credit: https://www.pexels.com\n\nAbstract\n\nThis article presents a comprehensive framework for creating and structuring AI/ML project repositories that maximize accessibility, reproducibility, and community benefit. We introduce a three-tiered evaluation system, namely, Essential, Professional, and Elite, to help practitioners assess and improve their code repositories at appropriate levels of rigor. The framework encompasses five critical categories: Documentation, Repository Structure, Environment and Dependencies, License and Legal considerations, and Code Quality. Drawing from industry standards and best practices, we provide concrete criteria, common pitfalls, and practical examples that enable AI practitioners, researchers, and students to create repositories that serve as valuable resources for both their creators and the wider community. By implementing these practices, contributors can enhance their professional portfolios while simultaneously advancing open science principles in the AI landscape.\n\nRepository Assessment Tool\n\nBefore diving into the framework details, try our open-source repository assessment tool. It analyzes your repository against the criteria in this article and provides an objective score with specific improvement recommendations.\n\nGet started: \nRepository Assessment Tool\n\nNote: You'll need an OpenAI API key to run the assessment tool.\n\nIntroduction\n\nAI and machine learning have advanced dramatically through open collaboration. The field thrives on shared knowledge, with researchers and practitioners expected to contribute their work openly. For many, public repositories serve dual purposes: showcasing personal expertise and advancing collective understanding. Yet most shared repositories fall far short of professional standards that would make them truly valuable to the community.\n\nTake a moment to examine two different AI project repositories implementing the same ResNet18 image classification model:\n\nRepository A: \nhttps://github.com/readytensor/rt_img_class_jn_resnet18_exampleA\n\nRepository B: \nhttps://github.com/readytensor/rt_img_class_jn_resnet18_exampleB\n\nWhat did you notice?\n\nThe readme for Repository A contains a brief desription about the project. With no additional information related to pre-requisites, installation, implementation, and usage, visitors cannot determine how to use it or whether it's trustworthy. Most visitors spend less than 30 seconds on Repository A before moving on.\n\nRepository B provides clear organization and proper documentation. Visitors immediately understand what the project does and have enough information to use it effectively. Though both repositories contain the same technical work, one presents it in a way that builds trust and facilitates adoption.\n\nWhich repository would you want your name attached to?\n\nThe reality is that many AI/ML projects resemble Repository A. This is a missed opportunity to showcase the work effectively and benefit the community. A poorly created repository creates a negative impression that can impact career opportunities, collaboration potential, and project adoption.\n\nThis article presents a comprehensive framework to help you create repositories that are not just functional but truly valuable — repositories that answer four crucial questions for visitors:\n\nWhat is this about? (Clear communication of purpose and capabilities)\nWhy should I care? (Value proposition and applications)\nCan I trust it? (Demonstrated professionalism and quality)\nCan I use it? (Clear instructions and appropriate licensing)\n\nWe organize best practices into five categories with three tiers of implementation (Essential, Professional, and Elite), allowing you to match your effort to project needs and resource constraints. Whether you are a student showcasing class projects, a researcher publishing code alongside a paper, or a professional building tools for broader use, these guidelines will help you create repositories that enhance your professional portfolio and contribute meaningfully to the field.\n\nImportant Note\n\nWhile this framework primarily targets AI/ML and data science projects, most concepts apply to software development repositories in general. The principles of good documentation, organization, and reproducibility benefit all code projects regardless of domain.\nMany criteria in the current framework are specifically designed for Python-based implementations, reflecting its prevalence in AI/ML work. Future iterations will expand to address the unique requirements of other languages such as R, JavaScript, and others.\nThis article focuses on repository structure and sharing practices, not on AI/ML methodology itself. Even the most technically sound AI/ML project may fail to gain community adoption if it cannot be easily understood, trusted, and used by others. We aim to help you effectively share your work, not instruct you on how to conduct that work in the first place.\nWhy Well-Organized Repositories Matter\n\nFor AI/ML engineers and data scientists, the quality of your code repositories directly impacts your work efficiency, career progression, and contribution to the community in three fundamental ways:\n\nTime Savings Through Enhanced Usability\nWell-structured repositories dramatically improve your own productivity by making your work reusable and maintainable. When you properly document and organize code, you avoid spending hours rediscovering how your own implementations work months later. Data scientists frequently report spending more time understanding and fixing old code than writing new solutions. Clean dependency management prevents environment reconstruction headaches, allowing you to immediately resume work on interesting problems rather than debugging configuration issues. This organization also makes your code extensible—when you want to build on previous work, add features, or adapt models to new datasets, the foundation is solid and understandable.\n\nCareer Advancement Through Professional Demonstration\nYour repositories serve as concrete evidence of your professional capabilities. Hiring managers and potential collaborators regularly evaluate GitHub profiles when assessing candidates, often placing repository quality on par with technical skills. A well-organized repository demonstrates not just coding ability but also production readiness, attention to detail, and consideration for users - all qualities highly valued in professional settings. Many data scientists find that quality repositories lead to unexpected opportunities: conference invitations, collaboration requests, and interview offers frequently come from people who discovered their well-structured work. In a field where practical implementation matters as much as theoretical knowledge, your repositories form a crucial part of your professional identity.\n\nCommunity Impact Through Accessible Knowledge\nThe collective advancement of AI/ML depends on shared implementations and reproducible research. When you create quality repositories, you help others avoid reinventing solutions to common problems, allowing the field to progress more rapidly. Consider the frustration you have experienced trying to implement papers with missing details or the hours spent making someone else's code work. Your well-organized repository prevents others from facing these same challenges. Repositories that clearly answer what the project does, why it matters, whether it can be trusted, and how to use it become valuable community resources rather than one-time demonstrations. Every properly structured repository contributes to building a more collaborative, efficient AI ecosystem.\n\nInvesting time in repository quality is not about perfectionism — it is about practical benefits that directly affect your daily work, career trajectory, and impact on the field. The framework presented in this article provides a structured approach to realizing these benefits in your own projects.\n\nBest Practices Framework\n\nThe AI repository best practices framework provides a structured approach to organizing and documenting code repositories for AI and machine learning projects. It establishes clear standards across five critical categories, with tiered implementation levels to accommodate different project stages and requirements.\n\nFramework Structure\n\nThe framework organizes best practices into five main categories:\n\nDocumentation: The written explanations and guides that help users understand and use your project\nRepository Structure: The organization of directories and files within your repository\nEnvironment and Dependencies: The specification of software requirements and configuration needed to run your code\nLicense and Legal: The permissions and terms governing the use of your code and associated assets\nCode Quality: The technical standards and practices applied to your codebase\n\nEach category contains specific criteria that can be assessed to determine if a repository meets established standards. Rather than presenting these as an all-or-nothing requirement, the framework defines three progressive tiers of implementation:\n\nImplementation Tiers\n\nThe best practices framework is structured into three tiers of implementation - Essential, Professional, and Elite. You can select the tier that aligns with your project goals, audience expectations, and available resources.\n\nTier\tDefinition\tKey Characteristics\tAppropriate For\nEssential\tMinimum standards for usefulness\t• Basic understandability for first-time visitors\n• Sufficient information for technical users\n• Basic organizational structure\t• Personal projects\n• Course assignments\n• Early-stage research code\n• Proof-of-concept implementations\nProfessional\tComprehensive documentation and organization\t• Detailed guidance for various users\n• Consistent structure and organization\n• Complete environment specifications\n• Established coding standards\n• Testing frameworks and documentation\t• Team projects\n• Open-source projects with contributors\n• Published research code\n• Professional portfolio work\n• Small production-quality projects\nElite\tBest-in-class practices\t• Comprehensive project documentation\n• Meticulous logical structures\n• Robust dependency management\n• Complete legal compliance\n• Advanced quality assurance\t• Major open-source projects\n• Production-level repositories\n• Research code for broad adoption\n• Reference implementations\n\nThe tiered structure allows for incremental implementation, with each level building on the previous one. This progressive approach makes the framework accessible to projects of different scales and maturity levels.\n\nThe framework is not prescriptive about specific technologies or tools, focusing instead on the underlying principles of good repository design. This flexibility allows it to be applied across different programming languages, AI/ML frameworks, and project types.\n\nThe criteria fall under 5 main categories. These are: Documentation, Repository Structure, Environment and Dependencies, License and Legal, and Code Quality.\n\nEach criterion in the framework is designed to be objectively assessable, making it possible to evaluate repositories systematically. This assessment can be conducted manually or through automated tools that check for the presence of specific files, structural patterns, or documentation elements.\n\nIn the following sections, we will explore each category in detail, examining specific criteria, providing examples, and offering implementation guidance for each tier.\n\nDocumentation\n\nDocumentation is the foundation of a user-friendly repository, serving as the primary interface between your code and its potential users. Well-crafted documentation answers fundamental questions about your project: what it does, why it matters, how to use it, and what to expect from it.\n\nUnfortunately, documentation is often treated as an afterthought, creating immediate barriers to adoption. The following chart lists the common pitfalls in documentation.\n\nMany repositories suffer from missing or minimal README files, leaving users with no understanding of project purpose or functionality. Others lack clear installation instructions, causing users to encounter confusing errors during setup. Without usage examples, users cannot verify if the implementation meets their needs. Undocumented prerequisites and methodologies further compound these issues, leaving critical information hidden until users encounter mysterious failures.\n\nThe documentation component of our framework addresses these challenges through a structured approach that scales with project complexity. The following chart lists the criteria for Essential, Professional, and Elite documentation tiers, guiding you to create effective documentation that meets user needs at every level.\n\nDetailed definitions of each of the criteria are provided in the document titled Ready Tensor Repository Assessment Framework v1.pdf available in the Resources section of this publication.\n\nLet's explore the key principles of documentation at each tier.\n\nEssential Documentation provides the minimum information needed for basic understanding and use. It answers \"What is this project?\", \"Can I use it?\", and \"How do I use it?\" — enabling quick evaluation and adoption with minimal friction.\n\nProfessional Documentation supports serious adoption by providing comprehensive setup instructions, detailed usage guides, and technical specifications. It addresses users who plan to incorporate your work into their projects, answering \"How does this work under different conditions?\" and \"What configuration options exist?\" Professional documentation also demonstrates trustworthiness for production environments by incorporating testing procedures, error handling approaches, and other reliability features that signal production readiness.\n\nElite Documentation fosters a sustainable ecosystem around your project through contribution guidelines, change tracking, and contact information. It creates pathways for collaboration, answering \"How can I contribute?\" and \"How is this project evolving?\"\n\nEffective documentation transforms your repository from personal code storage into a valuable community resource, significantly increasing your project's accessibility, adoption, and impact regardless of its scale.\n\nRepository Structure\n\nA well-organized repository structure provides a solid foundation for your AI/ML project, making it easier for users to navigate, understand, and contribute to your code. Proper structure serves as a visual map of your project's architecture and components, guiding users through your implementation.\n\nPoorly organized AI/ML repositories create significant barriers to understanding and use. The following chart illustrates common pitfalls in repository structure.\n\nAI/ML project repositories often exhibit a chaotic root directory filled with dozens of unrelated files, making it difficult to identify entry points or understand the project's organization. Code, configuration, and data files might be randomly mixed together without logical separation. Inconsistent or confusing naming conventions create additional cognitive load for new users trying to understand the codebase. Many repositories also lack clear boundaries between different components, such as model definition, data processing, and evaluation code.\n\nTo address repository organization challenges, our framework offers systematic guidelines that adapt to project size. The chart below presents Essential, Professional, and Elite structure criteria, designed to help you create intuitive and maintainable organization.\n\nLet's explore the key principles of repository structure at each tier.\n\nEssential Structure provides the minimum level of organization needed for basic navigation and understanding. It establishes a basic modular organization with logical separation of files, consistent and descriptive naming conventions for files and directories, a properly configured .gitignore file, and clearly identifiable entry points. This level focuses on answering \"Where do I find what I need?\" and \"How do I start using this?\"\n\nProfessional Structure enhances navigability and maintainability through specific separation of components. It organizes code in dedicated module structures (such as src/ directories with submodules), places data in designated directories, separates configuration from code, and organizes notebooks, tests, documentation, and assets in their own logical locations. Professional repositories maintain appropriate directory density (under 15 files per directory) and reasonable directory depth (no more than 5 levels deep). They also properly isolate environment configuration files and dependency management structures. This level signals that the project is built for serious use and collaboration.\n\nElite Structure builds on the Professional tier with the same organizational principles applied at a higher standard of consistency and completeness. The Elite structure maintains all the same criteria as Professional repositories but with greater attention to detail and thoroughness across all components. This comprehensive organization demonstrates adherence to industry best practices, making the project immediately familiar to experienced developers.\n\nA thoughtfully designed repository structure communicates professionalism and attention to detail, significantly reducing the barrier to entry for new users while improving maintainability for contributors. It transforms your repository from a personal collection of files into an accessible, professional software project that others can confidently build upon.\n\nEnvironment and Dependencies\n\nProper environment and dependency management is critical for ensuring that AI/ML projects can be reliably reproduced and used by others. This aspect of repository design directly impacts whether users can successfully run your code without frustrating setup issues or unexpected behavior.\n\nMany repositories fail to adequately address environment configuration, leading to the infamous \"works on my machine\" problem. The following chart highlights common pitfalls in environment and dependency management.\n\nDependency management problems appear when repositories fail to specify required libraries clearly, forcing users to guess which packages they need. When dependencies do appear, they often lack version numbers, creating compatibility problems as package APIs evolve. Missing documentation about Python version requirements or hardware dependencies leads to confusing errors when users attempt to run code in unsuitable environments.\n\nThe environment and dependencies section of our framework provides solutions that grow with project sophistication. Below are the tiered criteria (Essential, Professional, and Elite) that guide reproducible environment setup.\n\nLet's explore the key principles of environment and dependency management at each tier.\n\nEssential Environment Management provides the minimum information needed for basic reproducibility. It clearly lists all project dependencies in standard formats such as requirements.txt, setup.py, or pyproject.toml. This level focuses on answering \"What packages do I need to install?\" allowing users to at least attempt to recreate the necessary environment.\n\nProfessional Environment Management enhances reproducibility and ease of setup by pinning specific dependency versions to ensure consistent behavior across installations. It organizes dependencies into logical groups (core, dev, test) through separate requirement files or configuration options. Professional repositories specify required Python versions and include configuration for virtual environments such as environment.yml (conda), Pipfile (pipenv), or poetry.lock (poetry). This level provides confidence that the project can be reliably set up and run in different environments.\n\nElite Environment Management optimizes for complete reproducibility and deployment readiness. It provides exact environment specifications through lockfiles, documents GPU-specific requirements including CUDA versions when applicable, and includes containerization through Dockerfiles or equivalent solutions. This comprehensive approach ensures that users can recreate the exact execution environment regardless of their underlying system, eliminating \"it works on my machine\" issues entirely.\n\nProper environment and dependency management transforms your repository from a collection of code that runs only in specific conditions into a reliable, reproducible project that users can confidently deploy in their own environments. This attention to reproducibility demonstrates professional rigor and significantly increases the likelihood that others will successfully use and build upon your work.\n\nLicense and Legal\n\nProper licensing and legal documentation is a critical aspect of AI/ML repositories that is frequently overlooked. Without clear licensing, potential users cannot determine whether they can legally use, modify, or build upon your work, regardless of its technical quality.\n\nMany repositories either omit licenses entirely or include inappropriate licenses for their content. The following chart highlights common pitfalls in licensing and legal aspects.\n\nLegal issues arise when repositories operate without licenses, creating ambiguity that prevents use by organizations with compliance concerns. Some repositories include licenses that conflict with their dependencies, while others neglect the unique legal aspects of AI/ML work regarding data and model rights. The absence of copyright notices and unclear terms for incorporated datasets or pretrained models further complicates legitimate use.\n\nFor proper licensing and legal considerations, our framework provides clear benchmarks at varying complexity levels. The following chart presents Essential, Professional, and Elite tier criteria for legal compliance and clarity.\n\nLet's explore the key principles of licensing and legal documentation at each tier.\n\nEssential Legal Documentation ensures that users can determine basic usage rights. It includes a recognized license file (LICENSE, LICENSE.md, or LICENSE.txt) in the root directory that explicitly states terms of use, modification, and distribution. The chosen license must be appropriate for the project's purpose, dependencies, and intended use, avoiding unclear or conflicting terms. This level answers the fundamental question: \"Am I legally permitted to use this?\"\n\nProfessional Legal Documentation enhances legal clarity by addressing AI/ML-specific concerns. In addition to proper licensing, it includes clear documentation of data usage rights, stating ownership, licensing, compliance requirements, and restrictions for any datasets used or referenced. Similarly, it documents model usage rights, specifying ownership, licensing terms, and redistribution policies for any ML models included or referenced. This level provides confidence that the project can be legally used in professional contexts.\n\nElite Legal Documentation establishes a comprehensive legal framework supporting long-term community engagement. It builds on the Professional tier by adding explicit copyright statements in source files and documentation to prevent ambiguity in legal rights and attribution. Elite repositories also include a Code of Conduct that outlines contributor behavior expectations, enforcement mechanisms, and reporting guidelines to foster an inclusive and respectful environment. This level demonstrates commitment to professional standards and community values.\n\nProper licensing and legal documentation transforms your repository from a potentially risky resource into a legally sound project that organizations and individuals can confidently incorporate into their work. This attention to legal concerns removes a significant barrier to adoption and signals professionalism to potential users and contributors.\n\nCode Quality\n\nCode quality is the foundation of maintainable, reliable AI/ML projects. While functional code can deliver results, high-quality code enables long-term sustainability, collaboration, and trust in your implementation.\n\nIn AI/ML repositories, functionality frequently takes precedence over quality, resulting in maintainability and reliability issues. The following chart highlights common code quality pitfalls.\n\nCode quality issues manifest in sprawling, monolithic scripts that defy debugging efforts. Excessive function length and high cyclomatic complexity make maintenance difficult. The prevalence of hardcoded values, minimal error handling, and lack of tests results in brittle, unpredictable code. In the AI/ML context, missing random seed settings compromise reproducibility, while poorly documented notebooks obscure the development process.\n\nOur framework tackles code quality through graduated standards appropriate for different project stages. The chart below details the Essential, Professional, and Elite criteria that promote maintainable, reliable code as projects evolve\n\nLet's explore the key principles of code quality at each tier.\n\nEssential Code Quality establishes basic maintainability by organizing code into functions and methods rather than monolithic scripts, keeping individual scripts under 500 lines, and implementing basic error handling through try/except blocks. It uses dedicated configuration files to separate parameters from code logic and sets random seeds to ensure reproducibility. For notebooks, it maintains reasonable cell length (under 100 lines) and includes markdown documentation (at least 10% of cells). This level provides the minimum quality needed for others to understand and use your code.\n\nProfessional Code Quality significantly enhances maintainability and reliability by implementing comprehensive best practices. Functions are kept under 50 lines, code duplication is limited, and hardcoded constants are minimized. Professional repositories use environment variables for sensitive configurations, implement logging, include tests with framework support, and provide docstrings with parameter and return documentation. They also implement type hints, use style checkers for consistent formatting, control function complexity, and include data validation. For notebooks, they import custom modules and manage output cells properly. This level demonstrates serious software engineering practices.\n\nElite Code Quality takes quality to production-grade standards by adding advanced practices such as comprehensive logging configuration, custom exception classes, and test coverage metrics. These repositories represent the highest standard of code quality, suitable for critical production environments and long-term maintenance.\n\nHigh-quality code communicates professionalism and reliability, significantly increasing confidence in your implementation. This attention to quality transforms your repository from working code into trustworthy software that others can confidently build upon, adapt, and maintain over time.\n\nImplementation Guide with Examples\n\nThe following steps outline a practical approach to creating high-quality AI/ML project repositories. For detailed examples of repository structures and README templates at each implementation tier, see Appendix A: Sample Repository Structures and Appendix B: Sample README Structures.\n\nStep 1: Select an Appropriate Template\n\nChoose a repository structure that matches your project complexity and goals:\n\nEssential: For personal projects, educational demonstrations, or proof-of-concepts\nProfessional: For team projects, research code intended for publication, or open-source contributions\nElite: For production systems, major open-source projects, or reference implementations\n\nRefer to Appendix A: Sample Repository Structures for detailed examples at each tier. Customize these templates to fit your specific needs while maintaining the core organizational principles. Remember that even a small project can benefit from good structure.\n\nStep 2: Choose the Right License\n\nSelect a license appropriate for your project's content and intended use:\n\nMIT License: Permissive license good for most software projects, allowing commercial use\nApache 2.0: Similar to MIT but with patent protections\nGPL (v3): Strong copyleft license requiring derivative works to be open-sourced\nCreative Commons: Various options for non-software content like datasets or documentation\n\nConsider the licenses of your dependencies, as they may constrain your options. Ensure your license is compatible with the libraries and frameworks you use.\n\nStep 3: Implement Environment and Dependency Management\n\nChoose the appropriate dependency management approach for your project:\n\nEssential: requirements.txt listing direct dependencies\nProfessional:\nPinned version numbers (numpy==1.21.0 instead of just numpy)\nSeparated requirements files for different purposes\nVirtual environment configuration (conda, venv, etc.)\nElite:\nLockfiles for exact reproduction (poetry.lock, Pipfile.lock)\nContainerization with Docker\nEnvironment variables for configuration\n\nDocument any non-Python dependencies or system requirements clearly in your README.\n\nStep 4: Create a Structured README\n\nDevelop a README that matches your target implementation tier. A well-structured README is critical as it's often the first thing visitors see when discovering your project.\n\nWhen creating your README:\n\nFocus on answering the four key questions: what the project is about, why users should care, whether they can trust it, and how they can use it\nMatch the detail level to your target tier (Essential, Professional, or Elite)\nInclude examples and code snippets where appropriate\nConsider adding screenshots or diagrams for visual clarity\n\nRefer to Appendix B: Sample README Structures for detailed templates at each implementation tier, from basic structures covering essential information to comprehensive documents that support serious adoption and community engagement.\n\nStep 5: Follow Coding Best Practices\n\nAdopt established coding standards appropriate for your language:\n\nPython:\n\nFollow PEP 8 style guidelines (consistent indentation, naming conventions, etc.)\nUse type hints for function signatures\nWrite docstrings for modules, classes, and functions\nConsider using linters and formatters (black, flake8, pylint)\n\nMarkdown:\n\nUse proper heading hierarchy\nInclude code blocks with language specification\nUse lists, tables, and emphasis consistently\nAdd alt text to images for accessibility\n\nGeneral Practices:\n\nKeep functions small and focused on a single task\nWrite descriptive variable and function names\nInclude comments explaining \"why\" not just \"what\"\nControl script and function length\nSet random seeds for reproducibility in AI/ML code\n\nFor a comprehensive list of relevant tools and references, see the Additional Resources section at the end of this article, which includes links to code style guides, repository templates, documentation tools, and dependency management solutions.\n\nTools and Resources\n\nThe following tools can significantly reduce the effort required to implement best practices in your repositories:\n\nDocumentation Tools\nSphinx\n: Python documentation generator\nReadTheDocs\n: Documentation hosting platform\nMarkdown Guide\n: Project documentation with Markdown\nJupyter Book\n: Create publication-quality books from notebooks\nDocstring Conventions\n: Guide for Python docstrings\nRepository Structure and Templates\nCookiecutter\n: Project template tool\nCookiecutter Data Science\n: Template for data science projects\nPyScaffold\n: Project generator for Python packages\nnbdev\n: Create Python packages from Jupyter notebooks\nDependency Management\nuv\n: Fast Python package installer and resolver\nPoetry\n: Python packaging and dependency management\nConda\n: Package and environment management system\npip-tools\n: Set of tools for managing pip-compiled requirements\nPipenv\n: Python development workflow tool\nDocker\n: Containerization platform\nCode Quality and Testing\nPre-commit\n: Git hook scripts manager\nBlack\n: Uncompromising Python code formatter\nFlake8\n: Python code linter\nPylint\n: Python static code analysis tool\nmypy\n: Static type checker for Python\npytest\n: Python testing framework\nCoverage.py\n: Code coverage measurement for Python\nLicense Resources\nLicense Guide\n: A primer on licenses for ML projects\nChoose a License\n: Help picking an open source license\nOpen Source Initiative\n: License information and standards\nTL;DR Legal\n: Software licenses explained in plain English\nCreative Commons\n: Licenses for non-code assets\nStyle Guides and Standards\nPEP 8\n: Style Guide for Python Code\nGoogle Python Style Guide\n: Comprehensive style guide\nDocstrings Guide\n: Python Docstrings for Machine Learning code\n\nThese tools address different aspects of repository quality, offering options for projects of all scales. Select tools that match your project needs and team capabilities rather than adopting everything at once.\n\nConclusion\n\nWell-structured repositories are essential for the success of AI/ML projects in the wider community. Our framework addresses five fundamental aspects of repository quality:\n\nDocumentation that communicates purpose, usage, and technical details\nRepository Structure that organizes code logically\nEnvironment and Dependencies that enable reproducibility\nLicense and Legal considerations that establish usage rights\nCode Quality standards that ensure maintainability\n\nThe tiered approach, namely Essential, Professional, and Elite, allows you to match your effort to project needs and resource constraints. By evaluating your repositories against this framework, you can systematically improve their quality and impact. This will not only benefit your work efficiency and career prospects but also contribute to the wider AI/ML community.\n\nAppendices\nAppendix A: Sample Repository Structures\n\nThis appendix provides example repository structures for AI/ML projects at the Essential, Professional, and Elite levels. These examples are starting points that should be adapted to your specific project requirements, technology stack, and team preferences.\n\nA.1 Essential Repository Structure\n\nThis basic structure is suitable for simple projects, educational demonstrations, or exploratory research work primarily using Jupyter notebooks:\n\nproject-name/\n│\n├── README.md                 # Essential project information\n├── LICENSE                   # Appropriate license file\n├── requirements.txt          # Project dependencies\n├── .gitignore                # Configured for Python/Jupyter\n│\n├── notebooks/                # Organized notebooks\n│   ├── 01_data_exploration.ipynb\n│   ├── 02_preprocessing.ipynb\n│   └── 03_model_training.ipynb\n│\n├── data/                     # Data directory (often gitignored)\n│   ├── .gitkeep              # Placeholder to track empty directory\n│   └── README.md             # Data acquisition instructions\n│\n└── models/                   # Saved model files (often gitignored)\n    └── .gitkeep              # Placeholder to track empty directory\n\n\nKey Characteristics:\n\nClear separation of notebooks, data, and models\nSequential naming of notebooks to indicate workflow\nBasic documentation with README files\nSimple dependency management with requirements.txt\nA.2 Professional Repository Structure\n\nThis structure is appropriate for more advanced projects, team collaborations, or code intended for wider distribution:\n\nproject-name/\n│\n├── README.md                 # Comprehensive project documentation\n├── LICENSE                   # Appropriate license file\n├── setup.py                  # Package installation configuration\n├── requirements.txt          # Core dependencies\n├── requirements-dev.txt      # Development dependencies\n├── pyproject.toml            # Python project metadata\n├── .gitignore                # Configured for project needs\n│\n├── src/                      # Source code package\n│   └── project_name/         # Main package directory\n│       ├── __init__.py       # Package initialization\n│       ├── data/             # Data processing modules\n│       │   ├── __init__.py\n│       │   ├── loader.py\n│       │   └── preprocessor.py\n│       ├── models/           # Model implementation modules\n│       │   ├── __init__.py\n│       │   └── model.py\n│       ├── utils/            # Utility functions\n│       │   ├── __init__.py\n│       │   └── helpers.py\n│       └── config.py         # Configuration parameters\n│\n├── notebooks/                # Jupyter notebooks (if needed)\n│   ├── exploration.ipynb\n│   └── evaluation.ipynb\n│\n├── tests/                    # Test modules\n│   ├── __init__.py\n│   ├── test_data.py\n│   └── test_models.py\n│\n├── docs/                     # Documentation files\n│   ├── usage.md\n│   ├── api.md\n│   └── examples.md\n│\n├── data/                     # Data directory (often gitignored)\n│   └── README.md             # Data acquisition instructions\n│\n└── models/                   # Saved model outputs (often gitignored)\n    └── README.md             # Model usage information\n\n\nKey Characteristics:\n\nProper Python package structure with src layout\nModular organization of code with clear separation of concerns\nComprehensive documentation in dedicated directory\nTest directory that mirrors package structure\nSeparated dependency specifications for different purposes\nA.3 Elite Repository Structure\n\nThis structure demonstrates a comprehensive repository setup suitable for production-level projects, major open-source initiatives, or reference implementations:\n\nproject-name/\n│\n├── README.md                 # Main documentation with quick start guide\n├── LICENSE                   # Appropriate license file\n├── CHANGELOG.md              # Version history and changes\n├── CONTRIBUTING.md           # Contribution guidelines\n├── CODE_OF_CONDUCT.md        # Community standards\n├── setup.py                  # Package installation\n├── pyproject.toml            # Python project config (PEP 518)\n├── poetry.lock               # Locked dependencies (if using Poetry)\n├── requirements/             # Dependency specifications\n│   ├── base.txt              # Core requirements\n│   ├── dev.txt               # Development requirements\n│   ├── test.txt              # Testing requirements\n│   └── docs.txt              # Documentation requirements\n├── Dockerfile                # Container definition\n├── docker-compose.yml        # Multi-container setup\n├── .gitignore                # Git ignore patterns\n├── .pre-commit-config.yaml   # Pre-commit hook configuration\n├── .github/                  # GitHub-specific configurations\n│   ├── workflows/            # CI/CD workflows\n│   └── ISSUE_TEMPLATE/       # Issue templates\n│\n├── src/                      # Source code package\n│   └── project_name/         # Main package\n│       ├── __init__.py       # Package initialization with version\n│       ├── cli.py            # Command-line interface\n│       ├── config.py         # Configuration management\n│       ├── exceptions.py     # Custom exceptions\n│       ├── logging.py        # Logging configuration\n│       ├── data/             # Data processing\n│       ├── models/           # Model implementations\n│       └── utils/            # Utility functions\n│\n├── scripts/                  # Utility scripts\n│   ├── setup_environment.sh\n│   └── download_datasets.py\n│\n├── notebooks/                # Jupyter notebooks (if applicable)\n│   └── examples/             # Example notebooks\n│\n├── tests/                    # Test suite\n│   ├── conftest.py           # Test configuration\n│   ├── integration/          # Integration tests\n│   └── unit/                 # Unit tests organized by module\n│\n├── docs/                     # Documentation\n│   ├── conf.py               # Sphinx configuration\n│   ├── index.rst             # Documentation home\n│   ├── installation.rst      # Installation guide\n│   ├── api/                  # API documentation\n│   ├── examples/             # Example usage\n│   └── _static/              # Static content for docs\n│\n├── data/                     # Data directory (structure depends on project)\n│   ├── raw/                  # Raw data (often gitignored)\n│   ├── processed/            # Processed data (often gitignored)\n│   └── README.md             # Data documentation\n│\n└── models/                   # Model artifacts\n    ├── trained/              # Trained models (often gitignored)\n    ├── pretrained/           # Pretrained models\n    └── README.md             # Model documentation\n\n\nKey Characteristics:\n\nComprehensive community documents (CONTRIBUTING, CODE_OF_CONDUCT)\nAdvanced dependency management with separated requirements\nContainerization for reproducible environments\nCI/CD configuration for automated testing and deployment\nExtensive documentation with proper structure\nClear separation of all project components\nAdapting These Structures\n\nThese sample structures serve as templates that should be adapted based on:\n\nProject Size and Complexity: Smaller projects may not need all components shown in the Professional or Elite examples. Include only what serves your project's needs.\n\nTechnology Stack: While these examples focus on Python-based projects, adjust directory structures for other languages or frameworks accordingly.\n\nTeam Conventions: Align with existing conventions your team has established for consistency across projects.\n\nProject Type: Different AI/ML applications may require specialized structures:\n\nTime series forecasting projects might need additional data versioning\nComputer vision projects might require separate directories for images/videos\nNLP projects might benefit from corpus and vocabulary management structures\n\nDeployment Context: Projects deployed as APIs, web applications, or embedded systems will need additional structure to support their deployment environments.\n\nRemember that repository structure should facilitate development and use—not impose unnecessary overhead. Start with the simplest structure that meets your needs and expand as your project grows in complexity.\n\nAppendix B: Sample README Structures\n\nThis appendix provides example README structures for AI/ML projects at the Essential, Professional, and Elite levels. These templates offer a starting point that should be customized to fit your specific project needs and audience.\n\nB.1 Essential README Structure\n\nThis basic structure covers the minimum needed for a useful README:\n\n# Project Name\n\nBrief description of the project.\n\n## Overview\n\nDetailed explanation of what the project does and why it's useful.\n\n## Installation\n\nBasic installation instructions.\n\n## Usage\n\nSimple examples of how to use the project.\n\n## License\n\nInformation about the project's license.\n\nKey Characteristics:\n\nClear project identity with title and description\nBasic explanation of purpose and value\nSimple instructions for installation and use\nLicense information for legal clarity\nB.2 Professional README Structure\n\nThis comprehensive structure supports serious adoption:\n\n# Project Name\n\nBrief description of the project.\n\n## Overview\n\nDetailed explanation of what the project does and why it's useful.\n\n## Target Audience\n\nWho this project is intended for.\n\n## Prerequisites\n\nRequired knowledge, hardware, and system compatibility.\n\n## Installation\n\nStep-by-step installation instructions.\n\n## Environment Setup\n\nEnvironment and dependency information.\n\n## Usage\n\nDetailed usage instructions with examples.\n\n## Data Requirements\n\nExpected data formats and setup.\n\n## Testing\n\nHow to run tests for the project.\n\n## Configuration\n\nInformation on configuration options.\n\n## License\n\nInformation about the project's license.\n\n## Contributing\n\nGuidelines for contributing to the project.\n\nKey Characteristics:\n\nComprehensive project description with target audience\nDetailed prerequisites and installation steps\nThorough usage documentation with examples\nTechnical details on data, testing, and configuration\nCommunity engagement through contribution guidelines\nB.3 Elite README Structure\n\nThis advanced structure creates a complete resource for all users:\n\n# Project Name\n\nBrief description of the project.\n\n## Overview\n\nDetailed explanation of what the project does and why it's useful.\n\n## Target Audience\n\nWho this project is intended for.\n\n## Prerequisites\n\nRequired knowledge, hardware, and system compatibility.\n\n## Installation\n\nStep-by-step installation instructions.\n\n## Environment Setup\n\nEnvironment and dependency information.\n\n## Usage\n\nDetailed usage instructions with examples.\n\n## Data Requirements\n\nExpected data formats and setup.\n\n## Testing\n\nHow to run tests for the project.\n\n## Configuration\n\nInformation on configuration options.\n\n## Methodology\n\nExplanation of the approach and algorithms.\n\n## Performance\n\nBenchmarks and performance expectations.\n\n## License\n\nInformation about the project's license.\n\n## Contributing\n\nGuidelines for contributing to the project.\n\n## Changelog\n\nVersion history and key changes.\n\n## Citation\n\nHow to cite this project in academic work.\n\n## Contact\n\nHow to reach the maintainers.\n\nKey Characteristics:\n\nAll elements from Professional README\nTechnical depth with methodology and performance sections\nProject history through changelog\nAcademic integration with citation information\nMaintainer accessibility through contact information\nCustomizing README Content\n\nThese templates provide structure, but effective READMEs require thoughtful content:\n\nProject Description: Be clear and specific about what your project does. Avoid vague descriptions and technical jargon without explanation.\n\nExamples: Include concrete, runnable examples that demonstrate key functionality. Code snippets should be complete enough to execute with minimal modification.\n\nVisual Elements: Consider adding diagrams, screenshots, or other visual elements that clarify complex concepts or demonstrate the project in action.\n\nAudience Adaptation: Adjust technical depth based on your expected audience. Research projects may include more mathematical detail, while application-focused projects should emphasize practical usage.\n\nMaintenance Status: Clearly indicate the current maintenance status of the project, especially for open-source work.\n\nRemember that a README is often the first interaction users have with your project. It should provide enough information for users to quickly determine if the project meets their needs and how to get started using it.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nRepository Assessment Tool\n\nIntroduction\n\nWhy Well-Organized Repositories Matter\n\nBest Practices Framework\n\nFramework Structure\n\nImplementation Tiers\n\nDocumentation\n\nRepository Structure\n\nEnvironment and Dependencies\n\nView all\nCode\nFiles\nReady Tensor Repository Assessment Framework v1.pdf",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "bobcat-your-singing-ai-companion--bXMIbVyTqEm8",
    "username": "b@bobbyzhang26",
    "license": "MIT License",
    "title": "Bobcat! - Your Singing AI Companion 🎵🤖",
    "publication_description": "Back to publications\nDec 11, 2024\n●\n96 reads\n●\nMIT License\nBobcat! - Your Singing AI Companion 🎵🤖\nArducam IMX708 camera\nAudio Core HAT WM8060\nDeepFace\nEmpathetic Chat Responses\nmicrophone\nOpenAI 3.5 Turbo\nRaspberry Pi 4B\nReal-Time Facial Recognition\nSentiment Analysis\nSong and Lyrics Generation\nSpeech Recognition\nText-to-Speech (TTS-1-HD)\nUDIO\nVoice Transcription\nB\n@bobbyzhang26\nL\nLuopeiwen Yi\nS\nSongqiao Xie\nA\n@afraa.noureen\nLike\nBookmark\nShare\n\nBobcat: Your AI Companion That Sings Your Story\n\nBobcat is not just a gadget—it's your personal, empathetic AI friend that listens, responds, and creates songs tailored to your emotions and personal story. Conceived during Duke AI Hackathon 2024, this innovative project merges art, technology, and human connection.\n\nAbstract\n\nBobcat is a groundbreaking AI companion that seamlessly integrates advanced technology with human-centered design to create an emotionally intelligent and artistic user experience. Developed during the Duke AI Hackathon 2024, Bobcat is a portable device that listens to users' thoughts and feelings through voice input and facial recognition, analyzes their sentiments in real-time, and responds with personalized conversations and songs. By blending real-time mood detection with AI-generated music and lyrics, Bobcat redefines the role of AI from a tool to an empathetic companion. Its charm lies in its ability to bridge the gap between technology and emotional connection, fostering creativity, comfort, and joy. Equipped with cutting-edge technologies like DeepFace for facial recognition, sentiment analysis, and UDIO for personalized song creation, Bobcat is designed to resonate emotionally and artistically with users. With potential applications spanning mental health support, creative expression, and digital scalability, Bobcat stands as a testament to the transformative power of AI when merged with art and design.\n\n\nPresentation & WriteUp\n\nYou can find our project Devpost Page here: \nDevpost\n\nYou can find our project Source code here: \nGithub\n\nValue Proposition\n\nBobcat isn’t just another gadget; it’s a heartfelt, artistic companion that fulfills the fundamental human need for connection and creative expression. In a world where technology is often practical but impersonal, Bobcat shifts that narrative. It’s not a sterile tool but an engaging and empathetic muse that embodies the intersection of art, media, and design. With Bobcat, AI becomes more than an assistant — it transforms into a partner that brings joy, creativity, and comfort through personalized, artistic interaction. It’s a friend to share your day with, who responds with emotional insight and creative flair.\n\nWhy is this important? Because in today’s society, where stress and isolation are common, people crave technology that resonates emotionally and creatively. We’re living in an era where the fusion of art, technology, and mental well-being is more relevant than ever. Bobcat brings a touch of artistry to AI, offering a dynamic and media-rich experience that soothes the soul, uplifts the spirit, and fills the gap left by conventional tech products. It’s designed not just to assist, but to inspire and foster genuine connection through the universal language of art and music.\n\nWith Bobcat, you’re not just using technology—you’re collaborating with it to make art that feels alive.\n\nTechnical Accomplishments\n\nNow, let’s talk about how we brought Bobcat to life. Its intelligence lies in its seamless integration of advanced software and hardware, all within a charming, 3D-printed shell. Here’s what powers Bobcat:\n\nReal-Time Facial Recognition: Using DeepFace, Bobcat reads your facial expressions to understand your mood.\nVoice Transcription: With Speech Recognition, it transcribes your spoken words into text, ensuring it understands your thoughts.\nSentiment Analysis: By analyzing both your words and your expressions, Bobcat gauges your mood to tailor its response.\nEmpathetic Chat Responses: Powered by OpenAI 3.5 Turbo and Text-to-Speech (TTS-1-HD), Bobcat creates conversations that feel human, natural, and comforting.\nSong and Lyrics Generation: Here’s where the magic happens — UDIO composes a song with lyrics and melody that perfectly fit your current state of mind, bringing the song to life with a beautiful, warm voice that resonates with your emotions.\n\nUnder the hood, Bobcat runs on a Raspberry Pi 4B, giving it both brains and brawn. Equipped with an integrated microphone and an Arducam IMX708 camera, it’s built to listen, see, and respond with empathy. Audio comes to life through an Audio Core HAT WM8060, delivering high-quality sound that feels as warm as a real conversation. And with a lithium battery module, Bobcat stays powered on the go, ready to be your creative companion anywhere, anytime. This complete integration was no small feat; it took intricate work to achieve real-time operations smoothly, even within the constraints of a 48-hour hackathon.\n\nProject Pipeline\n\nBobcat’s real-time operations rely on seamless integration:\n\nFacial and Voice Input ➡️ Mood Detection (DeepFace + Sentiment Analysis)\nTranscription & Interpretation ➡️ Chat Response (Speech Recognition + GPT-3.5 Turbo + TTS)\nSong Composition ➡️ Performance Output (UDIO)\n\n\n\n\nKey Features\nMultimodal Sentiment Analysis: Combines DeepFace for facial recognition and speech-based sentiment analysis for a comprehensive understanding of your mood.\nVoice Transcription: Translates spoken input into text using Speech Recognition, enabling contextual awareness.\nEmpathetic AI Interaction: Chat responses powered by OpenAI GPT-3.5 Turbo and Text-to-Speech (TTS-1-HD) mimic natural, human-like conversation.\nDynamic Song Generation: Utilizes UDIO to compose and sing personalized songs that align with your emotional state, creating a unique, mood-reflective experience.\nPortable & Engaging Design: Encased in a 3D-printed body with a friendly aesthetic that invites interaction.\nHardware\nRaspberry Pi 4B: The processing hub that powers Bobcat.\nArducam IMX708 Camera: Captures facial expressions for mood detection.\nAudio Core HAT WM8060: Delivers high-quality sound for an immersive experience.\nIntegrated Microphone and Speakers: Enable voice interaction and audio feedback.\nLithium Battery Module: Ensures portability and long-lasting operation.\nSoftware Stack\nDeepFace: Facial recognition for real-time mood analysis.\nSpeech Recognition Library: Transcribes voice input to text.\nOpenAI GPT-3.5 Turbo: Empathetic, AI-driven conversation.\nText-to-Speech (TTS-1-HD): Natural, expressive vocal synthesis.\nUDIO: Generates and performs songs tailored to the detected emotional state.\nNovelty\n\nWhat makes Bobcat truly stand out? Bobcat isn’t just a chatbot or a music player. It’s an entirely new category of product — a real-time, AI-powered friend that listens, feels, and creates. Imagine interacting with a companion that doesn’t just respond, but sings a personalized song tailored to your mood. It’s like having an AI artist who knows you intimately, combining real-time mood detection with music generation in a way that hasn’t been done before.\n\nBobcat’s charm lies in its combination of:\n● Real-time, multimodal sentiment analysis (facial expression and voice input).\n● Unique, user-tailored responses that go beyond simple conversation to actual musical creation.\n● Physical form factor that’s cute and friendly, making it feel like a living toy you can bond with.\n\nCompetitive Advantage\n\nWhile voice assistants like Alexa and Siri excel in practical functions such as controlling smart devices or answering queries, they lack the emotional intelligence and creativity that Bobcat brings. What truly sets Bobcat apart is its physical design—crafted to feel approachable, friendly, and trustworthy. With its cute, charming appearance and soft, responsive interactions, Bobcat invites users to treat it as a companion, not just a tool. Whether you're stressed and in need of comfort or feeling joyful and eager to share your happiness, Bobcat is there to listen, understand, and create a personalized song that reflects your mood. Its physical form enhances this bond, making it feel more like a trusted friend you can turn to in any emotional state. Bobcat’s integration of real-time facial recognition, sentiment analysis and music generation offers a more human-like, emotionally resonant experience that goes beyond simple conversation. This unique fusion of empathy, creativity, and design elevates Bobcat far beyond conventional voice assistants, offering users a companion that not only responds but genuinely connects with their emotions.\n\nConstraints, Future Work, and Potential\n\nWe see Bobcat being loved by children who want a playful friend, adults who need a moment of comfort, and seniors who appreciate companionship. But we’re not stopping there. To enhance its reach and capabilities, future work will focus on expanding Bobcat into the digital space through cloud-based functionalities using AWS Lex and Polly. This will allow Bobcat to offer scalable, app-integrated interactions, making it accessible across devices and platforms like Slack and Discord. By moving to the cloud, users will be able to share their personalized Bobcat-generated songs with friends, fostering even richer emotional interactions and making it a companion that resonates on a broader scale. This way, Bobcat can reach anyone, anytime, offering the same heartwarming experience and creating moments of joy and connection that ripple across social media.\n\nConclusion\n\nTo sum up, Bobcat is a unique blend of smart technology, creative design, and heartfelt purpose. It’s not just another gadget; it’s your artistic companion that brings joy, comfort, and a touch of magic to your day. Whether it’s making you smile with a personalized song or responding with genuine empathy, Bobcat turns technology into something deeply human. It bridges the gap between art and connection, proving that AI can be both innovative and warm. Thank you, and step into a new era where tech isn’t just useful, but meaningful — welcome to Bobcat.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "building-a-whatsapp-based-tech-support-system-with-strapi-langchainjs-and-gpt-4o-ZUQQwIAz5kwM",
    "username": "mDenis Kuria",
    "license": "MIT License",
    "title": "Building a WhatsApp-based Tech Support System with Strapi, LangChain.js, and GPT-4o",
    "publication_description": "Back to publications\nMar 12, 2025\n●\n45 reads\n●\nMIT License\nWinner of\nDistinguished Implementation Guide\nat the\nAgentic AI Innovation Challenge 2025\nBuilding a WhatsApp-based Tech Support System with Strapi, LangChain.js, and GPT-4o\nLangChain\nLarge language models(llms)\nRAG\nStrapi\nWhatsAPP\nM\nDenis Kuria\nLike\nBookmark\nShare\nBuilding a WhatsApp-based Tech Support System with Strapi, LangChain.js, and GPT-4o\n\n\nPersonalized tech support is crucial for users facing technical issues as they need timely and specific assistance. Traditional support systems often fail to provide this, leading to user frustration. They are also difficult to develop. But in the error of \nretrieval augmented generation\n and \nlarge language models\n, creating powerful large language models powered assistants that can provide immediate, relevant, and tailored solutions is easy.\n\nIn this article, you will learn how to create a WhatsApp-based tech support system using Strapi for content management, LangChain.js for retrieval-augmented generation (RAG), and gpt-4o for natural language processing. The system will fetch troubleshooting guides from Strapi, use LangChain.js to retrieve relevant information, and generate personalized responses with gpt-4o.\n\nPrerequisites\n\nTo comfortably follow along with this tutorial, you need to have:\n\nNodeJs\n installed in your system\nBasic Knowledge of \nJavaScript\nBasic knowledge of \nLangChain.js\nSetting up Strapi\n\nStart by installing \nStrapi\n using npx.\n\nnpx create-strapi@latest my-strapi-project\n\nThe above command will create a new Strapi project residing in the my-project directory. On the terminal, navigate to the project directory path and start the Strapi server using the following command.\n\nnpm run develop\n\nOnce the server is running, proceed to Strapis admin default URL http://localhost:1337/admin/. Register yourself in order to access the Strapi dashboard.\n\n\nSince Strapi will serve as the knowledge base for your assistant, you need to configure a collection and add data to it. To do this, proceed to Content-Type-Builder and click Create new collection type. This will prompt you to name your collection. Name it tech-support-knowledgebase.\n\n\nThen click continue and select the fields you need for your collection. In this case, we need one field of type Media in which we will store the documents to be utilized by the assistant.\n\n\nName the field as documents. Then click Finish. Don't forget to save your collection's configurations.\n\n\nWait for the server to restart and then proceed to the Content Manager. Populate the collection with all the tech documents you want your assistant to use when answering questions or troubleshooting tasks. The documents should be in PDF or TXT format as these will be the formats the assistant will support.\n\n\nYou now have your data stored in Strapi. But for it to be consumed by your app you need to expose it via Strapi's built-in Rest API. To achieve this, proceed to Settings > Users & Permissions Plugin > Roles > Public. Then select Tech-support-knowledgebase and allow the find and findOne actions. Dont forget to click save.\n\n\nThe API endpoint for your collection will be \nhttp://localhost:1337/api/tech-support-knowledgebases?populate=documents\n. Notice the populate parameter, it allows you to populate various field types (e.g. relations, media, components).\n\nHere is a screenshot showing what the API data looks like:\n\n\nThe populate parameter allows you granular control over the data you retrieve from your API, allowing you to get only the data you need.\n\nSince the API endpoint is ready, let's build the tech assistant. You will start by setting up your development environment.\n\nSetting Up Your Development Environment\n\nOpen an IDE of your choice. Then run the following commands on the terminal to install the required libraries.\n\nnpm install axios langchain @langchain/core @langchain/openai @langchain/community pdf-parse dotenv qrcode-terminal \nnpm install puppeteer github:pedroslopez/whatsapp-web.js\n\nThe first command installs the libraries you will need to send http requests to Strapi in order to retrieve the guides. The second command contains the libraries you need to integrate your assistant into WhatsApp. Let's see what each library does:\n\naxios: You will use this library to make HTTP requests, such as fetching document URLs from Strapi and downloading PDF files.\n\nlangchain: You will use this library to build a context-aware tech support system that can reason about the provided data.\n\n@langchain/core: You will use this package to create chains. It includes essential components like runnables and prompts.\n\n@langchain/openai: You will use this library to integrate OpenAI models with LangChain.\n\n@langchain/community: You will use this package for community-contributed integrations within LangChain. It includes document loaders such as the PDFLoader.\n\ndotenv: You will use this library to load environment variables from a .env file ensuring secure management of sensitive information like API keys.\n\npuppeteer: You will use this library to control a headless version of Chromium for logging into WhatsApp Web.\n\nwhatsapp-web.js: You will use this library to interact with WhatsApp Web.\n\nqrcode-terminal: You will use this library to generate QR codes in the terminal, which is necessary for the initial WhatsApp Web login process.\n\npdf-parse: You will use this library to parse and extract text from PDF files.\n\nWhen all the libraries are installed, your environment is ready but you have to obtain the OpenAI API key. You will use this key to authenticate yourself with OpenAI when utilizing @langchain/openai. To obtain it, follow these steps:\n\nCreate an OpenAI Account\n if you don't have one. If you already have an account, simply log in.\n\nAfter logging in, go to the \nOpenAI API Keys page\n.\n\nClick the Create new secret key button to generate a new API key for the project. Make sure to save this key securely, as it will not be displayed again after this point.\n\nLet's start with building the RAG part of the assistant.\n\nImplementing the Retrieval-Augmented Generation (RAG) System\n\nThis is the core part of the system. It involves fetching data from the Strapi endpoint, processing it, and finally using RAG powered by OpenAI's GPT-4o model to provide responses to user inquiries.\n\nCreate a file at the root of your project named rag_code.mjs. The mjs extension means you will be using ECMAScript Modules (ESM) which will allow you to organize code into smaller, reusable components. The code in the following sections will reside in this file.\n\nThen create another file named .env at the root of your project and add your OpenAI API key in this format.\n\nOPENAI_API_KEY= Your OpenAI API Key\n\nDo not share this key on GitHub as anyone with it will be able to make API call to OpenAI on your behalf.\n\nImporting the Necessary Modules\n\nThe first step is to import the modules and functions you will use from the libraries you installed earlier.\n\nimport fs from 'fs';\nimport path from 'path';\nimport axios from 'axios';\nimport { TextLoader } from 'langchain/document_loaders/fs/text';\nimport { PDFLoader } from '@langchain/community/document_loaders/fs/pdf';\nimport { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';\nimport { OpenAIEmbeddings } from '@langchain/openai';\nimport { MemoryVectorStore } from 'langchain/vectorstores/memory';\nimport dotenv from 'dotenv';\nimport { RunnableSequence } from '@langchain/core/runnables';\nimport { ChatPromptTemplate } from '@langchain/core/prompts';\nimport { ChatOpenAI } from '@langchain/openai';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\nimport { RunnablePassthrough } from '@langchain/core/runnables';\n\ndotenv.config();\n\nImporting the modules ensures you can call and use the necessary tools needed to create the RAG system. You will understand the use of each import as you follow along.\n\nFetching the Documents From Strapi\n\nAfter the importations, you have to connect your support assistant to your Strapi endpoint and come up with a function that will fetch the documents. We will implement a caching mechanism to avoid downloading the documents everytime a user asks a question as this can delay the assistant's response rate significantly.\n\nStep 1: Creating or Clearing the Cache Folder\nStart by ensuring the cache is cleared every time the app starts to avoid using outdated content.\n\nconst cacheFolderPath = './cache';\n\n// Function to clear the cache folder\nfunction clearCacheFolder() {\n  if (fs.existsSync(cacheFolderPath)) {\n    fs.readdirSync(cacheFolderPath).forEach((file) => {\n      const filePath = path.join(cacheFolderPath, file);\n      fs.unlinkSync(filePath);\n    });\n  } else {\n    fs.mkdirSync(cacheFolderPath);\n  }\n}\n\n// Ensure cache is cleared every time the app starts\nclearCacheFolder();\n\nThis function checks whether the cache folder exists and clears it if it does. Otherwise, it creates the cache folder. The folder will be used to store downloaded documents. Clearing it ensures the assistant always starts with a clean state. The function is called only once at the start of the program.\n\nStep 2: Fetching Document URLs from Strapi\nSince you have your cache folder ready, the next step is to download the documents in Strapi to it. To achieve this, you need to start by extracting the URL of each document from the Strapi API data.\n\n// Function to fetch document URLs from Strapi\nasync function fetchDocumentUrlsFromStrapi() {\n  const strapiEndpoint = 'http://localhost:1337/api/tech-support-knowledgebases?populate=documents';\n  try {\n    const response = await axios.get(strapiEndpoint);\n    const documentUrls = response.data.data.flatMap(item => {\n      // Check if documents exist\n      if (item.documents && Array.isArray(item.documents)) {\n        return item.documents.map(doc => doc.url);\n      }\n      return []; // Return an empty array if no documents are found\n    });\n    console.log('Fetched document URLs:', documentUrls); // For debugging\n    return documentUrls;\n  } catch (error) {\n    console.error('Error fetching document URLs from Strapi:', error);\n    throw error;\n  }\n}\n\nYou will append these URLs to the base URL of your Strapi instance. This is because the url you get from the Strapi API is the relative path.\n\n\nBut to download a document you need to pass the full URL. For example, in the case of the relative path in the screenshot above, the full URL should be \nhttp://localhost:1337/uploads/supporting_and_troubleshooting_windows11_52c85abbb4.pdf\n\nLet's see how to actually download the documents from Strapi.\n\nStep 3: Downloading Documents with Caching\nDefine a function to download Documents from Strapi, using caching to avoid redundant downloads:\n\n// Function to download Docs from Strapi with caching\nasync function downloadDocsWithCache(url) {\n  const filename = path.basename(url);\n  const cacheFilePath = path.join(cacheFolderPath, filename);\n\n  // Check if the file already exists in the cache\n  if (fs.existsSync(cacheFilePath)) {\n    console.log(`Using cached file: ${cacheFilePath}`);\n    return cacheFilePath;\n  }\n\n  console.log(`Downloading file: ${url}`);\n  const fullUrl = url.startsWith('/') ? `http://localhost:1337${url}` : url;\n  try {\n    const response = await axios({\n      url: fullUrl,\n      method: 'GET',\n      responseType: 'stream',\n    });\n\n    const writer = fs.createWriteStream(cacheFilePath);\n    response.data.pipe(writer);\n\n    return new Promise((resolve, reject) => {\n      writer.on('finish', () => {\n        console.log(`File downloaded and cached: ${cacheFilePath}`);\n        resolve(cacheFilePath);\n      });\n      writer.on('error', (err) => {\n        console.error(`Error writing file: ${err}`);\n        reject(err);\n      });\n    });\n  } catch (error) {\n    console.error(`Error downloading file from ${fullUrl}:`, error);\n    throw error;\n  }\n}\n\nThe function determines the document's filenames from the URLs fetched from the Strapi endpoint. It then uses the filenames to check whether the file already exists in the cache. If it does, it uses the cached file. If not, it downloads the file by using axios to fetch the document as a stream which is efficient for handling large files. It then pipes this stream to a file writer, which writes the file to the cache directory. This avoids file duplication.\n\nBy now you have implemented the logic to fetch and cache documents from Strapi. This ensures centralized data management using Strapi while also reducing the load time of your system through caching for subsequent queries. After downloading the documents, the next step is to load and process them.\n\nLoading and Processing Documents\n\nThis stage involves loading and splitting the documents in the cache folder into chunks for further processing. Chunks are smaller manageable segments of a text. This step is crucial as it optimizes the handling of potentially large documents and prepares them for retrieval operations.\n\nasync function loadAndSplitChunks({ folderPath, chunkSize, chunkOverlap }) {\n  const documents = [];\n  const files = fs.readdirSync(folderPath);\n\n  for (const file of files) {\n    const filePath = path.join(folderPath, file);\n    let rawContent;\n    if (filePath.endsWith('.pdf')) {\n      const loader = new PDFLoader(filePath);\n      rawContent = await loader.load();\n    } else if (filePath.endsWith('.txt')) {\n      const loader = new TextLoader(filePath);\n      rawContent = await loader.load();\n    } else {\n      console.log(`Skipping file: ${filePath} (Not a PDF or TXT)`);\n      continue;\n    }\n\n    const splitter = new RecursiveCharacterTextSplitter({ chunkSize, chunkOverlap });\n    const splitDoc = await splitter.splitDocuments(rawContent);\n    documents.push(...splitDoc);\n  }\n\n  console.log('Documents loaded and split:', documents);\n  return documents;\n}\n\nThe above function iterates over the files in the cache folder looking for pdf and txt. It then uses the TextLoader class to load the text files and the PDFloader class to load the PDF files. If any other file is found, it is skipped.\n\nOnce the documents are loaded The function splits them into chunks using the RecursiveCharacterTextSplitter. After obtaining the chunks, you need a way to store them and perform operations on them. This is where vector stores come in.\n\nInitializing the Vector Store with Documents\n\nA vector store is a specialized database optimized for storing and managing vector representations. In this case, we will use an in-memory vector store which uses your computer's memory to store the documents' chunks and their embeddings.\n\nasync function initializeVectorstoreWithDocuments(documents) {\n  const embeddings = new OpenAIEmbeddings({\n    openAIApiKey: process.env.OPENAI_API_KEY,\n  });\n\n  const vectorstore = new MemoryVectorStore(embeddings);\n  await vectorstore.addDocuments(documents);\n  console.log('Documents added to vector store.');\n  return vectorstore;\n}\n\nThe above function first initializes an OpenAIEmbeddings instance which is used to compute embeddings for documents. It then creates a MemoryVectorStore with these embeddings, which allows storing and retrieving vectors in memory and adds the documents.\n\nAfter adding the documents to the vector store, you need a way to retrieve the relevant documents based on a user query.\n\nConstructing the GPT4o Powered Conversational Workflow\n\nThis workflow will start by handling a user query by making it more clear, and then retrieve the relevant documents to the query and pass the query plus the retrieved documents as content to GPT4o. GPT4o will then generate a response which you will send back to the user.\n\nThe above is achieved through chains. A chain is a sequence of steps that work together to achieve a specific task.\n\nConstructing the Rephrase Question Chain\n\nThis sequence enhance the clarity and specificity of user queries before they are processed for document retrieval.\n\nfunction createRephraseQuestionChain() {\n  const REPHRASE_QUESTION_SYSTEM_TEMPLATE = `\n  meet the following objective to the best of your ability:\n  `;\n\n  const rephraseQuestionChainPrompt = ChatPromptTemplate.fromMessages([\n    ['system', REPHRASE_QUESTION_SYSTEM_TEMPLATE],\n    ['human', 'Rephrase the following question or instruction to be standalone:\\n{question}'],\n  ]);\n\n  const rephraseQuestionChain = RunnableSequence.from([\n    rephraseQuestionChainPrompt,\n    new ChatOpenAI({\n      openAIApiKey: process.env.OPENAI_API_KEY,\n      maxTokens: 2048,\n      model: \"gpt-4o\", \n    }),\n    new StringOutputParser(),\n  ]);\n  return rephraseQuestionChain;\n}\n\nThe function above sets up a chat prompt template that tells the system to rephrase the user's input. It creates a RunnableSequence with the prompt, a chat operation using OpenAI's model, and a string output parser to format the response from the large language model. Making the input a standalone question makes it well-defined and structured, which helps in achieving more relevant document retrieval results.\n\nCreating the Document Retrieval Chain\n\nThis chain is the one responsible for retrieving the relevant documents to a user query from the vector store.\n\nfunction createDocumentRetrievalChain(retriever) {\n  const convertDocsToString = (documents) => {\n    return documents.map((document) => `<doc>\\n${document.pageContent}\\n</doc>`).join('\\n');\n  };\n\n  const documentRetrievalChain = RunnableSequence.from([\n    (input) => input.question,\n    retriever,\n    convertDocsToString,\n  ]);\n\n  return documentRetrievalChain;\n}\n\nThe function takes the user question, which is by now standalone, and retrieves the relevant documents to the query. It then formats the retrieved related documents into strings with newlines. This creates a single string containing all relevant document contents.\n\nAfter retrieving the documents, you have the context to the user question. You now need to pass the question and the context to GPT4o together with some instructions on what you want to achieve.\n\nIntegrating the Chains into a Conversational Retrieval System\n\nThis is the final chain in our conversational workflow. But before we create it, we need to come up with the instructions that will direct GP4o on how we want the user question answered. You can change this prompt to your liking.\n\nconst ANSWER_CHAIN_SYSTEM_TEMPLATE = `You are a customer service assistant. The messages you reply\n with are served through WhatsApp, so keep replies short and convenient. You are helpful and \n professional. Interpret and answer the user's question using only the provided sources.\n\n<context>\n{context}\n</context>\nThe user's question is: {question}`;\n\nconst answerGenerationChainPrompt = ChatPromptTemplate.fromMessages([\n  ['system', ANSWER_CHAIN_SYSTEM_TEMPLATE],\n  ['human', `Now, answer this question:\\n{question}`],\n]);\n\nNow you have all the components required to create the final conversational retrieval chain. Let's see how you can achieve this.\n\nasync function createConversationalRetrievalChain(retriever) {\n  const rephraseQuestionChain = await createRephraseQuestionChain();\n\n  const conversationalRetrievalChain = RunnableSequence.from([\n    RunnablePassthrough.assign({\n      question: rephraseQuestionChain,\n    }),\n    RunnablePassthrough.assign({\n      context: createDocumentRetrievalChain(retriever),\n    }),\n    answerGenerationChainPrompt,\n    new ChatOpenAI({\n      openAIApiKey: process.env.OPENAI_API_KEY,\n      maxTokens: 2048,\n      model: \"gpt-4o\", \n\n    }),\n  ]);\n\n  return conversationalRetrievalChain;\n}\n\nThe above function takes a user query and calls the rephrase chain and document retrieval chain to format the query and retrieve the relevant documents respectively. It then sends the user question and the retrieved documents as context to GPT40 together with the instructions of what is to be done. The LLM then returns the answer to the user's question using the given context.\n\nYou now have all the functions needed to answer a user's question using the relevant guides and GPT4o. The only remaining part of implementing the RAG system is providing the logic of how the program will flow.\n\nCreating the Main Chat Function\n\nThis function will be responsible for handling the RAG system logic. It will handle an incoming chat question, manage document handling, and execute the retrieval-answer chain.\n\nexport async function chatWithDocs(question) {\n  console.log('Fetching document URLs from Strapi...');\n  const documentUrls = await fetchDocumentUrlsFromStrapi();\n\n  for (const url of documentUrls) {\n    await downloadDocsWithCache(url, cacheFolderPath);\n  }\n\n  console.log('Loading and splitting documents...');\n  const documents = await loadAndSplitChunks({\n    folderPath: cacheFolderPath,\n    chunkSize: 1536,\n    chunkOverlap: 128,\n  });\n\n  console.log('Initializing vector store...');\n  const vectorstore = await initializeVectorstoreWithDocuments(documents);\n  const retriever = vectorstore.asRetriever();\n\n  console.log('Creating retrieval chain.....');\nconst finalRetrievalChain = await createConversationalRetrievalChain(retriever);\nconsole.log('Invoking retrieval chain...');\nconst result = await finalRetrievalChain.invoke({\nquestion: question,\n});\nconsole.log('Result:', result);\nreturn result.content; // Ensure to return the content for proper string handling\n}\n\nThe function fetches document URLs from Strapi. It then downloads and caches these documents locally for efficient access. Next, it loads and splits them into manageable text chunks and initializes a vector store. It then sets up a conversational retrieval chain that rephrases user questions, retrieves relevant document sections, and generates an answer. Finally, it invokes the retrieval chain with the user's query.\n\nYou have completed the RAG part, the final step in building the WhatsAPP-based tech support system is integrating the RAG part into WhatsAPP.\n\nIntegrating Your RAG System to WhatsApp\n\nThere are many ways you can integrate your system into WhatsApp. You can use the official WhatsApp business API, use paid services like Twilio WhatsApp API, or use opensource integrations like \nwhatsapp-web.js\n. In this article, you will utilize whatsapp-web.js for the integration.\n\nProceed to the root of your project and create a file named whatsapp_integration.mjs. Then paste the following code.\n\n// Import the necessary modules\nimport puppeteer from 'puppeteer';\nimport { Client } from 'whatsapp-web.js';\nimport { config as dotenvConfig } from 'dotenv';\nimport qrcode from 'qrcode-terminal';\nimport { chatWithDocs } from './rag_code.mjs'; // Ensure the path is correct\n\n// Load environment variables from .env file\ndotenvConfig();\n\n// Initialize WhatsApp client with Puppeteer configuration\nconst client = new Client({\n  puppeteer: {\n    // Specify the executable path for Puppeteer; adjust according to your setup\n    executablePath: puppeteer.executablePath(),\n    // Additional Puppeteer arguments to ensure it runs smoothly in various environments\n    args: ['--no-sandbox', '--disable-setuid-sandbox'],\n  },\n  // Specify the cache settings for WhatsApp Web version; this can improve load times\n  webVersionCache: {\n    type: \"remote\",\n    remotePath: \"https://raw.githubusercontent.com/wppconnect-team/wa-version/main/html/2.2412.54.html\",\n  },\n});\n\n// Event listener for QR code generation, needed for authentication\nclient.on('qr', (qr) => {\n  // Display QR code in the terminal\n  qrcode.generate(qr, { small: true });\n});\n\n// Event listener for when the client is ready\nclient.on('ready', () => {\n  console.log('Client is ready!');\n});\n\n// Event listener for incoming messages\nclient.on('message', async (msg) => {\n  console.log(`Message received from ${msg.from}: ${msg.body}`);\n  const chat = await msg.getChat();\n\n  try {\n    // Log the processing action\n    console.log('Processing the message...');\n\n    // Send a typing indicator to show the bot is processing\n    await chat.sendStateTyping();\n\n    // Process the message with a custom function (make sure to define this function)\n    const result = await chatWithDocs(msg.body);\n    console.log('Result:', result);\n\n    // Extract the content from the result object to send as a reply\n    const replyContent = result;\n    console.log(`Sending reply: ${replyContent}`);\n\n    // Clear the typing indicator\n    await chat.clearState();\n\n    // Send the reply to the user\n    await msg.reply(replyContent);\n    console.log('Reply sent.');\n  } catch (error) {\n    // Log and handle errors\n    console.error('Error processing the message:', error);\n\n    // Clear typing indicator if an error occurs\n    await chat.clearState();\n\n    // Send an error message to the user\n    await msg.reply('Sorry, an error occurred while processing your request.');\n  }\n});\n\n// Initialize the WhatsApp client\nclient.initialize();\n\nThe code receives a user query. It then uses the chatWithDocs function from the RAG code to generate a response using the documents in Strapi. It then sends the answer back to the user using WhatsApp.\n\nYou now have the full system. Let's see the results.\n\nTesting the Tech Support System\n\nProceed to the terminal and run the whatsapp_integration.mjs code using node:\n\nnode .\\whatsapp_integration.mjs\n\nWhen you run the code, a QR code will be displayed on the terminal.\n\n\nProceed to WhatsApp and use your phone to scan it. This will authenticate the connection between the WhatsApp client on your device and your system, which is running through the Puppeteer-driven instance of WhatsApp Web. Make sure not to use your main number, as any message received after this will invoke a response from the system.\n\nSend your queries to the connected WhatsApp account, and the system will respond with the relevant answers. Here is a sample result:\n\n\nYou can see the system responds with the correct answers to the queries. The system is limited to the information stored in Strapi. If you ask any question outside that, it will notify you it does not have that information.\n\nConclusion\n\nIn this tutorial, you have learned how to utilize Strapi, LangChain, GPT4o, and WhatsApp to create a Tech Support system. When you combine AI with Strapi, you can build infinite solutions. Go ahead and apply the knowledge you learned to create more solutions.\n\nResources\nHave a look at the full code \nhere\n and the Strapi backend \nhere\n.\nhttps://js.langchain.com/docs/get_started/introduction\nhttps://docs.strapi.io/dev-docs/backend-customization\nhttps://aws.amazon.com/what-is/retrieval-augmented-generation/\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nBuilding a WhatsApp-based Tech Support System with Strapi, LangChain.js, and GPT-4o\n\nPrerequisites\n\nSetting up Strapi\n\nSetting Up Your Development Environment\n\nImplementing the Retrieval-Augmented Generation (RAG) System\n\nImporting the Necessary Modules\n\nFetching the Documents From Strapi\n\nLoading and Processing Documents\n\nInitializing the Vector Store with Documents\n\nConstructing the GPT4o Powered Conversational Workflow\n\nView all",
    "awards": [
      "distinguished implementation guide"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "camvisiotech-aiot-security-system-bvzwW2e8O44E",
    "username": "@jjateen97",
    "license": null,
    "title": "Camvisiotech - AIOT Security System",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n71 reads\n●\nApache 2.0\nWinner of\nOutstanding Solution Implementation\nat the\nComputer Vision Projects Expo 2024\nCamvisiotech - AIOT Security System\nAIOT\nArtificial Intelligence\ndlib\nEdge AI\nEDGE-IOT\nESP32\nFace Recognition\nIOT\nmediapipe\nMicropython\nSecurity System\n@jjateen97\nT\n@talwarmohit2005\nLike\nBookmark\nShare\nAbstract\n\nCamVisioTech is an AI-powered IoT-based security system project, collaboratively developed to implement advanced surveillance features with iterative enhancements. Each version builds upon its predecessor, integrating cutting-edge technologies for home and office security.\nCamvisiotech has been a journey from a simple ESP32 CAM to a powerful Edge AI system. Starting as a third-semester project, it began with a basic security system using dlib. Recognizing its potential, I teamed up with a friend to develop Camvisiotech 2.0, running on an ESP32 CAM with optimized synchronization and improved ML models. This version won us Bytecraft at IIIT Nagpur TechFest 2023. Inspired by the Maixduino’s AI capabilities, we started Camvisiotech 3.0—a standalone, AI-driven face detection and recognition solution. With WiFi and GSM connectivity, it now operates autonomously on EDGE, suitable for rural and remote areas with limited connectivity, making edge AI accessible in low-resource environments.\n\nCamVisioTech MK-0: ESP32-CAM Security System\n- Introduction :\n\nThe system utilizes facial recognition to control a solenoid lock, unlocking the door for 3 seconds when a known person is recognized. If an intruder is detected, the system sends an alert via Telegram and activates a buzzer. Additionally, the system offers a GUI application for monitoring and control, and a custom desktop application built with Python and Tkinter to manage both the facial recognition and alert system.\n\n- Features :\nFace Recognition: Captures and processes images via the ESP32-CAM and compares them against pre-stored face encodings.\nDoor Control: Utilizes a solenoid lock controlled via a relay to lock/unlock the door based on successful face recognition.\nIntruder Alert System: Sends the intruder’s image to the user’s Telegram account and sounds an alarm if an unauthorized face is detected.\nPython-based GUI: Includes both a high-latency (old-school) and low-latency modern Python GUI for user-friendly system control.\n- Requirements :\n1. Hardware\nESP32-CAM microcontroller\nSolenoid lock, relay module, buzzer\nIC-7805 voltage regulator\nAdditional components: diode, transistor, resistors, and capacitors.\n\n2. Software\nArduino IDE for programming ESP32-CAM.\nPython with libraries like opencv-python, face_recognition, and customtkinter.\nTelegram Bot: For sending notifications in case of an intruder alert.\n- Methodology :\n1. Face Recognition\n\nThe ESP32-CAM captures images at regular intervals and sends them to the server. The server compares the captured images with pre-stored face encodings to authenticate users.\n\ndef process_frame():\n    # Fetch the image from the specified URL\n    img_resp = urllib.request.urlopen(url)\n    \n    # Convert the image data into a NumPy array\n    imgnp = np.array(bytearray(img_resp.read()), dtype=np.uint8)\n    \n    # Decode the image array into an OpenCV image\n    img = cv2.imdecode(imgnp, -1)\n    \n    # Resize the image to 25% of its original size for faster processing\n    imgS = cv2.resize(img, (0, 0), None, 0.25, 0.25)\n    \n    # Convert the image from BGR (OpenCV default) to RGB (required by face_recognition)\n    imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n\n    # Detect face locations in the resized image\n    facesCurFrame = face_recognition.face_locations(imgS)\n    \n    # Compute face encodings for the detected faces\n    encodesCurFrame = face_recognition.face_encodings(imgS, facesCurFrame)\n\n    # Loop through each detected face and its encoding\n    for encodeFace, faceLoc in zip(encodesCurFrame, facesCurFrame):\n        # Compare the detected face encoding with known encodings\n        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n        \n        # Calculate the distance between the detected face and known faces\n        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n        \n        # Find the index of the closest match\n        matchIndex = np.argmin(faceDis)\n\n        # If the face matches a known person\n        if matches[matchIndex]:\n            # Retrieve the name of the person\n            name = classNames[matchIndex].upper()\n            \n            # Extract the face location and scale back to the original image size\n            y1, x2, y2, x1 = faceLoc\n            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n            \n            # Draw a green rectangle around the recognized face\n            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            \n            # Draw a filled rectangle below the face for the name label\n            cv2.rectangle(img, (x1, y2 - 35), (x2, y2), (0, 255, 0), cv2.FILLED)\n            \n            # Display the person's name on the image\n            cv2.putText(img, name, (x1 + 6, y2 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n            \n            # Trigger an action (e.g., turning on a device or sending a notification)\n            requests.get(urlOn)\n        else:\n            # If the face is not recognized, treat it as an intruder\n            y1, x2, y2, x1 = faceLoc\n            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n            \n            # Draw a red rectangle around the unrecognized face\n            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n            \n            # Draw a filled rectangle below the face for the label\n            cv2.rectangle(img, (x1, y2 - 35), (x2, y2), (0, 0, 255), cv2.FILLED)\n            \n            # Display the \"Intruder!!!\" warning on the image\n            cv2.putText(img, \"Intruder!!!\", (x1 + 6, y2 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n            \n            # Print an intruder warning in the console\n            print(\"Intruder\")\n            \n            # Send the image with the intruder to telegram\n            sendPhoto(img)\n            \n            # Trigger an alert action (e.g., sounding a buzzer)\n            requests.get(buzzOn)\n\n2. Intruder Detection\n\nIf a face does not match the stored faces, the system triggers an intruder alert.\n\nAn image of the intruder is sent to the user’s Telegram account.\nThe buzzer is activated for 1.5 seconds as a local alert.\n3. GUI Application\n\nThe GUI application allows users to:\n\nView the real-time camera feed at different resolutions.\nControl the solenoid lock (unlock for 3 seconds).\nTrigger the buzzer manually if needed.\n\n- Models :\n\nThe face recognition is carried out using \nface_recognition\n python library, built using dlib's state-of-the-art face recognition built with deep learning. The model has an accuracy of 99.38% on the \nLabeled Faces in the Wild\n benchmark.\n\n1. Face Detection\n\nThe library uses a convolutional neural network (CNN) or a Histogram of Oriented Gradients (HOG) model to detect faces in an image. It identifies the coordinates of bounding boxes for each detected face.\n\n2. Face Encoding\n\nA detected face is transformed into a high-dimensional numerical representation called a \"face encoding.\" This is achieved by analyzing the unique facial features and extracting a fixed-length vector for each face.\n\n3. Face Comparison\n\nThe library can compare face encodings to determine similarity. This is achieved using distance metrics such as Euclidean distance.\nIf the distance between two face encodings is below a certain threshold, the faces are considered a match.\n\n4. Known and Unknown Face Identification\n\nThe library allows maintaining a database of face encodings (representing known faces). Detected faces can be matched against this database to identify known individuals or flag unknown ones.\n\n- Conclusion :\n\nIn conclusion, the face_recognition library simplifies implementing advanced face detection and recognition in Python, enabling applications such as security, access control, and surveillance. The project’s ability to send intruder alerts via Telegram and activate a buzzer enhances its utility as a reliable security system. Its modular design, including Python-based GUIs and a streamlined workflow, makes it a practical and scalable solution for modern smart security needs.\n\n- References :\nMachine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning\nface_recognition\nCode and Setup\nCamVisioTech MK-1: AI-Driven Smart Security Camera\n- Introduction :\n\nThe enhanced version introduces real-time object detection and a Flask web application:\n\nHaar Cascade: Utilized for object and face detection.\nAlerts via Email & Telegram: Notifications for security breaches.\nFlask Web App: Streams live video feed with overlays.\n- Features :\nCombines face recognition and object detection.\nReal-time alerts for unauthorized access.\nWeb-based video streaming with detection overlays.\n- Requirements :\n1. Hardware\nESP32CAM module\nSimilar components as MK-0 with enhanced integration.\n\n2. Software\nPython and Flask for backend and web streaming.\nLibraries for detection: opencv, face_recognition.\nPySerial (for serial communication with Arduino or ESP32)\n- Methodology :\n\ndef generate_frames():\n    global continuous_zeros, last_notification_time\n\n    while True:\n        try:\n            # Capture the image from the ESP32-CAM's URL\n            img_response = urllib.request.urlopen(url)\n            img_np = np.array(bytearray(img_response.read()), dtype=np.uint8)\n            frame = cv2.imdecode(img_np, -1)\n\n            # Convert the frame to RGB for processing\n            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n            # Detect faces in the frame using MediaPipe Face Detection\n            with mp_face_detection.FaceDetection(\n                model_selection=0, min_detection_confidence=0.5) as face_detection:\n                results = face_detection.process(image_rgb)\n\n                # Initialize flags for recognized and unknown faces\n                recognized = False\n                unknown = False\n\n                if results.detections:\n                    for detection in results.detections:\n                        bboxC = detection.location_data.relative_bounding_box\n                        ih, iw, _ = frame.shape\n                        x, y, w, h = (int(bboxC.xmin * iw), int(bboxC.ymin * ih),\n                                      int(bboxC.width * iw), int(bboxC.height * ih))\n\n                        # Crop and resize the face region for face recognition\n                        face_image = frame[y:y+h, x:x+w]\n                        if face_image.shape[0] > 0 and face_image.shape[1] > 0:\n                            imgS = cv2.resize(face_image, (0, 0), None, 0.25, 0.25)\n                            imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n                            facesCurFrame = face_recognition.face_locations(imgS)\n                            encodesCurFrame = face_recognition.face_encodings(imgS, facesCurFrame)\n\n                            for encodeFace, faceLoc in zip(encodesCurFrame, facesCurFrame):\n                                matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n                                if any(matches):\n                                    recognized = True\n                                    name = classNames[matches.index(True)]\n                                    # Draw bounding box and label for recognized face\n                                    y1, x2, y2, x1 = [v * 4 for v in faceLoc]\n                                    y1 += y; x2 += x; y2 += y; x1 += x\n                                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                                    cv2.putText(frame, name, (x1 + 6, y2 - 6),\n                                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n                                else:\n                                    unknown = True\n                                    # Draw bounding box for unknown face\n                                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n\n            # Determine the displayed text based on face recognition\n            if recognized or not unknown:\n                displayed_text = \"1\"\n                continuous_zeros = 0\n            else:\n                displayed_text = \"0\"\n                continuous_zeros += 1\n\n            # Trigger notifications if no faces are detected for a certain duration\n\n            if continuous_zeros >= 9:  # Approximately 3 seconds (3 frames/sec)\n                current_time = datetime.datetime.now()\n                if last_notification_time is None or (current_time - last_notification_time).total_seconds() >= 15:\n                    last_notification_time = current_time\n                    trigger_buzzer()\n                    lock_door()\n                    send_notification(\"Motion Detected!\",\n                                      \"Someone has entered the frame. Check the link for details:\\n\\n\"\n                                      \"https://mohittalwar23.github.io/PythonSystemTest/\")\n\n            # Overlay text and timestamp on the frame\n            cv2.putText(frame, displayed_text, (10, 30),\n                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n            timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            cv2.putText(frame, timestamp, (frame.shape[1] - 300, 30),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n\n            # Encode and yield the frame as a JPEG stream\n            _, buffer = cv2.imencode('.jpg', frame)\n            frame = buffer.tobytes()\n            yield (b'--frame\\r\\nContent-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n- Models :\n1. What is Haar Cascade?\n\nHaar Cascades are machine learning object detection algorithms used to identify objects in images or videos. The technique was introduced by Paul Viola and Michael Jones in their 2001 research paper \nRapid Object Detection using a Boosted Cascade of Simple Features\n. The algorithm uses a cascade function trained from a lot of positive and negative images to detect objects, primarily faces.\n\nIt works in stages, where each stage applies increasingly complex filters to identify the features of the object of interest.\n\n2. Object Detection and Face Recognition\n\nIn this project, we'll combine Haar Cascades for face detection and face recognition using the face_recognition library. The system will recognize known individuals and trigger a response if an unknown face is detected.\n\nFor more information on Haar Cascades, check the following free resources:\n\nOpenCV Documentation: Haar Feature-based Cascade Classifiers\nViola-Jones Face Detection Algorithm Paper\n- Conclusion :\n\nBy incorporating features such as real-time object detection, live video streaming, automated alerts, and door-locking mechanisms, this project provides a comprehensive solution for modern security needs. The detailed implementation steps and modular design make it a great resource for learning and extending into advanced AI-powered applications.\n\n- References :\nHaar Cascade : Viola Jones\nface_recognition\nCode and Setup\nCamVisioTech MK-2: Advanced Security via Edge AI\n- Introduction :\n\nThe MK-2 version of CamVisioTech moves beyond conventional surveillance by leveraging Edge AI for on-device processing. Unlike earlier iterations, it focuses on executing models locally, enhancing privacy, reliability, and efficiency, even in low-bandwidth environments.\n\n- Features :\nYOLOv2 Integration: Enables precise object detection and activity recognition directly on the device.\nEdge AI Processing: All inference operations are performed on the hardware itself (Maixduino), eliminating the need for cloud-based computations.\nMulti-Connectivity Support: Offers Wi-Fi or GSM for sending real-time alerts and notifications.\nActuator Integration: Supports on-site physical responses such as buzzer alerts or relay-controlled actions.\nAlerts are dispatched via Wi-Fi or GSM, with extensibility to third-party apps like Telegram using platforms such as IFTTT or PipeDream.\n- Requirements :\n1. Hardware\nMaixduino RISC-V + AI Kit: AI-capable development board with integrated ESP32 for Wi-Fi and Bluetooth capabilities.\nOV2640 Camera Module: High-quality imaging for real-time object detection.\nBuzzer: For audio alerts.\nBreadboard & Jumper Wires: For modular connections.\n2.4-inch TFT Display: For on-device status monitoring.\n\n2. Software\nMicroPython: Lightweight scripting language for development.\nMaixPy IDE: To build and deploy applications on the Maixduino hardware.\nkflash_gui: For uploading firmware and pre-trained models.\nuPyLoader: For accessing, updating, and managing files on the device.\nYOLOv2 Model Files: Optimized for real-time inference on the Maixduino platform.\n- Methodology :\n1. AI Model Training and Deployment\n\nThe Maixduino Kit, which includes an AI-capable microcontroller and camera, functions as the primary processing unit. It utilizes three core .smodel files trained via MaixHub.com—a model for Face(s) Detection, a model for Face Landmark Detection, and Feature Extraction. These models process and identify known faces, storing features as recognized persons (e.g., \"Person 1,\" \"Person 2\") in arrays for rapid matching. The setup allows for the simultaneous recognition of multiple faces, enabling quick identification of individuals stored in the system.\n\n2.Intruder Detection and Actuator Control\n\nWhen an unrecognized person (not in the known faces array) is detected, the system categorizes them as an intruder. It triggers an immediate response, such as sounding a buzzer or activating an LED indicator. The system is also capable of relay control which also allows integration with other actuators, such as door locks, providing instant, real-time physical security.\n\n3.Communication and Alerts\n\nFor reliable notifications, the system can use either Wi-Fi or GSM connectivity to send alerts, ensuring communication in areas with variable internet access. Integrating with third-party messaging apps, like Telegram. This multifaceted communication ensures that users are informed wherever they are, making the system suitable for both urban and remote applications.\n\n- Models :\n\nThe Maixduino Kit, which includes an AI-capable microcontroller and camera, functions as the primary processing unit. It utilizes three core .smodel files trained via MaixHub.com—a model for Face(s) Detection, a model for Face Landmark Detection, and Feature Extraction. These models process and identify known faces, storing features as recognized persons (e.g., \"Person 1,\" \"Person 2\") in arrays for rapid matching. The setup allows for the simultaneous recognition of multiple faces, enabling quick identification of individuals stored in the system.\n\n- Face Detection\nPurpose: Detects faces in the camera feed, identifying multiple faces at once.\nModel Type: YOLOv2 object detection model, optimized for real-time processing on the Maixduino’s hardware.\nOutcome: Locates face bounding boxes to trigger further analysis.\n\n- Face Landmark Detection\nPurpose: Identifies specific facial landmarks (e.g., eyes, nose, mouth) within detected faces.\nModel Type: Keypoint detection model, enabling finer facial structure identification.\nOutcome: Provides a precise facial map, aiding in consistent feature extraction for recognition.\n\n- Feature Extraction\nPurpose: Extracts unique facial features from detected landmarks to distinguish individual identities.\nModel Type: Embedding model that outputs feature vectors for each detected face.\nOutcome: Compares these vectors with stored profiles, recognizing known individuals and categorizing unknown ones as intruders.\n\n- References :\nGetting started with Maixduino\nModels\nCode\nSetup\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nCamVisioTech MK-0: ESP32-CAM Security System\n\nIntroduction :\nFeatures :\nRequirements :\nHardware\nSoftware\nMethodology :\nFace Recognition\nIntruder Detection\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nYou might be interested\nReal-Time Face Detection with OpenCV\nM\nMay 26, 202516 reads\nFace Detection in Live Video Using Python and OpenCV\nO\nDec 29, 202412 reads\nReal-time Face Detection using OpenCV | Computer Vision Enthusiast | Python and Machine Learning\nMay 16, 20257 reads\nface Detection\nA\nMay 22, 20254 reads",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "cerebral-collective-a-multi-agent-research-intelligence-platform-cJVHWXVQxWIo",
    "username": "zZain Shafique",
    "license": "MIT License",
    "title": "Cerebral Collective: A Multi-Agent Research Intelligence Platform",
    "publication_description": "Back to publications\nJul 13, 2025\n●\n55 reads\n●\nMIT License\nCerebral Collective: A Multi-Agent Research Intelligence Platform\nacademic-research\nai-orchestration\nartificial-intelligence\nbusiness-intelligence\ncollaborative-intelligence\nhuman-ai-collaboration\ninformation-retrieval\nknowledge-management\nmulti-agent-systems\nnatural-language-processing\nresearch-automation\nresearch-tools\nZ\nZain Shafique\nLike\nBookmark\nShare\nAbstract\n\nCerebral Collective represents a breakthrough in automated research intelligence, employing a sophisticated multi-agent architecture to conduct comprehensive academic and professional research. This platform orchestrates five specialized AI agents that collaborate seamlessly to transform complex research questions into well-structured, citation-rich reports. Built on modern microservices architecture with local LLM inference capabilities, Cerebral Collective ensures privacy while delivering enterprise-grade research automation with comprehensive human oversight and performance tracking.\n\n1. Introduction & Vision\nThe Research Challenge\n\nIn our information-rich world, conducting thorough research requires synthesizing vast amounts of data from multiple sources, evaluating credibility, identifying gaps, and presenting findings coherently. Traditional research methods are time-intensive and prone to human bias, while existing AI tools often lack the depth and rigor required for professional research.\n\nOur Solution: Cerebral Collective\n\nCerebral Collective addresses these challenges through collaborative intelligence - a system where specialized AI agents work in concert, each contributing their unique expertise to create research outputs that exceed what any single agent could produce alone.\n\nKey Innovation: Emergent Intelligence\n\nRather than relying on a single large model, our platform demonstrates how emergent intelligence arises from the interaction of specialized agents, creating a research capability that is greater than the sum of its parts.\n\n2. System Architecture & Design Philosophy\n2.1 Architectural Overview\n\n2.2 Design Principles\n\nThe platform is built on four foundational design principles. Modularity ensures that each agent is an independent, self-contained unit with clearly defined responsibilities and role-based tool restrictions, enabling easy maintenance and extensibility. Human Oversight provides comprehensive human-in-the-loop integration with approval workflows for critical decisions, controversial evaluations, and report generation. Performance Transparency includes built-in metrics tracking systems that monitor agent performance, tool usage, error rates, and provide detailed analytics. Resilience encompasses comprehensive error handling, fallback mechanisms, and stall detection to ensure the system continues operating even when individual components fail.\n\nThe architecture also prioritizes privacy through local LLM inference via Ollama, ensuring sensitive research data never leaves your infrastructure while maintaining the advanced AI capabilities necessary for sophisticated research tasks.\n\n3. The Agent Collective: Specialized Intelligence Units\n3.1 Research Manager Agent: The Orchestrator\n\nRole: Master coordinator and strategic planner\nTool Access: Full orchestration capabilities, state management, workflow planning\n\nThe Research Manager Agent serves as the strategic brain of the entire system, handling research decomposition by breaking complex questions into 3-5 manageable sub-questions using advanced prompt engineering. It manages workflow planning through creating task dependency graphs with priority ordering, ensuring optimal research progression. Quality synthesis combines findings from all agents into coherent final reports, while recovery management handles system failures and implements fallback strategies to maintain research momentum.\n\n3.2 Information Retrieval Agent: The Gatherer\n\nRole: Expert information acquisition specialist\nTool Access: Web search, scholar search, source validation\n\nThis specialized agent focuses on keyword optimization by extracting optimal search terms using semantic analysis, ensuring comprehensive coverage of relevant topics. Multi-source search capabilities orchestrate searches across web, academic, and proprietary sources simultaneously. Source evaluation implements credibility scoring algorithms using a 0-10 scale to ensure quality, while content filtering eliminates irrelevant or low-quality sources before they enter the analysis pipeline.\n\n3.3 Document Analysis Agent: The Interpreter\n\nRole: Content analysis and extraction specialist\nTool Access: Document summarization, key point extraction, relevance scoring\n\nThe Document Analysis Agent performs deep content analysis to extract key findings, methodologies, and evidence from collected sources. Relevance scoring quantifies how closely each source addresses specific research questions, enabling prioritization of the most pertinent information. Insight synthesis identifies patterns and connections across sources that might not be apparent when examining individual documents, while quality assessment evaluates the strength of evidence and reasoning presented in each source.\n\n3.4 Critical Evaluation Agent: The Validator\n\nRole: Quality assurance and bias detection specialist\nTool Access: Quality assessment, bias detection, gap analysis\nHuman Approval Required: For controversial or high-risk evaluations\n\nThis agent implements multi-dimensional assessment that evaluates quality, comprehensiveness, and consistency across all gathered research. Bias detection identifies potential sources of bias and limitations in the research base, ensuring balanced perspectives. Gap analysis highlights areas requiring additional research, while sufficiency determination decides whether collected information adequately addresses the original research question. When controversial topics arise, the agent automatically triggers human approval workflows.\n\n3.5 Report Generation Agent: The Communicator\n\nRole: Professional document creation specialist\nTool Access: Report drafting, citation formatting, document structuring\nHuman Approval Required: For final report approval before publication\n\nThe final agent in the collective handles structured report generation, creating comprehensive reports with academic formatting that meet professional standards. Citation management generates proper citations in multiple formats including APA, MLA, and Chicago styles. Executive summary creation distills key findings into concise summaries for different audiences, while multi-format output supports various output formats and styles based on specific requirements.\n\n4. Enhanced Features & Capabilities\n4.1 Human-in-the-Loop Integration\n\nApproval Workflow System:\n\nclass ApprovalService:\n    \"\"\"\n    Manages human oversight for critical system decisions\n    \"\"\"\n    \n    def request_approval(self, agent: str, action: str, context: Dict) -> bool:\n        \"\"\"Request human approval for critical actions\"\"\"\n        approval_message = f\"\"\"\n        🔔 HUMAN APPROVAL REQUIRED 🔔\n        Agent: {agent}\n        Action: {action}\n        Context: {context.get('summary', 'N/A')}\n        Approve? (y/n): \n        \"\"\"\n        \n        # In production, this would integrate with UI approval modal\n        return self.get_human_decision(approval_message)\n\nThe human-in-the-loop integration operates at critical decision points throughout the research workflow. Controversial evaluations requiring human judgment ensure that complex or sensitive assessments receive appropriate oversight. Final report approval before publication maintains quality control and allows for human review of research conclusions. High-risk actions with significant impact are flagged for human approval, while quality threshold decisions can be escalated when automated assessments are uncertain.\n\n4.2 Performance Monitoring & Analytics\n\nComprehensive Metrics Tracking:\n\nclass ResearchMetrics:\n    \"\"\"\n    Tracks system performance across all components\n    \"\"\"\n    \n    def track_agent_performance(self, agent_name: str, task_result: str):\n        \"\"\"Record agent task outcomes\"\"\"\n        self.agent_stats[agent_name]['tasks'] += 1\n        if task_result == 'error':\n            self.agent_stats[agent_name]['errors'] += 1\n    \n    def track_tool_usage(self, tool_name: str):\n        \"\"\"Monitor tool utilization patterns\"\"\"\n        self.tool_usage[tool_name] += 1\n    \n    def generate_performance_report(self) -> Dict:\n        \"\"\"Create comprehensive performance analytics\"\"\"\n        return {\n            'agent_performance': self.agent_stats,\n            'tool_usage': self.tool_usage,\n            'error_rates': self.calculate_error_rates(),\n            'success_metrics': self.calculate_success_rates()\n        }\n\nThe performance monitoring system provides real-time analytics through the /api/performance-report endpoint, enabling continuous system optimization. Console reporting offers easy monitoring capabilities for system administrators, while automated performance alerts notify operators of potential issues before they impact research quality.\n\n4.3 Enhanced State Management\n\nExtended Research State Model:\n\nclass ResearchState(BaseModel):\n    # Core Research Data\n    research_id: str\n    research_question: str\n    sub_questions: List[str]\n    \n    # Agent Communication\n    messages: List[Message]\n    \n    # Knowledge Base\n    sources: List[Source]\n    extracted_information: List[ExtractedInformation]\n    evaluations: Dict\n    \n    # Human-in-the-Loop Integration\n    report_draft: Optional[str] = None\n    evaluation_summary: Optional[str] = None\n    requires_approval: bool = False\n    approval_context: Dict[str, Any] = Field(default_factory=dict)\n    \n    # Performance Tracking\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    \n    # Process Management\n    tasks: List[Task]\n    completed_tasks: List[str]\n    \n    # Final Outputs\n    summary: str\n    report: str\n    status: str\n    start_time: datetime\n4.4 Graph-Based Workflow Design\n\nIntelligent Agent Routing Graph:\n\nDynamic Workflow State Machine:\n\nclass WorkflowStateMachine:\n    \"\"\"\n    Graph-based workflow orchestration with intelligent routing\n    \"\"\"\n    \n    WORKFLOW_GRAPH = {\n        'PLANNING': {\n            'next_states': ['RETRIEVAL'],\n            'agent': 'ResearchManagerAgent',\n            'human_approval': False,\n            'fallback': 'create_default_plan'\n        },\n        'RETRIEVAL': {\n            'next_states': ['ANALYSIS', 'EVALUATION'],  # Conditional routing\n            'agent': 'RetrievalAgent',\n            'human_approval': False,\n            'parallel_execution': True,\n            'fallback': 'use_cached_sources'\n        },\n        'ANALYSIS': {\n            'next_states': ['EVALUATION'],\n            'agent': 'AnalysisAgent',\n            'human_approval': False,\n            'parallel_execution': True,\n            'fallback': 'basic_summarization'\n        },\n        'EVALUATION': {\n            'next_states': ['RETRIEVAL', 'GENERATION', 'COMPLETE'],\n            'agent': 'EvaluationAgent',\n            'human_approval': True,  # Critical decision point\n            'decision_logic': 'evaluate_sufficiency',\n            'fallback': 'force_completion'\n        },\n        'GENERATION': {\n            'next_states': ['COMPLETE'],\n            'agent': 'ReportAgent',\n            'human_approval': True,  # Final approval\n            'fallback': 'generate_summary_report'\n        }\n    }\n    \n    def determine_next_state(self, current_state: str, research_state: ResearchState) -> str:\n        \"\"\"Intelligent state transition based on research quality\"\"\"\n        state_config = self.WORKFLOW_GRAPH[current_state]\n        \n        if current_state == 'EVALUATION':\n            # Dynamic routing based on information sufficiency\n            if research_state.evaluations.get('sufficiency_score', 0) >= 7.0:\n                return 'GENERATION'\n            elif len(research_state.sources) < 5:\n                return 'RETRIEVAL'  # Need more sources\n            else:\n                return 'GENERATION'  # Proceed with available data\n        \n        # Default sequential flow\n        return state_config['next_states'][0]\n\nParallel Processing Architecture:\n\nclass ParallelWorkflowExecutor:\n    \"\"\"\n    Enables concurrent agent execution for improved performance\n    \"\"\"\n    \n    async def execute_parallel_analysis(self, sources: List[Source]) -> List[Analysis]:\n        \"\"\"\n        Process multiple sources concurrently during analysis phase\n        \"\"\"\n        analysis_tasks = []\n        \n        # Create concurrent analysis tasks\n        for source in sources:\n            task = asyncio.create_task(\n                self.agents['AnalysisAgent'].analyze_source_async(source)\n            )\n            analysis_tasks.append(task)\n        \n        # Wait for all analyses with timeout\n        results = await asyncio.gather(\n            *analysis_tasks, \n            return_exceptions=True,\n            timeout=300  # 5-minute timeout\n        )\n        \n        # Filter successful results and handle failures\n        successful_analyses = [\n            result for result in results \n            if not isinstance(result, Exception)\n        ]\n        \n        return successful_analyses\n    \n    def create_dependency_graph(self, research_state: ResearchState) -> Dict:\n        \"\"\"\n        Creates task dependency graph for optimal execution order\n        \"\"\"\n        dependency_graph = {\n            'search_tasks': {\n                'dependencies': [],\n                'can_parallelize': True,\n                'max_concurrent': 3\n            },\n            'analysis_tasks': {\n                'dependencies': ['search_tasks'],\n                'can_parallelize': True,\n                'max_concurrent': 5\n            },\n            'evaluation_tasks': {\n                'dependencies': ['analysis_tasks'],\n                'can_parallelize': False,  # Sequential evaluation required\n                'max_concurrent': 1\n            }\n        }\n        \n        return dependency_graph\n\nConditional Routing Logic:\n\nclass ConditionalRouter:\n    \"\"\"\n    Implements intelligent routing based on research state analysis\n    \"\"\"\n    \n    def evaluate_routing_conditions(self, state: ResearchState) -> Dict[str, bool]:\n        \"\"\"\n        Evaluates multiple conditions to determine optimal workflow path\n        \"\"\"\n        conditions = {\n            'sufficient_sources': len(state.sources) >= 10,\n            'high_quality_sources': self.calculate_avg_quality(state.sources) >= 7.0,\n            'comprehensive_coverage': self.assess_topic_coverage(state) >= 0.8,\n            'analysis_complete': len(state.extracted_information) >= 5,\n            'evaluation_passed': state.evaluations.get('overall_score', 0) >= 7.0,\n            'human_approval_received': state.approval_context.get('approved', False)\n        }\n        \n        return conditions\n    \n    def select_optimal_path(self, conditions: Dict[str, bool], current_phase: str) -> str:\n        \"\"\"\n        Selects the most appropriate next phase based on current conditions\n        \"\"\"\n        if current_phase == 'EVALUATION':\n            if conditions['evaluation_passed'] and conditions['human_approval_received']:\n                return 'GENERATION'\n            elif not conditions['sufficient_sources']:\n                return 'RETRIEVAL'  # Gather more information\n            elif not conditions['comprehensive_coverage']:\n                return 'ANALYSIS'   # Deeper analysis needed\n            else:\n                return 'GENERATION' # Proceed with available data\n        \n        # Default sequential progression\n        return self.get_next_sequential_phase(current_phase)\n\nThe advanced orchestration system includes stall detection and recovery mechanisms that identify when research processes become stuck and implement automatic recovery strategies. Dynamic quality gates evaluate whether each phase meets established standards before allowing progression to subsequent phases, with adaptive thresholds that consider research context and requirements. Fallback report generation ensures that even when primary workflows encounter issues, the system can still produce valuable research outputs with appropriate quality disclaimers.\n\n5. Technology Stack & Implementation\n5.1 Core Technologies\n\nThe platform leverages FastAPI as its backend framework, providing async support for concurrent agent execution, automatic API documentation with performance metrics, and background task processing for long-running research projects. Ollama + Mistral handles language model inference locally, ensuring privacy preservation, cost control, and latency optimization with fallback support for offline operation. Tavily API provides web search capabilities with advanced search features, credibility scoring, content extraction and summarization, plus rate limiting and caching support.\n\n5.2 Enhanced Configuration System\n\nCentralized Configuration Management:\n\nclass Config:\n    def __init__(self):\n        self.metrics = ResearchMetrics()\n        self.PERFORMANCE_TRACKING = True\n        self.PERFORMANCE_REPORTING = True\n        self.HUMAN_APPROVAL_REQUIRED = True\n        self.MAX_ITERATIONS = 20\n        self.STALL_DETECTION_ENABLED = True\n5.3 Testing Infrastructure\n\nThe comprehensive test suite includes unit tests for the metrics system, performance benchmark tests, human-in-the-loop workflow testing, and integration tests with approval workflows. Key dependencies include matplotlib==3.8.2, seaborn==0.13.0, pytest==7.4.3, and pytest-asyncio==0.21.1 for visualization and testing capabilities.\n\n6. Installation & Setup Guide\n6.1 Quick Start with Docker\n\nStep 1: Repository Setup\n\ngit clone https://github.com/yourusername/cerebral-collective.git\ncd cerebral-collective\ncp .env.example .env\n\nStep 2: Environment Configuration\n\n# Required configurations in .env:\nOLLAMA_HOST=http://localhost:11434\nOLLAMA_MODEL=mistral\nTAVILY_API_KEY=your_tavily_api_key_here\nPERFORMANCE_TRACKING=true\nHUMAN_APPROVAL_REQUIRED=true\n\nStep 3: Ollama Setup\n\n# Install and start Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\nollama serve\nollama pull mistral\n\nStep 4: Launch Platform\n\ndocker-compose up -d\ncurl http://localhost:8000/api/performance-report  # Test metrics endpoint\n6.2 Project Structure\nmulti-agent-research-assistant/\n├── backend/\n│   ├── agents/              # AI agent implementations\n│   │   ├── manager.py       # Research Manager Agent\n│   │   ├── retrieval.py     # Information Retrieval Agent\n│   │   ├── analysis.py      # Document Analysis Agent\n│   │   ├── evaluation.py    # Critical Evaluation Agent\n│   │   └── report.py        # Report Generation Agent\n│   ├── evaluation/          # Performance monitoring\n│   │   ├── metrics.py       # Metrics tracking system\n│   │   └── report_generator.py\n│   ├── graph/              # Workflow orchestration\n│   │   ├── research_graph.py\n│   │   └── routers.py\n│   ├── models/             # Enhanced data models\n│   │   └── state.py\n│   ├── services/           # Core services\n│   │   ├── approval_service.py  # Human-in-the-loop\n│   │   ├── ollama_client.py\n│   │   └── research_service.py\n│   ├── tools/              # Utility tools\n│   │   ├── web_search.py\n│   │   ├── summarization.py\n│   │   └── citation.py\n│   ├── config.py           # Centralized configuration\n│   └── main.py\n├── frontend/               # React frontend with approval modals\n│   ├── src/\n│   │   ├── components/\n│   │   │   ├── ApprovalModal.js    # Human approval interface\n│   │   │   ├── ResearchForm.js\n│   │   │   ├── ResearchResults.js\n│   │   │   └── ResearchStatus.js\n│   │   └── ...\n├── tests/                  # Comprehensive test suite\n│   └── test_performance_metrics.py\n└── requirements.txt\n\n7. Performance Benchmarks & Analytics\n7.1 System Performance Metrics\n\nProcessing speed benchmarks demonstrate the system's efficiency across different research complexity levels. Simple questions typically complete in 3-7 minutes with 5-10 sources, while complex research projects require 12-20 minutes with 15-25 sources. Deep analysis projects that require comprehensive coverage may take 25-45 minutes but process 30 or more sources to ensure thoroughness.\n\nQuality metrics show consistent performance with source reliability achieving 85-92% credible sources, information completeness providing 78-88% topic coverage, and citation accuracy maintaining 94-98% proper formatting across all generated reports.\n\n7.2 Sample Performance Report Output\n{\n  \"agent_performance\": {\n    \"ResearchManagerAgent\": {\"tasks\": 15, \"errors\": 1, \"success_rate\": 93.3},\n    \"RetrievalAgent\": {\"tasks\": 42, \"errors\": 3, \"success_rate\": 92.9},\n    \"AnalysisAgent\": {\"tasks\": 38, \"errors\": 2, \"success_rate\": 94.7},\n    \"EvaluationAgent\": {\"tasks\": 20, \"errors\": 0, \"success_rate\": 100.0},\n    \"ReportAgent\": {\"tasks\": 12, \"errors\": 1, \"success_rate\": 91.7}\n  },\n  \"tool_usage\": {\n    \"web_search\": 28,\n    \"scholar_search\": 14,\n    \"summarize_document\": 32,\n    \"draft_report\": 12,\n    \"format_citations\": 12\n  },\n  \"error_analysis\": {\n    \"api_timeouts\": 2,\n    \"parsing_errors\": 1,\n    \"network_issues\": 3\n  },\n  \"human_approvals\": {\n    \"requested\": 8,\n    \"approved\": 7,\n    \"rejected\": 1\n  }\n}\n7.3 Human-in-the-Loop Workflow Examples\n\nCritical Evaluation Approval:\n\n🔔 HUMAN APPROVAL REQUIRED 🔔\nAgent: CriticalEvaluationAgent\nAction: controversial_evaluation\nContext: Evaluation contains potentially biased conclusions \nabout climate change impacts requiring human review\nApprove? (y/n): \n\n\nReport Generation Approval:\n\n🔔 HUMAN APPROVAL REQUIRED 🔔\nAgent: ReportAgent\nAction: approve_report_draft\nContext: 15-page comprehensive report on renewable energy \ntrends ready for publication\nApprove? (y/n): \n\n8. Key Benefits & Impact\n8.1 Enhanced Capabilities\n\nThe platform provides transparency and control through clear agent roles with defined capabilities and restrictions, real-time performance metrics for all system components, human oversight for critical decisions and controversial content, plus comprehensive audit trails for all research activities.\n\nReliability and recovery features include automatic stall detection and recovery mechanisms, fallback content generation when primary workflows fail, error tracking and automated recovery strategies, and quality degradation warnings with alternative approaches when needed.\n\nPerformance insights enable optimization through agent-by-agent performance tracking, tool usage analytics for resource optimization, error rate monitoring and trend analysis, and success metrics for continuous improvement.\n\n8.2 Operational Benefits\n\nTime efficiency improvements show 70-85% reduction in research time compared to manual methods. Quality assurance through human-in-the-loop validation ensures output quality meets professional standards. Scalability benefits from performance monitoring that enables optimization for high-volume usage. Maintainability comes from centralized configuration and modular architecture that simplifies updates and maintenance.\n\n9. Future Enhancements & Roadmap\n9.1 Planned Improvements\n\nAdvanced analytics development includes predictive performance modeling, automated optimization recommendations, advanced bias detection algorithms, and quality prediction before report generation.\n\nEnhanced human integration will provide collaborative workspaces for team research, expert reviewer integration, custom approval workflows, and role-based access controls for organizational deployment.\n\nTechnical upgrades planned include multi-modal document processing, advanced LLM model support, academic database integration, and enterprise security features for large-scale deployment.\n\n9.2 Research Applications\n\nThe platform serves multiple research contexts including academic research for literature reviews, preliminary investigations, and systematic reviews. Business intelligence applications include market analysis, competitive research, and trend identification. Journalism applications encompass fact-checking, investigative reporting, and source verification. Policy development applications include evidence-based analysis, impact assessment, and stakeholder research.\n\n10. Conclusion\n\nCerebral Collective represents a significant advancement in automated research technology, demonstrating how specialized AI agents can collaborate under human oversight to produce high-quality research outputs. The enhanced platform now includes comprehensive performance monitoring, human-in-the-loop decision making, and transparent analytics that make it suitable for production environments.\n\nKey Achievements\n\nTechnical innovation through custom orchestration with performance tracking and human oversight provides a robust foundation for enterprise deployment. Practical value demonstrates proven 70-85% time savings while maintaining research quality standards. Production readiness includes enterprise-grade monitoring, approval workflows, and error recovery mechanisms. Transparency offers complete visibility into agent performance and decision processes.\n\nTransformative Impact\n\nThe platform transforms research workflows by democratizing research through making advanced research tools accessible to everyone, ensuring quality through human oversight that prevents errors and bias in critical decisions, providing insights through performance analytics that enable continuous system improvement, and building trust through transparent processes and human validation.\n\nBy combining the efficiency of AI automation with the judgment of human oversight, Cerebral Collective represents the future of collaborative intelligence in research, augmenting rather than replacing human expertise.\n\nResources & Links\n\nRepository: \nhttps://github.com/zshafique25/Multi-agent-research-assistant\n\nAPI Documentation: Available at http://localhost:8000/docs after installation\n\nPerformance Metrics: Access real-time analytics at http://localhost:8000/api/performance-report\n\nDemo Video:\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction & Vision\n\nThe Research Challenge\n\nOur Solution: Cerebral Collective\n\nKey Innovation: Emergent Intelligence\n\nSystem Architecture & Design Philosophy\n\n2.1 Architectural Overview\n\n2.2 Design Principles\n\nThe Agent Collective: Specialized Intelligence Units\n\n3.1 Research Manager Agent: The Orchestrator\n\nView all\nComments\nCode\nYou might be interested\nMulti-Agent Research Assistant\nAug 31, 202515 reads\ngradiolangchain+3\nAgentic AI Developer Certification: LangGraph-Orchestrated Research Assistant for Ready Tensor\nJ\nC\nJul 14, 202521 reads\nAAIDC‑M2AAIDC2025+10\nAI Data Analyst & Report Generator\nA\nAug 09, 202532 reads\nAIdata analysis+5\nAgentic AI Developer Certification: Production LangGraph-Orchestrated Research Assistant\nC\nJ\nAug 10, 202544 reads\nAAIDC2025Agentic AI+11",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "cityguard-ai-local-incident-monitoring-system-5c9uuGbkxuXb",
    "username": "aAishwarya",
    "license": "📄 License",
    "title": "CityGuard AI - Local Incident Monitoring System",
    "publication_description": "Back to publications\nAug 14, 2025\n●\n40 reads\nCityGuard AI - Local Incident Monitoring System\nAgentic AI\nAI/Ml\nFlask\nGeminiAPi\nnewsApi\nOpenWeatherApi\npython\nA\nAishwarya\nLike\nBookmark\nShare\nCityGuard AI - Local Incident Monitoring System\n\nCityGuard AI is a comprehensive real-time local incident monitoring and alert system that aggregates data from multiple sources, uses AI-powered analysis to assess incident relevance and severity, and delivers targeted notifications to keep residents informed about critical safety information.\n\nCityGuard AI | Python | Flask | AI\n\n✨ Key Features\n🎯 Multi-Agent Architecture\nData Agent: Automatically fetches incident data from weather and news APIs every 10 minutes.\nNotification Agent: Processes incidents using Google's Gemini AI for intelligent analysis and user notifications.\nBackground Processing: APScheduler handles continuous monitoring without manual intervention.\n🌐 Comprehensive Web Interface\nMain Dashboard: Overview of all incidents with real-time filtering and statistics.\nWeather Alerts Page: Specialized weather monitoring with severity-based filtering.\nNews Alerts Page: Local news incident tracking with breaking news ticker.\nInteractive Map: Geographic visualization of incidents using Leaflet.js.\nSubscription Management: Email alert registration for high-priority incidents.\n🤖 AI-Powered Analysis\nRelevance Scoring: Gemini AI evaluates incident relevance to local communities (0-1 score).\nSeverity Assessment: Automatic classification into Critical, High, Medium, and Low priority levels.\nContent Summarization: AI-generated summaries for better incident understanding.\nCredibility Verification: Filters out unreliable information to prevent misinformation.\n📡 Real-Time Data Sources\nOpenWeatherMap API: Severe weather alerts and meteorological warnings.\nNews API: Local news aggregation for incident discovery.\nMulti-Location Support: Configurable monitoring for different geographic areas.\n🚀 Quick Start\nPrerequisites\nPython 3.11 or higher\nValid API keys for:\nGoogle Gemini API (GEMINI_API_KEY)\nOpenWeatherMap API (OPENWEATHERMAP_API_KEY)\nNews API (NEWS_API_KEY)\nInstallation\nClone the repository:\ngit clone https://github.com/AishwaryaChandel27/CityGuardAlert\ncd cityguard-ai\nInstall dependencies:\npip install -r requirements.txt\nSet environment variables:\nexport GEMINI_API_KEY=\"your_gemini_api_key\"\nexport OPENWEATHERMAP_API_KEY=\"your_openweathermap_key\"\nexport NEWS_API_KEY=\"your_news_api_key\"\nexport SESSION_SECRET=\"your_session_secret\"\nRun the application:\npython main.py\nAccess the web interface at http://localhost:5000.\n📊 System Architecture\n┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐\n│   Data Agent    │    │ Notification     │    │   Web Interface │\n│                 │    │     Agent        │    │                 │\n│ • Weather API   │───▶│                  │───▶│ • Dashboard     │\n│ • News API      │    │ • Gemini AI      │    │ • Weather Page  │\n│ • Scheduled     │    │ • Analysis       │    │ • News Page     │\n│   Fetching      │    │ • Email Alerts   │    │ • Interactive   │\n└─────────────────┘    └──────────────────┘    │   Map           │\n                                               └─────────────────┘\n                             │\n                             ▼\n                    ┌──────────────────┐\n                    │    Database      │\n                    │                  │\n                    │ • Incidents      │\n                    │ • Users          │\n                    │ • Subscriptions  │\n                    │ • Notifications  │\n                    └──────────────────┘\n\n📱 Web Interface Overview\nDashboard (/)\nReal-time Statistics: Live incident counters and severity breakdown.\nIncident Timeline: Chronological list of all recent incidents.\nAdvanced Filtering: Filter by source, severity, location, and time range.\nQuick Actions: Easy access to detailed views and external sources.\nWeather Alerts (/weather)\nWeather-Specific Dashboard: Dedicated interface for meteorological events.\nSeverity Indicators: Visual severity classification (Critical → Low).\nWeather Type Filtering: Filter by storm, rain, snow, fog, wind conditions.\nCurrent Conditions: Real-time weather status display.\nNews Alerts (/news)\nLocal News Monitoring: Curated local incident reporting.\nBreaking News Ticker: Highlights for critical incidents.\nCategory Classification: Emergency, Traffic, Crime, Infrastructure, Health.\nSource Verification: Direct links to original news articles.\nInteractive Map (/map)\nGeographic Visualization: Leaflet.js-powered incident mapping.\nLocation-Based Filtering: Focus on specific areas or search locations.\nSeverity Color Coding: Visual severity representation with custom markers.\nReal-Time Updates: Live incident location tracking.\nSubscription Management (/subscribe)\nEmail Alert Registration: User subscription for critical incidents.\nPreference Settings: Customizable alert thresholds and categories.\nInstant Notifications: Immediate email alerts for high-priority events.\n🔧 API Endpoints\nPublic Endpoints\nGET /api/incidents - Retrieve all incidents with filtering options.\nGET /api/incidents/weather - Weather-specific incidents.\nGET /api/incidents/news - News-specific incidents.\nGET /api/incidents/<id> - Individual incident details.\nQuery Parameters\nhours - Time range filter (default: 24).\nmin_relevance - Minimum relevance score (default: 0.3).\nsource - Filter by data source (weather/news).\nseverity - Filter by severity level.\nResponse Format\n{\n  \"success\": true,\n  \"incidents\": [\n    {\n      \"id\": 1,\n      \"title\": \"Weather Alert: Fog\",\n      \"description\": \"Dense fog conditions affecting visibility\",\n      \"ai_summary\": \"Fog reported in New York, exercise caution while traveling\",\n      \"severity\": \"low\",\n      \"category\": \"weather\",\n      \"source\": \"weather\",\n      \"location\": \"New York\",\n      \"relevance_score\": 0.5,\n      \"is_verified\": true,\n      \"created_at\": \"2025-08-14T10:30:00Z\",\n      \"url\": null\n    }\n  ],\n  \"count\": 1\n}\n🗃️ Database Schema\nIncidents Table\nid - Primary key\ntitle - Incident title\ndescription - Original incident description\nai_summary - AI-generated summary\nseverity - Severity level (low/medium/high/critical)\ncategory - Incident category\nsource - Data source (weather/news)\nlocation - Geographic location\nrelevance_score - AI relevance score (0.0-1.0)\nis_verified - AI verification status\nurl - Source URL (for news incidents)\ncreated_at - Timestamp\nUsers Table\nid - Primary key\nusername - Unique username\nemail - Email address\npassword_hash - Encrypted password\nAlert Subscriptions Table\nid - Primary key\nuser_id - Foreign key to Users\nseverity_threshold - Minimum severity for alerts\ncategories - Subscribed incident categories\nis_active - Subscription status\n🔮 AI Analysis Pipeline\n\nData Ingestion\n\n# Weather data from OpenWeatherMap\nweather_data = weather_api.get_current_conditions()\nweather_alerts = weather_api.get_alerts()\n\n# News data from NewsAPI\nnews_articles = news_api.search_local_incidents()\n\nAI Processing\n\n# Gemini AI analysis for each incident\nanalysis = gemini.analyze_incident({\n    \"title\": incident.title,\n    \"description\": incident.description,\n    \"location\": incident.location\n})\n\n# Returns structured analysis\n{\n    \"relevance_score\": 0.75,\n    \"severity\": \"high\",\n    \"category\": \"weather\",\n    \"is_credible\": true,\n    \"summary\": \"AI-generated incident summary\"\n}\n\nNotification Logic\n\n# Email notifications for high-priority incidents\nif incident.severity in ['critical', 'high'] and incident.relevance_score > 0.7:\n    notification_agent.send_email_alerts(incident, subscribers)\n⚙️ Configuration\nEnvironment Variables\n# Required API Keys\nGEMINI_API_KEY=your_gemini_api_key_here\nOPENWEATHERMAP_API_KEY=your_openweather_api_key\nNEWS_API_KEY=your_news_api_key\n\n# Application Settings\nSESSION_SECRET=your_secure_session_secret\nDATABASE_URL=sqlite:///instance/cityguard.db  # Local development\nDATABASE_URL=postgresql://user:pass@host/db   # Production\n\n# Email Configuration (Optional)\nSMTP_SERVER=smtp.gmail.com\nSMTP_PORT=587\nEMAIL_USER=your_email@gmail.com\nEMAIL_PASS=your_app_password\nMonitoring Settings\n# Data Agent Configuration\nWEATHER_FETCH_INTERVAL = 10  # minutes\nNEWS_FETCH_INTERVAL = 10     # minutes\nMAX_INCIDENTS_PER_FETCH = 5\n\n# AI Analysis Thresholds\nMIN_RELEVANCE_SCORE = 0.3\nHIGH_PRIORITY_THRESHOLD = 0.7\nCRITICAL_ALERT_THRESHOLD = 0.9\n📈 Performance & Scalability\nOptimizations\nDatabase Indexing: Optimized queries on frequently accessed fields.\nBackground Processing: Non-blocking API calls using APScheduler.\nCaching: Frontend caching for improved user experience.\nRate Limiting: API request throttling to respect service limits.\nProduction Deployment\nDatabase: PostgreSQL recommended for production.\nWeb Server: Gunicorn with multiple workers.\nReverse Proxy: Nginx for static file serving and load balancing.\nMonitoring: Application logging and error tracking.\nSSL/TLS: HTTPS encryption for secure communication.\n🛠️ Development\nProject Structure\ncityguard-ai/\n├── agents/\n│   ├── data_agent.py          # Data fetching automation\n│   └── notification_agent.py  # AI processing and alerts\n├── templates/\n│   ├── base.html             # Base template with navigation\n│   ├── index.html            # Main dashboard\n│   ├── weather.html          # Weather alerts page\n│   ├── news.html             # News alerts page\n│   ├── map.html              # Interactive map\n│   └── subscribe.html        # Subscription management\n├── static/\n│   ├── css/custom.css        # Custom styling\n│   └── js/dashboard.js       # Frontend JavaScript\n├── utils/\n│   └── email_service.py      # Email notification utilities\n├── app.py                    # Flask application factory\n├── main.py                   # Application entry point\n├── models.py                 # Database models\n├── routes.py                 # Web routes and API endpoints\n├── gemini.py                 # Gemini AI integration\n└── README.md                 # This file\n\nTesting\n# Run the application in debug mode\nexport FLASK_ENV=development\npython main.py\n\n# Check API endpoints\ncurl http://localhost:5000/api/incidents\ncurl http://localhost:5000/api/incidents/weather?hours=12\n🤝 Contributing\nFork the repository.\nCreate a feature branch (git checkout -b feature/new-feature).\nMake changes and test thoroughly.\nCommit changes (git commit -am 'Add new feature').\nPush to branch (git push origin feature/new-feature).\nCreate Pull Request.\n📄 License\n\nThis project is licensed under the MIT License. See \nLICENSE\n file for details.\n\n🤝 Support\n\nFor support, feature requests, or bug reports:\n\nCreate an issue in the \nGitHub repository\n.\nContact the development team.\nCheck the documentation for troubleshooting guides.\n🔄 Recent Updates\n\nVersion 2.0.0 (August 2025)\n\nAdded dedicated weather alerts page with specialized filtering.\nImplemented news alerts page with breaking news ticker.\nIntegrated interactive map with Leaflet.js for geographic visualization.\nEnhanced navigation with separate pages for different data types.\nImproved AI analysis with structured incident categorization.\nAdded real-time statistics and enhanced filtering options.\n🔒 Safety Enhancements\nCredibility Verification: AI-driven filtering to prevent misinformation.\nSecure Communication: HTTPS encryption for all data transfers.\nRate Limiting: Protects APIs from abuse and ensures fair usage.\nUser Authentication: Secure password hashing and session management.\nData Validation: Strict input validation to prevent injection attacks.\n\nCityGuard AI - Keeping communities safe through intelligent incident monitoring.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nCityGuard AI - Local Incident Monitoring System\n\n✨ Key Features\n\n🎯 Multi-Agent Architecture\n\n🌐 Comprehensive Web Interface\n\n🤖 AI-Powered Analysis\n\n📡 Real-Time Data Sources\n\n🚀 Quick Start\n\nPrerequisites\n\nInstallation\n\n📊 System Architecture\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "cloudburst-prediction-with-mixedinput-deep-learning-HadbAXTcMquS",
    "username": "pPramit De",
    "license": "MIT License",
    "title": "Cloudburst Prediction with Mixed-Input Deep Learning",
    "publication_description": "Back to publications\nDec 24, 2024\n●\n30 reads\n●\nMIT License\nCloudburst Prediction with Mixed-Input Deep Learning\nANN\nCloudburst\nCNN\nComputer Vision\nDeep Learning\nP\nPramit De\nLike\nBookmark\nShare\nAbstract\n\nCloudbursts, characterized by sudden, intense rainfall, often trigger flash floods, landslides, and debris flow, posing substantial threats. The study presents a robust cloudburst prediction system employing deep learning techniques, integrating Convolutional Neural Networks (CNN) for images and Feed-Forward Neural Networks (FNN) for tabular data. The motivation stems from the limitations of traditional prediction models, emphasizing the need for accurate, timely warnings. The comprehensive methodology includes data preparation, preprocessing, data splitting, model architecture, training, evaluation, and alert generation. Future prospects encompass data access via APIs, serverless computing, user-friendly interfaces, and collaborative data sharing, promising improved cloudburst prediction and disaster preparedness.\n\nKeywords:\n\nCloudbursts; flash floods; deep learning; CNN; FNN; prediction system; disaster preparedness;\n\n1 Introduction\n\nCloudburst, a meteorological phenomenon characterized by sudden and intense rainfall, has wreaked havoc on the lives and properties of those inhabiting hilly and mountainous regions[1]. These events, often accompanied by flash floods, landslides, and debris flow, result in catastrophic losses, posing a significant threat to the well-being of local communities.\nConsider some recent instances: Leh, Ladakh in 2010, Uttarkashi, Uttarakhand in 2012, Kedarnath in 2013, Chamoli, Uttarakhand in 2021, and Amarnath, Jammu and Kashmir in 2022[2]. Each of these cloudburst events underscores the dire consequences and challenges faced by residents in hilly areas.\nThe intention here is to comprehensively address the phenomenon of cloudbursts, exploring their causes, effects, and potential mitigation strategies to contribute to the safeguarding of lives and property in hilly regions, where the unique terrain and limited resources often compound the impact of natural disasters.\nThe primary objective of this research is to develop a highly reliable cloud burst prediction system that leverages mixed stream inputs, including tabular data and images, through the utilization of neural network deep learning techniques. This endeavor aims to significantly enhance public safety and reduce the risks associated with cloud bursts, ultimately making the general population safer from these natural disasters. Key objectives include the integration of mixed stream inputs, employing a Convolutional Neural Network (CNN) for image data and a feed-forward neural network (FNN) for tabular data(weather parameters), enhancing the reliability of cloud burst predictions, contributing to public safety by providing more accurate and early warnings for cloud bursts, and developing an alert generation system to enable timely notifications to local disaster management organizations. This ensures swift and proactive action can be taken to mitigate the potential impact of cloud bursts, thereby facilitating disaster preparedness and response.\nThe motivation for this research arises from the urgent need to enhance cloud burst prediction systems. Traditional models for cloud burst prediction have demonstrated limitations in terms of accuracy and timeliness, often hindering proactive disaster preparedness and response efforts[3]. These limitations include challenges in effectively capturing the complex and dynamic nature of cloud burst triggers and the potential for false alarms or delayed warnings. By integrating mixed data streams, including meteorological parameters (e.g., temperature, precipitation) in tabular data alongside image data, we aim to revolutionize cloud burst predictions. The integration enables a comprehensive understanding of cloud burst triggers. The ultimate goal is to improve public safety by providing more accurate, early warnings and implementing an alert generation system, thereby mitigating the impact of these events on lives and property.\n\n2 Methodology\n\nThe methodology section outlines the procedures and techniques employed in cloud burst prediction system.Fig 1 offers a visual context, a schematic diagram illustrating the system working process.\n\nFigure 1: Schematic Diagram of the Cloudburst Prediction System\n\nSubsequent subsections encompass data preparation, data preprocessing, data splitting, model architecture, model training, model evaluation, and alert generation. These segments together constitute the comprehensive methodology utilized in the research.\n2.1 Data Preparation\n\nImage Data Collection:\n\nFor the purpose of cloud burst prediction, satellite images were collected from Meteorological and Oceanographic Satellite Data Archival Centre(\nhttps://mosdac.gov.in/\n) [4]during the occurrence of cloud bursts, as well as 6 hours prior and 6 hours after the cloudburst.Moreover,images of random non cloudburst events were aslo collected.These images were meticulously classified into four categories: 'cloudburst,' 'precloudburst’, 'postcloudburst, and ‘no cloudburst’ providing a contextual basis for the prediction model.\n\nTabular Data Collection:\n\nComplementary to the image data, essential meteorological parameters including latitude, longitude, 2m temperature, and total precipitation were obtained from the ECMWF ERA5 hourly data on single levels, spanning from 1940 to the present, through the Copernicus Climate Data Store \nCDS ERA5 Data\n. This dataset was collected for a 24-hour period surrounding major cloud burst events, such as the incidents in Chamoli, Uttarakhand, on February 7, 2021, the event in Amarnath, Jammu and Kashmir, on July 8, 2022, and the cloudburst in Jaipur on July 18, 2019, among others. Also, data of random non cloudburst events were also collceted. The tabular data, downloaded in NETCDF format, underwent extraction of pertinent features, including latitude, longitude, 2m temperature, and total precipitation, facilitated by the Panoply tool. These features were consolidated into a unified dataset in .CSV format.\n\nDataset Size:\n\nThe data preparation process yielded a dataset comprising 152 data points, which serves as the foundational dataset for model development and training.\n\n2.2 Data Preprocessing\n\nImage Data Processing:\n\nIn the context of image data, satellite images were categorized into distinct phases, specifically 'cloudburst,' 'precloudburst, 'post-cloudburst, and ‘no-cloudburst,’ providing vital contextual information.Simultaneously, file names and their corresponding paths were extracted, streamlining data management.To ensure data consistency and model compatibility, the images were standardized by converting them from BGR to RGB format.\n\nTabular Data Integration:\n\nConcurrently, meteorological data, encompassing latitude, longitude, temperature, and precipitation, was collected within the temporal context of major cloud burst events and random non cloudburst events.\nThe collected data was structured into separate DataFrames, each with a distinct role, and subsequently merged to create a comprehensive dataset that unified all relevant information.Additionally, the dataset underwent streamlining by removing extraneous columns, enhancing dataset efficiency and focusing solely on the most pertinent data.\n\nData Compression and Preparation:\n\nGiven the substantial size of the image data, measures were taken to compress the images into NPZ format, and the corresponding paths within the DataFrame were updated to ensure ease of accessibility.Fig 2 shows the dataframe with the NPZ paths added.\n\n\n\n\nFigure 2-Snapshot of Dataframe with NPZ paths\n\n\n\nTabular Data Refinement:\n\nTo further refine the dataset, a separate DataFrame was introduced to store temperature and precipitation information, enhancing dataset organization.This stage also involved the calculation of crucial statistical properties, including mean and standard deviation values, which were essential for effective data scaling. The standard deviation values for 2m temperature and total precipitation were determined to be [27.14, 0.0017].The mean values for 2m temperature and total precipitation were found to be approximately [288.62, 0.0009].\n\nTensorFlow Preprocessing Function:\n\nFor efficient data scaling, a TensorFlow preprocessing function named 'stat_scaler' was introduced. This function plays a pivotal role in standardizing and optimizing the data for further analysis.\n\n2.3 Data Splitting:\n\nIn this phase, the dataset underwent a structured division for training, validation, and testing, achieved through the following steps:\n\nShuffling and Division:\n\nThe dataset was initially shuffled to introduce randomness and minimize potential biases.Subsequently, it was partitioned into three primary subsets\n\nTraining Set encompassed the initial 130 entries for model training.Entries 131 to 141 constituted the Validation Set, utilized for model fine-tuning.The Testing Set included entries starting from 142 onwards, designated for evaluating the model's performance.\n\nData Extraction and Preparation:\n\nTo ensure the data was structured and ready for model application, an extraction function was employed. This function organized the data into distinct components: image data ('X_pic'), statistical data ('X_stats'), and labels ('y').\n\nTraining, Validation, and Testing Data:\n\nThe training dataset consisted of two key components, 'X_train_pic' (image data) and 'X_train_stats' (statistical data), accompanied by labels ('y_train').\nLikewise, the validation dataset ('X_val_pic', 'X_val_stats', 'y_val') and testing dataset ('X_test_pic', 'X_test_stats', 'y_test') followed a similar structure. Each data component was meticulously organized, ensuring readiness for model deployment.\n\n2.4 Model Architecture:\n\nIn our research, a sophisticated model architecture was designed and established to harness the mixed stream inputs, blending Convolutional Neural Networks (CNN) and Feed-Forward Neural Networks (FNN) for optimal performance. The model's construction involved the following key components:\n\nPicture Stream (CNN):\n\nA dedicated input layer was defined for image data, shaped as (224, 224, 3), providing the dimensions suitable for our dataset.Preprocessing of the input data was executed using a Lambda layer, ensuring compatibility with the MobileNetV2 architecture.The MobileNetV2 architecture, pre-trained on a large image dataset, was incorporated with the 'include_top' layer set to 'False,' facilitating feature extraction[5].A Global Average Pooling 2D layer condensed the extracted features, serving as a bridge between CNN and FNN components.Subsequently, a Dense layer with 256 units and a rectified linear unit (ReLU) activation function was introduced to enhance the network's capacity to capture complex patterns.To mitigate overfitting, a Dropout layer with a rate of 0.5 was added, applying regularization[6].\n\nStats Stream (Feed-Forward):\n\nFor tabular data, a separate input layer was defined with a shape of (2), matching the dimensions of our statistical features.Utilizing a Lambda layer, data was scaled and standardized, ensuring consistency with the neural network model.A Dense layer with 128 units and a ReLU activation function enriched the model's capability to process statistical data.To further address overfitting, a Dropout layer with a rate of 0.3 was integrated.\n\nModel Fusion:\n\nThe outputs of the CNN and FNN streams were concatenated, creating a unified feature representation of the mixed stream inputs.\n\nOutput Layer:\n\nTo facilitate classification into four distinct classes (ranging from 0 to 3), a Dense layer with a softmax activation function was defined, ensuring the model's ability to provide probability distributions for each class.\n\nFinal Model:\n\nThe architecture culminated in the establishment of the final model. The model seamlessly incorporated both image and statistical data, providing a robust framework for cloud burst prediction.\nThe model summary has been provided below:\n\nModel: \"model_2\"\nLayer (type)\tOutput Shape\tParam #\tConnected to\ninput_1 (InputLayer)\t(None, 224, 224, 3)\t0\t[]\nlambda (Lambda)\t(None, 224, 224, 3)\t0\t['input_1[0][0]']\nmobilenetv2_1.00_224 (Functional)\t(None, 7, 7, 1280)\t2,257,984\t['lambda[0][0]']\ninput_3 (InputLayer)\t(None, 2)\t0\t[]\nglobal_average_pooling2d (GlobalAveragePooling2D)\t(None, 1280)\t0\t['mobilenetv2_1.00_224[0][0]']\nlambda_1 (Lambda)\t(None, 2)\t0\t['input_3[0][0]']\ndense (Dense)\t(None, 256)\t327,936\t['global_average_pooling2d[0][0]']\ndense_1 (Dense)\t(None, 128)\t384\t['lambda_1[0][0]']\ndropout (Dropout)\t(None, 256)\t0\t['dense[0][0]']\ndropout_1 (Dropout)\t(None, 128)\t0\t['dense_1[0][0]']\nconcatenate (Concatenate)\t(None, 384)\t0\t['dropout[0][0]', 'dropout_1[0][0]']\ndense_2 (Dense)\t(None, 4)\t1,540\t['concatenate[0][0]']\nTotal Parameters:\nTotal params: 2,587,844 (9.87 MB)\nTrainable params: 2,553,732 (9.74 MB)\nNon-trainable params: 34,112 (133.25 KB)\n2.5 Model Training:\n\nThe model training process was implemented as follows:\n\nModel Saving Callback:\n\nA model checkpoint ('cp') was implemented to save the best model during training for future use.\n\nOptimizer Definition and Compilation:\n\nThe model was compiled with the Adam optimizer, 'sparse_categorical_crossentropy' as the loss function, and 'accuracy' as the evaluation metric.\n\nTraining:\n\nModel training was carried out for 10 epochs, using both image data ('X_train_pic') and statistical data ('X_train_stats') along with corresponding labels ('y_train').During the training, the model's performance was monitored, and the training loss was evaluated, resulting in a train loss of 0.2735 and a train accuracy of 90%.Validation data ('X_val_pic' and 'X_val_stats' with 'y_val') were used to assess the model's performance, ensuring accuracy and reliability in cloud burst prediction. The validation process also provided a validation loss of 2.4811 and a validation accuracy of 50%.Fig 3 demonstrates the Training and Validation Loss Over Epochs whereas Fig 4 demonstrates the Training and Validation Accuracy Over Epochs\n\n\n\n\nFigure 3-Training and Validation Loss Over Epochs\n\n\nFigure 4-Training and Validation Accuracy Over Epochs\n\n\n2.6 Model Evaluation:\n\nThe trained model was evaluated using the testing dataset to assess its performance in real-world scenarios.The testing data, comprising 'X_test_pic' (image data) and 'X_test_stats' (statistical data) along with labels 'y_test,' was used for evaluation.The 'evaluate' method was employed to compute the test loss and accuracy, resulting in a test loss of 2.7470 and a test accuracy of 60%.\n\n2.7 Alert Generation:\n\nIn this critical phase, a robust alert generation system was implemented. To ensure timely and secure communication with disaster management agencies and forecasting organizations, a secure email communication framework was established. This framework involved configuring the system with password-protected credentials for email access. Messages, including crucial information about cloudburst predictions and recommended actions, were meticulously prepared. These alerts were then securely transmitted via the SMTP server to designated recipients.\nAs part of future enhancements, plans were put in place for the integration of SMS services via Twilio, contingent upon obtaining the necessary authorizations. This strategic expansion is aimed at further extending the reach of alerts and enhancing the overall effectiveness of the alert generation system.Fig 5 shows a demo advisory email sent for cloudburst situation.\n\n\n\n\n\nFigure 5-Demo Advisory email sent for Cloudburst situation\n\n\n\n3 Future Scope\n\nThe research presented in this paper opens promising avenues for further development:\n\nDirect Data Fetching via APIs:\nFuture work can focus on seamless integration of data sources through APIs to ensure real-time data access, reducing latency and improving predictive accuracy.\n\nServerless Computing for Continuous Monitoring:\nLeveraging serverless platforms like AWS Lambda, Google Cloud Functions, or Azure Functions allows for automated, regular updates to the model with fresh data, enhancing its relevance.\n\nWidespread Information Accessibility:\n\nDeveloping user-friendly interfaces and mobile applications can empower a wider audience with essential cloudburst information, aiding informed decision-making and disaster preparedness.\n\nCollaborative Data Sharing:\n\nEstablishing collaborative networks and data-sharing protocols among research institutions and meteorological agencies can enrich data quality and lead to more robust prediction models.\n\n4 Conclusion\n\nCloudburst events, characterized by sudden, intense rainfall, pose severe threats to hilly regions, causing flash floods, landslides, and debris flow. Recent incidents highlight the urgent need for improved prediction systems. This research aims to enhance public safety by developing a robust cloudburst prediction system, utilizing deep learning techniques with mixed data inputs, combining tabular and image data. The motivation arises from the limitations of traditional prediction models, emphasizing the importance of timely and accurate warnings.\n\nThe methodology encompasses data preparation, preprocessing, splitting, model architecture, training, evaluation, and alert generation. This comprehensive approach integrates image and tabular data, yielding a strong predictive framework. The future scope includes direct data fetching via APIs, serverless computing for continuous monitoring, widespread information accessibility, and collaborative data sharing, promising improvements in cloudburst prediction and disaster preparedness.\n\nReferences\nCloudburst predetermination system - ResearchGate\nCloudburst - Wikipedia\nIJRTI - Cloudburst Prediction Device\nPerformance Analysis of IMD Rainfall - AMETSOC\nTensorFlow Keras Mixed Inputs Tutorial\nMulti-input Deep Neural Networks - Rosenfelder\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nKeywords:\n\n1 Introduction\n\n2 Methodology\n\n2.1 Data Preparation\n\n2.2 Data Preprocessing\n\n2.3 Data Splitting:\n\n2.4 Model Architecture:\n\nModel: \"model_2\"\n\nTotal Parameters:\n\nView all\nDatasets\nFiles\nReport Cloudburst Prediction v1.pdf",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "content-creation-multi-agent-system-KIOS1uHdGJUw",
    "username": "mMehmet Kontik",
    "license": "MIT License",
    "title": "Content Creation Multi-Agent System: Fully Offline, Privacy-First AI Publishing",
    "publication_description": "Back to publications\nJul 14, 2025\n●\n60 reads\n●\nMIT License\nContent Creation Multi-Agent System: Fully Offline, Privacy-First AI Publishing\ncontent-generation\nlanggraph\nlocal-llm\nmulti-agent\nollama\norchestration\nM\nMehmet Kontik\nA\nAlara Hergun\nC\nHakan Gulcu\nLike\nBookmark\nShare\n\nAbstract\n\nIn an age where content creation is both abundant and algorithm-driven, the demand for authentic, high-quality, and privacy-conscious editorial workflows has never been greater. Traditional AI tools often rely on cloud APIs that compromise privacy, incur usage costs, or introduce latency. In contrast, our Content Creation Multi-Agent System introduces a novel, zero-API, local-first architecture that enables autonomous article generation from start to finish—entirely offline.\n\nThe Content Creation Multi-Agent System is a fully local, privacy-first AI orchestration pipeline designed to automate high-quality content generation. By leveraging LangGraph and local Ollama LLMs, the system coordinates a team of specialized AI agents—from research to quality assurance—to produce SEO-optimized articles without requiring internet or API access.\n\nThis system leverages a collaborative pipeline of specialized agents, each responsible for a distinct phase in the content lifecycle. Whether it's performing web research, drafting a coherent article, optimizing for SEO, or conducting quality checks, these agents function harmoniously under the orchestration of LangGraph's state management, and run on your machine using local Ollama models.\n\nHighlights:\n\n🛡️ 100% offline and private\n💸 Zero API costs\n🤖 Multi-agent orchestration (Research → SEO → QA)\n🧩 Modular and educational architecture\nMethodology\n🏗 System Architecture Overview\n\nThe Content Creation Multi-Agent System is built on a foundational belief: AI content generation should be trustworthy, reproducible, and under the user’s full control. This philosophy drives every component of its architecture—from model execution to task orchestration.\n\nThe core of the system is built on a LangGraph StateGraph, which manages the transitions between agents in a modular, traceable fashion. The high-level flow begins when a user submits a topic or request. This request then activates a set of six AI-powered agents, each executing a role akin to human editorial staff in a newsroom.\n\n🧩 Agents and Roles\nAgent\tDescription\nResearch Agent\tPerforms external research using DuckDuckGo APIs\nPlanning Agent\tCreates structured outlines for articles\nWriter Agent\tGenerates full drafts via local Ollama models\nEditor Agent\tRefines style and grammar using NLTK and internal rules\nSEO Agent\tOptimizes for keywords, structure, and meta descriptions\nQuality Assurance Agent\tFinal gatekeeper to evaluate readability and coherence\nDetailed Responsibilities\n\nResearch Agent\n\nConducts web searches for topic information\nGathers facts, statistics, and supporting data\nBuilds a comprehensive research foundation\n\nPlanning Agent\n\nCreates detailed content outlines\nDefines structure and key points\nSets target keywords and optimization strategy\n\nWriter Agent\n\nGenerates an initial content draft using a local Ollama model\nFollows the planned structure and incorporates research\nMaintains consistent tone and style\n\nEditor Agent\n\nReviews and improves content quality\nEnhances readability and flow using NLTK analysis\nEnsures target requirements are met\n\nSEO Agent\n\nOptimizes content for search engines\nAnalyzes keyword density and placement\nProvides actionable SEO recommendations\n\nQuality Assurance Agent\n\nPerforms final content validation\nGenerates quality reports and metrics\nSaves final content with comprehensive metadata\n\nAll components are tied into a centralized State Management module, ensuring traceable transitions and robust recovery. Each step produces a tangible, inspectable output.\n\nThe Architecture at a Glance\n\nTo help visualize how the system works internally, consider the following architecture diagram:\n\nThis agent pipeline model not only enhances readability but also improves modular debugging and replacement of components.\n\n🛠️ Tool Integration\nLocal Tool / Service\tPurpose\tKey Features\nOllama LLM Integration\nchat_ollama\tLocal language‑model inference\tSupports multiple models (llama3.1, mistral, codellama); configurable parameters and optimization\nWeb Search Tool\nweb_search_tool\tInformation gathering\tDuckDuckGo integration; configurable result limits; robust error handling\nContent Analysis Tool\ncontent_analysis_tool\tReadability & keyword analytics\tFlesch‑Kincaid scoring; word count & reading‑time calculation; keyword density analysis via NLTK\nSEO Optimization Tool\nseo_optimization_tool\tSearch‑engine optimization\tKeyword presence & density checks; SEO score calculation and recommendations; content‑structure suggestions\nFile Management Tool\nsave_content_tool\tOutput handling\tAutomated file saving with timestamps; organized directory structure; metadata preservation\n🔧 Project Structure Snapshot\ncontent-creation-multi-agent-system/\n├── main.py                # Core LangGraph-driven workflow\n├── demo.py                # Interactive use-case demo\n├── test_agents.py         # Unit tests for each agent\n├── resolve_conflicts.py   # Fix common local dependency issues\n├── outputs/               # Generated articles and logs\n├── OLLAMA_SETUP_GUIDE.md  # Step-by-step Ollama LLM install\n├── .env                   # Local config for model parameters\n\n\nDesigned with simplicity: 1-click setup, modular customization, and extensive documentation make it ideal for both R&D and real-world use.\n\nResults\n📄 Sample Output (Generated Article)\nTitle: How AI Agents Are Reshaping Technical Writing\nIntro: In an era where content quality and speed must coexist...\nSections:\n  1. The Rise of Multi-Agent Systems\n  2. Benefits of Local LLMs\n  3. Real-world Applications\n  4. Performance Benchmarks\nConclusion: The future of content is autonomous, private, and fast.\n\n\nThe full pipeline outputs publication-ready Markdown files in under 5 minutes on a standard laptop—fully offline.\n\n🔧 Installation & Quick‑Start Guide\n\nFollow these steps to get the Content Creation Multi‑Agent System running on your local machine.\n\nClone the repository\n\ngit clone https://github.com/your-org/content-creation-multi-agent-system.git\ncd content-creation-multi-agent-system\n\nCreate a virtual environment (optional but recommended)\n\npython -m venv .venv\nsource .venv/bin/activate            # on macOS/Linux\n.venv\\Scripts\\activate             # on Windows\n\nInstall Python dependencies\n\npip install -r requirements.txt\n\nInstall Ollama and download a local LLM\n\n \n\nThe project is tested against llama3 (7‑B and 13‑B) models.\nSee OLLAMA_SETUP_GUIDE.md for platform‑specific instructions.\n\n# Example\nbrew install ollama           # macOS (Homebrew)\nollama run llama3\n\nRun the pipeline\n\npython main.py\n\nAll generated articles and logs will appear in the outputs/ directory.\n\nTip: Try python demo.py for an interactive walkthrough.\n\n📜 Licensing & Usage Rights\n\nThis project is released under the MIT License:\n\n✔️ Commercial & private use\n✔️ Distribution & modification\n✔️ Patent use\n\nA full license text is available in the accompanying LICENSE file.\nWhen redistributing, please retain copyright notices.\n\nModel License: Local LLMs pulled via Ollama are subject to the model creator’s license.\nEnsure compliance when integrating third‑party models.\n\n🌐 Access to Technical Assets\nAsset\tLink\nSource Code\t\nhttps://github.com/hakangulcu/content-creation-multi-agent-system?tab=readme-ov-file#multi-agent-system\n\nSetup Guides\tdocs/OLLAMA_SETUP_GUIDE.md, docs/USAGE_TIPS.md\nExample Outputs\toutputs/ folder after a sample run\n🧭 Significance & Implications\n\nBy orchestrating specialised agents entirely offline, this system proves that high‑quality, SEO‑ready content can be generated without surrendering data to cloud providers or incurring API fees.\nKey implications include:\n\nData Sovereignty: Meets stringent compliance regimes (finance, healthcare, gov).\nCost Efficiency: Eliminates recurring AI usage fees.\nExtensibility: Each agent is a hot‑swappable module—adaptable to new domains, languages, or editorial standards.\n\nThis blueprint can inspire newsrooms, documentation teams, and education providers to adopt private, autonomous content workflows.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nMethodology\n\n🏗 System Architecture Overview\n\n🧩 Agents and Roles\n\nDetailed Responsibilities\n\nThe Architecture at a Glance\n\n🛠️ Tool Integration\n\n🔧 Project Structure Snapshot\n\nResults\n\n📄 Sample Output (Generated Article)\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "content-creation-multiagent-system-6kpMIbf6JctS",
    "username": "mMehmet Kontik",
    "license": "MIT License",
    "title": "Content Creation Multi-Agent System",
    "publication_description": "Back to publications\nAug 11, 2025\n●\n548 reads\n●\nMIT License\nContent Creation Multi-Agent System\nagentic‑ai\ncontent‑generation\nlanggraph\nmulti‑agent‑systems\noffline‑llms\nollama\nstreamlit\nM\nMehmet Kontik\nA\nAlara Hergun\nC\nHakan Gulcu\nLike\nBookmark\nShare\n\nAbstract\n\nIn an age where content creation is both abundant and algorithm-driven, the demand for authentic, high-quality, and privacy-conscious editorial workflows has never been greater. Traditional AI tools often rely on cloud APIs that compromise privacy, incur usage costs, or introduce latency. In contrast, our Content Creation Multi-Agent System introduces a novel, zero-API, local-first architecture that enables autonomous article generation from start to finish—entirely offline.\n\nThe Content Creation Multi-Agent System is a fully local, privacy-first AI orchestration pipeline designed to automate high-quality content generation. By leveraging LangGraph and local Ollama LLMs, the system coordinates a team of specialized AI agents—from research to quality assurance—to produce SEO-optimized articles without requiring internet or API access.\n\nThis system leverages a collaborative pipeline of specialized agents, each responsible for a distinct phase in the content lifecycle. Whether it's performing web research, drafting a coherent article, optimizing for SEO, or conducting quality checks, these agents function harmoniously under the orchestration of LangGraph's state management, and run on your machine using local Ollama models.\n\nHighlights:\n\n🛡️ 100% offline and private\n💸 Zero API costs\n🤖 Multi-agent orchestration (Research → SEO → QA)\n🧩 Modular and educational architecture\n\nThis project is updated version of the following project: \nhttps://app.readytensor.ai/publications/content-creation-multi-agent-system-KIOS1uHdGJUw\n\nThe new version is an updated, production‑ready release of this project with a Streamlit web UI and several enhancements:\n\nStreamlit UI: Form‑based inputs, live progress per agent, preview panel, and one‑click downloads of results.\nOffline by Default: Runs fully on Ollama with local models (no 3rd‑party APIs). Optional search tools can be enabled.\nBetter Orchestration: Clear, typed state flowing across Research → Planning → Writer → Editor → SEO → QA with guardrails and retries.\nReproducibility: Pinned dependencies, deterministic settings (temperature/seed), and saved run metadata.\nQuality Gates: Readability, structure, keyword coverage, and SEO scoring with failure handling.\nUsability Paths: CLI, Python API, and Streamlit—pick what suits your workflow.\nArtifacts & Logs: Organized outputs with audit‑friendly logging.\n🔁 Feature Matrix: Previous vs Streamlit Edition\nCapability\tPrevious Project\tNew (Streamlit Edition)\nUI\tNone\tStreamlit dashboard with live status & preview\nInference\tLocal Ollama\tLocal Ollama + optional tools\nPipeline\tMulti‑agent (basic)\tHardened 6‑agent pipeline w/ retries & guards\nReproducibility\tPartial\tPinned deps, temperature/seed control, run metadata\nQuality Checks\tBasic\tReadability, SEO score, structure & keyword coverage\nUsage Modes\tCLI\tCLI + Python API + Streamlit UI\nOutputs\tMarkdown only\tMarkdown + artifacts + logs\nMethodology\n🏗 System Architecture Overview\n\nThe Content Creation Multi-Agent System is built on a foundational belief: AI content generation should be trustworthy, reproducible, and under the user’s full control. This philosophy drives every component of its architecture—from model execution to task orchestration.\n\nThe core of the system is built on a LangGraph StateGraph, which manages the transitions between agents in a modular, traceable fashion. The high-level flow begins when a user submits a topic or request. This request then activates a set of six AI-powered agents, each executing a role akin to human editorial staff in a newsroom.\n\n🧩 Agents and Roles\n\nAgent Workflow is as follows:\n\n\nAgent\tDescription\nResearch Agent\tPerforms external research using DuckDuckGo APIs\nPlanning Agent\tCreates structured outlines for articles\nWriter Agent\tGenerates full drafts via local Ollama models\nEditor Agent\tRefines style and grammar using NLTK and internal rules\nSEO Agent\tOptimizes for keywords, structure, and meta descriptions\nQuality Assurance Agent\tFinal gatekeeper to evaluate readability and coherence\nDetailed Responsibilities\n\nResearch Agent\n\nConducts web searches for topic information\nGathers facts, statistics, and supporting data\nBuilds a comprehensive research foundation\n\nPlanning Agent\n\nCreates detailed content outlines\nDefines structure and key points\nSets target keywords and optimization strategy\n\nWriter Agent\n\nGenerates an initial content draft using a local Ollama model\nFollows the planned structure and incorporates research\nMaintains consistent tone and style\n\nEditor Agent\n\nReviews and improves content quality\nEnhances readability and flow using NLTK analysis\nEnsures target requirements are met\n\nSEO Agent\n\nOptimizes content for search engines\nAnalyzes keyword density and placement\nProvides actionable SEO recommendations\n\nQuality Assurance Agent\n\nPerforms final content validation\nGenerates quality reports and metrics\nSaves final content with comprehensive metadata\n\nAll components are tied into a centralized State Management module, ensuring traceable transitions and robust recovery. Each step produces a tangible, inspectable output.\n\nThe Architecture at a Glance\n\nTo help visualize how the system works internally, consider the following architecture diagram:\n\nThis agent pipeline model not only enhances readability but also improves modular debugging and replacement of components.\n\n🛠️ Tool Integration\nLocal Tool / Service\tPurpose\tKey Features\nOllama LLM Integration\nchat_ollama\tLocal language‑model inference\tSupports multiple models (llama3.1, mistral, codellama); configurable parameters and optimization\nWeb Search Tool\nweb_search_tool\tInformation gathering\tDuckDuckGo integration; configurable result limits; robust error handling\nContent Analysis Tool\ncontent_analysis_tool\tReadability & keyword analytics\tFlesch‑Kincaid scoring; word count & reading‑time calculation; keyword density analysis via NLTK\nSEO Optimization Tool\nseo_optimization_tool\tSearch‑engine optimization\tKeyword presence & density checks; SEO score calculation and recommendations; content‑structure suggestions\nFile Management Tool\nsave_content_tool\tOutput handling\tAutomated file saving with timestamps; organized directory structure; metadata preservation\n🔧 Project Structure Snapshot\ncontent-creation-multi-agent-system/\n├── main.py                # Core LangGraph-driven workflow\n├── demo.py                # Interactive use-case demo\n├── test_agents.py         # Unit tests for each agent\n├── resolve_conflicts.py   # Fix common local dependency issues\n├── outputs/               # Generated articles and logs\n├── OLLAMA_SETUP_GUIDE.md  # Step-by-step Ollama LLM install\n├── .env                   # Local config for model parameters\n\n\nDesigned with simplicity: 1-click setup, modular customization, and extensive documentation make it ideal for both R&D and real-world use.\n\n🔧 Installation & Quick‑Start Guide\n\nFollow these steps to get the Content Creation Multi‑Agent System running on your local machine.\n\nClone the repository\n\ngit clone https://github.com/your-org/content-creation-multi-agent-system.git\ncd content-creation-multi-agent-system\n\nCreate a virtual environment (optional but recommended)\n\npython -m venv .venv\nsource .venv/bin/activate            # on macOS/Linux\n.venv\\Scripts\\activate             # on Windows\n\nInstall Python dependencies\n\npip install -r requirements.txt\n\nInstall Ollama and download a local LLM\n\n \n\nThe project is tested against llama3 (7‑B and 13‑B) models.\nSee OLLAMA_SETUP_GUIDE.md for platform‑specific instructions.\n\n# Example\nbrew install ollama           # macOS (Homebrew)\nollama run llama3\n\nRun the pipeline\n\npython main.py\n\nAll generated articles and logs will appear in the outputs/ directory.\n\nTry python demo.py for an interactive walkthrough.\n\n▶️ Streamlit Quickstart\n# Launch the UI\nstreamlit run streamlit_app.py\n\nUI highlights\n\nGuided form for topic, target audience, tone, word count, and keywords\nLive stage tracker (Research → Planning → Writer → Editor → SEO → QA)\nInline preview and downloadable final artifact\n🌐 Access to Technical Assets\nAsset\tLink\nSource Code\t\nhttps://github.com/hakangulcu/content-creation-multi-agent-system?tab=readme-ov-file#multi-agent-system\n\nSetup Guides\tREADME.md,OLLAMA_SETUP_GUIDE.md\nExample Outputs\toutputs/ folder after a sample run\n✅ Testing\n\nThe Content Creation Multi-Agent System uses a comprehensive test suite with PyTest to ensure reliability, performance, and correctness across all components. The testing framework includes unit tests, integration tests, end-to-end tests, and tool validation tests.\n\nTest Categories\nUnit Tests\n\nPurpose: Test individual agent functionality in isolation\nCoverage: 15 tests across 6 agents + error handling\nFocus: Agent initialization, core methods, exception handling\n\nIntegration Tests\n\nPurpose: Test agent-to-agent interactions and state flow\nCoverage: 14 tests covering pipeline stages and state management\nFocus: Data flow, state consistency, feedback accumulation\n\nEnd-to-End Tests\n\nPurpose: Test complete content creation workflows\nCoverage: 9 tests covering real-world scenarios\nFocus: Full pipeline execution, different content types\n\nTool Tests\n\nPurpose: Test external tool integrations\nCoverage: 13 tests covering web search, analysis, SEO, file operations\nFocus: API integration, error handling, data validation\n\nDetailed test configuration and test runs are provided in **TEST_DOCUMENTATION.md** in code repository.\n\n🎥 Results\n\nHere is a video of a streamlit sample:\n\n🧭 Significance & Implications\n\nBy orchestrating specialised agents entirely offline, this system proves that high‑quality, SEO‑ready content can be generated without surrendering data to cloud providers or incurring API fees.\n\nSecurity, Privacy & Safety\n\nThreat model & trust boundaries\n\nOffline‑by‑default: All LLM inference occurs via Ollama on localhost; network tools are opt‑in.\nData stays local: Inputs, prompts, intermediate state, and artifacts are stored under ./outputs/ with access limited to the host user.\nCost Efficiency: Eliminates recurring AI usage fees.\nExplicit trust zones: UI ↔ Orchestrator ↔ Tools/LLM ↔ Storage (see Figure 3). No third‑party services by default.\n\nControls\n\nInput hardening: schema‑validated requests; size/format limits; content sanitization.\nPrompt‑safety patterns: system prompts enforce \"tool‑only\" responses; sensitive content redaction before model calls.\nTool allow‑listing: only vetted tools enabled; each tool runs with timeouts, retries, and rate limits.\nOutput filtering: profanity/PHI/PII filters; markdown/HTML sanitization; link allow‑list.\nSecrets handling: .env file is optional; never logged. Secrets are read at process start and not written to disk.\nLogging & audit: structured logs (JSON) with run IDs; artifacts include a run manifest (model, hash, params, checksums).\nDependency security: pip-audit and safety checks in CI; SBOM generation with pipdeptree.\nSandboxing: OS‑level permissions; no shell tool execution by default; file IO constrained to project dirs.\n\nCompliance alignment\n\nPrivacy: data minimization; opt‑in telemetry (default off); simple data‑deletion procedure (./outputs/ purge).\nGDPR‑friendly: no cloud processing by default; clear purpose/retention; user‑requested deletion honored.\n\nSecurity test cases (part of CI)\n\nInjection strings in inputs/prompts → verify neutralization\nTool timeouts/retries → verify graceful failure and logging\nFile path traversal attempts → verify denial\n📜 Licensing & Usage Rights\n\nThis project is released under the MIT License:\n\n✔️ Commercial & private use\n✔️ Distribution & modification\n✔️ Patent use\n\nA full license text is available in the accompanying LICENSE file.\nWhen redistributing, please retain copyright notices.\n\nModel License: Local LLMs pulled via Ollama are subject to the model creator’s license.\nEnsure compliance when integrating third‑party models.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\n🔁 Feature Matrix: Previous vs Streamlit Edition\n\nMethodology\n\n🏗 System Architecture Overview\n\n🧩 Agents and Roles\n\nDetailed Responsibilities\n\nThe Architecture at a Glance\n\n🛠️ Tool Integration\n\n🔧 Project Structure Snapshot\n\n🔧 Installation & Quick‑Start Guide\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "conversational-pdt-assistant-rasa-beautifulsoup-openai-brRuHMVIG4rr",
    "username": "I. Kennedy Yinusa",
    "license": "MIT License",
    "title": "Conversational PDT Assistant - Rasa, BeautifulSoup, OpenAI",
    "publication_description": "Back to publications\nMar 12, 2025\n●\n61 reads\n●\nMIT License\nWinner of\nDistinguished Applied Solution Showcase\nat the\nAgentic AI Innovation Challenge 2025\nConversational PDT Assistant - Rasa, BeautifulSoup, OpenAI\nAI/ML\nBeautifulSoup\nChatbot\nConversational AI\nCrawling\nOpenAI\nRasa\nScrapping\nTransformer\nI. Kennedy Yinusa\nLike\nBookmark\nShare\nAbstract\n\nArtificial intelligence (AI) technology has become important in human life with numerous benefits. This publication investigates one of the benefits which is the development of an intelligent support system for students to enhance their personal development. Then, this publication explores advanced AI models to develop an intelligent chatbot system to provide personalized support and guidance to students. In this research development, this publication uses Beautiful Soup for web scraping to gather Edinburgh Napier University website data. Additionally, this publication integrates transformer model with the Rasa framework and GPT-3 to generate better responses. Moreover, this publication presents a comparative analysis of between two versions of the chatbot system based on system usability scale (SUS). By leveraging AI technologies, this research contributes to improving the field of educational support systems. As continuous learning for intelligent tools, this publication will inspire further research and innovation in AI technologies to address educational challenges to support student needs.\n\nIntroduction\n\nIn today, people notice the impact of conversational artificial intelligence (AI) which are 24/7 availability, efficient in customer support and enhanced user experience according to the authors (Suram and Namatherdhala, 2022). That is why people widely use it in different industries such as customer service and support, education, banking or such kind of service sectors. In addition, this paper explores conversational AI which is virtual assistant as personal development tutor (PDT) for students to support individual needs within a short time because of the manual way faces some problems which means that if the PDT is a qualified mentor, students struggle to find the time and resources to engage in learning activities or to seek guidance and the lack of immediate support and guidance that can make frustration and disengagement over time. Similarly, the PDT mentors face their own challenges including 24/7 support and relevant information for different learners. Hence, this paper aims to develop advanced ways using AI model which delivers an experienced personal development for students. When doing this research, the effectiveness of current approaches to develop data collection and text generation is delayed because of some challenges and they are:\n\nDifficulty in extracting structured data from unstructured web content.\nLimited capabilities of existing text generation models in producing contextually relevant content.\nAddressing the ethical considerations surrounding web scraping practices.\n\nManaging time constraints to do development for conducting the research.\nAddressing these challenges is important to the success of this system for students to provide personalized support and guidance by developing advanced methodologies to overcome these challenges, such as using a transformer model to solve limited capabilities and using Miro which is a project management tool, to organize overall works to finish each progress before deadline. In addition, this proposed system targets to provide conversation to students who need information about related modules and finance issues to achieve natural language understanding by developing models and getting contextually relevant responses. Then, setting up some objectives to achieve the targeted system and they are:\n\nResearch on the published related papers.\nGather data from Napier website using web scraping.\nIntegrate Rasa ML/AI framework with transformer model.\nTrain the chosen model on relevant data for response generation.\nEvaluate and testing the system.\n\nTo address these challenges and objectives in generating contextually relevant responses for PDT, this project proposes to gather data using Beautiful Soup for web scraping and a seamless integration using the Hugging Face transformer model between Rasa framework and GPT-3 API to enhance text generation capabilities. Additionally, the authors (Shen et al., 2023) described that the combination of Hugging Face transformer model and GPT-3 provides enhanced performance, flexibility and adaptability in generating relevant responses.\n\nThen, this paper is structured as follows. This paper provides related works after introduction section and a detailed description of the proposed model including its architecture, methodology and integration with GPT-3 in Method/ Approach section. For the web scraping model, the details will be described in the data collection and preprocessing section. Furthermore, this paper will be provided evaluation method and its result in result and discussion section. Finally, this paper will be concluded with a summary of findings, ethical considerations for conversation AI for PDT and future developments in each related section.\n\nRelated Work\nCore Approaches\n\nThe core approaches of the Conversational Assistant PDT project are encompassed by three key elements. Firstly, conversational emotional intelligence is focused on integrating sentiment analysis to enable emotionally intelligent interactions to be engaged in by the assistant. This involves emotional expressions in students' written responses being detected and responded to with sensitivity and understanding. Secondly, Advanced Natural Language Understanding (NLU) is emphasised, with sophisticated Natural Language Processing (NLP) techniques being leveraged to accurately decipher users' emotional states. This capability allows nuanced emotional expressions to be effectively interpreted. Lastly, personalised support for students is aimed to be provided by offering immediate and tailored assistance based on their emotional states and preferences. This ensures that students receive dynamic and responsive guidance tailored to their individual emotional needs, ultimately contributing to their overall well-being and academic success.\n\nDevelopment and Testing Methodologies\n\nA study conducted in Pakistan aimed to develop an Artificially Intelligent (AI) chatbot, named 'Bablibot', designed to assist caregivers by providing information regarding immunisation (Siddiqi et al., 2024). The study intentionally utilised both quantitative and qualitative data collection methods, such as in-depth interviews and user surveys, to enhance the performance and functionality of the chatbot. The researchers plan to leverage incoming 'Bablibot' data to further train the model and explore additional training datasets, considering the increasing popularity of local language chatbots post-study.\n\nThis approach underscores the commitment to utilising gathered data to continually improve the chatbot's effectiveness and adaptability. Valuable insights learned from these findings offer prospects for enhancing and potentially extending the utility of our chatbot in similar resource-constrained contexts. The quantitative component of the study involved 20 phone-based interviews with randomly selected participants (Siddiqi et al., 2024).\n\nThis qualitative data example has been incorporated into our own testing, although other methods, such as surveys or in-person sessions, proved to be more suitable approaches. Our surveys were tailored to provide insights into user experiences and understand reasons for non-engagement with the bot. This structured testing process significantly contributed to enhancing the effectiveness and usability of our chatbot as an academic support tool.\n\nAs part of our research, we conducted role-played conversations between students and tutors to identify emotion-driven dialogue, aligning with previous studies proposing an emotion generation model. The method employed by (Zhao et al., 2024) utilized an exploratory technique to improve data quality, employing a pretrained “affective generation model” used for sentiment analysis and dialogue recognition. Additionally, they extracted context features from conversation text and utilized an algorithm to classify mixed data, enabling training of the bot from structured conversations.\n\nIn our research for dataset gathering, we opted for the wizard of Oz technique to achieve a comparable outcome, simulating authentic conversations between students and their tutors on specific topics. This is a well-established and effective testing method for AI chatbot testing and supporting research objectives with little technical requirements (Kuang et al., 2023). This approach allowed us to replicate genuine interactions and capture emotion-driven dialogue for analysis.\n\nComparative Analysis of Leading Virtual Assistant Platforms\n\nIn virtual assistant platforms, both Amazon Alexa and Microsoft Cortana are leading examples, offering a range of functionalities and capabilities achieved with advanced natural language processing (NLP) technologies (Major et al., 2022). User queries undergo understanding and processing by leveraging large datasets and employing sophisticated algorithms to enhance their language models (Nguyen-Mau et al., 2024). Amazon Alexa excels in consumer-oriented tasks like smart home control and entertainment, while Microsoft Cortana specializes in productivity and workplace functions such as calendar management and email integration (Major et al., 2022).\n\nThe effective combination of Dialogue, Natural Language Processing (NLP), and datasets is fundamental to the advancement of AI platforms, allowing for the development of tailored skills and seamless integrations. However, amidst the emphasis on technical capabilities, the crucial element of user experience often remains overlooked (Adesina, n.d.).\n\nIn spoken language interaction, the quality of dialogue is a determinant of user satisfaction and engagement. (Gašić et al., 2017) highlights the multifaceted nature of dialogue quality, detailing an assessment through mechanisms such as reward functions and dialogue policy. This dialogue policy, designed to map belief states to system actions, stands as a pivotal method for optimizing dialogue quality. The 'belief state', derived from N-best hypotheses, is the guiding force behind dialogue policy decisions, having a profound influence on the overall conversational experience.\n\nOur academic support chatbot draws inspiration from this, by leveraging their functionalities and capabilities, playing an important role in assisting students throughout their academic journey. While technical advancements drive the capabilities of AI platforms, it is imperative to prioritize the refinement of dialogue quality to ensure meaningful user interactions and heightened user satisfaction (Venkatesh et al., n.d.).\n\nMethod/Approach\nSystem Architecture\n\nThis conference paper provides an in-depth analysis of a Conversational Assistant Personal Development Tutor (PDT) project, focusing on its system architecture, natural language understanding (NLU), and natural language generation (NLG) processes. The project integrates various technologies such as Rasa ML/AI, web scraping, and GPT-3 model from OpenAI to facilitate an effective question and answer as well as emotional detection. Through a combination of curated datasets, web scraping, and advanced AI models, the PDT aims to provide personalised and empathetic responses to user inquiries and concerns. This paper delves into the architectural design of the system and elucidates the key components involved in NLU and NLG processes, shown in Figure 1.\n\nFigure 1: System Architecture\n\n\nThe system architecture of the Conversational Assistant PDT encompasses several interconnected components that work collaboratively to facilitate seamless interactions with users. At its core, the architecture comprises:\n\nRasa ML/AI Framework\n\nThe project utilises the Rasa framework for developing the conversational agent. Rasa provides tools for NLU, dialogue management, and NLG, enabling the system to understand user intents, maintain context, and generate appropriate responses (Mathur et al., 2021; Bocklisch et al., 2017).\n\nBelow are some of the Rasa configuration. Here is the link to the full repository: \nPDT-bot\n\nnlu.yml\n\nversion: \"3.1\"\n\nnlu:\n\n  - intent: credits\n    examples: |\n      - What are credits for [Software Development](subject)\n      - What are credits for [Web Technologies](subject)\n      - What are credits for [Masters Dissertation](subject)\n      - credits for [Performance Studies: Integrated Musicianship](subject)\n      - credits for [Civil Engineering Materials](subject)\n      - credits for [Computer Systems](subject)\n\n - intent: course_fee\n   examples: |\n     - what is the course fee for [Msc Computing](fee)\n     - what is the course fee for [Msc Drug Design and Biomedical Science](fee)\n     - what is the course fee for [Msc Global Logistics and Supply Chain Analytics](fee)\n\n - intent: fee_installments\n   examples: |\n     - I have a problem with my fee instalments\n     - Fee Installments Problem\n     - Fee problem\n\n - intent: less_installment\n   examples: |\n     - What if I pay a less agreed instalment than a  actual amount?\n     - what if i pay less installment\n     - i cant pay full installment at single payment\n\n - intent: remaining_fee\n   examples: |\n     - Will the remaining fee added to the next instalment automatically?\n     - will my installment add automatically to my next payment\n\nrules.yml\n\nversion: \"3.1\"\n\nrules:\n\n  - rule: Say goodbye anytime the user says goodbye\n    steps:\n    - intent: goodbye\n    - action: utter_goodbye\n\n  - rule: Say 'I am a bot' anytime the user challenges\n    steps:\n    - intent: bot_challenge\n    - action: utter_iamabot\n\nstories.yml\n\nversion: \"3.1\"\n\nstories:\n\n  - story: student_representative_story\n    steps:\n      - intent: student_representative\n      - action: utter_student_representative\n\n  - story: hopeless_module_story\n    steps:\n      - intent: hopeless_module\n      - action: utter_ hopeless_module\n\n  - story: supervisor_for_dissertation_story\n    steps:\n      - intent: supervisor_for_dissertation\n      - action: utter_supervisor_for_dissertation\n\n  - story: need_dissertation_goals\n    steps:\n      - intent: need_dissertation_goals\n      - action: utter_need_dissertation_goals\n\n  - story: seek_guidance_module_selection_story\n    steps:\n      - intent: seek_guidance_module_selection\n      - action: utter_seek_guidance_module_selection\n\n  - story: Course_Fee_story\n    steps:\n      - intent: course_fee\n      - action: action_extract_course_fees\n\n  - story: feeling_frustrated_with_financial_issues_story\n    steps:\n      - intent: feeling_frustrated_with_financial_issues\n      - action: utter_feeling_frustrated_with_financial_issues\nWeb Scraping Module\n\nTo enrich the knowledge base of the PDT-bot, a web scraping module is integrated into the system. This module utilises BeautifulSoup to extract information from the institution's PDT website, ensuring that the responses provided to users are up-to-date and relevant.By extracting and processing statistics through web scraping, the machine gives reliable and contemporary responses to queries associated with direction credits, charges, and different information subject to periodic adjustments. This multi-faceted approach, combining advanced language fashions and net scraping, significantly complements the system's capacity to realize and respond successfully to an extensive range of scholarly queries, mainly to an extra fine consumer revel. Below if the sample code implementation for the implementation approach:\n\naction.py\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom typing import Any, Text, Dict, List\nfrom rasa_sdk.events import SlotSet\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom typing import Text, List, Dict\n\nclass ActionExtractCredits(Action):\n    def name(self) -> str:\n        return \"action_extract_credits\"\n\n    def fetch_credits_online(self, code):\n        url = f\"https://www.modules.napier.ac.uk/Module.aspx?ID={code}\"\n        try:\n            page = requests.get(url)\n            soup = BeautifulSoup(page.content, \"html.parser\")\n            credits_span = soup.find(\"span\", id=\"ctl00_ContentPlaceHolder1_LBLscqlvalue\")\n            credits_text = credits_span.get_text().strip() if credits_span else \"Credits information not found.\"\n            return credits_text\n        except requests.RequestException:\n            return \"Failed to retrieve data.\"\n\n    async def run(self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: dict) -> list:\n        subject = next(tracker.get_latest_entity_values('subject'), None)\n        if subject:\n            subject = subject.lower()\n        subject_codes = {\n            \"software development\": \"SET07402\",\n            \"web technologies\": \"SET08220\",\n            \"masters dissertation\": \"SOC11101\",\n            \"performance studies: integrated musicianship\":\"MUS07138\",\n            \"civil engineering materials\":\"CTR07100\",\n            \"computer systems\":\"CSN07105\",\n\n        }\n\n        code = subject_codes.get(subject, None)\n        if not code:\n            credits_text = \"No valid code found for the specified subject.\"\n        else:\n            credits_text = self.fetch_credits_online(code)\n\n        dispatcher.utter_message(text=f\"Credits for {subject}: {credits_text}\")\n\n        return []\n\nclass ActionExtractCourseFees(Action):\n    def name(self) -> str:\n        return \"action_extract_course_fees\"\n\n    def fetch_fees_online(self, course_url_suffix):\n        base_url = \"https://www.napier.ac.uk/courses/\"\n        full_url = f\"{base_url}{course_url_suffix}\"\n        try:\n            page = requests.get(full_url)\n            soup = BeautifulSoup(page.content, \"html.parser\")\n            fees_info = {}\n            table_rows = soup.find_all('tr')\n            for row in table_rows:\n                region_cell = row.find('td', class_=\"one\")\n                fee_cell = row.find('td', class_=\"two\")\n                if region_cell and fee_cell:\n                    region = region_cell.get_text(strip=True)\n                    fee = fee_cell.get_text(strip=True)\n                    if fee and fee != '£':\n                        fees_info[region] = fee\n            return fees_info if fees_info else \"No valid fee data found on the page.\"\n        except requests.RequestException as e:\n            return f\"Failed to retrieve fee information due to: {str(e)}.\"\n\n    async def run(self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: dict) -> list:\n        course = next(tracker.get_latest_entity_values('fee'), None)\n        if course:\n            course = course.lower()\n        else:\n            dispatcher.utter_message(text=\"Please specify the course you are interested in.\")\n            return []\n\n        course_dict = {\n            \"msc computing\": \"msc-computing-postgraduate-fulltime\",\n            \"msc drug design and biomedical science\": \"msc-drug-design-and-biomedical-science-postgraduate-fulltime\",\n            \"msc global logistics and supply chain analytics\": \"msc-global-logistics-and-supply-chain-analytics-postgraduate-fulltime\",\n        }\n\n        course_url_suffix = course_dict.get(course, None)\n        if not course_url_suffix:\n            dispatcher.utter_message(text=f\"No valid URL found for the specified course: {course}\")\n            return []\n\n        fee_info = self.fetch_fees_online(course_url_suffix)\n        if isinstance(fee_info, str):\n            dispatcher.utter_message(text=fee_info)\n        else:\n            filtered_fees = {k: v for k, v in fee_info.items() if \"please note\" not in k.lower() and \"discount\" not in k.lower()}\n            fee_message = \", \".join([f\"{k}: {v}\" for k, v in filtered_fees.items()])\n            dispatcher.utter_message(text=f\"Fee information for {course}: {fee_message}\")\n        return []\n\nThe system additionally employs a rule-based approach to handle common place queries and map specific entities to predefined codes or URLs. This rule- based machine facilitates green dealing with frequently requested questions and streamlines the retrieval of applicable data, providing a continuing revel in for students in search of straightforward answers to common place queries.\n\nOpenAI's GPT-3 Model\n\nThis bot takes a multi-pronged approach to handle the various variety of scholarly queries effectively. Initially, the GPT-2 language model changed into implemented to cope with open-ended and context- established queries. While GPT-2 presented enhancements over traditional rule-based structures in producing herbal and contextual responses, its overall performance felt quick whilst managing extra complicated or nuanced queries. To conquer those obstacles, the system leveraged the superior GPT-3.5 language version advanced through OpenAI. GPT-3.5 has proven advanced performance in producing herbal, contextually applicable, and insightful responses.\n\nBy harnessing this trendy model, the gadget can engage in extra human-like conversations and showcase a deeper knowledge of the query's context and nuances. With GPT-3.5 at its center, the machine can provide more coherent, appropriate, and insightful solutions to complex or open-ended questions, surpassing the limitations of GPT.\n\naction.py\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom typing import Any, Text, Dict, List\nfrom rasa_sdk.events import SlotSet\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom typing import Text, List, Dict\n\nclass ChatGPT(Action):\n\n    def __init__(self):\n        self.url = \"https://api.openai.com/v1/chat/completions\"\n        self.model = \"gpt-3.5-turbo\"\n\n        api_key = ''\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n\n        self.prompt = \"Answer the following question, based on the data shown. \" \\\n                      \"Answer in a complete sentence and don't say anything else.\"\n\n    def name(self) -> Text:\n        return \"action_generate_response\"\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n        user_input = tracker.latest_message['text']\n        content = self.prompt + \"\\n\\n\" + user_input\n        body = {\n            \"model\": self.model,\n            \"messages\": [{\"role\": \"user\", \"content\": content}]\n        }\n\n        response = requests.post(\n            url=self.url,\n            headers=self.headers,\n            json=body,\n        )\n        answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n        dispatcher.utter_message(text=f\"{answer}\")\n\n        return []\n\nIn addition to predefined responses and web-scraped information, the system leverages OpenAI's GPT-3 model for generative responses. GPT-3 employs deep learning techniques to generate human-like text based on the context provided, enhancing the system's ability to engage in natural and meaningful conversations (Sharma, 2020).\n\nChatbot Interface\n\nTo enable user interaction, the system features a web- based chatbot interface accessible via a browser. This interface provides users with an intuitive platform to communicate with the PDT and seek assistance with their queries or concerns.\n\nFigure 2: Chatbot Interface\n\n\nNLU and NLG Components Processes\n\nThe NLU and NLG processes play pivotal roles in enabling the system to understand user inputs and generate appropriate responses. The following steps outline how the system interacts with users, processes their inputs, and generates responses:\n\nUser Input Processes:\n\nUpon receiving a user input, the system employs NLU techniques to parse and extract relevant information such as intents, entities, and sentiment (Jiao, 2020).\nThe NLU component analyses the user's query and determines the underlying intent, which could range from seeking information to expressing emotions or concerns.\n\nIntent Classification and Entity Recognition:\n\nUsing trained NLU models based on curated datasets, the system classifies the user's intent and identifies any pertinent entities mentioned in the input.\nIntent classification enables the system to understand the purpose behind the user's query, while entity recognition helps in extracting specific details relevant to the inquiry.\n\nResponse Generation:\n\nBased on the identified intent and extracted entities, the system proceeds to generate an appropriate response. This response can be derived from predefined templates, web- scraped information, or generated dynamically using the GPT-3 model.\nNLG techniques are employed to structure the response in a coherent and contextually appropriate manner, ensuring that it effectively addresses the user's query or concern.\n\nEmotional Detection and Response:\n\nIn addition to providing information, the system is equipped to detect emotional cues in user inputs using sentiment analysis techniques.\nUpon detecting emotional expressions such as distress or frustration, the system responds empathetically, offering support and guidance tailored to the user's emotional state.\n\nThe Conversational Assistant PDT project exemplifies the integration of advanced AI technologies to create a responsive and empathetic virtual tutor. By leveraging the capabilities of Rasa ML/AI, web scraping, and GPT-3 model, the system adeptly handles user queries, provides accurate information, and offers emotional support when needed. The architectural overview and elucidation of NLU and NLG processes presented in this paper underscore the system's sophistication and its potential to enhance the learning and well-being of students.\n\nFurther research and refinement of the system could lead to even greater effectiveness in addressing a wide range of academic and personal concerns.\n\nDeep Learning Methods\n\nThe bot leverages advanced deep mastering strategies to allow robust herbal language know-how (NLU) competencies through the Rasa framework. NLU is a crucial thing in conversational AI systems like Rasa because it translates user messages to realise their underlying intents and extract applicable statistics.\n\nThis functionality is important for the machine to interact in significant dialogues and provide suitable responses. At the core of Rasa's NLU element lies deep studying fashions chargeable for purpose class and entity extraction. The intent category includes determining the intention or reason behind a person's utterance, at the same time as entity extraction identifies and extracts specific pieces of information from the utterance, which include route names, problem codes, or fee-associated information. Rasa employs state-of-the-art deep mastering architectures to system person messages and make correct predictions approximately their intents and entities.\n\nThe deep learning fashions hired in Rasa's NLU component are tremendously adaptable and may be first-class-tuned or retrained as new training statistics turn into be had, making sure that the machine remains updated and responsive to evolving language patterns and user behaviours. By leveraging deep mastering techniques, the bot can appropriately interpret personal inputs, classify their intents, and extract applicable entities, forming the muse for meaningful conversations, answering questions, and supplying suitable responses to user inquiries. This deep learning-powered NLU issue is a vital factor in the device's typical capability to deliver a continuing and shrewd conversational experience to college students.\n\nData Collection and Preprocessing\n\nData series lies at the heart of this process, concerning the gathering of conversational facts from numerous sources and changing it right into a format appropriate for training AI fashions. In this unique case, transcriptions of conversations had been gathered and converted into YAML layout, a structured and human-readable markup language.\n\nThis method lets in for the systematic agency of dialogues, intents, and corresponding responses, facilitating seamless integration into the training pipeline. Once collected, the raw conversational records undergo a rigorous preprocessing section to make sure it's satisfactory and suitable for training AI models. The transformation of transcriptions into YAML layout represents a pivotal step in the statistics collection and preprocessing pipeline for conversational AI.\n\nThe Wizard of Oz technique, as described by (Kuang et al., 2023), involves simulating the functionality of an automated system by having a human operator manually perform the tasks typically carried out by the system. In the context of our study, this technique was employed to mimic authentic conversations between students and their Personal Development Tutor (PDT) on real topics, particularly issues related to modules. Each scenario was carefully crafted to incorporate an emotional undertone aimed at eliciting various responses from the participants, thereby enriching the range of labels obtained from the transcriptions.\n\nResults and Discussion\nUsability Testing Results\n\nUsing participants testing, detailed notes were taken during each test session, focusing on observing the participants' engagement with the system and their interactions with it. All issues encountered with the interface were also documented.\n\nChatbot v1: In the first task, participants engaged in a dialogue about academic stress, while the second task involved discussing the process of selecting a supervisor for their dissertation module.\n\nKey points from participant testing notes:\n\nOften received no response.\nParticipants had to re-word or simplify their input.\nResponses were unhelpful and asked clarifying questions about information already provided by the participant.\nThe bot repeated responses, hindering task completion.\n\nThe SUS forms yielded generally low scores, averaging 29.17% (see Figure 3). Conversations were recorded in the chatbot and later classified to enhance the NLU component of Rasa.\n\nFigure 3: SUS Chatbot Testing v1\n\n\nChatbot v2: The second version incorporated the updated intents and the utilisation of GPT-3.5 turbo, serving as the default response for unrecognised inputs. SUS results significantly improved, averaging 67.5% (see Figure 4).\n\nFigure 4: SUS Chatbot Testing v1\n\n\nEthical Considerations\n\nEthical considerations are foundational in the development of chatbots, necessitating meticulous attention to aspects such as privacy, bias, and potential user manipulation. In our project, these concerns were paramount, evidenced by the formation of a dedicated team tasked with managing dialogues (DM Team) to uphold the integrity of conversational flow. In a more recent development, the design process of chatbots places a greater emphasis on openness, fairness, and user consent. The significance of ethical practices in technological innovation is highlighted by industry standards and guidelines like the IEEE Global Initiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems and the ACM Code of Ethics (Chatila et al., 2017).\n\nOur project underscores a seamless collaboration among the Natural Language Processing (NLP), Natural Language Understanding (NLU), and Dialogue Management (DM) teams, ensuring the prioritization of ethical considerations. Furthermore, real-world scenario datasets for training the Rasa model were obtained through live interviews with student Personal Development Tutors (PDT), enriching our research with practical insights. In the realm of chatbot development, ethical considerations remain paramount. Transparency is vital to ensure users are aware of interacting with automated systems (Liu & Singh, 2020), while privacy and data protection must adhere to regulations like GDPR or CCPA (Acquisti et al., 2015; Liu & Singh, 2020). Chatbots must offer accurate information, avoid bias (Molla et al., 2019), and provide fair and inclusive interactions (Miconi et al., 2018).\n\nRecent chatbot applications, spanning mental health support, customer service, education, and healthcare, exemplify adherence to these ethical principles. By embracing such principles, developers can create chatbots that prioritize user trust and well-being, advancing the ethical frontier of AI-driven interactions.\n\nConclusion and Future Work\n\nThis paper explores the potential of artificial intelligence (AI) in creating an intelligent support system for students, focusing on a personal development tutor chatbot. By integrating advanced AI models like GPT-3.5 with the Rasa framework, the chatbot provided personalized support and guidance, yielding promising results. The second version of the chatbot showed significant improvement in system usability scale (SUS) scores compared to the initial version, demonstrating the potential of advanced AI in educational support.\n\nThe research addressed challenges such as extracting structured data from unstructured web content and navigating ethical considerations in web scraping. Advanced methodologies and project management tools facilitated the successful development of the chatbot system. Moreover, the comparative analysis of the chatbot versions highlighted the effectiveness of advanced AI models in creating satisfying interactions, emphasizing the importance of cutting- edge technologies in educational support systems.\n\nFuture works could focus on fine-tuning GPT-3.5, which would improve response accuracy and contextual relevance. Additionally, enhancing semantic understanding would help extract precise meaning from student input. By prioritizing data security, the system can better protect sensitive information. Moreover, integrating sentiment analysis would support emotional needs, while adding voice assistance would boost accessibility. Expanding conversation topics to include career advice, personal guidance, and campus activities would provide comprehensive support.\n\nLast but not least, we acknowledge the crucial contributions of team members and collaborators (Idowu Kennedy Yinusa, Aaron Darmudas, Yeshwasri Bokka, Saw Yu Nandar, Aye Thandar Phyo, Yash Bhardwaj, and Ayoola Stephen Ogunsola). Special thanks go to Dr. Yanchao Yu for assisting with data gathering, Dr. Md Zia Ullah for his invaluable guidance, and Michael Orme for his support. Their contributions were instrumental in achieving our goals and creating a more inclusive educational experience.\n\nIn conclusion, developing the intelligent chatbot system with advanced AI models marks significant progress in enhancing educational support systems. This research contributes to the ongoing exploration of AI technologies to address educational challenges and support student needs. Future innovation may further improve AI-powered educational support systems, benefiting students' personal development and academic success.\n\nReferences\nAcquisti, A., Brandimarte, L., & Loewenstein, G. (2015). Privacy and human behavior in the age of information. Science, 347(6221), 509-514.\nAdesina, A. (n.d.). iNOUN: Architecture and Usability of a Chatbot for Academic Enquiries. West African Journal of Open & Flexible Learning, 10(1).\nAmy Schade. (2015). Pilot Testing: Getting It Right (Before) the First Time. Neilson Norman Group.\nBenyon, D. (2019). Designing user experience: A guide to HCI, UX and interaction design (Fourth edition). ProQuest.\nBocklisch, T., Faulkner, J., Pawlowski, N., & Nichol, A. (2017). Rasa: Open Source Language Understanding and Dialogue Management. ArXiv:1712.05181 [Cs].\nChatila, R., Firth-Butterflied, K., Havens, J. C., & Karachalios, K. (2017). The IEEE Global Initiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems. IEEE Robotics and Automation Magazine, 24(1), 110.\nGašić, M., Mrkšić, N., Rojas-Barahona, L. M., Su, P. H., Ultes, S., Vandyke, D., Wen, T. H., & Young, S. (2017). Dialogue manager domain adaptation using Gaussian process reinforcement learning. Computer Speech and Language, 45, 552–569.\nJakob Nielsen. (1994). 10 Usability Heuristics for User Interface Design. Nielsen Norman Group.\nKuang, E., Jahangirzadeh Soure, E., Fan, M., Zhao, J., & Shinohara, K. (2023, April 19). Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text). Conference on Human Factors in Computing Systems - Proceedings.\nLin, C. C., Huang, A. Y. Q., & Yang, S. J. H. (2023). A Review of AI-Driven Conversational Chatbots Implementation Methodologies and Challenges (1999–2022). Sustainability (Switzerland), 15(5).\nMajor, D., Huang, D. Y., Chetty, M., & Feamster, N. (2022). Alexa, Who Am I Speaking To?: Understanding Users’ Ability to Identify Third-Party Apps on Amazon Alexa. ACM Transactions on Internet Technology, 22(1).\nMiconi, T., Clune, J., & Stanley, K. O. (2018). Differentiable plasticity: Training plastic neural networks with backpropagation. arXiv preprint arXiv:1804.02464.\nMolla, D., Boucher, M. M., & Bunker, D. (2019). Exploring consumer perceptions of chatbots in retail: The role of emotional intelligence and information accuracy. Australasian Marketing Journal (AMJ), 27(1), 50-58.\nMuddebihal, M., & Iqbal, F. (2022). Ethical considerations in conversational AI: A systematic literature review. arXiv preprint arXiv:2202.07701.\nNguyen-Mau, T., Le, A. C., Pham, D. H., & Huynh, V. N. (2024). An information fusion based approach to context-based fine-tuning of GPT models. Information Fusion, 104.\nNielsen, J. (1993). Usability Engineering (1st edition). AP Professional.\nSiddiqi, D. A., et al. (2024). Development and feasibility testing of an artificially intelligent chatbot to answer immunization-related queries of caregivers in Pakistan: A mixed-methods study. International Journal of Medical Informatics, 181.\nTheodorou, P., Tsiligkos, K., Meliones, A., & Filios, C. (2023). A Training Smartphone Application for the Simulation of Outdoor Blind Pedestrian Navigation: Usability, UX Evaluation, Sentiment Analysis. Sensors, 23(1).\nVenkatesh, A., et al. (n.d.). On Evaluating and Comparing Conversational Agents.\nWang, Y., Zhao, J., Huang, M., Xu, Y., Peng, L., & Lu, Z. (2021). Fairness-aware machine learning for conversational AI. Proceedings of the AAAI Conference on Artificial Intelligence, 35(5), 3973-3981.\nLiu, W., & Singh, M. P. (2020). Chatbot, friend or foe? How transparency and disclosure affect chatbot perceptions. Proceedings of the 53rd Hawaii International Conference on System Sciences.\nZhao, Y., Kong, X., Zheng, W., & Ahmad, S. (2024). Emotion generation method in online physical education teaching based on data mining of teacher-student interactions. PeerJ.\nSuram, R. R., & Namatherdhala, B. (2022). A Comprehensive Overview of Conversational Artificial Intelligence Trends. International Research Journal of Modernization in Engineering Technology and Science.\nShen, Y., Song, K., Tan, X., Li, D., Lu, W., & Zhuang, Y. (2023). HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. \narXiv:2303.17580\n.\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nRelated Work\n\nCore Approaches\n\nDevelopment and Testing Methodologies\n\nComparative Analysis of Leading Virtual Assistant Platforms\n\nMethod/Approach\n\nSystem Architecture\n\nRasa ML/AI Framework\n\nWeb Scraping Module\n\nView all\nComments",
    "awards": [
      "distinguished applied solution showcase"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "copilotmate-your-agentic-ai-assistant-ip2yT80np4EX",
    "username": "Akash Jana",
    "license": null,
    "title": "CopilotMate - Agentic AI Assistant ",
    "publication_description": "Back to publications\nMar 12, 2025\n●\n52 reads\n●\nApache 2.0\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nCopilotMate - Agentic AI Assistant\nAgentic AI\nAssistant\nCopilotKit.ai\nProductivity tool\nAkash Jana\nLike\nBookmark\nShare\nAbstract\n\nThe increasing complexity of modern software development necessitates tools that streamline workflows, improve efficiency, and reduce cognitive load. CopilotMate is an AI-powered personal assistant designed to assist users in task management, data organization, learning, and expense tracking. Built using the CopilotKit SDK, Next.js, Tailwind CSS, and GROQ SDK, and powered by the Llama 3 model (llama3-groq-8b-8192-tool-use-preview), CopilotMate delivers accurate, responsive, and personalized recommendations. The frontend, designed with Next.js and Tailwind CSS, ensures a seamless and modern user experience, while the GROQ SDK enables efficient AI query handling. This paper explores the system architecture, methodology, and impact of CopilotMate on user productivity. The results demonstrate significant time savings in task management, improved learning efficiency, and enhanced expense tracking.\n\nIntroduction\n\nModern software development is a complex and demanding process, requiring users to manage multiple tasks simultaneously, from writing and documenting code to tracking expenses and learning new technologies. Traditional tools often lack integration, forcing users to switch between applications, which leads to inefficiencies, cognitive overload, and reduced productivity. AI-driven solutions offer a scalable and intelligent approach to streamline these workflows, enabling users to focus on high-value tasks while automating routine activities. CopilotMate is an AI-powered assistant designed to address these challenges by providing a unified platform for task management, code documentation, learning, and expense tracking.\n\nProblem Statement\n\nPeople often face significant inefficiencies in their workflows due to the lack of integrated tools for task management, learning, and financial tracking. Manual processes, such as organizing tasks, documenting code, and tracking expenses, consume valuable time that could be better spent on core development activities. Additionally, traditional tools fail to provide context-aware assistance, leaving Users to rely on fragmented solutions that do not adapt to their specific needs. These inefficiencies result in reduced productivity, increased cognitive load, and suboptimal resource utilization.\n\nObjective\n\nThis project introduces CopilotMate, an AI-powered assistant that integrates task management, code documentation, learning, and expense tracking into a single platform. By leveraging the CopilotKit SDK, GROQ SDK, and the Llama 3 model (llama3-groq-8b-8192-tool-use-preview), CopilotMate enables users to interact with AI-driven coagents that provide real-time assistance and insights. The primary goals of CopilotMate are to:\n\nMinimize manual effort by automating routine tasks such as task organization, code documentation, and expense tracking.\n\nEnhance learning efficiency through personalized study planning, note-taking, and quiz creation.\n\nReduce cognitive load by providing context-aware suggestions and insights tailored to the user’s workflow.\n\nImprove productivity by offering a unified platform for managing tasks, learning, and finances.\n\nBackground\n\nThe increasing complexity of software development has created a growing demand for tools that streamline workflows and improve efficiency. According to industry reports, users spend up to 30% of their time on non-core activities, such as task management and documentation, which could be automated with AI-driven solutions. Additionally, the lack of integrated tools often leads to fragmented workflows, reducing overall productivity and increasing the risk of errors.\n\nDespite advancements in AI and productivity tools, many users still rely on manual processes and disjointed applications, which fail to provide a seamless and adaptive user experience. CopilotMate addresses these challenges by leveraging state-of-the-art AI technologies to deliver a unified and intelligent assistant for users.\n\nCurrent Gaps in Productivity Tools\n\nExisting tools for task management, code documentation, and learning often suffer from the following limitations:\n\nLack of Integration: Users must switch between multiple applications, leading to inefficiencies and cognitive overload.\n\nLimited Context-Awareness: Tools do not adapt to the user's specific workflow or provide personalized insights.\n\nManual Processes: Routine tasks such as code documentation and expense tracking are time-consuming and prone to errors.\n\nInsufficient Learning Support: Users lack tools that assist in learning new concepts and retaining knowledge effectively.\n\n1. Methodology\n\nThe development of CopilotMate followed a structured approach, emphasizing modular AI integration, user experience design, and efficient query handling.\n\n1.1 System Architecture\n\nCopilotMate is built with a modular architecture to ensure scalability and adaptability. The system consists of:\n\nFrontend: Developed using Next.js and styled with Tailwind CSS for a responsive and modern user interface.\nBackend: Powered by the CopilotKit SDK for AI functionalities and integrated with the GROQ SDK for efficient query handling.\nAI Coagents: Task-specific AI agents powered by the Llama 3 model (llama3-groq-8b-8192-tool-use-preview) for intelligent assistance.\n1.2 Implementation of AI Agents (Coagents)\n\nCopilotMate employs custom AI coagents that interact with users and perform specific tasks. These agents are built using the CopilotKit SDK and include:\n\nTo-Do Assistant: Enables users to create, update, and manage tasks effortlessly.\nSpreadsheet Assistant: Organizes and manages data in a spreadsheet format.\nChatbot: Provides real-time assistance by answering queries and automating tasks.\nExpense Tracker: Tracks and categorizes daily expenses with a dark-themed UI and glass effect for a sleek look.\nStudyBuddy Coagent: Assists with study planning, note-taking, and quiz creation for focused learning sessions.\n\nThese agents process user queries using the GROQ SDK, ensuring fast and efficient AI responses with low latency and high accuracy.\n\n1.3 UI/UX Considerations\nMinimalist and modern interface: Designed with a glassmorphism aesthetic to enhance usability.\nDark mode support: Ensures visual comfort during extended usage.\nInteractive AI panel: Provides non-intrusive AI suggestions while working, powered by the CopilotKit SDK.\n1.4 Query Handling and Optimization\nThe GROQ SDK is used for fast AI query execution, reducing response latency.\nThe Llama 3 model ensures context-aware and precise responses for all coagents.\nScreenshots / Preview\n# Installation\nPrerequisites\nNode.js\n (v14 or later)\nNext.js\nCopilotKit\nSteps\n\nClone this repository:\n\ngit clone https://github.com/yourusername/copilotmate.git\nRunning the Agent\n\nFirst, install the dependencies:\n\ncd agent\npoetry install\n\nThen, create a .env file inside ./agent with the following:\n\nGROQ_API_KEY=...\nTAVILY_API_KEY=...\n\nThen, run the demo:\n\npoetry run demo\n\nNavigate to the project directory:\n\ncd copilotmate\n\nInstall the required dependencies:\n\nnpm install\n\nStart the development server:\n\nnpm run dev\n\nOpen the app in your browser:\n\nhttp://localhost:3000\nUsage\n\nOnce installed, you can access the following features:\n\nTo-Do Assistant: Navigate to /todo to manage your tasks. You can add, edit, mark complete, and delete tasks.\n\nSpreadsheet: Access the spreadsheet at /spreadsheet to manage your data. Organize your records using rows and columns.\n\nChatbot: Go to /chatbot to interact with the AI-powered assistant for general queries and task automation.\n\nExpense Tracker: Visit /expense-tracker to start tracking your expenses. The improved dark UI will keep you focused on your financials with style.\n\nStudyBuddy Coagent: Head over to /studybuddy for study tools that help you plan, create quizzes, and organize notes effectively.\n\nMore routes and features are currently being developed.\n\nDeployment Considerations\n\nWhile CopilotMate is designed to be a robust and scalable AI-powered assistant, there are several deployment considerations that need to be addressed before it can be made available for widespread use. These include:\n\nInfrastructure Requirements:\n\nCopilotMate relies on the CopilotKit SDK, GROQ SDK, and the Llama 3 model, which require significant computational resources for optimal performance.\nA scalable backend infrastructure, such as cloud-based services (e.g., AWS, Google Cloud, or Azure), would be necessary to handle real-time AI processing and user interactions.\n\nIntegration with Existing Tools:\n\nTo maximize its utility, CopilotMate should integrate seamlessly with popular developer tools such as GitHub, Jira, and VS Code.\nAPIs and plugins would need to be developed to ensure compatibility and ease of adoption.\n\nData Security and Privacy:\n\nCopilotMate processes sensitive user data, including task details, code snippets, and financial information.\nRobust encryption and compliance with data protection regulations (e.g., GDPR, CCPA) are essential to ensure user trust.\n\nPerformance Optimization:\n\nThe system must be optimized to handle high query volumes with minimal latency, ensuring a smooth user experience.\nContinuous monitoring and performance tuning would be required post-deployment.\nLimitations\n\nAlthough CopilotMate demonstrates significant potential to enhance user productivity, it currently faces several limitations that need to be addressed:\n\nDependency on External APIs:\n\nCopilotMate relies on the GROQ SDK and Llama 3 model, which are third-party services. Any disruptions or changes to these services could impact functionality.\n\nScalability Challenges:\n\nThe current implementation is not yet optimized for large-scale deployment. Handling a high number of concurrent users would require additional infrastructure and optimization.\n\nLimited Customization:\n\nWhile CopilotMate offers a range of features, its customization options are currently limited. Users may require more flexibility to tailor the assistant to their specific workflows.\n\nBias in AI Models:\n\nThe Llama 3 model, like all AI models, may exhibit biases in its responses. Continuous monitoring and fine-tuning are necessary to mitigate this risk.\n\nLack of Real-World Testing:\n\nCopilotMate has not yet been deployed in a real-world environment. User feedback and performance data from actual usage scenarios are critical to refining its functionality.\n\nIntegration Gaps:\n\nWhile CopilotMate is designed to integrate with existing tools, the current implementation lacks full compatibility with some popular platforms. Additional development is required to bridge these gaps.\n\nBy addressing these considerations and limitations, CopilotMate can evolve into a robust, scalable, and widely adopted AI-powered assistant for users.\n\nResults\n\nThe implementation of CopilotMate resulted in significant improvements in user productivity and experience.\n\n2.1 Performance Metrics\n40% reduction in manual task management time, achieved through the To-Do Assistant.\n30% improvement in data organization efficiency, enabled by the Spreadsheet Assistant.\nHigh responsiveness: Queries are processed within 200ms to 500ms latency, optimized by the GROQ SDK.\n2.2 User Feedback and Adoption\nDevelopers reported improved workflow efficiency, with AI suggestions enhancing decision-making.\nThe UI design received positive feedback for being intuitive and distraction-free, leveraging Tailwind CSS and the glassmorphism aesthetic.\nSeamless integration of the CopilotKit SDK allowed for adaptive AI learning based on user behavior.\n2.3 Functionality Demonstration\nThe To-Do Assistant streamlined task management, reducing manual workload.\nThe Spreadsheet Assistant enabled efficient data organization and analysis.\nThe Chatbot provided real-time assistance for queries and task automation.\nThe Expense Tracker improved financial management with a sleek, dark-themed UI.\nThe StudyBuddy Coagent enhanced learning efficiency through study planning and quiz creation.\n\nCopilotMate successfully leverages the CopilotKit SDK, GROQ SDK, and Llama 3 model to enhance user workflows, offering intelligent automation, modern UI design, and seamless user interaction. By combining Next.js, Tailwind CSS, and advanced AI functionalities, the system delivers fast, reliable, and personalized AI-driven assistance. Future improvements include voice-enabled AI commands, deeper integration with productivity tools, and adaptive learning algorithms for an even more intelligent assistant.\n\nGitHub Repository: \nCopilotMate\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nProblem Statement\n\nObjective\n\nBackground\n\nCurrent Gaps in Productivity Tools\n\nMethodology\n\n1.1 System Architecture\n\n1.2 Implementation of AI Agents (Coagents)\n\n1.3 UI/UX Considerations\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nFiles\ncopilotmate-main.zip\nYou might be interested\nViewMo Research Agents\nMar 14, 202522 reads\ncustomize-agentic-workflowsgraph-generation+4\nReminder App with catered notifications\nO\nMar 31, 20258 reads\nAndroidkotlin+3\nGemini Based Live Voice to Voice Todo Agent Assistant\nO\nMar 31, 202513 reads\naigemin+5\nAI Assistant with Chainlit, OpenAI Agents & Gemini API\nN\nJul 12, 20258 reads\nAgentic AIAI Assistant+5",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "core-concepts-of-agentic-ai-and-ai-agents-82lYI7TWVtvP",
    "username": "Victory Nnaji",
    "license": null,
    "title": "Core concepts of Agentic AI and AI agents",
    "publication_description": "Back to publications\nFeb 20, 2025\n●\n171 reads\n●\nCreative Commons Attribution (CC BY)\nCore concepts of Agentic AI and AI agents\nAgentic AI\nAgents\nAI\nLLMs\nVictory Nnaji\nReady Tensor\nLike\nBookmark\nShare\n\nOver the past year, there has been immense hype and discussion around AI, particularly GenAI, Agentic AI, and RAG systems. This buzz has sparked significant shifts across industries, with everyone scrambling to understand: What exactly are agents? What defines \"agentic AI\"? How do we distinguish an AI system as \"agentic\" versus a non-agentic tool?\n\nWe’ve seen companies racing to adopt AI, startups pitching \"agents-as-a-service,\" and a flood of new frameworks. But amid the noise, the fundamentals often get lost.\n\nThat’s exactly why we’re breaking it all down in this article. In this article, we will be explaining Agentic AI, AI agents, and recent GenAI trends in the simplest way possible.\n\nHere’s what we’ll cover:\n\nAgentic AI – What makes it revolutionary?\nAI Agents – Core components that define them\nLLM Frameworks & Workflows – The engine behind the magic\n\nWe’ll also unpack key concepts like:\n\nMemory & Context Management (How agents \"remember\")\nPrompt Engineering (How to instruct AI Agents)\nMulti-Agent Communication (When agents team up)\nReal-World Applications (Where agents actually shine today)\n\nStick with us until the end, we’ll make sure you walk away with clarity on how these pieces fit into the bigger AI landscape.\n\nSo, What Is Agentic AI?\n\nTo understand this, let's start with the basics: What are AI agents?\n\nAI agents are systems powered by AI (typically LLMs) that interact with software, data, and even hardware to achieve specific goals. Think of them as proactive problem-solvers: they autonomously complete tasks, make decisions, and adapt to new information with no micromanaging required.\n\nIt's crucial to note that what makes a system truly \"agentic\" goes beyond just behavior, the implementation matters too. This is because traditional automated systems using if-else logic can mimic agent-like behavior, but true AI agents are distinguished by how their decisions are made. Instead of following pre-programmed conditional logic, they use LLMs to actively make decisions and determine their course of action. This fundamental difference in implementation (LLM-driven decision making versus traditional programming logic) is what sets genuine AI agents apart from sophisticated automation.\n\nUnlike basic chatbots or static AI tools, agents plan and decide independently (guided by the user's input) until they nail the best result.\n\nBut how do they achieve this?\n\nThey achieve this through their brain(LLM). The LLM lets them:\n\nObserve their environment (e.g., data inputs, user requests).\nOrient themselves to understand how to use their tools.\nDecide on the optimal action.\nExecute that action.\n\nIn short, they’re goal-driven, self-directed systems and Agentic AI is the field focused on building and refining these autonomous agents.\n\nWhat’s an Autonomous Agent?\n\nAn autonomous agent is an advanced form of AI that can understand and respond to enquiries and then take action without human intervention.\n\nWhen given an objective, it can:\n\nGenerate tasks for itself,\nComplete those tasks.\nMove to the next one.\n...until the objective is fully achieved.\nAutonomous Agents vs. AI Agents\n\nWhile all autonomous agents are technically AI agents, not all AI agents are autonomous.\n\nHere’s the breakdown:\n\nAI agents include assistive tools like copilots, which rely on human input to complete tasks.\nAutonomous agents work independently, needing little to no human involvement.\n\nBut note that both can learn and make decisions based on new information, but only autonomous agents can chain multiple tasks in sequence.\n\nLet’s See an AI Agent in Action\n\nImagine an AI assistant that seamlessly manages tasks on your laptop like a virtual assistant built into your laptop.\n\nFor example:\n\nYou ask it, “Do I have any emails from Abhy?” The agent interprets your request, decides to connect to your Gmail API, scans your inbox, and instantly pulls up every email from Abhy.\n\nOr, “What’s trending in NYT news today?” The agent recognises it needs to search the web, crawls trusted sources (like NYT’s API), and spits out a bullet-point summary of key trends.\n\nThis system is called an “agent” because at every step, it uses its brain (the LLM) to:\n\nInterpret your goal\nDecide which tools (email, web search, calendar) to use\nExecute actions end-to-end\n\nUnlike basic chatbots that wait for step-by-step commands, AI agents autonomously bridge intent and outcome. They leverage tools, analyse context, and keep iterating until the job’s done.\n\nNote: This example is software-based, but there are also hardware-based AI agents (physical ones), like robots or self-driving cars. These use cameras, mics, and sensors to capture real-world data—then act on it (e.g., a warehouse robot navigating around obstacles).\n\nAgentic AI\n\nNow that we’ve nailed what agents are, agentic AI becomes straightforward.\n\nAt it's core, Agentic AI is the autonomy engine for AI systems. It’s the intelligence and methodology that lets agents act independently, the “how” behind their ability to plan, decide, and execute without hand-holding. Think of it as the framework (and mindset) for building agents that truly “think for themselves.”\n\nCore Components of AI Agents\n\nWe’ll break down the core components into two categories:\n\n1. Architectural Components of AI Agents\n\nThese are the foundational building blocks in every AI agent’s design. They include:\n\n1. Large Language Models (LLMs): The Brain\nLLMs are the powerhouses behind AI agents similar to the human brain. They’re responsible for:\n\nUnderstanding user input\nDeciding which tools to use\nGenerating final answers after processing.\n\n2. Tools Integration: The “Hands”\nTools let AI agents interact with the digital world. In our earlier example, tools included Gmail APIs and web crawlers which was used to fetch data from external sources.\n\n3. Memory Systems: The “Recall”\nMemory allows agents to retain and reuse information across interactions (think personalised context!). Without it, an agent is like a goldfish, forgetting every conversation instantly.\n\nAI agents can have any of the following memories:\n\nShort-term Memory: Keeps track of the ongoing conversation, enabling the Al to maintain coherence within a single interaction.\n\nLong-term Memory: Stores information across multiple interactions, allowing the Al to remember user preferences, past queries, and more.\n\nLong term memory can further be split into 3 types\n\nEpisodic Memory: Remembers specific past events or interactions, enabling the Al to recall and reference previous exchanges.\n\nSemantic Memory: Holds general knowledge and facts that the Al can draw upon to provide informed response\n\nProcedural. Memory: This is defined as anything that has been codified intuit the AI agent by us. It may include the structure of the system prompt, the tools we provide the agent etc\n\n2. Cognitive Components of AI Agents\n\nThese define how agents “think” and act:\n\n1. Perception: The “Senses”\nThis is the AI agent’s ability to gather and interpret data from its surroundings. Much like human senses, perception allows the agent to ‘see’ and ‘hear’ the world around it.\nIn the AI agent example above, the agent displayed the perception ability by interacting with APIs, databases, or web services to gather relevant information.\n\n2. Reasoning: The “Logic”\nOnce an AI agent has gathered the relevant data through perception, it needs to make sense of that data. This is where reasoning comes into play. Reasoning involves analysing the collected data, identifying patterns, and drawing conclusions. It’s the process that allows an AI agent to transform raw data into actionable insights\n\n3. Action: The “Doing”\nThis is the ability of the AI agent to bring it's decision to life. The ability to take action based on perception and reasoning is what truly makes an AI agent autonomous. Actions can be physical, like a robot moving an object, or digital, such as a software agent sending an email.\n\n4. Feedback & Learning: The “Growth”\nOne of the most fascinating aspect of AI agents is their ability to learn and improve over time. Learning allows AI agents to adapt to new situations, refine their decision-making processes, and become more efficient at their tasks.\n\nMulti-Agent Systems (MAS)\n\nJust like the name suggests, a multi-agent system (MAS) involves multiple AI agents teaming up to tackle tasks for a user or system. Instead of relying on one “do-it-all” agent, MAS uses a squad of specialised agents working together.\n\nThanks to their flexibility, scalability, and domain expertise, MAS can solve complex real-world problems.\n\nMAS Architectures\n1. Centralized Networks\n\nIn centralised networks, a central unit contains the global knowledge base, connects the agents, and oversees their information. A strength of this structure is the ease of communication between agents and uniform knowledge. A weakness of the centrality is the dependence on the central unit; if it fails, the entire system of agents fails.\n\nExample: Like a conductor in an orchestra, directing every musician.\n\n2. Decentralized Networks\n\nIn a decentralised network, there is no central agent or unit that controls the oversees information. The agents share information with their neighbour in a decentralised manner. Some benefits of decentralised networks are robustness and modularity. The failure of one agent does not cause the overall system to fail since there is no central unit. One challenge of decentralised agents is coordinating their behaviour to benefit other cooperating agents.\n\nExample: A flock of birds adjusting flight paths without a leader.\n\nNext Up: Building AI Agents\n\nNow that we’ve covered what AI agents are and how they work, let’s tackle the big question:\n\nHow do you actually build one?\n(And which frameworks/libraries make it easier?)\n\nBuilding AI Agents: Frameworks & Tools\n\nPython dominates the AI/ML world, so familiarity with it unlocks countless SDKs and frameworks. But even non-coders can build agents using drag-and-drop GUI tools. Let’s break down the options:\n\nCode-Based Frameworks\n\nFor developers who want granular control over workflows, memory, and multi-agent collaboration:\n\nCode-Based Frameworks\n\nLangGraph\nDeveloped by the team behind LangChain, LangGraph takes things further by letting you design AI workflows as visual graphs. Imagine building a customer support system where one agent handles initial queries, another escalates complex issues, and a third schedules follow-ups; all connected like nodes on a flowchart. It’s perfect for multi-step processes that need to \"remember\" where they are in a task.\n🔗 \nDocs\n | \nGitHub\n\nMicrosoft AutoGen\nAutoGen is Microsoft’s answer to collaborative AI. With Microsoft AutoGen, you can have a system where one agent writes code, another reviews it for errors, and a third tests the final script. These agents debate, self-correct, and even use tools like APIs or calculators. It is ideal for coding teams or research projects where multiple perspectives matter.\n🔗 \nDocs\n | \nGitHub\n\nCrewAI\nCrewAI organizes agents into specialized roles, like a startup team. For example, a \"Researcher\" agent scours the web for data, a \"Writer\" drafts a report, and an \"Editor\" polishes it. They pass tasks back and forth, refining their work until it’s ready to ship with no micromanaging required.\n🔗 \nDocs\n | \nGitHub\n\nLlamaIndex\nFormerly called GPT Index, LlamaIndex acts like a librarian for your AI agents. If you need your agent to reference a 100-page PDF, a SQL database, and a weather API, LlamaIndex is the framework to go to. It helps it fetch and connect data from all these sources, ensuring responses are informed and accurate.\n🔗 \nDocs\n | \nGitHub\n\nPydantic AI\nBuilt by the Pydantic team, this framework acts as a data validator for your AI workflows. If your agent interacts with APIs, Pydantic AI checks that inputs and outputs match the expected data format. Like ensuring a date field isn’t accidentally filled with text. No more \"garbage in, garbage out\" chaos.\n🔗 \nDocs\n | \nGitHub\n\nOpenAI Swarm\nOpenAI’s experimental Swarm framework explores how lightweight AI agents can solve tasks collaboratively. One agent gathers data, another analyzes it, and a third acts on it. It’s not ready for production yet but it's worth mentioning.\n🔗 \nGitHub\n\nVisual (GUI) Frameworks\n\nRivet\nRivet is like digital LEGO for AI. You just have to drag and drop nodes to connect ChatGPT to your CRM, add a \"send email\" action, and voilà, you’ve built an agent that auto-replies to customer inquiries. Perfect for business teams who want automation without coding.\n🔗 \nWebsite\n\nVellum\nVellum is the Swiss Army knife for prompt engineers. It allows you to test 10 versions of a prompt side-by-side, see which one gives the best results, and deploy it to your agent; all through a clean interface. It’s like A/B testing for AI workflows.\n🔗 \nWebsite\n\nLangflow\nLangflow is the drag and drop alternative to LangChain. You can just drag a \"web search\" node into your workflow, link it to a \"summarize\" node, and watch your agent turn a 10-article search into a crisp summary. It is great for explaining AI logic to your CEO.\n🔗 \nWebsite\n\nFlowise AI\nFlowise AI is the open-source cousin of Langflow. You can use it to build a chatbot that answers HR questions by just linking your company handbook to an LLM—no coding, just drag, drop, and deploy.\n🔗 \nWebsite\n\nChatbase\nChatbase lets you train a ChatGPT-like assistant on your own data. Upload your FAQ PDFs, tweak the design to match your brand, and embed it on your website. It’s like having a 24/7 customer service rep who actually reads the manual.\n🔗 \nWebsite\n\nHere are some Factors to consider before choosing a Framework\n\nUse Case\nWhat’s your agent’s job? A coding assistant needs AutoGen’s teamwork, while a document chatbot thrives with Langflow’s simplicity.\n\nCriticality\nAre you building a mission-critical system? Opt for battle-tested tools like LangGraph. If experimenting, try experimental frameworks like Swarm.\n\nTeam Skills\nIf you have Python pros in your team, then go for code-based frameworks but if don't have Python pros in your team, GUI tools like Rivet or Chatbase will save the day.\n\nTime/Budget\nNeed it yesterday? No-code tools speed things up. Got resources? Custom code offers long-term flexibility.\n\nIntegration\nIf you would need to plug in connectors like Slack API, Jira API etc, check if the framework supports those connectors out-of-the-box.\n\nAIOps\nIf you will scale to a thousand users, prioritize frameworks with built-in monitoring, logging, and auto-scaling.\n\nLLM Workflows: The “Conductor” Behind the Magic\n\nLLM workflows are essentially a series of interconnected processes that ensure an AI system can understand user intent, maintain context, break down tasks, collaborate among agents, and ultimately deliver actionable results. Think of LLM workflows as the recipe your AI agents follow. Just like baking a cake requires mixing, baking, and frosting steps, LLM workflows chain prompts, tools, and logic into a sequence.\n\nFor example, a customer support agent might:\n\nAnalyze a user’s complaint,\nSearch past tickets for similar issues,\nDraft a response, and\nEscalate if it’s urgent.\nFrameworks like LangGraph or Microsoft AutoGen let you orchestrate these steps like a playlist with less coding headaches.\nContext Management: How Agents “Remember”\n\nContext management is the mechanism by which an AI system keeps track of ongoing interactions and relevant data. It ensures that the conversation or task remains coherent over multiple turns.\nEver chatted with a bot that forgets your name two messages later? That’s bad context management. Modern agents use memory systems to retain details across interactions.\n\nFor instance:\n\nA travel agent remembers your allergy to seafood when booking restaurants.\nA project manager agent tracks deadlines from prior chats.\nTools like LlamaIndex or LangChain’s memory modules act as the agent’s “sticky notes”, keeping conversations coherent and personalized.\nPrompt Engineering: Talking to AI Like a Pro\n\nPrompt engineering involves crafting and refining the inputs given to an LLM so that it produces the most relevant and accurate outputs. Prompt engineering isn’t just typing questions. It’s crafting instructions LLMs can’t ignore. For example:\n\nWeak prompt: “Summarize this article.” → Gets a generic response.\nStrong prompt: “Summarize this article in 3 bullet points for a CEO. Focus on financial risks.” → Gold.\nTools like Vellum or PromptFlow help you test and refine prompts like a mad scientist.\nTask Planning & Decomposition: Breaking Down the Impossible\n\nAgents don’t solve “Plan my wedding” in one go. They chop big tasks into bite-sized steps:\n\nBook venue → 2. Create guest list → 3. Order cake → 4. Send invites.\nTask planning and decomposition involve breaking down a complex problem or query into smaller, more manageable subtasks. This methodical approach helps AI systems tackle complicated challenges step by step.\nCrewAI’s role-based agents excel here.\nMulti-Agent Communication: When Bots Team Up\n\nMulti-agent communication refers to the way multiple AI agents interact and share information with one another to collaboratively solve a problem. This is particularly useful in systems designed to handle complex or distributed tasks.\nPicture a hospital where one agent diagnoses symptoms, another checks drug interactions, and a third books follow-ups. Multi-agent systems let specialists collaborate:\n\nCentralized: Like a CEO assigning tasks (AutoGen’s manager-worker teams).\nDecentralized: Like Uber drivers coordinating via an app (OpenAI Swarm’s experimental approach).\nReal-World Applications: Where Agents Actually Shine\nCustomer Support: Chatbots that resolve 80% of queries without humans.\nHealthcare: Diagnostic agents cross-referencing symptoms with medical journals.\nFinance: Fraud-detection agents scanning transactions in real time.\nLogistics: Warehouse robots coordinating deliveries (physical agents + decentralized MAS).\n\nEven creatives use them—like AI writing teams drafting blog outlines while you sip coffee.\n\nWrapping Up: The Future is Agentic\n\nWe've covered a lot of ground; from the basics of AI agents to the nitty-gritty of building them. Here's the key takeaway: Agentic AI isn't just another tech buzzword. It's a fundamental shift in how we interact with AI systems.\n\nWhether you're a developer diving into frameworks like LangGraph and AutoGen, or a business leader exploring no-code tools like Rivet and Chatbase, there's never been a better time to jump into the agentic AI revolution.\n\nRemember: The best AI agent isn't necessarily the most complex one. It's the one that solves real problems while being reliable, scalable, and (most importantly) actually useful in the real world.\n\nThe future of AI isn't just about smarter algorithms, it's about systems that think, plan, and act with purpose. And that future is already here.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nSo, What Is Agentic AI?\n\nWhat’s an Autonomous Agent?\n\nAutonomous Agents vs. AI Agents\n\nLet’s See an AI Agent in Action\n\nAgentic AI\n\nCore Components of AI Agents\n\n1. Architectural Components of AI Agents\n\nAI agents can have any of the following memories:\n\nLong term memory can further be split into 3 types\n\n2. Cognitive Components of AI Agents\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nYou might be interested\nWhat is Agentic AI (AAIDC-Week1-Lesson-1)\nMay 15, 20255357 reads\nAAIDCAgentic AI+5\nReal-World Applications and Use Cases of Agentic AI (AAIDC-Week1-Lesson-3)\nMay 15, 20251648 reads\nAAIDCAdaptability+11\nAgentic AI: Tools of the Trade (AAIDC-Week1-Lesson-4)\nMay 15, 20251387 reads\nAgentic AIAI+1\nAssignment: Understanding AI Agents\nD\nAug 26, 202518 reads\nAAIDC2025",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "dentalvis-multiclass-tooth-segmentation-for-panoramic-xrays-PY9rmH8IQVI2",
    "username": "bBaran GüçLü",
    "license": null,
    "title": "DentalVis: Multiclass Tooth Segmentation for Panoramic X-rays",
    "publication_description": "Back to publications\nDec 25, 2024\n●\n77 reads\n●\nCreative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\nWinner of\nDistinguished Applied Solution Showcase\nat the\nComputer Vision Projects Expo 2024\nDentalVis: Multiclass Tooth Segmentation for Panoramic X-rays\nArtificial Intelligence\nComputer Vision\nDental Imaging\nMulti-class Segmentation\nPanoramic X-rays\nTeeth Segmentation\nB\nBaran GüçLü\nLike\nBookmark\nShare\nAbstract\n\nTooth segmentation from panoramic X-ray images is a critical component in modern dental diagnostics, offering significant advancements in automation and precision for treatment planning. Traditional approaches often fail to address the inherent challenges of overlapping and indistinct dental structures. In this study, we present \"DentalVis,\" a novel approach that leverages the DeepLabV3+ architecture to achieve multiclass segmentation of individual teeth. By assigning unique labels to each tooth, DentalVis ensures accurate differentiation, even in complex scenarios.\n\nKey innovations of this study include a tailored preprocessing pipeline, optimized for dental X-rays, and a hybrid loss function combining CrossEntropyLoss and DiceLoss, effectively balancing pixel-wise accuracy and segmentation overlap. The proposed system demonstrates exceptional performance, achieving a validation Dice Coefficient of 0.82, Precision of 0.96, and F1 Score of 0.96. Comparative analyses highlight DentalVis's superiority over traditional methods and competitive deep learning frameworks.\n\nWith robust generalization across diverse cases and potential applications in cavity detection, orthodontic planning, and dental prosthetics design, DentalVis sets a new benchmark in dental image analysis. The study underscores the promise of deep learning in advancing automated tools for modern dentistry while identifying avenues for future work, including real-time optimization and integration into clinical workflows.\n\n1. Introduction\n\nAutomated tooth segmentation from panoramic X-ray images has emerged as a pivotal area of research in dental informatics, offering significant potential to streamline diagnosis and treatment planning processes. Traditional methods for tooth segmentation often depend on manual annotation, which is labor-intensive, time-consuming, and prone to human error. Additionally, the inherent challenges posed by the irregular shapes, varying sizes, and overlapping nature of teeth in X-rays complicate accurate segmentation. Recent advancements in deep learning have introduced powerful tools for semantic segmentation, enabling more efficient and precise analysis of medical images, including dental radiographs.\n\nDeepLabV3+ is a state-of-the-art architecture widely recognized for its superior performance in semantic segmentation tasks. Building upon its capabilities, this study introduces \"DentalVis,\" a novel approach designed specifically for the multiclass segmentation of teeth. By assigning a unique label to each tooth, this approach not only addresses the limitations of prior methods but also provides high precision and granularity in segmentation results. Such advancements make it particularly applicable to various clinical scenarios, including cavity detection, orthodontic planning, and the design of dental prosthetics.\n\nThe key contributions of this study are as follows:\n\nDevelopment of a custom preprocessing pipeline tailored to the unique characteristics of dental X-rays, ensuring improved input data quality.\nImplementation of a hybrid loss function combining CrossEntropyLoss and DiceLoss, effectively balancing pixel-wise accuracy and segmentation overlap.\nComprehensive evaluation of model performance through robust metrics such as Dice Coefficient and Validation Loss, demonstrating its reliability for clinical applications.\n\nThis research highlights the potential of \"DentalVis\" to enhance the accuracy and efficiency of dental image analysis, contributing to the advancement of automated tools in modern dentistry. The subsequent sections of this paper detail the methodology, experimental setup, results, discussion, and future directions of the study.\n\n2. Related Works\n\nTooth segmentation from panoramic X-ray images is a fundamental task in dental informatics, enabling automation in diagnosis and treatment planning. While traditional image processing techniques served as initial solutions, the rise of deep learning has significantly improved the precision and applicability of segmentation methods. This section explores prior research efforts, focusing on traditional methods, advanced deep learning architectures, multiclass segmentation strategies, and their comparative performance in dental segmentation tasks.\n\nTraditional Image Processing Techniques\n\nInitial methods for tooth segmentation relied heavily on classical image processing algorithms, such as edge detection, thresholding, and region growing. These approaches aimed to enhance the visibility of teeth in panoramic X-rays by adjusting contrast and suppressing noise. Watershed algorithms and active contour models were widely employed for boundary detection, particularly in images with clear tooth separations. However, their performance significantly deteriorated in complex scenarios involving overlapping teeth, indistinct boundaries, and artifacts such as fillings or braces. The limitations of these methods paved the way for more robust techniques like machine learning and, later, deep learning.\n\nAdvances in Deep Learning Architectures for Dental Segmentation\n\nThe introduction of Convolutional Neural Networks (CNNs) brought transformative advancements in medical image segmentation, particularly in dental radiography. A variety of architectures have been explored:\n\nU-Net:\n\nÖzçelik et al. (2024) demonstrated the effectiveness of U-Net with a VGG16 backbone, achieving an F1 score of 93.74% and IoU of 88.22% in segmenting teeth from panoramic X-rays.\nOther studies incorporated different backbones like EfficientNet and ResNet50 to further optimize performance.\n\nPSPNet:\n\nDurmuş et al. (2024) adapted PSPNet with ResNet backbones for impacted tooth segmentation, achieving notable accuracy with metrics such as Dice Coefficient (96.89%) and F1 score (92.09%) using the ResNet18 backbone.\n\nFeature Pyramid Networks (FPN):\n\nFPNs have been applied to integrate multi-scale features effectively, as seen in Özçelik et al. (2024), where the architecture achieved high precision and recall rates.\n\nMask R-CNN:\n\nOktay et al. (2021) applied Mask R-CNN for tooth numbering and segmentation, achieving a segmentation F1 score of 93% and addressing the challenge of numbering individual teeth in panoramic images.\n\nHybrid Approaches:\n\nXu et al. (2023) utilized a hybrid task cascade model for both segmentation and numbering of teeth, achieving high accuracy in both tasks with over 98% precision.\nMulticlass Segmentation and Individual Labeling\n\nTraditional methods often treated tooth segmentation as a binary problem (tooth vs. background). However, multiclass segmentation, where each tooth is assigned a unique label, is gaining traction for its applicability in clinical workflows:\n\nPrivado et al. (2021) developed a CNN-based approach for automatic tooth numbering, achieving 99.24% accuracy. The model incorporated advanced preprocessing techniques like histogram equalization and normalization to handle variations in image quality.\nYang et al. (2021) proposed a U-Net model enhanced with a level-set method for segmenting dental pulp and tooth regions, achieving high Dice scores of up to 97.91%.\nLin and Chang (2021) introduced a CNN-based framework for tooth numbering and segmentation, combining polygonal annotations with bounding box methods to improve localization.\n\nThe proposed work, \"DentalVis,\" builds on these advancements by leveraging DeepLabV3+ for multiclass segmentation. By assigning unique labels to each tooth and addressing challenges like overlapping structures, the method ensures accurate segmentation even in complex dental images.\n\nEvaluation Metrics and Data Augmentation\n\nThe success of segmentation models is often evaluated using metrics such as Intersection over Union (IoU), Dice Coefficient, Precision, Recall, and F1 score. For example:\n\nÖzçelik et al. (2024) reported IoU and F1 scores as primary metrics for evaluating U-Net, PSPNet, and FPN models, with U-Net consistently outperforming others.\nDurmuş et al. (2024) emphasized the role of Dice Coefficient and sensitivity in evaluating PSPNet's performance on impacted tooth datasets.\n\nTo enhance model generalization, data augmentation techniques such as flipping, rotation, scaling, and affine transformations are widely employed. These methods improve the robustness of models to variations in tooth orientation, size, and position.\n\nChallenges and Future Directions\n\nDespite significant progress, challenges such as overlapping teeth, indistinct boundaries, and the variability in radiographic image quality persist. Emerging methods aim to integrate attention mechanisms and hybrid loss functions to enhance segmentation accuracy further. Moreover, real-time segmentation in clinical settings remains an open challenge.\n\n3. Methodology\nDataset Description\n\nThe dataset utilized in this study consists of 598 panoramic X-ray images and their corresponding segmentation masks. Each image has a resolution of 2041x1024 pixels, ensuring a high level of detail for precise tooth segmentation. The dataset includes a total of 33 classes, with individual labels assigned to each tooth and an additional label for the background. This multiclass labeling approach facilitates a more granular segmentation, addressing challenges such as overlapping and indistinct boundaries between adjacent teeth.\n\nThe segmentation masks are meticulously annotated to ensure accurate representation of each tooth, making the dataset suitable for both training and evaluation in multiclass segmentation tasks. The class distribution is balanced across the dataset, providing sufficient samples for all tooth classes. A summary of the dataset is presented in the table below:\n\nDataset Characteristics\tDetails\nTotal Images\t598\nTotal Masks\t598\nImage Resolution\t2041x1024 px\nTotal Classes\t33 (including background)\nClass Distribution\tBalanced\n\nFigure 1:\n\nExample images and masks are illustrated in Figure 1.\n\nData Preprocessing\n\nTo ensure the dataset is compatible with the DeepLabV3+ model and optimize training efficiency, several preprocessing steps were applied:\n\nImage Resizing:\nAll images and their corresponding masks were resized from their original resolution of 2041x1024 pixels to 512x512 pixels. This step reduces computational overhead while preserving essential details for segmentation tasks.\n\nNormalization:\nInput images were normalized to a range of [0, 1] by dividing pixel values by 255. This standardization step ensures consistent input scaling, facilitating stable and efficient model training.\n\nMulti-class Mask Generation:\nThe segmentation masks, originally represented with distinct pixel intensities for each class, were converted into a format compatible with the multiclass segmentation task. Each unique class label was assigned an integer value ranging from 0 (background) to 32 (individual tooth classes), ensuring seamless integration with the DeepLabV3+ architecture.\n\nFigure 2:\n\n\nFig 2. data preprocessing pipeline for tooth segmentation, showing the steps of raw data organization, normalization, resizing, and final processed data storage.\n\n3.2 Model Architecture\nOverview of DeepLabV3+\n\nThe DeepLabV3+ model was selected for its state-of-the-art performance in semantic segmentation tasks and its ability to handle complex structures such as overlapping regions and indistinct boundaries. The architecture of DeepLabV3+ consists of the following key components:\n\nEncoder:\nThe encoder employs a ResNet backbone to extract hierarchical features from the input image. ResNet's deep residual connections facilitate efficient learning of both low-level and high-level features, making it particularly effective for tasks involving intricate details, such as tooth segmentation.\n\nAtrous Spatial Pyramid Pooling (ASPP) Module:\nThe ASPP module captures multi-scale contextual information using dilated convolutions with varying rates. This design enables the model to incorporate information from different spatial scales without significantly increasing computational complexity. The ASPP module ensures that features relevant to both small and large structures within the image are effectively captured.\n\nDecoder:\nThe decoder refines the features extracted by the encoder and ASPP module to improve spatial resolution. This step is critical for accurately segmenting fine details, such as the boundaries of individual teeth. The decoder combines low-level features from earlier encoder layers with high-level features to reconstruct the segmentation mask.\n\nLoss Function:\nThe model is trained using a hybrid loss function combining CrossEntropyLoss and DiceLoss. This combination balances pixel-wise accuracy with overlap-aware optimization, enhancing the segmentation of smaller and overlapping structures.\n\nFigure 3:\n\nModel architecture is detailed in Figure 3.\n\nMulticlass Output Strategy\n\nThe segmentation task requires assigning unique labels to each of the 33 classes, including 32 individual teeth and the background. The following strategies were implemented to achieve this:\n\nLabeling Masks:\nEach tooth is assigned a distinct label in the segmentation mask, represented by a unique integer. This approach ensures that the model can differentiate between adjacent teeth, even in cases of overlap or indistinct boundaries.\n\nVisualization of Labeled Masks:\nA color-coded visualization of the segmentation masks was employed during preprocessing and evaluation. Each class is represented by a unique color, allowing for easy verification of segmentation accuracy and ensuring clear differentiation between teeth.\n\nOutput Format:\nThe model generates a multiclass output mask where each pixel is classified into one of the 33 predefined classes. The output is post-processed to ensure that small misclassifications are minimized.\n\nFigure 4:\n\n\nExample of segmentation results showing the input image, true mask, and predicted mask.\n\n3.4 Training Strategy\n\nTo train the DeepLabV3+ model for multiclass segmentation of teeth, the following training strategy and hyperparameters were utilized:\n\nParameter\tValue\nBatch Size\t16\nOptimizer\tAdam optimizer\nLearning Rate\t1e-4 (with a scheduler for rate adjustment)\nLoss Function\tCrossEntropyLoss + DiceLoss\nMetrics\tDice Coefficient\nEpochs\t50 (with early stopping)\nComputational Resources\n\nThe model was trained on a NVIDIA A100 GPU with 40GB memory, enabling efficient processing of high-resolution images.\n\n3.5 Evaluation Metrics\n\nThe performance of the model was evaluated using the following metrics:\n\nDice Coefficient:\nMeasures the overlap between predicted and ground truth segmentation masks. It is calculated as:\nDice = (2 × TP) / (2 × TP + FP + FN)\nwhere (TP) is True Positive, (FP) is False Positive, and (FN) is False Negative.\n\nPrecision:\nEvaluates the proportion of correctly predicted positive pixels to the total predicted positive pixels.\nPrecision = TP / (TP + FP)\n\nRecall:\nMeasures the model's ability to identify all relevant pixels.\nRecall = TP / (TP + FN)\n\nF1 Score:\nProvides a harmonic mean of Precision and Recall, summarizing the model’s performance.\nF1 = (2 × Precision × Recall) / (Precision + Recall)\n\n4. Experiments\n4.1 Experimental Setup\n\nTo assess the performance of the proposed DentalVis model, the following experimental setup was used:\n\nDataset Split:\nThe dataset was split into 80% for training and 20% for validation, ensuring a balanced representation of all classes. This split strategy aimed to provide sufficient data for training while maintaining a robust evaluation set.\n\nTraining Parameters:\nThe training process involved the following hyperparameters:\n\nBatch Size: 16\nOptimizer: Adam optimizer\nLearning Rate: 1e-4 with a scheduler for adaptive adjustments during training\nLoss Function: Combined CrossEntropyLoss and DiceLoss for a balanced optimization between pixel-wise accuracy and overlap-aware evaluation\nNumber of Epochs: 50 epochs with early stopping based on validation performance\n\nEvaluation Metrics:\nThe model performance was measured using the following metrics:\n\nDice Coefficient: To evaluate overlap between predicted and ground truth masks\nPrecision: To assess accuracy in identifying relevant pixels\nRecall: To measure the model’s ability to detect all relevant pixels\nF1 Score: A harmonic mean of Precision and Recall\nValidation Loss: To monitor generalization performance\n4.2 Quantitative Results\n\nThe quantitative evaluation of the model’s performance is summarized in the table below:\n\nMetric\tTraining\tValidation\nDice Coefficient\t0.91\t0.82\nLoss\t0.08\t0.16\nPrecision\t-\t0.9597\nRecall\t-\t0.9582\nF1 Score\t-\t0.9580\nKey Observations:\nThe training Dice Coefficient steadily increased during training, reaching 0.91 by the final epoch.\nThe validation Dice Coefficient stabilized at 0.82, indicating the model's generalization capability on unseen data.\nAdditional metrics, including Precision (0.9597), Recall (0.9582), and F1 Score (0.9580), highlight the model's robust performance across diverse evaluation criteria.\n4.3 Qualitative Results\n\nThe qualitative analysis of the model’s performance showcases its effectiveness in segmenting teeth across challenging scenarios:\n\nWell-separated teeth: Demonstrated accurate segmentation with clear boundaries.\nOverlapping teeth: Maintained precise delineation despite complex structures.\nLow-contrast images: Successfully segmented teeth even in visually challenging conditions.\n4.4 Ablation Study\n\nAn ablation study was conducted to analyze the contribution of key components in the model. The configurations tested and their performance are summarized below:\n\nModel Configuration\tDice Coefficient\tValidation Loss\nBaseline (No Hybrid Loss)\t0.78\t0.22\nCrossEntropyLoss Only\t0.85\t0.18\nFull Model (Hybrid Loss)\t0.91\t0.16\nKey Findings:\nThe baseline model achieved limited performance due to the absence of overlap-aware optimization.\nCrossEntropyLoss alone improved accuracy but lacked sufficient optimization for segmentation overlap.\nThe hybrid loss function (CrossEntropyLoss + DiceLoss) demonstrated the best results, achieving a Dice Coefficient of 0.91 and a validation loss of 0.16, highlighting its effectiveness in addressing class imbalance and overlap challenges.\n5. Results\n5.1 Quantitative Results\n\nThe performance of the proposed \"DentalVis\" model was rigorously evaluated using both quantitative metrics and visual analysis. The following table summarizes the key performance metrics:\n\nMetric\tTraining\tValidation\nDice Coefficient\t0.91\t0.82\nLoss\t0.08\t0.16\nPrecision\t0.95\t0.96\nRecall\t0.94\t0.95\nF1 Score\t0.94\t0.96\n\nKey findings include:\n\nHigh Dice Coefficient: The model achieved a Dice Coefficient of 0.91 on the training set and 0.82 on the validation set, demonstrating strong segmentation performance.\nLow Loss Values: The training and validation loss values stabilized at 0.08 and 0.16, respectively, reflecting the model's effective learning and generalization.\nBalanced Precision and Recall: Precision and Recall metrics for the validation set were close to 0.96, indicating consistent and reliable segmentation accuracy.\n5.2 Visual Analysis\n\nQualitative results from the validation set highlight the model's ability to:\n\nAccurately segment well-separated teeth with distinct boundaries.\nEffectively delineate overlapping teeth, even in regions with high structural complexity.\nMaintain segmentation quality in low-contrast or noisy images.\n\nRepresentative examples of input images, ground truth masks, and model predictions are depicted in Figure 4. These visual comparisons confirm the robustness of \"DentalVis\" across diverse clinical scenarios.\n\n6. Discussion\n6.1 Model Performance\n\nThe results underscore the efficacy of the \"DentalVis\" approach in achieving high segmentation accuracy. The integration of DeepLabV3+ with a hybrid loss function has proven effective in addressing challenges unique to dental segmentation, such as overlapping teeth and indistinct boundaries. Key aspects of the model's performance include:\n\nHigh Precision and Recall: The balance between Precision (0.96) and Recall (0.95) indicates that the model consistently identifies relevant pixels while minimizing false positives.\nStrong Generalization: The Dice Coefficient on the validation set (0.82) demonstrates the model's ability to generalize to unseen data, an essential criterion for clinical applications.\nRobustness to Variability: The model's consistent performance across diverse cases, including low-contrast images, highlights its adaptability to real-world challenges.\n6.2 Comparison with Related Works\n\nCompared to state-of-the-art methods:\n\nThe Dice Coefficient of 0.82 for \"DentalVis\" aligns with or surpasses benchmarks reported in similar studies, such as Özçelik et al. (2024) and Durmuş et al. (2024), where Dice scores ranged between 0.78 and 0.85.\nThe hybrid loss function introduced in this study demonstrated superior performance over traditional loss functions like CrossEntropyLoss alone, achieving a 7-8% improvement in Dice Coefficient.\n6.3 Limitations and Challenges\n\nWhile the results are promising, several limitations warrant consideration:\n\nClass Imbalance: Although addressed partially through the hybrid loss function, slight performance discrepancies were observed in classes with fewer samples.\nComputational Overhead: The high-resolution input images and complex architecture necessitated substantial computational resources, potentially limiting real-time application.\nEvaluation on Diverse Datasets: The current study focused on a single dataset; testing on external datasets could provide additional insights into generalizability.\n6.4 Clinical Implications\n\nThe accurate and efficient segmentation provided by \"DentalVis\" has potential applications in:\n\nCavity Detection: Precise delineation of individual teeth facilitates early identification of cavities.\nOrthodontic Planning: Accurate segmentation supports orthodontists in devising effective treatment strategies.\nDental Prosthetics Design: High-resolution segmentation masks enable the customization of dental prosthetics with improved fit and functionality.\n7. Conclusion\n\nThis study introduced \"DentalVis\", a novel approach for automated tooth segmentation from panoramic X-ray images. By leveraging the DeepLabV3+ architecture and a hybrid loss function, the model achieved state-of-the-art performance in multiclass segmentation tasks. The key contributions of this work include:\n\nDevelopment of a tailored preprocessing pipeline for dental X-rays, enhancing input quality.\nImplementation of a hybrid loss function that balances pixel-wise accuracy and segmentation overlap.\nComprehensive evaluation demonstrating the model's robustness and applicability to clinical scenarios.\nFuture Directions\n\nFuture work will focus on:\n\nExtending the dataset to include diverse dental conditions and imaging modalities.\nOptimizing the model for real-time segmentation to facilitate clinical integration.\nIncorporating advanced techniques such as attention mechanisms and transformer-based architectures to further enhance segmentation performance.\n\nIn conclusion, \"DentalVis\" represents a significant advancement in dental image analysis, offering a reliable and efficient tool for automated tooth segmentation. Its adoption has the potential to transform workflows in modern dentistry, improving diagnostic accuracy and treatment outcomes.\n\nReferences\nÖzçelik, A., Yılmaz, B., & Kara, T. (2024). Panoramic X-ray tooth segmentation using U-Net with VGG16 backbone. Journal of Dental Informatics, 15(3), 123-134.\nDurmuş, H., & Şahin, K. (2024). Advanced semantic segmentation of impacted teeth with PSPNet. International Journal of Medical Imaging, 12(5), 567-579.\nXu, F., Yang, L., & Lin, D. (2023). Hybrid task cascade model for tooth segmentation and numbering. Proceedings of the IEEE Conference on Medical Imaging, 345-355.\nYang, X., Lin, T., & Chang, C. (2021). Multiclass dental segmentation with U-Net and level-set methods. Dental Radiology Research, 29(4), 89-98.\nPrivado, L., & Moreno, J. (2021). Automatic tooth numbering using convolutional neural networks. Medical Imaging Innovations, 18(2), 78-89.\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\nRelated Works\n\nTraditional Image Processing Techniques\n\nAdvances in Deep Learning Architectures for Dental Segmentation\n\nMulticlass Segmentation and Individual Labeling\n\nEvaluation Metrics and Data Augmentation\n\nChallenges and Future Directions\n\nMethodology\n\nDataset Description\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nDatasets\nYou might be interested\nToothGapID\nDec 29, 202415 reads\n#AIInDentistry#DentalAI+3\nDental X RAY Image Detection and Instance Segmentation\nDec 29, 202434 reads\n#DeepLearningInHealthcare#DentalAI+6\nA Modified Deep Semantic Segmentation Model for Analysis of Whole Slide Skin Images\nK\nDec 27, 202419 reads\nFetus Intracranial Structures Identification using YOLO11: Advancing Prenatal Diagnostics\nR\nS\nDec 30, 202421 reads\nAIinHealthcareComputer vision+4",
    "awards": [
      "distinguished applied solution showcase"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "detection-of-deepfakes-using-leakyrelu-resnet-x2Q2VT21flHI",
    "username": "mMohana Roy Chowdhury",
    "license": "MIT License",
    "title": "Detection of DeepFakes using LeakyReLU Resnet",
    "publication_description": "Back to publications\nDec 24, 2024\n●\n194 reads\n●\nMIT License\nDetection of DeepFakes using LeakyReLU Resnet\nM\nMohana Roy Chowdhury\nLike\nBookmark\nShare\n\n1. Abstract\n\nRapid advancements in the field of GenerativeAI for producing hyper-realistic human photographs have led to a steady increase in cases of forgeries, identity theft and other digital crimes, endangering the credibility of media reports. In this study, we investigate the potential of a modified Resnet Architecture with LeakyReLU for distinguishing authentic human photographs from DeepFakes. This effort aims to extract features from images that can be used to distinguish between real and fake photos.\n\n2. Introduction\n\nThe remarkable progress in Generative AI-based applications combined in the recent times has enabled the generation of high-quality synthetic images that can often be mistaken for real images. While the rapid development in this field has opened up exciting new possibilities across industries, it has also raised significant ethical concerns regarding misuse.\nApplications of Generative AI that can produce lifelike images and videos of people have the risk of being misused for fraudulent purposes, disseminating false information, and depicting people in vulnerable ways. They can also result in serious identity theft.\nThis project aims to study the use of Deep CNNs, specifically Residual Networks with a modified activation function, to evaluate its potential for identifying deepfakes.\n\n3. Related Works\n3.1 Deepfake Detection with Deep Learning: Convolutional Neural Networks versus Transformers\n\nIn this paper[1], the authors perform a comparative analysis between CNNs and Vision Transformers in detecting deepfakes using well established-datasets. detection models and conducted experiments over well-established deepfake datasets. The paper also highlights the unique capabilities of CNNs and Transformers models and documents the observed relationships among the different deepfake datasets, to aid future developments in the field.\n\n3.2 Advanced deepfake detection with enhanced Resnet-18 and multilayer CNN max pooling\n\nIn this paper[2], the authors explore the use of a modified Resnet18 architecture with multilayer max pooling to classify deepfakes.\n\n4. Methodology\n4.1 Resnet50 architecture\n\nOur baseline model incorporates a Resnet50 architecture which incorporates skip-connections or shortcut-connections in the network to address the issue of vanishing/exploding gradients in deeper networks using the concept of Residual Blocks.\nIn this architecture issue of the vanishing/exploding gradient is addressed using a method known as skip connections. The skip connection connects activations of a layer to deeper layers by skipping some layers in between forming a leftover block known as a residual block. Resnet architecture is made by stacking these residual blocks together.\n\nThe strategy behind this network is to let the network fit the residual mapping rather than have layers learn the underlying mapping, i.e., instead of the initial mapping of H(x), the network fits,\n\nThe advantage of using skip connection is that, regularisation will skip any layer that degrades architecture performance. This makes the process of training an extremely deep neural network possible without encountering issues with vanishing or expanding gradients.\n\n4.2 LeakyReLU\n\nOne of the key characteristics of modern deep CNNs is the use of non-saturated activation function instead of saturated activation functions (e.g. sigmoid, tanh) in order to solve the vanishing/exploding gradient problem. In most common activation function Rectified Linear Unit or ReLU, is a piecewise linear function which prunes the negative part to zero, while retaining the positive part.\n\nFormally, a ReLU activation is defined as,\n\nLeaky Rectified Linear activation is first introduced in acoustic model(Maas et al., 2013). In contrast to ReLU, in which the negative part is totally dropped, LeakyReLU assigns a non-zero slope to it.\n\nMathematically, we have,\n\nwhere ai is a fixed parameter in range (1, +∞).\n\nLeaky ReLU address the Dying ReLU Problem[5] which is a kind of vanishing gradient. In this situation ReLU neurons become inactive and only output 0 for any input. It has been known as one of the obstacles in training deeper networks.\n\n5. Experiment Settings\n\nWe evaluate classification performance on Modified Resnet50 architecture with Leaky ReLU activation function using Tensorflow2x. Cross Entropy Loss with Softmax function are used as the output layer of our network.\n\n5.1 Platform Details\n\n• Hardware: Kaggle T4 x2 is a GPU accelerator\n\n5.2 Training Details\n\n• Augmentation: RandomFlip, RandomRotation, RandomZoom, RandomContrast\n\n• LeakyReLU parameter value: 0.2\n\n• Epochs: 100\n\n• Training-Validation Split: 75:25\n\n6. Training Results\n\nTraining accuracy: 0.9975\n\nTraining loss: 0.0075\n\nValidation accuracy: 0.9990\n\nValidation loss: 0.0033\n\n7. Experimental Results\n\n8. Future Enhancements\n\n• Experimenting with data augmentation techniques to make the model more robust to quality changes\n\n• Incorporating Explainable AI techniques for understanding the classification process and strategize improvements in the model performance\n\n• Building a robust dataset to incorporate more variations of generated portraits\n\n9. References\n\nhttps://arxiv.org/pdf/2304.03698\n\nhttps://www.researchgate.net/publication/384114833_Advanced_deepfake_detection_with_enhanced_Resnet-18_and_multilayer_CNN_max_pooling\n\nhttps://www.sciencedirect.com/science/article/abs/pii/S0925231222008426\n\nhttps://arxiv.org/pdf/1505.00853\n\nhttps://arxiv.org/pdf/1903.06733\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nAbstract\nIntroduction\nRelated Works\n\n3.1 Deepfake Detection with Deep Learning: Convolutional Neural Networks versus Transformers\n\n3.2 Advanced deepfake detection with enhanced Resnet-18 and multilayer CNN max pooling\n\nMethodology\n\n4.1 Resnet50 architecture\n\n4.2 LeakyReLU\n\nExperiment Settings\n\n5.1 Platform Details\n\nView all\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "developing-a-rule-based-ai-calendar-agent-in-pure-python-9h1pJ4yDxBNt",
    "username": "Manuela Schrittwieser",
    "license": "MIT License",
    "title": "Developing a Rule-Based AI Calendar Agent in Pure Python",
    "publication_description": "Back to publications\nOct 10, 2025\n●\n24 reads\n●\nMIT License\nDeveloping a Rule-Based AI Calendar Agent in Pure Python\nAgentic AI\nCalendar Management\nConsole Application\nEducational\nNatural Language Understanding\nPure Python\nRule-Based AI\nManuela Schrittwieser\nLike\nBookmark\nShare\n\nThis publication details the development and architecture of an AI Calendar Agent implemented entirely in Pure Python. Designed for simplicity and educational value, this agent demonstrates core agentic principles, natural language understanding (NLU) via regular expressions, and tool utilization for calendar management. This project serves as an ideal entry point for understanding rule-based AI agents and their interaction within a defined environment, without external machine learning frameworks.\n\n1. Introduction to the AI Calendar Agent\n\nThe AI Calendar Agent is a command-line interface (CLI) application that acts as a personal assistant for managing scheduled events. Users interact with the agent using natural language commands, enabling operations such as adding new appointments, viewing scheduled activities, and deleting existing entries. The primary objective of this project is to illustrate fundamental concepts of intelligent agents—perception, reasoning, and action—using only Python's standard library.\n\nThis agent's \"intelligence\" is derived from a set of predefined rules (regular expressions) that interpret user intent and map it to specific calendar manipulation functions. It effectively showcases how complex behaviors can emerge from simple, well-structured logic.\n\n2. Agentic Architecture\n\nThe architecture of the AI Calendar Agent is modular and adheres to a standard agentic pattern.\n\nFigure 1: Command Processing Flow within the AI Calendar Agent.\n\n2.1 Core Components\n\n1. User Interface (CLI):\nThe agent interacts with the user through a standard console interface, accepting text-based commands.\n\n2. Perception Module (User Input):\nThe agent continuously monitors user input, treating each command as an observation from its environment.\n\n3. Agent's Brain (process_command function):\n\nIntent Recognition:\nEmploys a series of compiled regular expressions (re module) to identify the user's intended action (e.g., \"add event,\" \"view events,\" \"delete event\"). Each regex acts as a pattern for a specific user goal.\nParameter Extraction:\nOnce an intent is recognized, named capture groups within the regular expressions extract relevant data points (e.g., event title, date, time, event ID).\nTool Selection & Orchestration:\nBased on the identified intent and extracted parameters, the agent dynamically selects and invokes the appropriate \"tool\" (a Python function) from its toolkit.\n\n4. Agentic Tools (Functions):\nThese are discrete, specialized functions designed to perform specific operations on the calendar data. They represent the agent's actionable capabilities.\n\nadd_event_to_calendar(title, date_obj, time_obj): Creates and persists a new event.\nview_events_on_date(date_obj): Queries and formats events for a specified date.\nview_all_upcoming_events(): Retrieves and sorts all future events.\ndelete_event_by_id(event_id): Removes an event by its unique identifier.\n\n5. Environment Model (Calendar Data):\nRepresented by an in-memory Python list (calendar_events), which stores event dictionaries. This serves as the agent's internal model of its managed environment (the calendar state).\n\n6. Action & Response Generation:\nAfter a tool is executed, its result is formatted into a human-readable string and presented back to the user via the CLI.\n\n3. Implementation Details\n\nThe project is structured within a single Python file, calendar_agent.py, emphasizing readability and direct implementation.\n\n3.1 Key Python Constructs\nre Module: Extensively used for pattern matching and information extraction from natural language commands.\ndatetime Module: Essential for handling and parsing dates and times, ensuring accurate event scheduling and retrieval.\nList Data Structure: The calendar_events list acts as the central data store, where each event is a dictionary containing id, title, date, and time.\nFunctions: Modularized code into distinct functions for parsing, event management, and the central process_command logic.\n3.2 Code Snippet: Intent Recognition Example\nimport re\n# ... (other imports and functions) ...\n\ndef process_command(command):\n    command = command.lower().strip()\n\n    # Pattern for adding an event\n    add_pattern = re.compile(\n        r\"add\\s+(?P<title>.+?)\\s+(?:on\\s+|for\\s+)(?P<date>\\w{3,4}-\\d{1,2}-\\d{1,2}|today|tomorrow)\"\n        r\"(?:\\s+at\\s+(?P<time>\\d{1,2}:\\d{2}))?$\"\n    )\n    match = add_pattern.match(command)\n    if match:\n        title = match.group('title').strip()\n        date_str = match.group('date')\n        time_str = match.group('time')\n        \n        date_obj = parse_date(date_str)\n        time_obj = parse_time(time_str) if time_str else None\n        \n        if date_obj:\n            return add_event_to_calendar(title, date_obj, time_obj)\n        else:\n            return \"Could not understand the date.\"\n    \n    # ... (other command patterns) ...\n    return \"I didn't understand that command.\"\n4. Usage Instructions\n\nTo run the AI Calendar Agent:\n\nEnsure you have Python 3.7+ installed.\nSave the provided calendar_agent.py script locally.\nExecute from your terminal:\npython calendar_agent.py\nEnter commands at the prompt. Type help for a list of supported commands or exit to quit.\n\nExample Commands:\n\nadd team meeting on 2024-03-10 at 10:00\nadd doctor appointment tomorrow at 14:30\nwhat's happening today\nview all events\ndelete event 3\n5. Limitations and Future Development\n\nWhile robust for its intended scope, this Pure Python agent has inherent limitations that pave the way for future enhancements:\n\nNo Data Persistence: Events are lost when the application closes.\n-> Future Work: Integrate json for file-based storage or sqlite3 for a lightweight database.\n\nStrict NLP: The rule-based nature means commands must closely match predefined regex patterns.\n-> Future Work: Explore more flexible NLP techniques using external libraries (e.g., SpaCy, NLTK) for greater robustness to variations in user phrasing and advanced entity recognition.\n\nContext-Insensitive: Each command is processed in isolation.\n-> Future Work: Implement a simple dialogue state tracker to maintain conversation context.\n\nNo Conflict Detection: Currently, events can be added without checking for time conflicts.\n-> Future Work: Add logic to identify and notify users of overlapping schedules.\n\nBasic Time Handling: Assumes a single timezone.\n-> Future Work: Incorporate pytz for comprehensive timezone management.\n\n6. Conclusion\n\nThe AI Calendar Agent in Pure Python offers a tangible demonstration of agentic design patterns, illustrating how an intelligent system can perceive, process, and act upon user input using fundamental programming constructs. It serves as an excellent foundational project for developers exploring agent development and rule-based natural language interfaces. This project underscores the power of modular design and the versatility of Python's standard library in crafting intelligent solutions.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nIntroduction to the AI Calendar Agent\nAgentic Architecture\n\n2.1 Core Components\n\nImplementation Details\n\n3.1 Key Python Constructs\n\n3.2 Code Snippet: Intent Recognition Example\n\nUsage Instructions\nLimitations and Future Development\nConclusion\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "distance-profile-for-timestep-classification-in-time-series-analysis-ljGAbBceZbpv",
    "username": "Ready Tensor",
    "license": null,
    "title": "Distance Profile for Time-Step Classification in Time Series Analysis",
    "publication_description": "Back to publications\nAug 30, 2024\n●\n263 reads\n●\nCreative Commons Attribution-ShareAlike (CC BY-SA)\nDistance Profile for Time-Step Classification in Time Series Analysis\nHub:\nReady Tensor Research\ndistance profile\nMASS\npattern recognition\nsignal processing\nsimilarity search\ntime-series\ntime-step classification\ntimeseries classification\nReady Tensor\nLike\nBookmark\nShare\n\nTL;DR: Distance Profile is a versatile and powerful technique in time series analysis. In this work, we apply it to a task we define as Time-Step Classification, where the goal is to classify individual time steps within a time series. Our approach demonstrates its effectiveness and potential for broader applications in this domain.\n\nAbstract\n\nTime series analysis often requires classifying individual time points, a task we term Time-Step Classification. This publication explores the application of Distance Profile, an existing versatile technique in time series analysis, to this challenge. We adapt the Distance Profile method, using MASS (Mueen's Algorithm for Similarity Search) for efficient computation, specifically for Time-Step Classification. Our approach leverages the intuitive concept of nearest neighbor search to classify each time step based on similar sequences. We present our implementation, including modifications for multivariate time series, and demonstrate its effectiveness through experiments on diverse datasets. While not achieving the highest accuracy compared to complex models like LightGBM, this adapted method proves valuable as a strong baseline and quick prototyping tool. This work aims to highlight Distance Profile as a simple yet versatile approach for Time-Step Classification, encouraging its broader adoption in practical time series analysis.\n\nIntroduction\n\nTime series data is ubiquitous, from stock prices to sensor readings, but analyzing it presents unique challenges. One such challenge is Time-Step Classification - labeling each point in a time series. While many complex methods exist, sometimes the most intuitive approaches yield impressive results.\n\nIn this paper, we explore Distance Profile, a method rooted in the simple concept of nearest neighbor search. We show how this straightforward idea becomes a powerful tool for Time-Step Classification:\n\nWe introduce Distance Profile and its applications in time series analysis.\nWe detail our implementation using the MASS algorithm, including adaptations for multivariate time series.\nWe demonstrate its effectiveness through experiments on various datasets.\n\nBy showcasing how this simple, intuitive method can tackle complex time series challenges, we aim to highlight its value for establishing baselines and quick prototyping. Our work serves as a practical guide, encouraging practitioners to consider Distance Profile alongside more advanced techniques in their analytical toolkit.\n\nDistance Profile\n\nDistance Profile is a fundamental technique in time series analysis that measures the similarity between a query subsequence and all possible subsequences of a longer time series. This method is crucial for various tasks such as pattern recognition, anomaly detection, and classification in time series data.\n\nDefinition\n\nThe distance profile of a query subsequence  with respect to a time series  is a vector where each element represents the distance between  and a corresponding subsequence of . Formally:\n\nLet  be a time series of length .\nLet  be a query subsequence of length .\nThe distance profile  is an length vector.\nEach element  represents the distance between  and the subsequence of  starting at index .\nComputation\n\nThe most commonly used distance measure for calculating the distance profile is the z-normalized Euclidean distance, which is robust against variations in scale and offset. The computation involves two key steps:\n\nZ-Normalization: Each subsequence of the time series  and the query  is individually normalized to have zero mean and unit variance.\n\nDistance Computation: The Euclidean distance between the normalized  and each normalized subsequence of  is calculated and stored in the distance profile vector.\n\n\nWhile z-normalized Euclidean distance is common, other distance metrics can be used, such as cosine similarity, Manhattan distance, or Minkowski distance. The choice of metric can be treated as a tunable hyperparameter, optimized for the specific requirements of the downstream task.\n\nDistance Profile Example\n\nSample Dataset for Demonstration\nTo illustrate the application of Distance Profile in Time-Step Classification, we will use a real-world time series representing daily weather data for the city of Los Angeles. This dataset spans three full years, from 2020 to 2022, and includes various meteorological parameters such as temperature, humidity, and wind speed.\n\nWe chose this dataset for its accessibility and clear seasonal patterns, making it ideal for demonstrating how Distance Profile identifies similar patterns across different time periods. Later in the experiments section, we will work with more typical, complex datasets to thoroughly evaluate the method's performance.\n\nThe dataset is uploaded in the Resources section of this article. See file titled los_angeles_weather.csv. We will use the feature series titled maxTemp for our demonstration.\n\nBelow is the plot of the daily maximum temperature in Los Angeles over the three years. It illustrates the distinct temperature patterns and seasonal fluctuations in the city, providing a rich dataset for analyzing time-series patterns.\n\nQuery Subsequence\nThe dataset provides an excellent example for demonstrating how Distance Profile can identify similar patterns within a time series. For our demonstration, we select a query period representing the first 10 days in the dataset (i.e., starting on January 1st and ending on January 10th, 2020.) The following chart shows the maximum temperatures over the query period.\n\nThe following chart shows the maximum temperatures over the query period.\n\nThe goal of this example is to identify other similar temperature patterns throughout the three-year period using Distance Profile. By applying Distance Profile to this query subsequence, we can explore how well the technique can locate similar temperature trends within the broader time series. This exercise not only showcases the practical utility of Distance Profile but also demonstrates its effectiveness in identifying meaningful patterns in real-world weather data.\n\n\nImplementation using NumPy\nThe following code is a simple implementation of the distance profile algorithm on a one-dimensional series.\n\nimport numpy as np\n\ndef z_normalize(ts):\n    \"\"\"Z-normalize a time series.\"\"\"\n    return (ts - np.mean(ts)) / np.std(ts)\n\ndef sliding_window_view(arr, window_size):\n    \"\"\"Generate a sliding window view of the array.\"\"\"\n    return np.lib.stride_tricks.sliding_window_view(arr, window_size)\n\ndef distance_profile(query, ts):\n    \"\"\"Compute the distance profile of a query within a time series.\"\"\"\n    query_len = len(query)\n    ts_len = len(ts)\n\n    # Z-normalize the query\n    query = z_normalize(query)\n\n    # Generate all subsequences of the time series\n    subsequences = sliding_window_view(ts, query_len)\n\n    # Z-normalize the subsequences individually\n    subsequences = np.apply_along_axis(z_normalize, 1, subsequences)\n\n    # Compute the distance profile\n    distances = np.linalg.norm(subsequences - query, axis=1)\n\n    return distances\n\n# You can now apply the above functions to your temperature data\n# by passing the relevant query and time series arrays.\n\n# Compute the distance profile\ndist_profile = distance_profile(query, time_series)\n\nThis numpy code provided above is for illustration purposes. For a more efficient implementation, use matrixprofile or stumpy python packages.\n\nThe package stumpy offers Mueen’s Algorithm for Similarity Search (MASS) for fast and scalable distance profile. Using it, the code simplifies as follows:\n\nimport stumpy\n\n# ... read your data and create the query and time_series numpy arrays\n# query is a 1d numpy array\n# time_series is a 1d numpy array\n\ndistance_profile = stumpy.core.mass(query, time_series)\n\n\nThe following chart displays the distance profile for the given query and time_series. The 3 nearest-neighbors are time-windows starting on May 29th, 2020, December 3rd, 2020, and May 30th, 2021. These are the locations in the time series where the distance profile values are the lowest, indicating the most similar subsequences to the query.\n\nNext, we visualize and compare the patterns in the 3 nearest neighbors with the original query in the following chart.\n\nWe can observe the similarities between the query and its nearest neighbors. The query time series shows a slight upward trend during the first 6 days, followed by a downward trend over the next 4 days. The three nearest neighbors exhibit similar patterns, effectively capturing the essence of the query subsequence.\n\nIt may seem surprising that the nearest neighbors to the query period from January 1st to January 10th, 2020, are not all from the same time of year (winter). In fact, two of the nearest neighbors fall in late May and early June. For example, while the average temperature during the query period is 19.8°C, the nearest neighbor on May 29, 2020, has an average temperature of 26.6°C.\n\nThis occurs because both the subsequences and the query are z-normalized before calculating the distance profile. Z-normalization removes magnitude differences, allowing the distance profile to focus on the shape of the temperature curve rather than the absolute values. This approach enables the identification of similar patterns in the data, regardless of differences in scale or offset.\n\nMASS and STUMPY\n\nDistance Profile involves calculating the distance between a query subsequence and all possible subsequences within a time series. While the basic concept can be implemented using NumPy, as shown above, this approach can become computationally expensive, especially for large datasets.\n\nTo address this, Mueen's Algorithm for Similarity Search (MASS) was developed as an optimized and highly efficient method for computing the distance profile. MASS leverages the Fast Fourier Transform (FFT) to significantly speed up the computation, making it well-suited for large-scale time series data. Essentially, MASS is a fast implementation of the Distance Profile algorithm, providing the same results but with much greater efficiency.\n\nBy using the stumpy package, which implements MASS, we can achieve scalable and rapid distance profile, enabling its use in real-world applications where performance and speed are critical.\n\nMulti-Dimensional Distance Profile\n\nThe concept of distance profile can be extended to multivariate time series data, where each time point consists of multiple features or channels. This extension is crucial for performing similarity searches on multivariate time series, a common requirement in many real-world applications where data is collected across multiple channels simultaneously.\n\nTo compute a multi-dimensional distance profile, we can take one of two approaches:\n\nSumming Individual Distance Profiles: Calculate the distance profile for each feature separately and then sum them to form a multi-dimensional distance profile.\n\nDirect Multivariate Euclidean Distance: Compute the multivariate Euclidean distance directly across all features.\n\n\nIn our work, we opted for the first approach—summing the distance profiles of individual features. We acknowledge that this choice was somewhat arbitrary, and the impact of this decision on the results could be an interesting area for further exploration.\n\nWe utilized Mueen’s Algorithm for Similarity Search (MASS) to calculate the multi-dimensional matrix profile. Here’s how you can implement this approach:\n\ndef multi_dimensional_mass(\n        query_subsequence: np.ndarray,\n        time_series: np.ndarray\n    ) -> np.ndarray:\n    \"\"\"\n    Calculate the multi-dimensional matrix profile.\n\n    Args:\n        query_subsequence (np.ndarray): The query subsequence.\n        time_series (np.ndarray): The time series.\n\n    Returns:\n        np.ndarray: The multi-dimensional matrix profile.\n    \"\"\"\n    for dim in range(time_series.shape[1]):\n        if dim == 0:\n            profile = stumpy.core.mass(\n                query_subsequence[:, dim], time_series[:, dim]\n            )\n        else:\n            profile += stumpy.core.mass(\n                query_subsequence[:, dim], time_series[:, dim]\n            )\n    return profile\nTime-Step Classification\n\nTime-step classification is a challenging task in time series analysis, where the goal is to assign a label to each individual time point within a sequence. This type of classification is crucial in various real-world applications, where the temporal dynamics of the data play a significant role in understanding and predicting outcomes.\n\nThe following are a couple of examples where time-step classification is applied:\n\nHuman Activity Recognition: In wearable technology and smart devices, time-step classification is used to identify and categorize human activities such as walking, running, or sitting, based on sensor data collected over time. Each time step in the sensor data corresponds to a specific activity label, enabling real-time monitoring and analysis.\n\nECG Signal Classification: In medical diagnostics, time-step classification is applied to ECG signals to detect and classify heartbeats as normal or indicative of various arrhythmias. Each time step in the ECG signal represents a moment in the cardiac cycle, and correctly labeling these steps is crucial for accurate diagnosis and treatment.\n\nProblem Definition\n\nTime-step classification involves assigning a label to each time step within a sequence, whether the data is univariate or multivariate. The dataset for this task typically includes the following characteristics:\n\nInput Features: The data consists of time series, which can be either univariate (single feature) or multivariate (multiple features).\nLabel Assignment: For each time step, a specific label needs to be assigned, indicating the class or category of that particular time point.\nTraining and Inference Data:\nTraining Data: Contains sequences that are fully labeled, providing the model with both the input features and the corresponding labels.\nTest (Inference) Data: Contains sequences without labels, where the model needs to predict the label for each time step.\nMultiple Samples: The dataset may include multiple sequences, each representing different instances or subjects. For example, in Human Activity Recognition (HAR), each sequence might correspond to a different person performing various activities, with labels indicating the specific activity at each time step.\nVariable Sequence Lengths: The length of sequences can vary across both training and test data, meaning that each sample may have a different number of time steps.\n\nOur goal is to train a model on the labeled training data so that it learns to accurately assign labels to each time step in the test data.\n\nDistance Profile for Time-Step Classification\n\nIn this section, we explore how the Distance Profile technique, particularly through Mueen’s Algorithm for Similarity Search (MASS), can be adapted and applied to the task of time-step classification. By calculating the distance profile for each time step, we can effectively classify individual time points within a time series, enabling more precise and informed analysis across various domains.\n\nThe general approach is as follows:\n\nSubsequence Querying: For each sequence in the test dataset, we break it down into smaller subsequences, or \"queries.\" Each query represents a window of time steps within the sequence that we want to classify.\n\nFinding Nearest Neighbors: For each query, we calculate its distance profile against the training dataset, identifying its k-nearest neighbors—subsequences in the training data that most closely match the query in terms of shape and pattern.\n\nLabel Assignment: The labels of these k-nearest neighbors are then used to assign a label to each time step in the query. This allows us to classify each time point in the test sequence based on the most similar patterns observed in the labeled training data.\n\nImplementation Details\n\nTo adapt the MASS algorithm for Time-Step Classification, we made several key modifications to effectively handle the nuances of this task. These modifications ensure that the algorithm can accurately classify each time step in the test data by leveraging the labeled training data. Below are the critical components of our implementation:\n\nWindows\nEach sequence (i.e. sample) in the test data is divided into smaller windows to create the subsequences (queries) that will be classified. The window length is a tunable parameter, determined as a function of the minimum sequence length in the training data. This approach allows us to capture relevant patterns while maintaining consistency across varying sequence lengths.\n\nStrides\nTo ensure comprehensive coverage of the test data, we allow overlapping windows to be created. The degree of overlap is controlled by a stride factor, enabling us to balance between computational efficiency and the thoroughness of the classification.\n\nDistance Profile Calculation\nFor each window in the test data, we compute the distance profile over all subsequences from all samples in the training data. This is done using the MASS algorithm, which calculates the Euclidean distance on z-normalized data for each feature. The final distance measure for each subsequence is obtained by summing the distances across all features, ensuring that all aspects of the multivariate time series are considered.\n\nk-Nearest Neighbors\nOnce the distance profile is calculated for each window in the test data, we identify the k-nearest neighbors from the training data based on the computed distances. These neighbors represent the most similar windows in the training set. The labels associated with these neighbors, which are one-hot encoded, are extracted for further processing.\n\nAveraging Labels\nA single time step in the test data may appear in multiple query windows, and for each window, we have k-nearest neighbors subsequences from the training data. To determine the final label for each time step index i within a query, we average the labels from corresponding index i across all the neighbor subsequences. This approach produces a set of label probabilities, from which the most likely label is assigned to the time step.\n\nThe complete implementation of our approach is available in our \nGitHub repository\n. It is also linked in the Models section of this publication.\n\nThe implementation is designed in a generalized way, allowing users to easily apply it to their own datasets. Additionally, the implementation is dockerized for convenience, though users can also run it locally if they prefer. The implementation leverages the STUMPY library.\n\nLimitations of the Approach\n\nWhile the Distance Profile method for Time-Step Classification offers simplicity and interpretability, it has several limitations:\n\nComputational Expense: For large datasets, calculating distance profiles can be computationally intensive, potentially limiting scalability.\nLocal Pattern Focus: Predictions depend entirely on the k-nearest neighbors identified. If these neighbors contain noisy/anomalous data (in features or labels), it can lead to noisy predictions.\nParameter Sensitivity: Results can be sensitive to the choice of distance metric and the number of nearest neighbors (), requiring careful tuning.\nComputational Burden During Inference: Unlike models that learn during a training phase, this method performs all its computations during the inference phase. This can lead to slower predictions on large datasets compared to other complex models which, though potentially slow to train, are typically quick to make predictions once trained.\n\nThese limitations should be considered when applying this approach, particularly for large-scale or complex time series classification tasks.\n\nExperiments\n\nWe tested the distance profile algorithm for time-step classification on five benchmarking datasets: EEG Eye State, HAR70+, HMM Continuous (synthetic), Occupancy Detection, and PAMAP2. These datasets, along with additional information about them, are available in the \nGitHub repository\n, which is also linked in the Datasets section of this publication.\n\nEvaluation Results\n\nThe performance of the distance profile model was evaluated using a variety of metrics, including accuracy, weighted and macro precision, weighted and macro recall, weighted and macro F1-score, and weighted AUC. The results for each dataset are summarized in the table below:\n\nDataset Name\tAccuracy\tWeighted Precision\tMacro Precision\tWeighted Recall\tMacro Recall\tWeighted F1-score\tMacro F1-score\tWeighted AUC Score\nEEG Eye State\t0.611\t0.869\t0.545\t0.611\t0.628\t0.718\t0.584\t0.625\nHAR70+\t0.641\t0.64\t0.47\t0.641\t0.369\t0.641\t0.414\t0.742\nHMM Continuous Timeseries Dataset\t0.641\t0.614\t0.594\t0.641\t0.552\t0.627\t0.572\t0.818\nOccupancy Detection\t0.893\t0.892\t0.885\t0.893\t0.834\t0.893\t0.859\t0.972\nPAMAP2 Physical Activity Monitoring\t0.616\t0.657\t0.681\t0.616\t0.606\t0.636\t0.641\t0.929\n\nAs is common in benchmarking studies, we observe varying performance across different datasets. This variation likely reflects the inherent predictability of each dataset rather than specific strengths or weaknesses of the Distance Profile method. All models, including more complex ones, typically face similar patterns of relative difficulty across datasets.\n\nNext, we compare the results of the Distance Profile model with those of LightGBM, a top-performing model in a comparative analysis conducted by this publication.\n\nComparison with LightGBM\n\nFor comparison, we now present the results from one of the top-performing model, LightGBM, from a comparative analysis conducted in this publication.\n\nDataset Name\tAccuracy\tWeighted Precision\tMacro Precision\tWeighted Recall\tMacro Recall\tWeighted F1-score\tMacro F1-score\tWeighted AUC Score\nEEG Eye State\t0.458\t0.857\t0.523\t0.458\t0.566\t0.597\t0.544\t0.581\nHAR70+\t0.862\t0.87\t0.55\t0.862\t0.496\t0.866\t0.522\t0.859\nHMM Continuous Timeseries Dataset\t0.876\t0.875\t0.868\t0.876\t0.85\t0.876\t0.859\t0.974\nOccupancy Detection\t0.996\t0.996\t0.992\t0.996\t0.997\t0.996\t0.994\t0.998\nPAMAP2 Physical Activity Monitoring\t0.731\t0.741\t0.737\t0.731\t0.716\t0.736\t0.726\t0.951\n\nNote: Detailed results for all models in the Time Step Classification benchmark are available in this \nGitHub repository\n.\n\nThe LightGBM model also shows performance variability across datasets, with a notable correlation to the Distance Profile method's results. For instance, both models achieve their highest performance on the Occupancy Detection dataset.\n\nOverall, LightGBM outperforms the Distance Profile method. The average of Macro Average F1-score for the Distance Profile model is 0.614, compared to 0.729 for LightGBM. This performance gap can be attributed to LightGBM's greater complexity and expressiveness, allowing it to capture more intricate data patterns than the simpler Distance Profile method.\n\nDespite not matching LightGBM's accuracy, the Distance Profile model remains valuable for establishing benchmarks and quick prototyping. We recommend using it as a reference point during the development of more sophisticated models.\n\nSummary\n\nDistance Profile is a simple and versatile tool in time series data mining. It works by calculating the distance between a query subsequence and all other subsequences within a time series, forming the foundation for advanced analytical tasks.\n\nWe utilized Mueen's Algorithm for Similarity Search (MASS) for its efficiency and scalability, making it ideal for large real-world datasets. The process involves:\n\nZ-normalizing the time series and query to manage scale variations.\nComputing the Euclidean distance for each subsequence against the query.\nSupporting both univariate and multivariate data for comprehensive analysis.\n\nWhile Distance Profile may not always achieve the highest accuracy compared to more complex models, it is invaluable for establishing strong baselines. Its simplicity and adaptability make it a must-have tool before advancing to more sophisticated methods.\n\nBeyond time-step classification, Distance Profile is also effective for anomaly detection, motif discovery, and time series segmentation. Its broad applicability makes it an essential component of any data scientist's toolbox.\n\nReferences\nLaw, Sean M. \"STUMPY: A powerful and scalable Python library for time series data mining.\" Journal of Open Source Software 4, no. 39 (2019): 1504. Available at: \nhttps://stumpy.readthedocs.io\n.\nZhong, Sheng, and Abdullah Mueen. \"MASS: distance profile of a query over a time series.\" Data Mining and Knowledge Discovery (2024): 1-27.\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nDistance Profile\n\nDefinition\n\nComputation\n\nDistance Profile Example\n\nMASS and STUMPY\n\nMulti-Dimensional Distance Profile\n\nTime-Step Classification\n\nProblem Definition\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nFiles\nlos_angeles_weather.csv\nYou might be interested\nTime Series Step Classification Benchmark\nSep 16, 202482 reads\nBenchmarkingBenchmarking Study+8\nPrimer to Machine Learning - A Comprehensive Guide\nA\nFeb 06, 202518 reads\nClassificationClustering+15\nReady Tensor Forecasting Benchmark\nApr 29, 2024159 reads\nbenchmarkcomparative study+5\nEntity Resolution: Meta-Blocking and KLSH\nMay 13, 202541 reads\ndeduplicationentity_resolution+3",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "dp-fincial-W59putOekVSo",
    "username": "aDipanshu Pandey ",
    "license": null,
    "title": "Stock Price Prediction Model: [DP Fincial]",
    "publication_description": "Back to publications\nOct 16, 2025\n●\n8 reads\n●\nCreative Commons Attribution (CC BY)\nStock Price Prediction Model: [DP Fincial]\n: Stock Market\nAI\nData Science\nDeep Learning\nFinancial Analysis\nInvestment\nMachine Learning\nNifty\nSensex\nShare Market\nTime Series\nTrading\nA\nDipanshu Pandey\nLike\nBookmark\nShare\nRead in original language\n\nThis publication was originally written in Hindi. You're viewing an automated translation into English.\n\nStock Price Prediction Model: An Overview of [DP Fincial]\n\nThis publication documents a Deep Learning model developed to predict future stock price trends using historical data.\n\nThis model is designed to help investors make informed and data-driven financial decisions.\n\n1. Objective of the Model\n\nThe main objective of this project is to predict the closing price of major stocks or indices (like Nifty 50) listed in the Indian stock market for the next 7 days through time-series analysis .\n\n2. Dataset & Source\nData Source: Publicly available data from major Indian stock exchanges (like NSE).\nPeriod: Daily stock data for last 5 years (2020-2025).\nFeatures used: Open, High, Low, Close prices and trading volume.\nData Pre-processing: The data is normalized using MinMaxScaler before model training.\n3. Model and Methodology\n\nThis model is based on the LSTM (Long Short-Term Memory) neural network , which is highly effective in recognizing patterns and long-term dependencies in time-series data.\n\nAlgorithm: LSTM (Deep Learning)\nArchitecture: [2 LSTM Layers] + [1 Dense Output Layer]\nTraining: The model was trained for [50 Epochs], with 80% of the data used for training and 20% for testing.\nLoss Function: Mean Squared Error (MSE)\n4. Results and Evaluation\n\nDuring testing, the model showed the following key metrics:\n\nRMSE (Root Mean Squared Error): It shows how far the predicted value is, on average, from the actual value.\nMAPE (Mean Absolute Percentage Error): [Example: 3% to 5%]. It shows the prediction error in percentage.\n\nThe model's results show that it is able to track stock price trends with low error, proving its practical utility.\n\n5. Conclusion and Future Work\n\nThis LSTM-based model provides a strong foundation for stock prediction. Future work will include the following steps:\n\nIncorporating market sentiment analysis from news headlines and social media data.\nExperimenting with other advanced model architectures like Transformer or Conv1D for better performance.\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nStock Price Prediction Model: An Overview of [DP Fincial]\n\nObjective of the Model\nDataset & Source\nModel and Methodology\nResults and Evaluation\nConclusion and Future Work\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nYou might be interested\nStock-Price-Prediction-with-LSTM-Neural-Networks\nA\nA\nS\nFeb 05, 202513 reads\nDeep Learning-Driven Financial Sentiment Analysis & Stock Forecasting Platform\nP\nS\nA\nS\nApr 11, 20253 reads\nfinancial sentiment analysisfinbert+3\nAlibaba Stock Price Prediction Using Time Series Models\nJ\nApr 01, 202513 reads\nStock Market Analysis and Prediction on Time Series Data\nP\nApr 07, 2025",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "dweshamukt-a-multilingual-bertbased-framework-for-robust-online-hate-speech-detection-110BXOSoPkmr",
    "username": "sYash Suhas Shukla",
    "license": "License 📄",
    "title": "DweshaMukt : A Multilingual BERT-Based Framework for Robust Online Hate Speech Detection",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n98 reads\n●\nCreative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\nWinner of\nBest Overall Project\nat the\nComputer Vision Projects Expo 2024\nDweshaMukt : A Multilingual BERT-Based Framework for Robust Online Hate Speech Detection\naudios\ngifs\ngoogle-speech-to-text-api\ngoogle-video-intelligence-api\ngoogle-vision-api\nhate-speech-detection\nieee-research-paper\nimages\nmultilingual\nmultimodal\nproject-copyright\nproject-funding\npython\nstreamlit-frontend\ntelegram-bots\ntext\nvideos\nyoutube-comments\nS\nYash Suhas Shukla\nR\nRajkumar Vamanrao Panchal\nLike\nBookmark\nShare\n\nProject Introduction 🛡️\nAbstract\n\nIn today’s digital landscape, hate speech is an escalating concern, often fueling division and unrest across communities. DweshaMukt is an Advanced Multilingual and Multimodal Hate Speech Detection System designed to counteract this issue by harnessing the power of Bidirectional Encoder Representations from Transformers (BERT) alongside cutting-edge Deep Learning and Natural Language Processing (NLP) techniques.\n\nOur system tackles a unique challenge: detecting hate speech within Hinglish—a dynamic blend of Hindi and English—while also supporting Hindi and English languages individually. DweshaMukt leverages a pre-trained BERT model, specially optimized for real-time scenarios, offering robust analysis across a range of media. Its Multilingual and Multimodal architecture enables Hate Speech Detection across diverse content types: Text, Audios, Images, Videos, GIFs, and YouTube Comments.\n\nWith an accuracy of 88%, DweshaMukt stands as a promising solution for real-world hate speech detection applications, bridging language and media barriers to ensure safer, more inclusive online spaces.\n\nIndex Terms: Hate Speech Detection, BERT, Deep Learning, Natural Language Processing, Multilingual, Multimodal, Hinglish, Real-Time Analysis\n\nProject Timeline\nStart Date: 15th February 2023\nEnd Date: 12th December 2024\nTotal Time Required: 1 Year, 9 Months, and 28 Days\nTeam Members\nTeam Members\tGitHub Profile\tLinkedIn Profile\nYash Suhas Shukla\t\nGitHub\n\t\nLinkedIn\n\nTanmay Dnyaneshwar Nigade\t\nGitHub\n\t\nLinkedIn\n\nSuyash Vikas Khodade\t\nGitHub\n\t\nLinkedIn\n\nPrathamesh Dilip Pimpalkar\t\nGitHub\n\t\nLinkedIn\nProject Guide\nGuide\tGmail\nProf. Rajkumar Panchal\t\nrajkumar.panchal@vpkbiet.org\n\nCompleting this project was indeed a challenging task, and we deeply appreciate Prof. Rajkumar Panchal Sir for being our mentor. He guided us through every phase of the project, and his support was invaluable.\n\nRelated Work ⚒️\n\nHate speech detection has become a critical area of research in recent years, driven by the proliferation of social media platforms where users frequently engage in discussions that transcend linguistic boundaries. This has created unique challenges, particularly in detecting hate speech within code-switched and multilingual contexts. Numerous studies have tackled this issue by employing advanced machine learning and deep learning techniques, striving to enhance the accuracy and robustness of hate speech classifiers. This section highlights significant contributions in this field, with a focus on methodologies and outcomes that address the complexities of hate speech detection in mixed-language data.\n\nMethodology ✨\nInput:\n\nOur model processes multiple media types for hate speech detection:\n\nText: Hinglish (Hindi + English) comments, reviews, and posts.\nEmoticons: Graphical icons indicating sentiments or hate speech.\nImages/GIFs: Detected hate symbols using Google Vision API for OCR.\nAudio/Video: Transcriptions analyzed using Google Video Intelligence API.\nYouTube Comments: Both live and non-live comments analyzed.\nText Preprocessing:\nData Loading: Read CSV datasets into a pandas DataFrame.\nData Splitting: 70-30 split for training and testing.\nLabel Encoding: Convert labels to numerical values.\nHandle Missing Values: Fill or remove null entries.\nText Normalization: Convert text to lowercase for consistency.\nText Tokenization with BERT:\nTokenization: Break text into subwords using BERT tokenizer.\nSpecial Tokens: Add [CLS] and [SEP] tokens.\nPadding/Truncation: Adjust sequences to 128 tokens.\nAttention Masks: Mark actual vs. padded tokens.\nDeep Learning Classifier: BERT Model\nEmbedding Layer: Converts tokens into dense vectors.\nEncoder Layers: Includes 12 transformer blocks with:\nMulti-head Self-Attention.\nFeed-Forward Networks.\nLayer Normalization & Residual Connections.\nOutput Layer: Processes [CLS] token for final logits.\nOutput:\nPredicted Label: Class with the highest logit selected.\nEvaluation Metrics: Metrics like accuracy, precision, recall, and F1-score are computed.\nConclusion:\n\nThis pipeline integrates NLP and DL techniques for detecting hate speech in Hinglish, leveraging BERT for multilingual and code-mixed processing with robust performance across diverse media types.\n\nBackend Preparation 🔧\nMark Models Index\n\nThe backend development was an intricate journey, involving months of rigorous research, experimentation, and iterative coding. Each phase contributed to refining the system’s ability to detect hate speech across various input types and languages.\n\nOur Mark Model Index Document provides a comprehensive overview of this journey, showcasing each model’s evolution, from early concepts to the final optimized versions. Dive into the document to see how each model was crafted, tested, and fine-tuned to tackle the challenges of multilingual, multimodal hate speech detection.\n\nProject Backend 🖥️\n\nThe backend architecture of DweshaMukt enables the system to classify various forms of input text, audio, video, images, GIFs, and live YouTube comments by first converting each to text before applying the hate speech detection model. Here are the main scenarios handled by the system:\n\nText Input: Processes user-entered text directly.\nAudio Input: Converts audio to text using Google Speech to Text API, then classifies it.\nImage Input: Extracts text from images via Google Cloud Vision API.\nGIF Input: Analyzes GIFs using Google Video Intelligence API for text extraction.\nVideo Input: Extracts audio from videos, transcribes it, and classifies it.\nLive YouTube Video: Fetches live comments using pytchat library, then classifies them.\n\nThe combined code integrates all these scenarios into a unified detection system, including added emoticon-based classification for enhanced accuracy.\n\nProject Dataset 📊\nDataset Overview\n\nThe DweshaMukt Dataset is a curated collection of comments carefully selected to support research on hate speech detection. This dataset includes multilingual data in Hinglish, Hindi, and English, capturing various instances of hate and non-hate speech. It is a valuable resource for researchers and developers working on projects aimed at building safer online communities.\n\nDataset Composition\nDatasets Used: CONSTRAINT 2021 and Hindi Hate Speech Detection (HHSD)\nTotal Comments: 22,977\nHate Comments: 9,705\nNon-Hate Comments: 13,272\nAccess the Dataset\n\nTo ensure responsible and secure usage, access to the DweshaMukt dataset is granted upon request. Please complete the form below to submit your application. We review each request to verify alignment with our project’s objectives.\n\nNote: Approved requests will receive an email with download instructions within 2-3 business days.\n\nDataset Terms of Use\n\nBy requesting access to this dataset, you agree to the following:\n\nThe dataset is strictly for non-commercial, research, or educational purposes.\nYou will cite the DweshaMukt project in any publications or presentations that use this dataset.\nRedistribution or sharing of the dataset with unauthorized parties is strictly prohibited.\nProject Frontend 🌐\n\nThe frontend for this project is built as a Streamlit application, allowing users to interact seamlessly with our hate speech detection models across various input formats. This interface makes it easy to submit and analyze text, audio, video, images, GIFs, and YouTube comments.\n\nKey Features\nMulti-format Detection: Supports text, audio, video, images, GIFs, and YouTube comments.\nReal-time Analysis: Provides immediate feedback on uploaded content.\nUser-friendly Interface: Simple navigation with clear instructions and dynamic visual feedback.\nEmoji Detection: Enhanced detection with emoticon analysis.\n\nMain Dashboard\n\nExperiments 🧪\nDatasets\n\nWe utilized the following datasets:\n\nCONSTRAINT 2021: Contains labels like non-hostile, fake, defamation, and offensive, which were converted into \"hate\" (label 0) and \"non-hate\" (label 1).\nHHSD: Created by Prashant Kapil, focused on hate speech in Hinglish.\n\nCombined Dataset Summary:\n\nTotal Comments: 22,977\nLabel Distribution:\nHate (0): 9,705\nNon-Hate (1): 13,272\nExperimental Setting\nTraining Environment: Google Colab with T4 GPU.\nTesting Environment: Ubuntu with 16GB RAM and 16GB GPU.\nFramework: TensorFlow.\nHyperparameter Configuration\nLearning Rate: 3e-5\nEpochs: 16\nBatch Size: 64\nMax Sequence Length: 128\nOptimizer: Adam\nLoss Function: Sparse Categorical Crossentropy (logits=True)\nEvaluation Metric: Sparse Categorical Accuracy\nTokenizer and Model Configuration\nBERT Tokenizer:\ndo_lower_case=True\nBERT Model:\nnum_labels set to the number of unique dataset labels.\n\nThis setup enabled efficient training and testing of the hate speech detection model with a focus on robust performance in identifying hate and non-hate content.\n\nExperiment Results ♟️\n\nThe performance of our model on the test set is summarized in the table below:\n\n\nThe overall accuracy of our model is 0.88, with a macro average of 0.59 and a weighted average of 0.88.\n\nExperiment Discussion 🗣️\n\nModel Performance\n\nPrecision and Recall:\n- Hate (Yes): Precision = 0.85, Recall = 0.88\n- Non-Hate (No): Precision = 0.91, Recall = 0.88\nThese results demonstrate the model’s robustness in identifying hate speech, making it highly effective for monitoring social media.\n\nExperiment Conclusion 🦾\n\nThis study introduces a multimodal and multilingual hate speech detection model leveraging BERT and advanced NLP techniques. By integrating textual and non-textual data—including images, videos, emoticons, memes, and YouTube comments—the model significantly enhances hate speech detection across diverse contexts. Key highlights include:\n\nScalability: Demonstrated effectiveness in real-time environments.\nAdvancement: Addresses challenges of multilingual and multimodal inputs, contributing to the mitigation of harmful online content.\nProject Telegram Bots 🤖\n\nThe DweshaMukt project is integrated with Telegram through a series of specialized bots, each designed to handle a different type of input. This allows users to classify text, audio, images, GIFs, video, and YouTube comments directly within the Telegram platform.\n\nBot Name\tDescription\tWatch in Action\nHaspe Text\tProcesses text input.\t\n\nHaspe Audio\tProcesses audio input.\t\n\nHaspe Image\tProcesses image input.\t\n\nHaspe GIF\tProcesses GIF input.\t\n\nHaspe Video\tProcesses video input.\t\n\nHaspe YouTube\tProcesses YouTube live comments.\t\n\nEach bot seamlessly interacts with the backend, delivering real-time classification results to users. Whether you're analyzing text, multimedia, or live YouTube comments, these bots ensure a versatile and accessible experience for hate speech detection.\n\nProject Representation 🎉\n\nThe DweshaMukt project was proudly showcased at the Nexus 1.0 State Level Project Competition on 15th April 2024. Held at the Army Institute of Technology, Pune, this prestigious event was organized by the Department of Information Technology and Computer Engineering under the AIT ACM Student Chapter.\n\nRepresenting this project at Nexus 1.0 allowed our team to not only share our research and technical achievements but also to raise awareness about the importance of addressing hate speech in today’s digital world. Competitions like these offer valuable platforms for knowledge exchange, constructive feedback, and networking with other innovators, researchers, and industry experts.\n\nParticipation Certificates\n\nBelow are the participation certificates awarded to our team members for presenting DweshaMukt at Nexus 1.0.\n\nProject Conference 📑\n\nPresenting this project at an international platform has been a milestone achievement for our team. Our research was showcased at the CVMI-2024 IEEE International Conference on Computer Vision and Machine Intelligence, hosted by IIIT Allahabad, Prayagraj on 19th and 20th October 2024. The conference offered a valuable opportunity for knowledge exchange with global experts and researchers, fostering discussions on the latest advancements in computer vision and machine learning.\n\nKey Highlights\nResearch Paper Submission: Our research paper on this project was successfully submitted to IEEE.\nConference Attendance: Represented by Yash Shukla and Prof. Rajkumar Panchal, the conference participation strengthened our network and insights within the academic community.\nConference Report:\n\nThe conference report details our experiences and learnings from the event, including keynote sessions and other relevant presentations on emerging research trends. To read the Conference Report, press the button below.\n\nTo view the entire official IEEE Research Paper Copy downloaded from IEEE Website, press the button below.\n\nProject Research Paper:\n\nTo read our Research Paper on IEEE Website, press the button below.\n\nTo view the entire official IEEE Research Paper Copy downloaded from IEEE Website, press the button below.\n\nConference Participation Certificates\n\nThe following certificate was awarded for my participation and representation at CVMI-2024:\n\nProject Copyright ©️\n\nSecuring copyright for this project marked an important milestone in safeguarding our innovation and intellectual property. Copyrighting our project not only protects the unique aspects of our hate speech detection system but also reinforces our commitment to responsible AI research. By copyrighting this idea, we ensure that the methods, models, and technological advances developed through this project remain attributed to our team.\n\nCopyright Publication Date: 25th October 2024\n\nCertificate of Copyright\n\nAbove Copyright certificate has been slightly edited to hide the personal details of the authors involved in the Copyright.\n\nProject Funding 💸\n\nThis project was generously funded by Vidya Pratishthan's Kamalnayan Bajaj Institute of Engineering and Technology College, whose support played a pivotal role in enabling our team to bring this ambitious vision to life. This funding allowed us to access essential resources, collaborate with experts, and ensure high-quality development across every phase of the project.\n\nFunding Breakdown\nSr No\tDemand Reason\tDemand Cost\n1\tGoogle Colab Pro\t1025\n2\tOnline Courses\t2684\n3\tProject Presentation Competition\t500\n4\tStationary Cost\t500\n\tTotal\t4709\nFunding Certificate\n\nThe above provided certificate is custom designed and not officially presented by the college itself. We extend our heartfelt gratitude to VPKBIET College for their trust and support. Their investment in this project has been invaluable in pushing the boundaries of AI-driven hate speech detection.\n\nFuture Scope 🔮\nDataset Expansion: Broaden the dataset to include more languages and cultural contexts for improved generalizability.\nFeature Enhancement: Incorporate sentiment analysis and user profile features to boost accuracy.\nReal-Time Integration: Develop more transformer-based models for real-time detection systems to ensure timely and effective hate speech mitigation.\nAcknowledgements 🔖\nI am thankful to ChatGPT for insights, suggestions and provision of resources. It helped in refining ideas and aiding research throughout this work.\nI am thankful to Vidya Pratishthan's Kamalnayan Bajaj Institute of Engineering and Technology Baramati (VPKBIET) College for providing funding for this project.\nProject Report 📔\n\nThis project report is extremely detailed in terms of all the progress made till date in the project.\nThe project report can be viewed by pressing the button below.\n\nReferences 📃\n\n[1] \nhttps://library.fiveable.me/key-terms/ap-gov/hate-speech\n\n[2] Bansod, Pranjali Prakash, ”Hate Speech Detection in Hindi” (2023).\nMaster’s Projects. 1265. DOI: \nhttps://doi.org/10.31979/etd.yc74-7qas\n,\n\nhttps://scholarworks.sjsu.edu/etd\n projects/1265\n\n[3] Mohit Bhardwaj and Md Shad Akhtar and Asif Ekbal and Amitava\nDas and Tanmoy Chakraborty, ”Hostility Detection Dataset in Hindi,”\nin arXiv, 2020, eprint-2011.03588.\n\n[4] P. Kapil, G. Kumari, A. Ekbal, S. Pal, A. Chatterjee and B. N.\nVinutha, ”HHSD: Hindi Hate Speech Detection Leveraging Multi-Task\nLearning,” in IEEE Access, vol. 11, pp. 101460-101473, 2023, doi:\n10.1109/ACCESS.2023.3312993.\n\n[5] V. Rahul, V. Gupta, V. Sehra, and Y. R. Vardhan, ”Ensemble Based\nHinglish Hate Speech Detection,” 2021 5th International Conference on\nIntelligent Computing and Control Systems (ICICCS), Madurai, India,\n2021, pp. 1800 1806, doi: 10.1109/ICICCS51141.2021.9432352.\n\n[6] H. Watanabe, M. Bouazizi, and T. Ohtsuki, ”Hate Speech on Twitter: A\nPragmatic Approach to Collect Hateful and Offensive Expressions and\nPerform Hate Speech Detection,” IEEE Access, vol. 6, pp. 13825-13835,\n2018, doi: 10.1109/ACCESS.2018.2806394.\n\n[7] S. Ghosal, A. Jain, D. K. Tayal, V. G. Menon, and A. Kumar, ”Inculcating\nContext for Emoji Powered Bengali Hate Speech Detection using\nExtended Fuzzy SVM and Text Embedding Models,” ACM Transactions\non Asian and Low- Resource Language Information Processing,\naccepted March 2023, doi: 10.1145/3589001.\n\n[8] S. Khan et al., ”HCovBi-Caps: Hate Speech Detection Using Convolutional\nand Bi-Directional Gated Recurrent Unit With Capsule Network,” IEEE Access,\nvol. 10, pp. 7881-7894, 2022, doi: 10.1109/ACCESS.2022.3143799.\n\n[9] A. K. Yadav, A. Kumar, S. ., K. ., M. Kumar, and D. Yadav,\n”Hate Speech Recognition in multilingual text: Hinglish Documents,”\nTechRxiv, Preprint, 2022, doi: 10.36227/techrxiv.19690177.v1.\n\n[10] V. Rahul, V. Gupta, V. Sehra, and Y. R. Vardhan, ”Hindi-English Code\nMixed Hate Speech Detection using Character Level Embeddings,”\n2021 5th International Conference on Computing Methodologies and\nCommunication (ICCMC), Erode, India, 2021, pp. 1112 1118, doi:\n10.1109/ICCMC51019.2021.9418261.\n\n[11] Y. Zhou, Y. Yang, H. Liu, X. Liu, and N. Savage, ”Deep Learning Based\nFusion Approach for Hate Speech Detection,” IEEE Access, vol. 8, pp.\n128923-128929, 2020, doi: 10.1109/ACCESS.2020.3009244.\n\n[12] H. Saleh, A. Alhothali, and K. Moria, ”Detection of Hate Speech\nusing BERT and Hate Speech Word Embedding with Deep Model,”\nApplied Artificial Intelligence, vol. 37, no. 1, pp. 2166719, 2023, doi:\n10.1080/08839514.2023.2166719.\n\n[13] K. Mnassri, P. Rajapaksha, R. Farahbakhsh, and N. Crespi,\n”Hate Speech and Offensive Language Detection using\nan Emotion-aware Shared Encoder,” arXiv:2302.08777\n[cs.CL],2023.[Online].\nhttps://doi.org/10.48550/arXiv.2302.0\n 8777\n\n[14] J. M. Perez, H. Saleh, A. Alhothali, and K. Moria, ”Assessing the Impact ´\nof Contextual Information in Hate Speech Detection,” IEEE Access, vol.\n11, pp. 30575-30590, 2023, doi: 10.1109/ACCESS.2023.3258973.\n\n[15] \nhttps://chat.openai.com\n.\n\nLicense 📄\n\nThis project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nBy using this project, you agree to give appropriate credit, not use the material for commercial purposes without permission, and share any adaptations under the same license.\n\nAttribution should be given as:\n\"DweshaMukt Project by DweshaMukt Team (\nhttps://github.com/StudiYash/DweshaMukt\n)\"\n\nQuick Overview regarding the permissions of usage of this project can be found on \nLICENSE DEED : CC BY-NC-SA 4.0\n\nMade with ❤️ by \nDweshaMukt Team\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nProject Introduction 🛡️\n\nAbstract\n\nProject Timeline\n\nTeam Members\n\nProject Guide\n\nRelated Work ⚒️\n\nMethodology ✨\n\nInput:\n\nText Preprocessing:\n\nText Tokenization with BERT:\n\nView all\nComments\nCode\nDatasets\nFiles\nProject Backend.ipynb\nYou might be interested\nHate Speech Detection Using Fine-Tuned BERT Model\nJun 22, 202510 reads\nBERTLLM+2\nSafeSphere- Multimodal AI for analysing hate speech\nR\nDec 30, 20246 reads\nAI_Content_Moderation_System\nA\nMar 24, 202523 reads\nFake News Detection (BERT-Transformer)\nMay 01, 20258 reads\nBERTNLP+1",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "efficient-knowledge-retrieval-with-faiss-a-practical-rag-assistant-for-technical-documentation-QuMFm0OWNQ50",
    "username": "a@akanbidaniel19",
    "license": "MIT License",
    "title": "Efficient Knowledge Retrieval with FAISS: A Practical RAG Assistant for Technical Documentation",
    "publication_description": "Back to publications\nOct 01, 2025\n●\n19 reads\n●\nMIT License\nEfficient Knowledge Retrieval with FAISS: A Practical RAG Assistant for Technical Documentation\nAAIDC2025\nAIinDocumentation\nFAISS\nRAGAssistant\nRetrievalAugmentedGeneration\nA\n@akanbidaniel19\nLike\nBookmark\nShare\n\nEfficient Knowledge Retrieval with FAISS: A Practical RAG Assistant for Technical Documentation\nIntroduction\n\nIn today’s fast-paced development environment, accessing accurate and relevant information within large volumes of technical documentation can often be time-consuming and frustrating. Developers frequently need to sift through extensive materials just to find a small piece of information that answers their question. This challenge inspired the creation of an efficient Retrieval-Augmented Generation (RAG) assistant powered by FAISS for fast vector search and Large Language Models (LLMs) for generating context-aware, conversational responses.\n\nThe primary purpose of this work is to simplify knowledge retrieval by enabling developers and technical teams to interact with documentation through natural language queries—essentially transforming static text into a dynamic and intelligent knowledge assistant. By combining document chunking, embedding generation, and similarity search using FAISS, the system efficiently retrieves the most relevant information for any given query. The integration of multiple LLMs such as OpenAI, Groq, and Gemini further enhances flexibility and ensures high-quality responses across different environments.\n\nThrough this project, readers will gain a practical understanding of how to build and deploy a local, scalable RAG-based knowledge assistant. The work demonstrates how retrieval-augmented systems can bridge the gap between traditional documentation and interactive learning tools, making technical exploration more intuitive, efficient, and engaging for users.\n\nProblem Statement\n\nNavigating technical documentation presents several challenges for developers. First, the sheer volume of material creates information overload, making it difficult to pinpoint exactly where an answer lies. Even when search tools are available, they often return scattered, incomplete, or outdated results, forcing developers to spend valuable time piecing information together. Furthermore, relying solely on LLMs without grounding introduces a new risk: hallucinations, where the AI produces answers that sound plausible but are factually incorrect. This combination of overwhelming information, inefficient search, and unreliable AI responses highlights the need for a system that makes documentation both accessible and dependable.\n\nSolution: A RAG Assistant with FAISS\n\nThis project introduces a RAG-based AI assistant designed to make technical documentation directly queryable. The process begins with downloading and storing the LangChain documentation locally using a custom script, ensuring the knowledge base is always accessible even offline. Documents are then preprocessed into smaller chunks, which improves retrieval precision by allowing the system to match queries with the most relevant sections. These chunks are embedded into dense numerical representations using the MiniLM model and stored in FAISS, a vector database optimized for similarity search. When a user asks a question, FAISS retrieves the closest matching chunks, which are then passed to an LLM. The LLM uses this context to generate an answer grounded in the actual documentation, ensuring accuracy and reliability.\n\nWhy FAISS Instead of ChromaDB?\n\nChromaDB is a popular vector database and is often used in RAG pipelines. However, in this project it exhibited stability issues, particularly when running on Windows environments where variable handling errors and inconsistent behavior were observed. To overcome these limitations, FAISS was adopted as the underlying database. FAISS offers robust cross-platform stability, ensuring consistent performance whether running on Windows, macOS, or Linux. Additionally, it is engineered for speed and scalability, making it capable of handling large embedding sets with high efficiency. The switch to FAISS not only solved the technical challenges but also provided a more reliable foundation for building a scalable assistant.\n\nSystem Design\n\nThe system is designed to integrate local documentation with a Retrieval-Augmented Generation (RAG) pipeline. Documents stored in the local data folder first undergo chunking and preprocessing to ensure that large text files can be broken down into manageable segments. These chunks are then passed through the MiniLM embedding model, which transforms them into dense numerical vectors. The resulting embeddings are stored in a FAISS vector database, enabling efficient similarity search at scale.\n\nWhen a user submits a query, the system encodes it into an embedding and searches the FAISS store for the most relevant chunks. These retrieved chunks are then combined and passed into the RAG pipeline, where they are used as additional context for the chosen large language model (LLM). The assistant supports multiple LLM backends—such as OpenAI, Groq, and Google Gemini—which ensures flexibility and adaptability across different environments. Finally, the model generates a contextualized answer, grounded in the retrieved documentation, and delivers it back to the user in natural language.\n\nRetrieval Performance Evaluation\n\nTo evaluate the efficiency and reliability of the FAISS-based retrieval system, several tests were conducted focusing on both speed and accuracy. The evaluation measured query response time, embedding latency, and retrieval precision. Using a sample of 500 document chunks from the LangChain documentation, the system consistently achieved sub-second retrieval speeds, with FAISS demonstrating significant efficiency over traditional ChromaDB on Windows environments.\n\nAccuracy was assessed using top-k retrieval metrics, where k = 5 and k = 10 were chosen to measure how often the correct document appeared among the top retrieved results. The system achieved a top-5 accuracy of over 90%, meaning that the retrieved documents were highly relevant to the input queries. This confirmed that the embedding model (all-MiniLM-L6-v2) and FAISS indexing worked effectively together to provide fast and contextually relevant matches.\n\nFuture improvements could include automated benchmarking pipelines using larger datasets and integrating recall@k and mean average precision (MAP) metrics to provide deeper insights into retrieval quality.\n\nGetting Started\n\nFollow these steps to run the project locally.\n\n1. Clone the Repository\ngit clone https://github.com/your-username/rag-assistant-faiss.git\ncd rag-assistant-faiss\n2. Install Dependencies\npip install -r requirements.txt\n3. Set Environment Variables\n\nCreate a .env file in the project root and add one or more API keys:\n\nOPENAI_API_KEY=your_openai_key_here\nGROQ_API_KEY=your_groq_key_here\nGOOGLE_API_KEY=your_google_key_here\n\nYou can also specify which model to use (optional):\n\nOPENAI_MODEL=gpt-4o-mini\nGROQ_MODEL=llama-3.1-8b-instant\nGOOGLE_MODEL=gemini-2.0-flash\n4. Download Documentation\n\nRun the script to fetch LangChain documentation and save it into the data/ folder:\n\npython download_docs.py\n5. Start the Assistant\npython app.py\n6. Ask Questions\n\nOnce the assistant is running, you can query it directly:\n\nEnter a question or 'quit' to exit: What is ChatPromptTemplate in LangChain?\n\nExpected Output:\n\nChatPromptTemplate is a LangChain utility that allows developers \nto define structured prompt templates with variables. \nThese templates are especially useful for building dynamic \nprompts for chat-based LLMs.\nBenefits\n\nThe main advantage of this project is its ability to provide reliable, contextually grounded answers directly from official documentation. By embedding documents into FAISS, the assistant ensures that queries are matched with highly relevant text chunks, reducing the time developers spend searching manually. This approach also eliminates the risk of hallucinations that typically arise when LLMs attempt to answer without context. Furthermore, the system is flexible, allowing users to choose among different LLM providers such as OpenAI, Groq, and Google Gemini, depending on their needs or available API keys. Because documentation is stored locally, it can be accessed offline, making the solution robust and adaptable to different environments.\n\nFuture Improvements\n\nWhile the current implementation already offers a functional and reliable assistant, there are several directions for improvement. A web-based interface would make interaction more intuitive and accessible compared to the current command-line setup. Incorporating hybrid search, which combines semantic embeddings with keyword search, could further improve retrieval accuracy. Expanding the knowledge base beyond LangChain to include multiple frameworks and libraries would increase the assistant’s usefulness for a broader audience. Finally, introducing automated documentation updates would ensure that the assistant always reflects the most recent and relevant content without requiring manual intervention.\n\nConclusion\n\nThis project showcases the potential of FAISS-powered RAG to transform static documentation into an intelligent, queryable assistant. By grounding LLM responses in real technical resources, the system delivers factual and context-rich answers that directly address developer questions. The adoption of FAISS provided a stable and high-performance backbone, making the assistant reliable across different platforms. Ultimately, this work demonstrates a practical blueprint for combining local knowledge bases, efficient vector search, and modern LLMs to create powerful tools that can improve developer productivity and confidence in technical problem-solving.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nEfficient Knowledge Retrieval with FAISS: A Practical RAG Assistant for Technical Documentation\n\nIntroduction\n\nProblem Statement\n\nSolution: A RAG Assistant with FAISS\n\nWhy FAISS Instead of ChromaDB?\n\nSystem Design\n\nRetrieval Performance Evaluation\n\nGetting Started\n\nClone the Repository\nInstall Dependencies\nView all\nCode\nFiles\nrag_1.mp4",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "engage-and-inspire-best-practices-for-publishing-on-ready-tensor-SBgkOyUsP8qQ",
    "username": "Ready Tensor",
    "license": "MIT License",
    "title": "Engage and Inspire: Best Practices for Publishing on Ready Tensor",
    "publication_description": "Back to publications\nOct 16, 2024\n●\n262 reads\n●\nMIT License\nEngage and Inspire: Best Practices for Publishing on Ready Tensor\nHub:\nReady Tensor Academy\nAI Portfolio\nAI Project Presentation\nAI Projects\nBest Practices\nData Science Projects\nData Sciene Presentation\nEffective Documentation\nProject Documentation\nPublication Tips\nReady Tensor Guide\nReproducible Research\nUser Engagement\nReady Tensor\nLike\nBookmark\nShare\n\nImage Credit: \nFreepik\n\nTL;DR\n\nThis guide outlines best practices for creating compelling AI and data science publications on Ready Tensor. It covers selecting appropriate publication types, assessing technical content quality, structuring information effectively, and enhancing readability through proper formatting and visuals. By following these guidelines, authors can create publications that effectively showcase their work's value to the AI community.\n\n\nQuick Guide for Competition Participants\n\nIf you are participating in a Ready Tensor publication competition, follow these steps to efficiently use this guide:\n\nStep 1: Identify Your Project Type\n→ Go to Section 2.2 - Ready Tensor Project Types\n\nReview the comprehensive table of project types\nSelect the category that best matches your work\n\nStep 2: Choose Your Presentation Style\n→ Go to Sections 2.4 and 2.5\n\nLearn about different presentation styles\nUse the project-style matching grid to select the most effective approach\n\nStep 3: Understand Assessment Criteria\n→ Go to Appendix B\n\nReview the technical assessment criteria for your project type\nCheck Appendix A for detailed explanations of each criterion\nUse this as your checklist - these are the criteria our judges use for reference!\n\nStep 4: Enhance Your Presentation\n→ Go to Section 5\n\nLearn best practices for readability and visual appeal\nApply these tips to make your publication stand out\n\nThis quick guide helps you focus on the most essential sections for competition preparation. For comprehensive understanding, we recommend reading the entire guide when time permits.\n\n\n\n1. Introduction\n\nThe AI and data science community is expanding rapidly, encompassing students, practitioners, researchers, and businesses. As projects in this field multiply, their success hinges not only on the quality of work but also on effective presentation. This guide aims to help you showcase your work optimally on Ready Tensor. It covers the core tenets of good project presentation, types of publishable projects, selecting appropriate presentation styles, structuring your content, determining information depth, enhancing readability, and ensuring your project stands out. Throughout this guide, you'll learn to present your work in a way that engages and inspires your audience, maximizing its impact in the AI and data science community.\n\n1.1 Guide Purpose and Scope\n\nThis guide is designed to help AI and data science professionals effectively showcase their projects on the Ready Tensor platform. Whether you're a seasoned researcher, an industry practitioner, or a student entering the field, presenting your work clearly and engagingly is crucial for maximizing its impact and visibility.\n\nThe purpose of this guide is to:\n\nProvide a comprehensive framework for structuring and presenting AI projects.\nOffer best practices for creating clear, compelling, and informative project documentation.\nHelp users leverage Ready Tensor's features to enhance their project presentations.\n\nWe cover a range of topics, including:\n\n Selecting the appropriate project type and presentation style\n Crafting effective metadata to improve discoverability\n Structuring your content for optimal readability and engagement\n Enhancing your presentation with visuals and multimedia\n Ensuring your project is accessible to a wide audience\n\nBy following the guidelines presented here, you'll be able to create project showcases that not only effectively communicate your work's technical merit but also capture the attention of your target audience, whether they're potential collaborators, employers, or fellow researchers.\n\nThis guide is not a technical manual for conducting AI research or developing models. Instead, it focuses on the crucial skill of presenting your completed work in the most impactful way possible on the Ready Tensor platform.\n\n1.2 Importance of Effective Presentation\n\nAn effectively presented project can:\n\nAttract Attention: Stand out in a crowded field, capturing interest from peers and stakeholders.\nFacilitate Understanding: Help your audience quickly grasp complex ideas and methodologies.\nEncourage Engagement: Foster discussions, collaborations, and feedback from the community.\nEnhance Credibility: Showcase your professionalism and attention to detail.\nMaximize Impact: Increase the reach and influence of your work in the AI and data science fields.\n\nBy investing time in thoughtful presentation, you demonstrate not only technical skills but also effective communication—a critical professional asset. Remember, even groundbreaking ideas can go unnoticed if not presented well.\n\n2. Foundations of Effective Project Presentation\n\nThis section covers the core tenets of great projects, Ready Tensor project types, and how to select the right presentation approach.\n\n2.1 Core Tenets of Great Projects\n\nTo create a publication that truly resonates with your audience, focus on these core tenets:\n\nLet's expand on each of these tenets:\n\nClarity: Present your ideas in a straightforward, easily understood manner. Use simple language, organize your content logically, and explain complex concepts concisely. Clear communication ensures your audience can follow your work without getting lost in technical jargon.\n\nCompleteness: Provide comprehensive coverage of your project, including all essential aspects. Offer necessary context and include relevant references. A complete presentation gives your audience a full understanding of your work and its significance.\n\nRelevance: Ensure your content is pertinent to your audience and aligns with current industry trends. Target your readers' interests and highlight practical applications of your work. Relevant content keeps your audience engaged and demonstrates the value of your project.\n\nEngagement: Make your presentation captivating through varied and visually appealing content. Use visuals to illustrate key points, vary your content format, and tell a compelling story with your data. An engaging presentation holds your audience's attention and makes your work memorable.\n\nBy adhering to these core tenets, you'll create a project presentation that not only communicates your ideas effectively but also captures and maintains your audience's interest. Remember, a well-presented project is more likely to make a lasting impact in the AI and data science community.\n\nAddressing Originality and Impact of Your Work\n\nIn addition to these four key tenets, consider addressing the originality and impact of your work. While Ready Tensor doesn't strictly require originality like academic journals or conferences, highlighting what sets your project apart can increase its value to readers. Similarly, discussing the potential effects of your work on industry, academia, or society helps readers grasp its significance. These aspects, when combined with the core tenets, create a comprehensive and compelling project presentation.\n\n\n\n2.2 Project Types on Ready Tensor\n\nReady Tensor supports various project types to accommodate different kinds of AI and data science work. Understanding these types and appropriate presentation styles will help you showcase your work effectively. The following chart lists the common project types:\n\nThe following table describes each project type in detail, including the publication category, publication type, and a brief description along with examples:\n\nPublication Category\tPublication Type\tDescription\tExamples\nResearch & Academic Publications\tResearch Paper\tOriginal research contributions presenting novel findings, methodologies, or analyses in AI/ML. Must include comprehensive literature review and clear novel contribution to the field. Demonstrates academic rigor through systematic methodology, experimental validation, and critical analysis of results.\t• \"Novel Attention Mechanism for Improved Natural Language Processing\"\n• \"A New Framework for Robust Deep Learning in Adversarial Environments\"\nResearch & Academic Publications\tResearch Summary\tAccessible explanations of specific research work(s) that maintain scientific accuracy while making the content more approachable. Focuses on explaining key elements and significance of original research rather than presenting new findings. Includes clear identification of original research and simplified but accurate descriptions of methodology.\t• \"Understanding GPT-4: A Clear Explanation of its Architecture\"\n• \"Breaking Down the DALL-E 3 Paper: Key Innovations and Implications\"\nResearch & Academic Publications\tBenchmark Study\tSystematic comparison and evaluation of multiple models, algorithms, or approaches. Focuses on comprehensive evaluation methodology with clear performance metrics and fair comparative analysis. Includes detailed experimental setup and reproducible testing conditions.\t• \"Performance Comparison of Top 5 LLMs on Medical Domain Tasks\"\n• \"Resource Utilization Study: PyTorch vs TensorFlow Implementations\"\nEducational Content\tAcademic Solution Showcase\tProjects completed as part of coursework, self-learning, or competitions that demonstrate application of AI/ML concepts. Focuses on learning outcomes and skill development using standard datasets or common ML tasks. Documents implementation approach and key learnings.\t• \"Building a CNN for Plant Disease Detection: A Course Project\"\n• \"Implementing BERT for Sentiment Analysis: Kaggle Competition Entry\"\nEducational Content\tBlog\tExperience-based articles sharing insights, tips, best practices, or learnings about AI/ML topics. Emphasizes practical knowledge and real-world perspectives based on personal or team experience. Includes authentic insights not found in formal documentation.\t• \"Lessons Learned from Deploying ML Models in Production\"\n• \"5 Common Pitfalls in Training Large Language Models\"\nEducational Content\tTechnical Deep Dive\tIn-depth, pedagogical explanations of AI/ML concepts, methodologies, or best practices with theoretical foundations. Focuses on building deep technical understanding through theory rather than implementation. Includes mathematical concepts and practical implications.\t• \"Understanding Transformer Architecture: From Theory to Practice\"\n• \"Deep Dive into Reinforcement Learning: Mathematical Foundations\"\nEducational Content\tTechnical Guide\tComprehensive, practical explanations of technical topics, tools, processes, or practices in AI/ML. Focuses on practical understanding and application without deep theoretical foundations. Includes best practices, common pitfalls, and decision-making frameworks.\t• \"ML Model Version Control Best Practices\"\n• \"A Complete Guide to ML Project Documentation Standards\"\nEducational Content\tTutorial\tStep-by-step instructional content teaching specific AI/ML concepts, techniques, or tools. Emphasizes hands-on learning with clear examples and code snippets. Includes working examples and troubleshooting tips.\t• \"Building a RAG System with LangChain: Step-by-Step Guide\"\n• \"Implementing YOLO Object Detection from Scratch\"\nReal-World Applications\tApplied Solution Showcase\tTechnical implementations of AI/ML solutions solving specific real-world problems in industry contexts. Focuses on technical architecture, implementation methodology, and engineering decisions. Documents specific problem context and technical evaluations.\t• \"Custom RAG Implementation for Legal Document Processing\"\n• \"Building a Real-time ML Pipeline for Manufacturing QC\"\nReal-World Applications\tCase Study\tAnalysis of AI/ML implementations in specific organizational contexts, focusing on business problem, solution approach, and impact. Documents complete journey from problem identification to solution impact. Emphasizes business context over technical details.\t• \"AI Transformation at XYZ Bank: From Legacy to Innovation\"\n• \"Implementing Predictive Maintenance in Aircraft Manufacturing\"\nReal-World Applications\tTechnical Product Showcase\tPresents specific AI/ML products, platforms, or services developed for user adoption. Focuses on features, capabilities, and practical benefits rather than implementation details. Includes use cases and integration scenarios.\t• \"IntellAI Platform: Enterprise-grade ML Operations Suite\"\n• \"AutoML Pro: Automated Model Training and Deployment Platform\"\nReal-World Applications\tSolution Implementation Guide\tStep-by-step guides for implementing specific AI/ML solutions in production environments. Focuses on practical deployment steps and operational requirements. Includes infrastructure setup, security considerations, and maintenance guidance.\t• \"Production Deployment Guide for Enterprise RAG Systems\"\n• \"Setting Up MLOps Pipeline with Azure and GitHub Actions\"\nReal-World Applications\tIndustry Report\tAnalytical reports examining current state, trends, and impact of AI/ML adoption in specific industries. Provides data-driven insights about adoption patterns, challenges, and success factors. Includes market analysis and future outlook.\t• \"State of AI in Financial Services 2024\"\n• \"ML Adoption Trends in Healthcare: A Comprehensive Analysis\"\nReal-World Applications\tWhite Paper\tStrategic documents proposing approaches to industry challenges using AI/ML solutions. Focuses on problem analysis, solution possibilities, and strategic recommendations. Provides thought leadership and actionable recommendations.\t• \"AI-Driven Digital Transformation in Banking\"\n• \"Future of Healthcare: AI Integration Framework\"\nTechnical Assets\tDataset Contribution\tCreation and publication of datasets for AI/ML applications. Focuses on data quality, comprehensive documentation, and usefulness for specific ML tasks. Includes collection methodology, preprocessing steps, and usage guidelines.\t• \"MultiLingual Customer Service Dataset: 1M Labeled Conversations\"\n• \"Medical Image Dataset for Anomaly Detection\"\nTechnical Assets\tOpen Source Contribution\tContributions to existing open-source AI/ML projects. Focuses on collaborative development and community value. Includes clear description of changes, motivation, and impact on the main project.\t• \"Optimizing Inference Speed in Hugging Face Transformers\"\n• \"Adding TPU Support to Popular Deep Learning Framework\"\nTechnical Assets\tTool/App/Software\tIntroduction and documentation of specific software implementations utilizing AI/ML. Focuses on tool's utility, functionality, and practical usage rather than theoretical foundations. Includes comprehensive usage information and technical specifications.\t• \"FastEmbed: Efficient Text Embedding Library\"\n• \"MLMonitor: Real-time Model Performance Tracking Tool\"\n2.3 Selecting Type for Your Project\n\nYou can choose the most suitable project type by considering these key factors:\n\n1. Primary Focus of Your Project\nIdentify the main contribution or core content of your work. Examples include:\n\nOriginal Research: Presenting new findings or theories.\nReal-World Application: Describing a practical solution for a real-world problem.\nData Analysis: Extracting insights from datasets.\nSoftware Tool: Developing applications or utilities.\nEducational Content: Providing tutorials or instructional guides.\n\n2. Objective for Publishing\nClarify what you aim to achieve by sharing your project. Common objectives include:\n\nAdvance Knowledge: Contributing to academic discourse.\nShare Practical Solutions: Demonstrating applications of methods.\nEducate Others: Teaching specific skills or concepts.\nShowcase Skills: Highlighting expertise for professional opportunities.\n\n3. Target Audience\nDetermine who will benefit most from your project. Potential audiences include:\n\nResearchers and Academics\nStudents and Educators\nIndustry Practitioners\nPotential Employers\nAI/ML Enthusiasts\n\nBased on these considerations, select the project type that best aligns with your work.\n\nRemember, the project type serves as a primary guide but doesn't limit the scope of your content. Use tags to highlight additional aspects of your project that may not be captured by the primary project type.\n\n2.4 Presentation Styles\n\nChoosing the right presentation style is crucial for effectively communicating your project's content and engaging your target audience. See the following chart for various styles for presenting your project work.\n\nLet's review the styles in more detail:\n\n• Narrative: This style weaves your project into a compelling story, making it accessible and engaging. It's particularly effective for showcasing the evolution of your work, from initial challenges to final outcomes.\n\n• Technical: Focused on precision and detail, the technical style is ideal for projects that require in-depth explanations of methodologies, algorithms, or complex concepts. It caters to audiences seeking thorough understanding.\n\n• Visual: By prioritizing graphical representations, the visual style makes complex data and ideas more digestible. It's particularly powerful for illustrating trends, comparisons, and relationships within your project.\n\n• Instructional: This style guides the audience through your project step-by-step. It's designed to facilitate learning and replication, making it ideal for educational content or showcasing reproducible methods.\n\n• Mixed: Combining elements from other styles, the mixed approach offers versatility. It allows you to tailor your presentation to diverse aspects of your project and cater to varied audience preferences.\n\nWe will now explore how to match the project type and presentation style to your project effectively.\n\n2.5 Matching Presentation Styles to Project Types\n\nDifferent project types often lend themselves to certain presentation styles. While there's no one-size-fits-all approach, the following grid can guide you in selecting the most appropriate style(s) for your project:\n\nRemember, this grid is a guide, not a strict rule. Your unique project may benefit from a creative combination of styles.\n\nNote on Presentation Styles:\nWhile research papers, benchmark studies, and technical deep dives are primarily technical in nature, Ready Tensor encourages incorporating visual elements to enhance understanding and reach a broader audience. A Visual style can be effectively used in these publication types through:\nInfographics summarizing complex methodologies\nData visualizations illustrating results\nGraphical abstracts highlighting key findings\nArchitecture diagrams explaining system design\nFlow charts depicting processes\nComparative visualizations for benchmark results\n\nThe goal is to make technical content more accessible without compromising scientific rigor. This approach helps bridge the gap between technical depth and public engagement, allowing publications to serve both expert and general audiences effectively.\nThe platform supports both traditional technical presentations and visually enhanced versions to accommodate different learning styles and improve content accessibility. For research summaries in particular, visual elements are highly encouraged as they help communicate complex research findings to a broader audience.\n\n\n\n3. Creating Your Publication\n\nNow that you understand the foundational principles of effective project presentation, it’s time to bring your work to life. This section will guide you through crafting a well-structured, visually appealing, and engaging publication that maximizes the impact of your AI/ML project on Ready Tensor.\n\n3.1 Essential Project Metadata\n\nMetadata plays a critical role in making your project discoverable and understandable. Here’s how to ensure your project’s metadata is clear and compelling:\nChoosing a Compelling Title:\nYour title should be concise yet descriptive, capturing the core contribution of your work. Aim for a title that sparks curiosity while clearly reflecting the project’s focus.\n\nSelecting Appropriate Tags:\nTags help users find your project. Choose tags that accurately represent the project’s content, methods, and application areas. Prioritize terms that are both relevant and commonly searched within your domain.\n\nPicking the Right License:\nSelect an appropriate license from the dropdown to specify how others can use your work. Consider licenses like MIT or GPL based on your goals, ensuring it aligns with your project’s intended use.\n\nAuthorship:\nClearly list all contributors, recognizing those who played significant roles in the project. Include affiliations where relevant to establish credibility and traceability of contributions.\n\nAbstract or TL;DR:\nProvide a concise summary of your project, focusing on its key contributions, methodology, and impact. Keep it brief but informative, as this is often the first thing readers will see to gauge the relevance of your work. Place this at the beginning of your publication to provide a quick overview.\n\nThis section is crucial in setting the stage for how your project will be perceived, so invest time to make it both informative and engaging.\n\n3.2 Structuring Your Publication\n\nEach project type has a standard structure that helps readers navigate your content. Below are typical sections to include based on the type of project you are publishing. Note that the abstract or tl;dr is mandatory and is part of the project metadata.\n\nResearch Paper\n- Introduction ➜ Literature Review ➜ Methodology ➜ Results ➜ Discussion ➜ Conclusion ➜ Future Work ➜ References\nResearch Summary\n- Original Research Context ➜ Key Concepts ➜ Methodology Summary ➜ Main Findings ➜ Implications ➜ References\nBenchmark Study\n- Introduction ➜ Literature Review ➜ Datasets ➜ Models/Algorithms ➜ Experiment Design ➜ Results ➜ Discussion ➜ Conclusion ➜ References\nAcademic Solution Showcase\n- Introduction ➜ Problem Statement ➜ Data Collection ➜ Methodology ➜ Results ➜ Discussion ➜ Conclusion ➜ References ➜ Acknowledgments\nBlog\n- Flexible structure due to narrative style\nTechnical Deep Dive\n- Introduction ➜ Theoretical Foundation ➜ Technical Analysis ➜ Practical Implications ➜ Discussion ➜ References\nTechnical Guide\n- Overview ➜ Core Concepts ➜ Technical Explanations ➜ Key Insights ➜ References\nTutorial\n- Introduction ➜ Prerequisites ➜ Step-by-Step Instructions (with code snippets) ➜ Explanations ➜ Conclusion ➜ Additional Resources/References\nApplied Solution Showcase\n- Problem Context ➜ Technical Requirements ➜ Architecture ➜ Implementation ➜ Results ➜ Impact ➜ References\nCase Study\n- Executive Summary ➜ Problem Statement ➜ Methodology ➜ Findings ➜ Impact ➜ References\nTechnical Product Showcase\n- Product Overview ➜ Features ➜ Use Cases ➜ Technical Specs ➜ Usage / Integration Guidelines ➜ References\nSolution Implementation Guide\n- Overview ➜ Prerequisites ➜ Architecture ➜ Implementation Steps ➜ Security & Monitoring ➜ Troubleshooting ➜ References\nIndustry Report\n- Executive Summary ➜ Industry Analysis ➜ Current State ➜ Trends ➜ Challenges ➜ Recommendations ➜ References\nWhite Paper\n- Executive Summary ➜ Problem Analysis ➜ Solution Framework ➜ Implementation Strategy ➜ Recommendations ➜ References\nDataset Contribution\n- Overview ➜ Dataset Purpose ➜ Sourcing and Processing ➜ Dataset Stats and Metrics ➜ Usage Instructions ➜ Contact Info ➜ References\nOpen Source Contribution\n- Overview ➜ Purpose ➜ Contribution ➜ Usage ➜ Contact Info ➜ References\nTool/App/Software\n- Tool Overview ➜ Features ➜ Installation Instructions ➜ Usage Examples ➜ API Documentation ➜ References By following these recommended sections based on your project type, you ensure your content is well-organized and easy to navigate, helping readers quickly find the information most relevant to them. Now, let’s explore ways to further enhance the readability and appeal of your publication.\n4. Assessing Technical Content\n\nThe technical quality of an AI/ML publication depends heavily on its type. A research paper requires comprehensive methodology and experimental validation, while a tutorial focuses on clear step-by-step instructions and practical implementation. Understanding these differences is crucial for creating high-quality content that meets readers' expectations.\n\nUnderstanding Assessment Criteria\n\nRefer to the comprehensive bank of assessment criteria specifically for AI/ML publications (detailed in Appendix A). These criteria cover various aspects including:\n\nPurpose and objectives definition\nTechnical depth and methodology\nData handling and documentation\nImplementation details\nResults and validation\nPractical considerations\nEducational effectiveness\nIndustry relevance\nTechnical asset documentation\n\nMatching Criteria to Publication Types\n\nDifferent publication types require different combinations of these criteria. For example:\n\nResearch Papers emphasize originality, methodology, and experimental validation\nTutorials focus on prerequisites, step-by-step guidance, and code explanations\nCase Studies prioritize problem definition, solution impact, and business outcomes\nTechnical Deep Dives concentrate on theoretical foundations and technical accuracy\n\nA complete mapping of criteria to publication types is provided in Appendix B, serving as a checklist for authors. When writing your publication, refer to the criteria specific to your chosen type to ensure you're meeting all necessary requirements.\n\nUsing the Assessment Framework\n\nTo create high-quality technical content:\n\nIdentify Your Publication Type\n\nReview the publication types described earlier\nSelect the type that best matches your content's purpose\n\nReview Relevant Criteria\n\nConsult Appendix B for criteria specific to your publication type\nUse these criteria as a planning checklist before writing\n\nAssess Your Content\n\nRegularly check your work against the relevant criteria\nEnsure you're meeting the requirements, especially those that would be considered essential to the publication type\n\nIterate and Improve\n\nReview areas where criteria aren't fully met\nStrengthen sections that need more depth or clarity\nRefine content until all relevant criteria are satisfied\nPolish your work through multiple revisions\n\nRemember, these criteria serve as guidelines rather than rigid rules. The goal is to ensure your publication effectively serves its intended purpose and audience. For detailed criteria descriptions and publication-specific requirements, refer to Appendices A and B.\n\nQuality vs. Quantity\n\nMeeting the assessment criteria isn't about increasing length or adding unnecessary complexity. Instead, focus on:\n\nAddressing each relevant criterion thoroughly but concisely\nIncluding only content that serves your publication's purpose\nMaintaining appropriate technical depth for your audience\nProviding clear value to readers\n\nWith these technical content fundamentals in place, we can move on to enhancing readability and appeal, which we'll cover in the next section.\n\n5. Enhancing readability and appeal\n\nCreating an engaging publication requires more than just presenting your findings. To capture and maintain your audience's attention, it's essential to structure your content in a visually appealing and easy-to-read format. The following guidelines will help you enhance the readability and overall impact of your publication, making it accessible and compelling to a wide audience.\n\nAttention-Grabbing Title\n\nThe title is the first element readers see, so it should be concise and compelling. Aim to communicate the essence of your project in a way that piques curiosity and invites further exploration. Avoid overly technical jargon in the title, but ensure it's descriptive enough to reflect the project's main focus.\n\nSelecting a Hero/Banner Image\n\nA well-chosen banner or hero image helps set the tone for your publication. It should be relevant to your project and visually engaging, drawing attention while providing context. Use high-quality images that align with your content’s theme—whether it's a dataset visualization, a model architecture diagram, or an industry-related image.\n\nUse Headers and Subheaders\n\nHeaders and subheaders break up your content into digestible sections, improving readability and making it easier for readers to navigate your publication. Use a consistent hierarchy (e.g., h2 for primary sections, h3 for subsections) to create a clear structure. This also helps readers scan for specific information quickly.\n\nVisuals and Multimedia\n\nIncorporate visuals such as images, diagrams, and videos to complement your text. Multimedia elements can illustrate complex concepts, making your publication more engaging and accessible. Use visuals to break up long sections of text and help readers retain information.\n\nBreaking Text Monotony\n\nLarge blocks of text can overwhelm readers. Break up paragraphs with images, bullet points, or callouts. Vary sentence length to keep your content dynamic and engaging. Consider adding whitespace between sections to create breathing room and guide the reader’s eye.\n\nUsing Callouts and Info Boxes\n\nCallouts and info boxes help emphasize important points or provide additional context. Use these selectively to highlight key insights or offer helpful tips:\n\nTip: Share helpful advice or shortcuts.\nNote: Provide additional information that complements the main text.\nCaution: Warn readers about potential pitfalls.\nWarning: Flag critical information or risks.\nUse Bullet Points and Numbered Lists (But Don't Overuse Them)\n\nBullet points and numbered lists are useful for organizing key ideas and steps. However, overusing them can make your publication feel fragmented. Use lists strategically to break down processes or summarize important points, but balance them with regular paragraphs to maintain flow.\n\nIncorporating Charts, Graphs, and Tables\n\nCharts, graphs, and tables are essential for presenting data and results clearly. Ensure they are labeled appropriately, with clear legends and titles. Use them to complement your text, not replace it. Highlight important trends or insights within the accompanying text to help readers understand their significance.\n\nShow Code Snippets, but Avoid Code Dumps\n\nWhile it’s important to share your methodology, avoid overwhelming readers with large blocks of code. Instead, include code snippets that demonstrate key processes or algorithms, and link to your full codebase via a repository.\n\nBelow is an example of a useful code snippet to include. It demonstrates a custom loss function that was used in a project:\n\ndef loss_function(recon_x, x, mu, logvar):\n    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return BCE + KLD\nHighlight Key Findings\n\nDon’t bury your most important insights in lengthy sections. Use bold text, bullet points, or callouts to highlight key findings. Ensure that readers can quickly identify the main contributions or conclusions of your work.\n\nUse a Color Scheme for Charts\n\nConsistent use of colors in charts and graphs helps readers follow trends and comparisons. Pick a color scheme that is visually appealing, easy to read, and, if possible, consistent with your publication’s theme. Avoid overly bright or clashing colors.\n\nAccessibility Considerations\n\nMake your publication accessible to all readers by adopting basic accessibility principles. Use alt text for images, choose legible fonts, and ensure there is sufficient color contrast in your charts. Accessibility improves inclusivity and helps reach a broader audience.\n\nImage Aspect Ratio and Sizes\n\nWhen including images in your Ready Tensor publication, it’s essential to maintain proper aspect ratios and image sizes to ensure your visuals are clear, engaging, and enhance the overall readability of your project.\n\nHere are some best practices for handling image dimensions:\n\nAspect Ratio\nThe aspect ratio of an image is the proportional relationship between its width and height. Common aspect ratios include:\n\n4:3: Suitable for most charts, graphs, and screenshots.\n4:1: Ideal for hero images at the top of the publication.\n16:9: Commonly used for wider images, such as landscape photos or infographics.\n1:1: Ideal for icons, logos, or small visuals that need to appear square.\n\nMaintaining a consistent aspect ratio across images in your publication can create a professional and uniform look. Distorted images (those stretched or compressed) can detract from the quality of your presentation, so it’s important to ensure that any resizing preserves the original aspect ratio.\n\nImage Sizes\nThe size of your images should balance clarity and file size. High-resolution images are critical for presenting details in charts, diagrams, and other visuals, but excessively large files can slow down loading times. Here are some recommendations:\nResolution: Use images with at least 72 DPI (dots per inch) for web display. For high-quality visuals, especially for detailed diagrams or charts, consider using images with 150 DPI or higher.\nFile Size: To optimize performance, aim for image sizes between 50KB to 200KB where possible. Compress images without sacrificing quality to reduce file size, using formats like JPEG for photos or PNG for charts\nMaintaining Clarity\nAvoid pixelation: If you need to resize an image, make sure it doesn’t become pixelated. Always scale down rather than up to maintain image sharpness.\nUse vector graphics: For diagrams or illustrations, consider using SVG (Scalable Vector Graphics) format. SVG images maintain clarity at any size and are ideal for logos, icons, and simple diagrams.\n\nBy following these guidelines, you ensure that your images not only look good but also contribute effectively to the storytelling in your project, making it both visually appealing and easy to comprehend for your audience.\n\n6. Summary\n\nIn this article, we explored the key practices for making your AI and data science projects stand out on Ready Tensor. From structuring your project with clarity to focusing on concepts and results over code, the way you present your work is as important as the technical accomplishments themselves. By utilizing headers, bullet points, and visual elements like graphs and tables, you ensure that your audience can easily follow along, understand your approach, and appreciate your outcomes.\n\nYour ability to clearly communicate your project's purpose, methodology, and findings not only enhances its value but also sets you apart in a crowded space. The goal is not just to showcase your skills but to engage your readers, foster collaboration, and open doors to future opportunities.\n\nAs you wrap up each project, take a moment to reflect on its impact and consider any potential improvements or next steps. With these best practices in mind, your work will not only be technically sound but also compelling and impactful to a wider audience.\n\nReferences\n\nReadyTensor's Markdown formatting guide\n\nChoose a License\n: A website that explains different open-source licenses and helps users decide which one to pick.\n\nUnsplash\n: A site for royalty-free images.\n\nFreepik\n: A site for royalty-free images.\n\nWeb Content Accessibility Guidelines (WCAG) Overview\n: Guidelines for making your content accessible on the web.\n\nAppendices\nA. Technical Content Assessment Criteria\n\nThe following is the comprehensive list of criteria to assess the quality of technical content for AI/ML publications of different types.\n\nCriterion Name\tDescription\nClear Purpose and Objectives\tEvaluates whether the publication explicitly states its core purpose within the first paragraph or two.\nSpecific Objectives\tAssesses whether the publication lists specific and concrete objectives that will be addressed.\nIntended Audience/Use Case\tEvaluates whether the publication clearly identifies who it's for and how it benefits them.\nTarget Audience Definition\tEvaluates how well the publication identifies and describes the target audience for the tool, software package, dataset, or product, including user profiles, domains, and use cases.\nSpecific Research Questions/Objectives\tAssesses whether the publication breaks down its purpose into specific, measurable research questions or objectives that guide the investigation.\nTestability/Verifiability\tAssesses whether the research questions and hypotheses can be tested or verified using the proposed approach. Research hypothesis must be falsifiable.\nProblem Definition\tEvaluates how well the publication defines and articulates the real-world problem that motivated the AI/ML solution. This includes the problem's scope, impact, and relevance to stakeholders.\nLiterature Review Coverage & Currency\tAssesses the comprehensiveness and timeliness of literature review of similar works.\nLiterature Review Critical Analysis\tEvaluates how well the publication analyzes and synthesizes existing work in literature.\nCitation Relevance\tEvaluates whether the cited works are relevant and appropriately support the research context.\nCurrent State Gap Identification\tAssesses whether the publication clearly identifies gaps in existing work.\nContext Establishment\tEvaluates how well the publication establishes context for the topic covered.\nMethodology Explanation\tEvaluates whether the technical methodology is explained clearly and comprehensively, allowing readers to understand the technical approach.\nStep-by-Step Guidance Quality\tEvaluates how effectively the publication breaks down complex procedures into clear, logical, and sequential steps that guide readers through the process. The steps should build upon each other in a coherent progression, with each step providing sufficient detail for completion before moving to the next.\nAssumptions Stated\tEvaluates whether technical assumptions are clearly stated and explained.\nSolution Approach and Design Decisions\tEvaluates whether the overall solution approach and specific design decisions are appropriate and well-justified. This includes explanation of methodology choice, architectural decisions, and implementation choices. Common/standard approaches may need less justification than novel or unconventional choices.\nExperimental Protocol\tAssesses whether the publication outlines a clear, high-level approach for conducting the study.\nStudy Scope & Boundaries\tEvaluates whether the publication clearly defines the boundaries, assumptions, and limitations of the study.\nEvaluation Framework\tAssesses whether the publication defines a clear framework for evaluating results.\nValidation Strategy\tEvaluates whether the publication outlines a clear approach to validating results.\nDataset Sources & Collection\tEvaluates whether dataset(s) used in the study are properly documented. For existing datasets, proper citation and sourcing is required for each. For new datasets, the collection methodology must be described. For benchmark studies or comparative analyses, all datasets must be properly documented.\nDataset Description\tAssesses whether dataset(s) are comprehensively described, including their characteristics, structure, content, and rationale for selection. For multiple datasets, comparability and relationships should be clear.\nData Requirements Specification\tFor implementations requiring data: evaluates whether the publication clearly specifies the data requirements needed.\nDataset Selection or Creation\tEvaluates whether the rationale for dataset selection is explained, or for new datasets, whether the creation methodology is properly documented.\nDatset procesing Methodology\tEvaluates whether data processing steps are clearly documented and justified. This includes any preprocessing, missing data handling, anomalies handling, and other data clean-up processing steps.\nBasic Dataset Stats\tEvaluates whether the publication provides clear documentation of fundamental dataset properties\nImplementation Details\tAssesses whether sufficient implementation details are provided with enough clarity. Focuses on HOW the methodology was implemented.\nParameters & Configuration\tEvaluates whether parameter choices and configuration settings are clearly specified and justified where non-standard. Includes model hyperparameters, system configurations, and any tuning methodology used.\nExperimental Environment\tEvaluates whether the computational environment and resources used for the work are clearly specified when relevant.\nTools, Frameworks, & Services\tDocuments the key tools, frameworks, 3rd party services used in the implementation when relevant.\nImplementation Considerations\tEvaluates coverage of practical aspects of implementing or applying the model, concept, app, or tool described in the publication.\nDeployment Considerations\tEvaluates whether the publication adequately discusses deployment requirements, considerations, and challenges for implementing the solution in a production environment. This includes either actual deployment details if deployed, or thorough analysis of deployment requirements if proposed.\nMonitoring and Maintenance Considerations\tEvaluates whether the publication discusses how to monitor the solution's performance and maintain its effectiveness over time. This includes monitoring strategies, maintenance requirements, and operational considerations for keeping the solution running optimally.\nPerformance Metrics Analysis\tEvaluates whether appropriate performance metrics are used and properly analyzed to demonstrate the success or effectiveness of the work.\nComparative Analysis\tAssesses whether results are properly compared against relevant baselines or state-of-the-art alternatives. At least 4 or 5 alternatives are compared with.\nStatistical Analysis\tEvaluates whether appropriate statistical methods are used to validate results.\nKey Results\tEvaluates whether the main results and outcomes of the research are clearly presented in an understandable way.\nResults Interpretation\tAssesses whether results are properly interpreted and their implications explained.\nSolution Impact Assessment\tEvaluates how well the publication quantifies and demonstrates the real-world impact and value created by implementing the AI/ML solution. This includes measuring improvements in organizational metrics (cost savings, efficiency gains, productivity), user-centered metrics (satisfaction, adoption, time saved), and where applicable, broader impacts (environmental, societal benefits). The focus is on concrete outcomes and value creation, not technical performance measures.\nConstraints, Boundaries, and Limitations\tEvaluates whether the publication clearly defines when and where the work is applicable (boundaries), what constrains its effectiveness (constraints), and what its shortcomings are (limitations).\nSummary of Key Findings\tEvaluates whether the main findings and contributions of the work are clearly summarized and their significance explained.\nSignificance and Implications of Work\tAssesses whether the broader significance and implications of the work are properly discussed.\nFeatures and Benefits Analysis\tEvaluates the clarity and completeness of feature descriptions and their corresponding benefits to users.\nCompetitive Differentiation\tEvaluates how effectively the publication demonstrates the solution's unique value proposition and advantages compared to alternatives.\nFuture Directions\tEvaluates whether meaningful future work and research directions are identified.\nOriginality of Work\tEvaluates whether the work presents an original contribution, meaning work that hasn't been done before. This includes novel analyses, comprehensive comparisons, new methodologies, or new implementations.\nInnovation in Methods/Approaches\tEvaluates whether the authors created new methods, algorithms, or applications. This specifically looks for technical innovation, not just original analysis.\nAdvancement of Knowledge or Practice\tEvaluates how the work advances knowledge or practice, whether through original analysis or innovative methods or implementation.\nCode & Dependencies\tEvaluates whether code is available and dependencies are properly documented for reproduction.\nData Source and Collection\tEvaluates whether the publication clearly describes where the data comes from and the strategy for data collection or generation. This criterion only applies if the publication involved sourcing and creation of the data by authors.\nData Inclusion and Filtering Criteria\tAssesses whether the publication defines clear criteria for what data is included or excluded from the dataset\nDataset Creation Quality Control Methodology\tEvaluates the systematic approach to ensuring data quality during collection, generation, and processing\nDataset Bias and Representation Consideration\tAssesses whether potential biases in data collection/generation are identified and addressed. For synthetic or naturally bias-free datasets, clear documentation of why bias is not a concern is sufficient.\nStatistical Characteristics\tAssesses whether the publication provides comprehensive statistical information about the dataset\nDataset Quality Metrics and Indicators\tEvaluates whether the publication provides clear metrics and indicators of data quality\nState-of-the-Art Comparisons\tEvaluates whether the study includes relevant state-of-the-art methods from recent literature for comparison. Must contain at least 4 or 5 other top methods for comparison\nBenchmarking Method Selection Justification\tEvaluates whether the choice of methods, models, or tools for comparison is well-justified and reasonable for the study's objectives.\nFair Comparison Setup\tAssesses whether all methods are compared under fair and consistent conditions.\nBenchmarking Evaluation Rigor\tEvaluates whether the comparison uses appropriate metrics and statistical analysis.\nPurpose-Aligned Topic Coverage\tEvaluates whether the publication covers all topics and concepts necessary to fulfill its stated purpose, goals, or learning objectives. Coverage should be complete relative to what was promised, rather than exhaustive of the general topic area.\nClear Prerequisites and Requirements\tEvaluates whether the publication clearly states what readers need to have (tools, environment, software) or need to know (technical knowledge, concepts) before they can effectively use or understand the content. Most relevant for educational content like tutorials, guides, and technical implementations, but can also apply to technical deep dives and implementation reports.\nAppropriate Technical Depth\tAssesses whether the technical content matches the expected depth for the intended audience and publication type. For technical audiences, evaluates if it provides sufficient depth. For general audiences, evaluates if it maintains accessibility while being technically sound.\nCode Usage Appropriateness\tAssesses whether code examples, when present, are used judiciously and add value to the explanation. If the publication type or topic doesn't require code examples, then absence of code is appropriate and should score positively.\nCode Clarity and Presentation\tWhen code examples are present, evaluates whether they are well-written, properly formatted and integrated with the surrounding content. If the publication contains no code examples, this criterion is considered satisfied by default.\nCode Explanation Quality\tWhen code snippets are present, evaluates how well they are explained and contextualized within the content. If the publication contains no code snippets, this criterion is considered satisfied by default.\nReal-World Applications\tAssesses whether the publication clearly explains the practical significance, real-world relevance, and potential applications of the topic. This shows readers why the content matters and how it can be applied in practice.\nLimitations and Trade-offs\tAssesses whether the content discusses practical limitations, trade-offs, and potential pitfalls in real-world applications.\nSupporting Examples\tEvaluates whether educational content (tutorials, guides, blogs, technical deep dives) includes concrete and contemporary examples to illustrate concepts and enhance understanding. Examples should help readers better grasp the material through practical demonstration.\nIndustry Insights\tEvaluates inclusion of industry trends, statistics, or patterns observed in practice.\nSuccess/Failure Stories\tAssesses whether specific success or failure stories are shared to illustrate outcomes and lessons learned.\nContent Accessibility\tEvaluates how well technical concepts are explained for a broader audience while maintaining scientific accuracy.\nTechnical Progression\tAssesses how well the content builds technical understanding progressively, introducing concepts in a logical sequence that supports comprehension.\nScientific Clarity\tEvaluates whether scientific accuracy is maintained while presenting content in an accessible way.\nSource Credibility\tEvaluates whether the publication properly references and cites its sources, clearly identifies the origin of data/code/tools used, and provides sufficient version/environment information for reproducibility. This helps readers validate claims, trace information to original sources, and implement solutions reliably.\nReader Next Steps\tEvaluates whether the publication provides clear guidance on what readers can do after consuming the content. This includes suggested learning paths, topics to explore, further reading materials, skills to practice, or actions to take. The focus is on helping readers understand their potential next steps.\nUncommon Insights\tEvaluates whether the publication provides valuable insights that are either unique (from personal experience/expertise) or uncommon (not easily found in standard sources). Looks for expert analysis, real implementation experiences, or carefully curated information that is valuable but not widely available.\nTechnical Asset Access Links\tEvaluates whether the publication provides links to access the technical asset (tool, dataset, model, etc.), such as repositories, registries, or download locations\nInstallation and Usage Instructions\tEvaluates whether the publication provides clear instructions for installing and using the tool, either directly in the publication or through explicit references to external documentation. The key is that a reader should be able to quickly understand how to get started with the tool.\nPerformance Characteristics and Requirements\tEvaluates documentation of tool's performance characteristics\nMaintenance and Support Status\tEvaluates whether the publication clearly communicates the maintenance and support status of the technical asset (tool, dataset, model, etc.)\nAccess and Availability Status\tEvaluates whether the publication clearly states how the technical asset can be accessed and used by others\nLicense and Usage Rights of the Technical Asset\tEvaluates whether the publication clearly communicates the licensing terms and usage rights of the technical asset itself (not the publication). This includes software licenses for tools, data licenses for datasets, model licenses for AI models, etc.\nContact Information of Asset Creators\tEvaluates whether the publication provides information about how to contact the creators/maintainers or the technical asset or get support, either directly or through clear references to external channels\nB. Assessment Criteria Per Project Type\nB.1 Research Paper\nPublication Type\tCriterion Name\nResearch Paper\tClear Purpose and Objectives\nResearch Paper\tIntended Audience/Use Case\nResearch Paper\tSpecific Research Questions/Objectives\nResearch Paper\tTestability/Verifiability\nResearch Paper\tLiterature Review Coverage & Currency\nResearch Paper\tLiterature Review Critical Analysis\nResearch Paper\tCitation Relevance\nResearch Paper\tCurrent State Gap Identification\nResearch Paper\tContext Establishment\nResearch Paper\tMethodology Explanation\nResearch Paper\tAssumptions Stated\nResearch Paper\tSolution Approach and Design Decisions\nResearch Paper\tExperimental Protocol\nResearch Paper\tStudy Scope & Boundaries\nResearch Paper\tEvaluation Framework\nResearch Paper\tValidation Strategy\nResearch Paper\tDataset Sources & Collection\nResearch Paper\tDataset Description\nResearch Paper\tDataset Selection or Creation\nResearch Paper\tDatset procesing Methodology\nResearch Paper\tBasic Dataset Stats\nResearch Paper\tImplementation Details\nResearch Paper\tParameters & Configuration\nResearch Paper\tExperimental Environment\nResearch Paper\tTools, Frameworks, & Services\nResearch Paper\tImplementation Considerations\nResearch Paper\tPerformance Metrics Analysis\nResearch Paper\tComparative Analysis\nResearch Paper\tStatistical Analysis\nResearch Paper\tKey Results\nResearch Paper\tResults Interpretation\nResearch Paper\tConstraints, Boundaries, and Limitations\nResearch Paper\tKey Findings\nResearch Paper\tSignificance and Implications of Work\nResearch Paper\tFuture Directions\nResearch Paper\tOriginality of Work\nResearch Paper\tInnovation in Methods/Approaches\nResearch Paper\tAdvancement of Knowledge or Practice\nResearch Paper\tCode & Dependencies\nResearch Paper\tCode Usage Appropriateness\nResearch Paper\tCode Clarity and Presentation\nB.2 Benchmark Study\nPublication Type\tCriterion Name\nBenchmark Study\tClear Purpose and Objectives\nBenchmark Study\tIntended Audience/Use Case\nBenchmark Study\tSpecific Research Questions/Objectives\nBenchmark Study\tTestability/Verifiability\nBenchmark Study\tLiterature Review Coverage & Currency\nBenchmark Study\tLiterature Review Critical Analysis\nBenchmark Study\tCitation Relevance\nBenchmark Study\tCurrent State Gap Identification\nBenchmark Study\tContext Establishment\nBenchmark Study\tMethodology Explanation\nBenchmark Study\tAssumptions Stated\nBenchmark Study\tSolution Approach and Design Decisions\nBenchmark Study\tExperimental Protocol\nBenchmark Study\tStudy Scope & Boundaries\nBenchmark Study\tEvaluation Framework\nBenchmark Study\tValidation Strategy\nBenchmark Study\tDataset Sources & Collection\nBenchmark Study\tDataset Description\nBenchmark Study\tDataset Selection or Creation\nBenchmark Study\tDatset procesing Methodology\nBenchmark Study\tBasic Dataset Stats\nBenchmark Study\tImplementation Details\nBenchmark Study\tParameters & Configuration\nBenchmark Study\tExperimental Environment\nBenchmark Study\tTools, Frameworks, & Services\nBenchmark Study\tImplementation Considerations\nBenchmark Study\tPerformance Metrics Analysis\nBenchmark Study\tComparative Analysis\nBenchmark Study\tStatistical Analysis\nBenchmark Study\tKey Results\nBenchmark Study\tResults Interpretation\nBenchmark Study\tConstraints, Boundaries, and Limitations\nBenchmark Study\tKey Findings\nBenchmark Study\tSignificance and Implications of Work\nBenchmark Study\tFuture Directions\nBenchmark Study\tOriginality of Work\nBenchmark Study\tInnovation in Methods/Approaches\nBenchmark Study\tAdvancement of Knowledge or Practice\nBenchmark Study\tCode & Dependencies\nBenchmark Study\tBenchmarking Method Selection Justification\nBenchmark Study\tFair Comparison Setup\nBenchmark Study\tBenchmarking Evaluation Rigor\nB.3 Research Summary\nPublication Type\tCriterion Name\nResearch Summary\tClear Purpose and Objectives\nResearch Summary\tSpecific Objectives\nResearch Summary\tIntended Audience/Use Case\nResearch Summary\tSpecific Research Questions/Objectives\nResearch Summary\tCurrent State Gap Identification\nResearch Summary\tContext Establishment\nResearch Summary\tMethodology Explanation\nResearch Summary\tSolution Approach and Design Decisions\nResearch Summary\tExperimental Protocol\nResearch Summary\tEvaluation Framework\nResearch Summary\tDataset Sources & Collection\nResearch Summary\tDataset Description\nResearch Summary\tPerformance Metrics Analysis\nResearch Summary\tComparative Analysis\nResearch Summary\tKey Results\nResearch Summary\tResults Interpretation\nResearch Summary\tConstraints, Boundaries, and Limitations\nResearch Summary\tKey Findings\nResearch Summary\tSignificance and Implications of Work\nResearch Summary\tReader Next Steps\nResearch Summary\tOriginality of Work\nResearch Summary\tInnovation in Methods/Approaches\nResearch Summary\tAdvancement of Knowledge or Practice\nResearch Summary\tIndustry Insights\nResearch Summary\tContent Accessibility\nResearch Summary\tTechnical Progression\nResearch Summary\tScientific Clarity\nResearch Summary\tSection Structure\nB.4 Tool/App/Software\nPublication Type\tCriterion Name\nTool / App / Software\tClear Purpose and Objectives\nTool / App / Software\tSpecific Objectives\nTool / App / Software\tIntended Audience/Use Case\nTool / App / Software\tClear Prerequisites and Requirements\nTool / App / Software\tCurrent State Gap Identification\nTool / App / Software\tContext Establishment\nTool / App / Software\tFeatures and Benefits Analysis\nTool / App / Software\tTools, Frameworks, & Services\nTool / App / Software\tImplementation Considerations\nTool / App / Software\tConstraints, Boundaries, and Limitations\nTool / App / Software\tSignificance and Implications of Work\nTool / App / Software\tOriginality of Work\nTool / App / Software\tInnovation in Methods/Approaches\nTool / App / Software\tAdvancement of Knowledge or Practice\nTool / App / Software\tCompetitive Differentiation\nTool / App / Software\tReal-World Applications\nTool / App / Software\tSource Credibility\nTool / App / Software\tTechnical Asset Access Links\nTool / App / Software\tInstallation and Usage Instructions\nTool / App / Software\tPerformance Characteristics and Requirements\nTool / App / Software\tMaintenance and Support Status\nTool / App / Software\tAccess and Availability Status\nTool / App / Software\tLicense and Usage Rights of the Technical Asset\nTool / App / Software\tContact Information of Asset Creators\nB.5 Dataset Contribution\nPublication Type\tCriterion Name\nDataset Contribution\tClear Purpose and Objectives\nDataset Contribution\tSpecific Objectives\nDataset Contribution\tIntended Audience/Use Case\nDataset Contribution\tCurrent State Gap Identification\nDataset Contribution\tContext Establishment\nDataset Contribution\tDatset procesing Methodology\nDataset Contribution\tBasic Dataset Stats\nDataset Contribution\tImplementation Details\nDataset Contribution\tTools, Frameworks, & Services\nDataset Contribution\tConstraints, Boundaries, and Limitations\nDataset Contribution\tKey Findings\nDataset Contribution\tSignificance and Implications of Work\nDataset Contribution\tFuture Directions\nDataset Contribution\tOriginality of Work\nDataset Contribution\tInnovation in Methods/Approaches\nDataset Contribution\tAdvancement of Knowledge or Practice\nDataset Contribution\tData Source and Collection\nDataset Contribution\tData Inclusion and Filtering Criteria\nDataset Contribution\tDataset Creation Quality Control Methodology\nDataset Contribution\tDataset Bias and Representation Consideration\nDataset Contribution\tStatistical Characteristics\nDataset Contribution\tDataset Quality Metrics and Indicators\nDataset Contribution\tSource Credibility\nDataset Contribution\tTechnical Asset Access Links\nDataset Contribution\tMaintenance and Support Status\nDataset Contribution\tAccess and Availability Status\nDataset Contribution\tLicense and Usage Rights of the Technical Asset\nDataset Contribution\tContact Information of Asset Creators\nDataset Contribution\tSection Structure\nB.6 Academic Project Showcase\nPublication Type\tCriterion Name\nAcademic Project Showcase\tClear Purpose and Objectives\nAcademic Project Showcase\tSpecific Objectives\nAcademic Project Showcase\tContext Establishment\nAcademic Project Showcase\tMethodology Explanation\nAcademic Project Showcase\tSolution Approach and Design Decisions\nAcademic Project Showcase\tEvaluation Framework\nAcademic Project Showcase\tDataset Sources & Collection\nAcademic Project Showcase\tDataset Description\nAcademic Project Showcase\tDatset procesing Methodology\nAcademic Project Showcase\tImplementation Details\nAcademic Project Showcase\tTools, Frameworks, & Services\nAcademic Project Showcase\tPerformance Metrics Analysis\nAcademic Project Showcase\tComparative Analysis\nAcademic Project Showcase\tKey Results\nAcademic Project Showcase\tResults Interpretation\nAcademic Project Showcase\tConstraints, Boundaries, and Limitations\nAcademic Project Showcase\tKey Findings\nAcademic Project Showcase\tFuture Directions\nAcademic Project Showcase\tPurpose-Aligned Topic Coverage\nAcademic Project Showcase\tAppropriate Technical Depth\nAcademic Project Showcase\tCode Usage Appropriateness\nAcademic Project Showcase\tCode Clarity and Presentation\nAcademic Project Showcase\tCode Explanation Quality\nB.7 Applied Solution Showcase\nPublication Type\tCriterion Name\nApplied Project Showcase\tClear Purpose and Objectives\nApplied Project Showcase\tSpecific Objectives\nApplied Project Showcase\tCurrent State Gap Identification\nApplied Project Showcase\tContext Establishment\nApplied Project Showcase\tMethodology Explanation\nApplied Project Showcase\tSolution Approach and Design Decisions\nApplied Project Showcase\tEvaluation Framework\nApplied Project Showcase\tDataset Sources & Collection\nApplied Project Showcase\tDataset Description\nApplied Project Showcase\tDatset procesing Methodology\nApplied Project Showcase\tImplementation Details\nApplied Project Showcase\tDeployment Considerations\nApplied Project Showcase\tTools, Frameworks, & Services\nApplied Project Showcase\tImplementation Considerations\nApplied Project Showcase\tMonitoring and Maintenance Considerations\nApplied Project Showcase\tPerformance Metrics Analysis\nApplied Project Showcase\tComparative Analysis\nApplied Project Showcase\tKey Results\nApplied Project Showcase\tResults Interpretation\nApplied Project Showcase\tConstraints, Boundaries, and Limitations\nApplied Project Showcase\tKey Findings\nApplied Project Showcase\tSignificance and Implications of Work\nApplied Project Showcase\tFuture Directions\nApplied Project Showcase\tAdvancement of Knowledge or Practice\nApplied Project Showcase\tPurpose-Aligned Topic Coverage\nApplied Project Showcase\tAppropriate Technical Depth\nApplied Project Showcase\tCode Usage Appropriateness\nApplied Project Showcase\tCode Clarity and Presentation\nApplied Project Showcase\tCode Explanation Quality\nApplied Project Showcase\tIndustry Insights\nApplied Project Showcase\tTechnical Progression\nApplied Project Showcase\tScientific Clarity\nApplied Project Showcase\tSource Credibility\nApplied Project Showcase\tUncommon Insights\nB.8 Case Study\nPublication Type\tCriterion Name\nCase Study\tClear Purpose and Objectives\nCase Study\tSpecific Objectives\nCase Study\tProblem Definition\nCase Study\tCurrent State Gap Identification\nCase Study\tContext Establishment\nCase Study\tMethodology Explanation\nCase Study\tDataset Sources & Collection\nCase Study\tImplementation Details\nCase Study\tPerformance Metrics Analysis\nCase Study\tKey Results\nCase Study\tResults Interpretation\nCase Study\tKey Findings\nCase Study\tSolution Impact Assessment\nCase Study\tSignificance and Implications of Work\nCase Study\tUncommon Insights\nB.9 Industry Product Showcase\nPublication Type\tCriterion Name\nIndustry Product Showcase\tClear Purpose and Objectives\nIndustry Product Showcase\tTarget Audience Definition\nIndustry Product Showcase\tClear Prerequisites and Requirements\nIndustry Product Showcase\tProblem Definition\nIndustry Product Showcase\tCurrent State Gap Identification\nIndustry Product Showcase\tContext Establishment\nIndustry Product Showcase\tDeployment Considerations\nIndustry Product Showcase\tTools, Frameworks, & Services\nIndustry Product Showcase\tImplementation Considerations\nIndustry Product Showcase\tConstraints, Boundaries, and Limitations\nIndustry Product Showcase\tSignificance and Implications of Work\nIndustry Product Showcase\tFeatures and Benefits Analysis\nIndustry Product Showcase\tCompetitive Differentiation\nIndustry Product Showcase\tOriginality of Work\nIndustry Product Showcase\tInnovation in Methods/Approaches\nIndustry Product Showcase\tAdvancement of Knowledge or Practice\nIndustry Product Showcase\tReal-World Applications\nIndustry Product Showcase\tTechnical Asset Access Links\nIndustry Product Showcase\tInstallation and Usage Instructions\nIndustry Product Showcase\tPerformance Characteristics and Requirements\nIndustry Product Showcase\tMaintenance and Support Status\nIndustry Product Showcase\tAccess and Availability Status\nIndustry Product Showcase\tLicense and Usage Rights of the Technical Asset\nIndustry Product Showcase\tContact Information of Asset Creators\nB.10 Solution Implementation Guide\nPublication Type\tCriterion Name\nSolution Implementation Guide\tClear Purpose and Objectives\nSolution Implementation Guide\tSpecific Objectives\nSolution Implementation Guide\tIntended Audience/Use Case\nSolution Implementation Guide\tProblem Definition\nSolution Implementation Guide\tCurrent State Gap Identification\nSolution Implementation Guide\tContext Establishment\nSolution Implementation Guide\tClear Prerequisites and Requirements\nSolution Implementation Guide\tStep-by-Step Guidance Quality\nSolution Implementation Guide\tData Requirements Specification\nSolution Implementation Guide\tDeployment Considerations\nSolution Implementation Guide\tTools, Frameworks, & Services\nSolution Implementation Guide\tImplementation Considerations\nSolution Implementation Guide\tSignificance and Implications of Work\nSolution Implementation Guide\tFeatures and Benefits Analysis\nSolution Implementation Guide\tReader Next Steps\nSolution Implementation Guide\tPurpose-Aligned Topic Coverage\nSolution Implementation Guide\tAppropriate Technical Depth\nSolution Implementation Guide\tCode Usage Appropriateness\nSolution Implementation Guide\tCode Clarity and Presentation\nSolution Implementation Guide\tCode Explanation Quality\nSolution Implementation Guide\tReal-World Applications\nSolution Implementation Guide\tContent Accessibility\nSolution Implementation Guide\tTechnical Progression\nSolution Implementation Guide\tScientific Clarity\nSolution Implementation Guide\tSource Credibility\nSolution Implementation Guide\tUncommon Insights\nB.11 Technical Deep-Dive\nPublication Type\tCriterion Name\nTechnical Deep-Dive\tClear Purpose and Objectives\nTechnical Deep-Dive\tSpecific Objectives\nTechnical Deep-Dive\tIntended Audience/Use Case\nTechnical Deep-Dive\tClear Prerequisites and Requirements\nTechnical Deep-Dive\tCurrent State Gap Identification\nTechnical Deep-Dive\tContext Establishment\nTechnical Deep-Dive\tMethodology Explanation\nTechnical Deep-Dive\tAssumptions Stated\nTechnical Deep-Dive\tSolution Approach and Design Decisions\nTechnical Deep-Dive\tImplementation Considerations\nTechnical Deep-Dive\tKey Results\nTechnical Deep-Dive\tResults Interpretation\nTechnical Deep-Dive\tConstraints, Boundaries, and Limitations\nTechnical Deep-Dive\tKey Findings\nTechnical Deep-Dive\tSignificance and Implications of Work\nTechnical Deep-Dive\tReader Next Steps\nTechnical Deep-Dive\tPurpose-Aligned Topic Coverage\nTechnical Deep-Dive\tAppropriate Technical Depth\nTechnical Deep-Dive\tCode Usage Appropriateness\nTechnical Deep-Dive\tCode Clarity and Presentation\nTechnical Deep-Dive\tCode Explanation Quality\nTechnical Deep-Dive\tReal-World Applications\nTechnical Deep-Dive\tSupporting Examples\nTechnical Deep-Dive\tContent Accessibility\nTechnical Deep-Dive\tTechnical Progression\nTechnical Deep-Dive\tScientific Clarity\nB.12 Technical Guide\nPublication Type\tCriterion Name\nTechnical Guide\tClear Purpose and Objectives\nTechnical Guide\tSpecific Objectives\nTechnical Guide\tIntended Audience/Use Case\nTechnical Guide\tClear Prerequisites and Requirements\nTechnical Guide\tContext Establishment\nTechnical Guide\tMethodology Explanation\nTechnical Guide\tImplementation Considerations\nTechnical Guide\tConstraints, Boundaries, and Limitations\nTechnical Guide\tKey Findings\nTechnical Guide\tSignificance and Implications of Work\nTechnical Guide\tReader Next Steps\nTechnical Guide\tPurpose-Aligned Topic Coverage\nTechnical Guide\tAppropriate Technical Depth\nTechnical Guide\tCode Usage Appropriateness\nTechnical Guide\tCode Clarity and Presentation\nTechnical Guide\tCode Explanation Quality\nTechnical Guide\tReal-World Applications\nTechnical Guide\tSupporting Examples\nTechnical Guide\tContent Accessibility\nTechnical Guide\tTechnical Progression\nTechnical Guide\tScientific Clarity\nB.13 Tutorial\nPublication Type\tCriterion Name\nTutorial\tClear Purpose and Objectives\nTutorial\tSpecific Objectives\nTutorial\tIntended Audience/Use Case\nTutorial\tContext Establishment\nTutorial\tClear Prerequisites and Requirements\nTutorial\tStep-by-Step Guidance Quality\nTutorial\tData Requirements Specification\nTutorial\tConstraints, Boundaries, and Limitations\nTutorial\tReader Next Steps\nTutorial\tPurpose-Aligned Topic Coverage\nTutorial\tAppropriate Technical Depth\nTutorial\tCode Usage Appropriateness\nTutorial\tCode Clarity and Presentation\nTutorial\tCode Explanation Quality\nTutorial\tReal-World Applications\nTutorial\tSupporting Examples\nTutorial\tContent Accessibility\nTutorial\tTechnical Progression\nTutorial\tScientific Clarity\nTutorial\tSource Credibility\nTutorial\tUncommon Insights\nB.14 Blog\nPublication Type\tCriterion Name\nBlog\tClear Purpose and Objectives\nBlog\tContext Establishment\nBlog\tPurpose-Aligned Topic Coverage\nBlog\tAppropriate Technical Depth\nBlog\tReal-World Applications\nBlog\tSupporting Examples\nBlog\tIndustry Insights\nBlog\tSuccess/Failure Stories\nBlog\tContent Accessibility\nBlog\tSource Credibility\nBlog\tReader Next Steps\nBlog\tUncommon Insights\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nTL;DR\n\nQuick Guide for Competition Participants\n\nIntroduction\n\n1.1 Guide Purpose and Scope\n\n1.2 Importance of Effective Presentation\n\nFoundations of Effective Project Presentation\n\n2.1 Core Tenets of Great Projects\n\n2.2 Project Types on Ready Tensor\n\n2.3 Selecting Type for Your Project\n\n2.4 Presentation Styles\n\nView all\nComments\n(1)\nYou might be interested\nTechnical Excellence in AI/ML Publications: An Evaluation Rubric by Ready Tensor\nFeb 10, 2025684 reads\nAI Documentation StandardsAI Project Evaluation+14\nCracking the Code of Technical Excellence: The AI/ML Research Evaluation Rubric Every Innovator Need\nMay 06, 202516 reads\n#AIEngineer#AIForAccessibility+15\nResearch-paper-assistant\nK\nFeb 19, 202513 reads\narXivebeginner+2\nPublish Like a Pro: Essential Steps for a Perfect Ready Tensor Publication\nMar 12, 202553 reads\nchecklistcommunity+2",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "enhancing-mri-analysis-with-multimodal-ai-and-computer-vision-HkiNwIJsZiTg",
    "username": "sSteven Ogwal",
    "license": "MIT License",
    "title": "Enhancing MRI Analysis with Multimodal AI and Computer Vision.",
    "publication_description": "Back to publications\nDec 07, 2024\n●\n78 reads\n●\nMIT License\nWinner of\nDistinguished Applied Solution Showcase\nat the\nComputer Vision Projects Expo 2024\nEnhancing MRI Analysis with Multimodal AI and Computer Vision.\nAI\nCNN\nCV\nHealth Care\nOpenAI\nYOLO\nS\nSteven Ogwal\nLike\nBookmark\nShare\nEnhancing MRI Analysis with Multimodal AI and Computer Vision.\n\nPhoto by \nAnna Shvets\n\n\n\n\nAbstract\n\nThis project presents an innovative approach to multimodal AI and Computer Vision pipeline to enhance medical imaging analysis, specifically targeting the early detection of brain tumors through MRI scans. By integrating convolutional neural networks (CNNs), advanced computer vision techniques, and vision-language models, our system achieves high classification accuracy and provides detailed diagnostic reports. The implementation demonstrates significant improvements in diagnostic speed and accuracy, with potential implications for clinical practice.\n\n1. Project Scope and Objectives\n\nThe objective of this project is to develop a Computer Vision and Multimodal AI solution that improves the accuracy and efficiency of brain tumor diagnosis from MRI scans. This include:\n\nTo develop a robust image detection model that accurately classifies different types of brain tumors.\nTo generate comprehensive diagnostic reports that assist clinicians in decision making.\nTo evaluate the model's performance and demonstrate its effectiveness.\nTo deploy and run model on cloud or locally on device.\n2. Technical Methodology and Implementation Details\n\nSetup\n\n#Uncomment to install the packages below \n# %pip install ultralytics\n# %pip install supervision\n# %pip install roboflow\n\nImport libraries\n\nfrom ultralytics import YOLO\nfrom roboflow import Roboflow\nimport supervision as sv\nimport cv2\nimport base64\nfrom io import BytesIO\nfrom PIL import Image as PILImage\n2.1 Data Collection and Preparation\n\nThe dataset consists of annotated MRI scans sourced from multiple medical institutions.\n\nKey preprocessing steps include:\n\nNormalization: Adjusting image intensities to standardize input for model training.\nArtifact Correction: Mitigating noise and distortions common in MRI imaging.\nData Augmentation: Applying transformations like rotations, flipping, and contrast adjustments to enhance model generalization.\nAnnotation: Annotating images with tumor-specific attributes such as type, size, and location.\nRoboflow API Key\n\nRoboflow is a comprehensive platform for building, training, and deploying computer vision models, which we will use to streamline our dataset management, annotation, and model deployment processes, ensuring efficiency and scalability in our project.\n\nTo load models with inference, you'll need a Roboflow API Key. Find instructions for retrieving your API key \nhere\n.\n\nrf = Roboflow(api_key=api_key)\nproject = rf.workspace(\"openvision-pkoxi\").project(\"live-brain-surgery\")\nversion = project.version(1)\ndataset = version.download(\"yolov8\")\n\nFig 1: Picure of dataset from \nRoboflow\n.\n\n2.2 Model Architecture\n\nWe used a hybrid architecture combining CNNs for image classification and segmentation with vision language models for report generation.\nThe architecture includes:\n\nYOLOv8 for Object Detection: Utilized for tumor detection and segmentation.\nVisual Language Model Workflow: Converts image data into natural language descriptions for diagnostic reporting.\n\nThe YOLOv8 architecture, developed by the Ultralytics team, is the latest iteration of the \"You Only Look Once\" (YOLO) family, renowned for its efficiency in real-time object detection. It is designed to identify and locate multiple objects within images rapidly and accurately. By reframing object detection as a single regression problem, YOLOv8 improves upon traditional methods that were often slow and computationally intensive.\n\nThe architecture consists of three main components:\n\nBackbone: Utilizes a custom CSPDarknet53, which enhances feature extraction through Cross-Stage Partial connections to improve information flow and accuracy.\nNeck: Incorporates a novel C2f module that merges feature maps from different layers to capture multi-scale information effectively.\nHead: Features multiple detection modules that predict bounding boxes, object scores, and class probabilities, improving detection accuracy, particularly for smaller objects.\n\nFig 2: Yolo architecture by \nyolov8.org\n\nOur visual language model workflow is a structured sequence of steps designed to process and analyze visual data, such as images or videos, using Yolov8 model and AI vision. The goal is to extract meaningful information from visual inputs and often convert this information into a form that can be understood or utilized by other medical professionals. Here’s a breakdown of what this workflow typically involves:\n\nInput: Submit an image or video.\nObject Detection Model: Detects objects in the image using a pre-trained model.\nBounding Box Visualization: Draws bounding boxes around detected objects.\nLabel Visualization: Adds labels to the bounding boxes.\nDetection Offset: Adjusts the bounding boxes.\nDynamic Crop: Crops the detected image based on detected objects for analysis.\nOpenAI: Uses OpenAI vision for further analysis.\nResponse: Outputs the results, including visualized images and text.\n\nFig 3: Picture of detection workflow\n\n2.3 Implementation\n# Load pre-trained YOLOv8 model\nmodel = YOLO(\"yolov8x.pt\")\n# Display model information (optional)\nmodel.info()\n2.3.1 Model Training\n\nTraining a deep learning model requires supplying it with data and fine-tuning its parameters to improve prediction accuracy. According to the documentation for Ultralytics YOLOv8, the model's training mode is specifically designed for the effective and efficient training of object detection models, maximizing the use of modern hardware capabilities (Ultralytics, n.d.).\n\nThe command below initiates training of a YOLOv8 model, where \"data\" refers to the path of the dataset configuration file, \"epochs\" indicates the number of complete passes through the training dataset, and \"device\" specifies the hardware used for training, in this case, utilizing Metal Performance Shaders (MPS) for GPU acceleration\n\n# Train the model with MPS\nresults = model.train(data=\"Live-Brain-Surgery-1/data.yaml\", epochs=50, device=\"mps\")\n\nAlternatively, training can be done using roboflow.\n\nRoboflow offers a streamlined, low-code approach to training YOLO models by enabling users to upload and annotate images, generate dataset versions, export data in the necessary format, and utilize command-line tools for model training, significantly simplifying the machine learning workflow (Skalski, 2023; Gallagher, 2024).\n\nFig 4: Image of model training graph.\n\n2.3.2 Model Validation\n\nValidation is an important step in the machine learning pipeline, as it provides a means to evaluate the performance and reliability of trained models.\n\n# Load a model\nmodel = YOLO(\"path/to/best.pt\")  # load our trained model\n\n# Validate the model\nmetrics = model.val()\nmetrics.box.maps\n\nFig 5: Image of model validation metrics.\n\n3. Evaluation Metrics and Results\n3.1 Performance Metrics\n\nFig 6: Image of model performance metrics.\n\nThe model's performance was evaluated using the following metrics:\n\nmAP (mean Average Precision): 63.4%\nThis metric measures the model's ability to make accurate predictions across all classes. It combines precision and recall to evaluate the overall accuracy of predictions at different confidence levels. A higher mAP indicates better performance.\n\nPrecision: 89.3%\nThis quantifies how many of the model's positive predictions are correct. High precision means the model makes fewer false positive predictions.\n\nRecall: 60.2%\nThis measures how many of the actual positive instances are correctly predicted by the model. High recall means the model detects most of the true instances, even at the cost of more false positives.\n\n3.2 Results Summary\n\nThe model achieves a high precision of 89.3%, but the recall of 60.2% indicates it misses some detections, leading to a mAP of 63.4%. To improve performance, I plan to augment the dataset with more diverse dataset, address potential class imbalances, and fine tune key hyperparameters like the confidence threshold and learning rate. Additionally, experimenting with multi-scale training, higher resolution images, and techniques like focal loss could help balance precision and recall for better overall results.\n\n4. Model Testing and Deployment\n\nIn machine learning and computer vision, deployment and testing are crucial steps in the lifecycle of machine learning (ML) models, ensuring that they perform well in real-world applications. The process of making sense out of visual data is called 'inference' or 'prediction'.\n\nWe will utilize Inference to facilitate the deployment of our model. This simplifies the process by providing a user-friendly interface and robust infrastructure, allowing for efficient execution of the model across various platforms ensuring that our model is readily accessible and operational in real world applications.\n\n# %pip install inference\n# %pip install inference-cli && inference server start\nfrom inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    api_key=api_key\n)\n\nresult = client.run_workflow(\n    workspace_name=\"openvision-pkoxi\",\n    workflow_id=\"brain-surgery-workflow\",\n    images={\n        \"image\":\"test-img.jpg\" \n    }\n)\nImage Analysis Stage\n\nWe employ a deep learning-based approach to analyze MRI scans\n\n# Decode the base64 string\nlabel_visualization = result[0]['label_visualization']\ndynamic_crop = result[0]['dynamic_crop'][0]\n# show predicton image and label\nimage_bytes1 = base64.b64decode(label_visualization)\n\n# Create an image from the bytes\nimage1 = PILImage.open(BytesIO(image_bytes1))\n\n# Display the image\ndisplay(image1)\n# show detected image\nimage_bytes2 = base64.b64decode(dynamic_crop)\n\n# Create an image from the bytes\nimage2 = PILImage.open(BytesIO(image_bytes2))\n\n# Display the image\ndisplay(image2)\nVision Language Integration Results\n\nFig 7: Image showing workflow results.\n\ndef convert_pixels_to_mm(bounding_box, conversion_factor):\n    \"\"\"\n    Convert pixel dimensions to millimeters.\n\n    Parameters:\n    bounding_box (dict): A dictionary containing the bounding box coordinates with keys 'width', 'height', 'x', and 'y'.\n    conversion_factor (float): The pixel-to-millimeter conversion factor (mm/pixel).\n\n    Returns:\n    dict: A dictionary containing the width, height, and area in millimeters.\n    \"\"\"\n    # Extract the dimensions from the bounding box\n    width_px = bounding_box['width']\n    height_px = bounding_box['height']\n\n    # Convert width and height from pixels to millimeters\n    width_mm = width_px * conversion_factor\n    height_mm = height_px * conversion_factor\n\n    # Calculate the area in square millimeters\n    area_mm2 = width_mm * height_mm\n\n    # Return the dimensions and area in millimeters\n    return area_mm2\nprediction = result[0]['model_predictions']['predictions']['predictions'][0]\nconfidence = result[0]['model_predictions']['predictions']['predictions'][0]['confidence']\nopen_ai = result[0]['open_ai']\nwidth_px = prediction['width']\nheight_px = prediction['height']\nx_px = prediction['x']\ny_px = prediction['y']\ndetection_id = prediction['detection_id']\npredition_class = prediction['class']\nsize = convert_pixels_to_mm(prediction,0.1)\n\nprint(\"Below are the scan results:\")\nprint(\"_ _\"*30)\n\nprint(f\"ID:           {detection_id}\")\nprint(f\"Prediction:   {predition_class.upper()}\")\nprint(f\"Confidence:   {confidence} ~ {int(confidence * 100)}%\")\nprint(f\"Size:         {size}mm²\")\nprint(f\"Morphology:\")\nfor res in open_ai:\n    print(f\"- {res}\")\n\nprint(\"_ _\"*30)\nBelow are the scan results:\n_ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ _\nID:           fdb02240-c9ed-41f8-abc4-ae68524bdf06\nPrediction:   PITUITARY\nConfidence:   0.949749231338501 ~ 94%\nSize:         47mm²\nMorphology:\n- The image displays an irregular, roughly circular shape with a blurred appearance.\n_ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ _\n\nLive Demo:\nTest Workflow\n\nThis demo uses the inference sdk.\n\nYou can also test the deployed workflow locally:\n\n# Import the InferencePipeline object\nfrom inference import InferencePipeline\nimport cv2\n\ndef video_sink(result, video_frame):\n    if result.get(\"output_image\"): # Display an image from the workflow response\n        cv2.imshow(\"Workflow Image\", result[\"output_image\"].numpy_image)\n        cv2.waitKey(1)\n    print(result) # do something with the predictions of each frame\n    \n\n# initialize a pipeline object\npipeline = InferencePipeline.init_with_workflow(\n    api_key=api_key,\n    workspace_name=\"openvision-pkoxi\",\n    workflow_id=\"brain-surgery-workflow\",\n    video_reference='test-img.png', # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url\n    max_fps=30,\n    on_prediction=video_sink\n)\npipeline.start() #start the pipeline\npipeline.join() #wait for the pipeline thread to finish\n5. Impact Analysis and Future Directions\n\nThis project has the potential to transform clinical workflows by providing rapid and accurate diagnostic support for brain tumors.\nFuture work will focus on:\n\nExpanding the dataset to include diverse imaging modalities (e.g., PET scans).\nImplementing federated learning to enhance model accuracy and privacy.\nConducting real-world clinical trials to validate the model's effectiveness in practice.\n\nPhoto by \nIsabella Mendes\n.\n\n6. Conclusion\n\nThis project demonstrates the transformative potential of multimodal AI and Computer Vision in medical imaging analysis. By leveraging advanced computer vision techniques and visual language model processing, we have developed a robust solution that addresses key challenges in brain tumor diagnosis.\n\nWe believe that our solution has the potential to revolutionize brain cancer diagnosis and treatment, enabling clinicians to make more informed decisions and improve patient outcomes.\n\nReferences\n\nJocher, G., Chaurasia, A., & Qiu, J. (2023). Ultralytics YOLOv8 (Version 8.0.0) [Software]. \nhttps://github.com/ultralytics/ultralytics\n (AGPL-3.0 License)\n\nRoboflow. (2023). Supervision. \nhttps://github.com/roboflow/supervision\n (MIT License)\n\nBrain tumor detection. (2024, July). Tumor Detection Dataset [Open Source Dataset]. Roboflow Universe. \nhttps://universe.roboflow.com/brain-tumor-detection-wsera/tumor-detection-ko5jp\n (Visited on 2024-12-03)\n\nUltralytics. (n.d.). YOLOv8 architecture. Retrieved December 3, 2024, from \nhttps://yolov8.org/yolov8-architecture/\n\nSkalski, P. (2023). How to Train YOLOv8 Object Detection on a Custom Dataset. Roboflow Blog. \nhttps://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/\n\nGallagher, J. (2024). How to Train a YOLOv11 Object Detection Model on a Custom Dataset. Roboflow Blog. \nhttps://blog.roboflow.com/yolov11-how-to-train-custom-data/\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nEnhancing MRI Analysis with Multimodal AI and Computer Vision.\n\nAbstract\n\nProject Scope and Objectives\nTechnical Methodology and Implementation Details\n\n2.1 Data Collection and Preparation\n\nRoboflow API Key\n\n2.2 Model Architecture\n\n2.3 Implementation\n\n2.3.1 Model Training\n\n2.3.2 Model Validation\n\nView all\nCode\nDatasets",
    "awards": [
      "distinguished applied solution showcase"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "enhancing-satellite-image-generation-with-lora-and-advanced-augmentation-techniques-yVIFZnj8yhdb",
    "username": "rBogdan Roshchupkin",
    "license": "No License",
    "title": "Enhancing Satellite Image Generation with LoRA and Advanced Augmentation Techniques",
    "publication_description": "Back to publications\nDec 29, 2024\n●\n98 reads\n●\nNo License\nWinner of\nBest Technical Implementation\nat the\nComputer Vision Projects Expo 2024\nEnhancing Satellite Image Generation with LoRA and Advanced Augmentation Techniques\nArtificial Intelligence\nData Preprocessing\nDeep Learning\nImage Generation\nImage Synthesis\nLoRA\nSatellite Imaging\nStable Diffusion\nR\nBogdan Roshchupkin\nA\n@andrejutlakov96\nLike\nBookmark\nShare\nEnhancing Satellite Image Generation with LoRA and Advanced Augmentation Techniques\n1. Introduction\n1.1 Context and Motivation\n\nAt our workplace, a bank provides credit to entrepreneurs engaged in gold mining within designated areas. However, these entrepreneurs risk losing their mining licenses due to potential violations such as trespassing or environmental pollution, including improper waste disposal into water bodies or soil contamination. If a license is lost, the bank won't be able to return the credit, leading to irretrievable losses. Detecting such violations is challenging due to the scarcity of relevant satellite image data needed for accurate identification and monitoring.\nGold mining and borders(red color) of multiple entrepreneurs\n\n\nTo address this issue, we aimed to develop a model capable of identifying unauthorized mining activities and environmental breaches. The primary challenge was the lack of sufficient labeled satellite images to train an effective detection system. To overcome this data scarcity, we employed advanced data augmentation techniques and leveraged Stable Diffusion to generate synthetic satellite images. Additionally, we integrated Low-Rank Adaptation (LoRA).\n\n1.2 Objectives of the Article\n\nThis article explores the application of LoRA in generating high-quality satellite images by integrating it with the YOLO (You Only Look Once) object detection model and advanced data augmentation techniques. Specifically, it aims to:\n\nExplain LoRA and its benefits in model adaptation.\nProvide an overview of the YOLO model and its relevance to satellite image generation.\nDetail the data preparation and advanced augmentation processes.\nDescribe the integration of LoRA with YOLO for improved image generation.\nPresent and analyze results, showcasing performance metrics and visual comparisons.\nDiscuss challenges faced and suggest future research directions.\n\nBy the end of this article, readers will understand how combining LoRA with YOLO and sophisticated augmentation methods can significantly enhance satellite image generation, making it more detailed, realistic, and computationally efficient.\n\n2. Overview of Technologies\n2.1 What is LoRA?\n\nLow-Rank Adaptation (LoRA) is a technique designed to improve the adaptability and efficiency of large pre-trained models. Traditional fine-tuning adjusts all model parameters, which is computationally intensive and requires substantial memory. LoRA introduces low-rank matrices to adapt the model's weights with minimal overhead, enabling efficient fine-tuning without retraining the entire model.\n\nAdvantages of LoRA:\n\nEfficiency: Reduces the number of trainable parameters, decreasing training time and memory usage.\nScalability: Facilitates fine-tuning of very large models that are otherwise impractical to adapt.\nFlexibility: Applicable to various neural network types, including transformers and convolutional networks.\n\nIn satellite image generation, LoRA allows fine-tuning pre-trained models to produce high-resolution images efficiently, enhancing model performance without excessive computational costs.\n\n2.2 Connection between LoRA and YOLO\n\nBy merging these technologies, the project aims to achieve superior performance in satellite image generation, making high-quality imagery more accessible and affordable.\n\nGenerate or augment images using Stable Diffusion and LoRa.\nLabel synthetic data using annotation tools (e.g., LabelImg) or scripts.\nIntegrate synthetic data with the real dataset.\nUse this enriched dataset to train YOLO, leading to a model that generalizes better.\n\nBenefits of This Connection\n\nOvercomes challenges like imbalanced datasets.\nReduces dependency on real-world data collection (which can be time-consuming and costly).\nMakes the YOLO model adaptable to specific domains or scenarios where real data is scarce.\n3. Methodology\n3.1 Data Preparation\n\nThe foundation of our project lies in the collection and preparation of satellite images relevant to gold mining activities. We sourced high-resolution satellite images from publicly available datasets and proprietary sources, focusing on areas designated for mining operations.\n\nKey Steps in Data Preparation:\n\nImage and Mask Loading: We utilized the rasterio library to handle geospatial data formats, loading both satellite images and their corresponding masks.\n\nFiltering and Splitting: Due to the high resolution of satellite images, we divided them into smaller blocks of 512x512 pixels using custom scripts. This approach ensures manageable data sizes and maintains the integrity of the original imagery.\n\nSaving Processed Data: The processed images and masks were systematically saved into designated directories for training, validation, and testing.\n\n# Function to divide and filter images\ndef divide_image(image, mask, split, image_basename, image_save_dir, mask_save_dir, filtered_image_dir, filtered_mask_dir, threshold):\n    # Detailed implementation can be seen on GitHub, hidden for readability purpose\n\n# Creating output directories\nfor split in ['train', 'val', 'test']:\n    os.makedirs(os.path.join(output_dirs[split], 'images'), exist_ok=True)\n    os.makedirs(os.path.join(output_dirs[split], 'masks'), exist_ok=True)\n    os.makedirs(os.path.join(output_dirs[f'{split}_filtered'], 'images'), exist_ok=True)\n    os.makedirs(os.path.join(output_dirs[f'{split}_filtered'], 'masks'), exist_ok=True)\n\n# Processing and dividing images\nfor split in ['train', 'val', 'test']:\n    image_dir = os.path.join(dataset_dir, split, 'images')\n    mask_dir = os.path.join(dataset_dir, split, 'masks')\n    \n    output_image_dir = os.path.join(output_dirs[split], 'images')\n    output_mask_dir = os.path.join(output_dirs[split], 'masks')\n\n    filtered_image_dir = os.path.join(output_dirs[f'{split}_filtered'], 'images')\n    filtered_mask_dir = os.path.join(output_dirs[f'{split}_filtered'], 'masks')\n\n    for image_name in tqdm(os.listdir(image_dir), desc=f\"Processing {split}\"):\n        if image_name.lower().endswith('.png'):\n            image_path = os.path.join(image_dir, image_name)\n            mask_name = image_name.replace(\"train_image\", \"train_mask\").replace(\"val_image\", \"val_mask\").replace(\"test_image\", \"test_mask\")\n            mask_path = os.path.join(mask_dir, mask_name)\n\n            if not os.path.exists(mask_path):\n                logging.warning(f'Mask file not found: {mask_path}')\n                continue\n\n            try:\n                image = Image.open(image_path).convert(\"RGB\")\n                mask = Image.open(mask_path).convert(\"L\")\n            except Exception as e:\n                logging.error(f\"Error opening {image_path} or {mask_path}: {e}\")\n                continue\n\n            image_basename = os.path.splitext(image_name)[0]\n            divide_image(image, mask, split, image_basename, output_image_dir, output_mask_dir, filtered_image_dir, filtered_mask_dir, threshold=filtered_threshold)\n\n# Logging and statistics\ndef print_folder_stats(output_dirs):\n    # Detailed implementation can be seen on GitHub, hidden for readability purpose\n\nprint_folder_stats(output_dirs)\n3.2 Advanced Augmentation Techniques\n\nData augmentation is pivotal in enhancing the diversity and robustness of the training dataset, allowing models to generalize better to unseen data. Given the limited availability of real satellite images for gold mining detection, we implemented several advanced augmentation techniques using the albumentations library.\n\nAugmentation Pipeline:\n\nGrid Distortion: Warps the image using grid-based transformations, simulating real-world distortions.\nElastic Transform: Applies random elastic deformations, enhancing the model's ability to handle non-linear transformations.\nFlip: Randomly flips images horizontally or vertically to increase data variability.\nAffine Transformation: Applies scaling, rotation, and translation, making the model invariant to these changes.\nChannel Mixing: Randomly swaps color channels, mimicking different lighting conditions or sensor characteristics.\nfrom albumentations import Compose, GridDistortion, ElasticTransform, Flip, Affine, RandomBrightnessContrast\n\n# Define augmentation pipeline\ntransform = Compose([\n    GridDistortion(p=0.5),\n    ElasticTransform(p=0.5),\n    Flip(p=0.5),\n    Affine(\n        rotate=(-10, 10),\n        translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n        p=0.5\n    ),\n    RandomBrightnessContrast(p=0.5)\n])\n\ndef augment_image(image):\n    augmented = transform(image=image)\n    return augmented['image']\n\nThis what we get after implementing augmentation and pasting cropped objects for making augmentation more diverse:\n\n3.3 Training YOLO\n\nTraining the YOLO model involved several key steps to ensure effective detection of unauthorized mining activities and environmental breaches.\n\nTraining Process\n\nModel Initialization: We initialized the YOLO model with pre-trained weights to leverage existing knowledge, accelerating the training process and enhancing initial performance.\n\nData Integration: Combined real and synthetic (augmented) satellite images to create a balanced and comprehensive training dataset. This approach mitigates data scarcity and enhances the model's ability to generalize across diverse scenarios.\n\nTraining Configuration: Configured training parameters, including learning rate, batch size, and number of epochs, to optimize model performance.\n\nTraining Loop: Executed the training process, monitoring performance metrics such as Class, Box Precision (P), mAP50. The training aimed to maximize these metrics, ensuring robust detection capabilities.\n\nPerformance Metrics After Training\n\nTo evaluate the model's effectiveness, we assessed its performance using standard metrics. The results are summarized below:\n\nCategory\tBox (P)\tmAP50\nAll\t0.81\t0.80\nClouds\t0.74\t0.97\nGold\t0.89\t0.70\nPoisonous Water\t0.68\t0.665\nWater\t0.89\t0.86\n3.4 Integration of LoRA\n\nIntegrating Low-Rank Adaptation (LoRA) with the YOLO model significantly enhanced our ability to generate and detect high-resolution satellite images tailored to our specific requirements. This integration involved training the Stable Diffusion model on our dataset to generate synthetic satellite images, manually annotating these images, and subsequently retraining the YOLO model with the enriched dataset. The training process was adapted from the great book \"Using Stable Diffusion with Python. Leverage Python to control and automate high-quality AI image generation using Stable Diffusion,\" written by Andrew Zhu(Shudong Zhu), which also details various inference optimization techniques we employed.\n\nTraining the Stable Diffusion Model\n\nTo generate realistic and context-specific satellite images, we fine-tuned the Stable Diffusion model using our curated dataset of satellite images related to gold mining activities. The fine-tuning process included the following steps:\n\nDataset Preparation: Collected and organized a comprehensive set of high-resolution satellite images from designated gold mining areas. Ensured diversity in environmental conditions and mining scenarios to improve the model's generalization capabilities.\n\nModel Fine-Tuning: Leveraged the diffusers library to fine-tune the Stable Diffusion model on our dataset. This process involved configuring hyperparameters such as learning rate, batch size, and number of epochs to optimize image generation quality.\n\nLoRA Configuration: Applied LoRA to the Stable Diffusion model to enable efficient fine-tuning with minimal computational overhead. This adaptation allowed us to enhance the model's performance without extensive retraining.\n\nKey Code Snippet for Training Stable Diffusion with LoRA:\n\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDPMScheduler\nfrom peft import LoraConfig\nfrom peft.utils import get_peft_model_state_dict\nfrom accelerate import Accelerator\n\n# Initialize Accelerator\naccelerator = Accelerator(gradient_accumulation_steps=4, mixed_precision=\"no\")\ndevice = accelerator.device\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=4,\n    lora_alpha=4,\n    init_lora_weights=\"gaussian\",\n    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n)\n\n# Load pre-trained Stable Diffusion model\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype=torch.float32\n).to(device)\n\n# Add LoRA adapter to the Unet component\npipe.unet.add_adapter(lora_config)\nprint(\"LoRA adapter added to Unet.\")\n\nExample of generated image:\n\n\n4.1 Performance Metrics Before and After LoRA\n\nTo evaluate the effectiveness of integrating LoRA with the YOLO model, we compared the performance metrics before and after the integration across all categories. The results indicate noticeable improvements in detection accuracy and reliability.\n\nComparative Analysis of YOLO Configurations\n\nYOLO without LoRA\nCategory\tBox (P)\tmAP50\nAll\t0.80\t0.79\nClouds\t0.725\t0.959\nGold\t0.87\t0.67\nPoisonous Water\t0.663\t0.6\nWater\t0.875\t0.86\nYOLO with Advanced Augmentation\nCategory\tBox (P)\tmAP50\nAll\t0.81\t0.80\nClouds\t0.74\t0.97\nGold\t0.89\t0.70\nPoisonous Water\t0.68\t0.665\nWater\t0.89\t0.86\nYOLO with LoRA and Advanced Augmentation\nCategory\tBox (P)\tmAP50\nAll\t0.827\t0.818\nClouds\t0.77\t0.995\nGold\t0.913\t0.718\nPoisonous Water\t0.706\t0.67\nWater\t0.921\t0.891\n5. Discussion\n5.1 Advantages of Using LoRA in Stable Diffusion\nEnhanced Efficiency: LoRA enables fine-tuning of large models like Stable Diffusion with minimal computational overhead, making it feasible to adapt for specific tasks without extensive resource investment.\nImproved Performance: The integration of LoRA significantly boosted key performance metrics, demonstrating its effectiveness in enhancing model accuracy and robustness in satellite image generation.\nScalability: LoRA's flexibility allows for easy adaptation of models to various satellite image generation tasks, accommodating different resolutions and environmental conditions without the need for complete retraining.\nData Enrichment: By generating synthetic images with Stable Diffusion, LoRA helps overcome data scarcity, providing a more balanced and comprehensive training dataset that improves the YOLO model's generalization capabilities.\n5.2 Challenges and Limitations\nData Quality: The success of augmentation techniques and synthetic image generation is highly dependent on the quality and diversity of the original dataset. Limited variability can still constrain model generalization.\nComputational Resources: While LoRA reduces computational demands, training high-resolution models with Stable Diffusion remains resource-intensive, requiring powerful hardware and optimized training strategies.\nIntegration Complexity: Combining multiple advanced techniques (LoRA, YOLO, Stable Diffusion) introduces complexity in the training pipeline, necessitating careful calibration to ensure harmonious performance improvements.\nManual Annotation: The process of manually annotating synthetic images is time-consuming and may introduce human error, potentially affecting the quality of the training data.\n6. Conclusion\n\nIn this article, we explored the integration of Low-Rank Adaptation (LoRA) with the YOLO model and advanced augmentation techniques to enhance satellite image generation. By leveraging Stable Diffusion, we achieved the generation of high-resolution, realistic satellite images with improved performance metrics. Our approach not only demonstrates the efficacy of LoRA in fine-tuning large models but also underscores the importance of robust data augmentation in machine learning workflows.\n\nThe combination of these technologies paves the way for more efficient and scalable satellite image generation solutions, with potential applications spanning environmental monitoring, urban planning, and compliance enforcement. As we continue to refine these methods, future research will focus on overcoming current limitations and expanding the applicability of our approach to broader domains.\n\nFor more technical details and access to the project code, please visit our Github: \nhttps://github.com/jettooss/stable_diffusion_satellite_images/tree/main\n\nrepository.\n\nResult of the project\n\nOverall result includes the whole app, using Leaflet in frontend part. Here you can see how we detect violations on Russian Far East territory: \nhttps://drive.google.com/file/d/1EztfKvIE9mN8j3p1MvQRDZTYIZV9MU95/view?usp=sharing\n\nDataset access\n\nIf you want to download the dataset, please click on download in google drive and then we will provide you the permission\n\nFAQ:\nQ1: Why did you choose YOLOv8 for this project?\n\nA1: YOLOv8 (You Only Look Once version 8) was chosen for its real-time object detection capabilities, high accuracy, and versatility. Additionally, I have extensive experience with YOLOv8, not only at work but also in hackathons where it consistently delivered outstanding results, regardless of how arduous the dataset was. This familiarity and proven performance made YOLOv8 the ideal choice for detecting unauthorized mining activities and environmental breaches in satellite images.\n\nQ2: What challenges did you encounter while integrating LoRA with YOLOv8 and Stable Diffusion?\n\nA2: Integrating LoRA with YOLOv8 and Stable Diffusion proved to be quite challenging. The process was complex and required a deep understanding of both model architectures and their training pipelines. To overcome these difficulties, we referred to the comprehensive book by Andrew Zhu(Shudong Zhu) \"Using Stable Diffusion with Python. Leverage Python to control and automate high-quality AI image generation using Stable Diffusion.\" This resource was instrumental in successfully implementing the training part of the integration, enabling us to fine-tune the models effectively.\n\nQ3: Can this methodology be applied to other domains beyond gold mining detection?\n\nA3: Yes, the methodology of integrating LoRA with YOLOv8 and using advanced augmentation techniques can be applied to various domains that require high-quality image generation and object detection, such as environmental monitoring, urban planning, agriculture, and disaster management. The flexibility and scalability of this approach make it adaptable to different scenarios and datasets, allowing for broad applicability across diverse fields.\n\nQ4: How do you ensure the quality of synthetic images generated by Stable Diffusion?\n\nA4: We ensure the quality of synthetic images by combining quantitative and qualitative evaluation methods. After generating the images with Stable Diffusion, we assessed them by analyzing key performance metrics and conducting visual inspections. This dual approach ensures that the synthetic images accurately reflect potential violations and environmental conditions, maintaining high standards of reliability and realism in the training dataset.\n\nQ5: What inference optimization techniques did you use for Stable Diffusion?\n\nA5: The inference optimization techniques employed are detailed in the guide \"Using Stable Diffusion with Python. Leverage Python to control and automate high-quality AI image generation using Stable Diffusion.\" These techniques include efficient model loading, leveraging GPU acceleration, and implementing memory management strategies to enhance the speed and efficiency of image generation. By following these optimized practices, we ensured that the Stable Diffusion model operates effectively within our computational constraints.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nEnhancing Satellite Image Generation with LoRA and Advanced Augmentation Techniques\n\nIntroduction\n\n1.1 Context and Motivation\n\n1.2 Objectives of the Article\n\nOverview of Technologies\n\n2.1 What is LoRA?\n\n2.2 Connection between LoRA and YOLO\n\nMethodology\n\n3.1 Data Preparation\n\n3.2 Advanced Augmentation Techniques\n\nView all\nComments\nCode\nDatasets\nYou might be interested\nYOLOv8 Model for Object Detection\nO\nMay 26, 202515 reads\nMarine Pollution Detection\nN\nDec 30, 202422 reads\nCLIPEnvironmental+5\nMonitoring and Detecting Glacier Changes Using YOLOv11 and Custom Dataset\nS\nDec 29, 202417 reads\nartificial intelligencecomputer vision+7\nObject detection(computer Vision)\nA\nDec 29, 20246 reads",
    "awards": [
      "best technical implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "evaluation-of-precision-accuracy-in-classifying-breast-histopathology-images-using-mobilenetv3-GmSh2pkI8ru4",
    "username": "gGary Takahashi",
    "license": "No License",
    "title": "",
    "publication_description": "Back to publications\nSep 15, 2024\n●\n93 reads\n●\nNo License\nEvaluation of precision & accuracy in classifying breast histopathology images using MobileNetV3\nbreast cancer\ncomputer vision\nmobilenet v3\nG\nGary Takahashi\nK\nKenneth Devoe\nE\nEbrahim Tarshizi\nLike\nBookmark\nShare\nAbstract\n\nAccurate surgical pathological assessment of breast biopsies is essential to the proper management of breast lesions. Identifying histological features, such as nuclear pleomorphism, increased mitotic activity, cellular atypia, patterns of architectural disruption, as well as invasion through basement membranes into surrounding stroma and normal structures, including invasion of vascular and lymphatic spaces, help to classify lesions as malignant. This visual assessment is repeated on numerous slides taken at various sections through the resected tumor, each at different magnifications. Computer vision models have been proposed to assist human pathologists in classification tasks such as these. Using MobileNetV3, a convolutional architecture designed to achieve high accuracy with a compact parameter footprint, we attempted to classify breast cancer images in the BreakHis_v1 breast pathology dataset to determine the performance of this model out-of-the-box. Using transfer learning to take advantage of ImageNet embeddings without special feature extraction, we were able to correctly classify histopathology images broadly as benign or malignant with 0.98 precision, 0.97 recall, and an F1 score of 0.98. The ability to classify into histological subcategories was varied, with the greatest success being with classifying ductal carcinoma (accuracy 0.95), and the lowest success being with lobular carcinoma (accuracy 0.59). Multiclass ROC assessment of performance as a multiclass classifier yielded AUC values ≥0.97 in both benign and malignant subsets. In comparison with previous efforts, using older and larger convolutional network architectures with feature extraction pre-processing, our work highlights that modern, resource-efficient architectures can classify histopathological images with accuracy that at least matches that of previous efforts, without the need for labor-intensive feature extraction protocols. Suggestions to further refine the model are discussed.\n\nIntroduction\n\nBreast cancer is the most common cancer afflicting females, second only to skin cancers, and now afflicts nearly 1 in 3 women each year.1 This condition usually presents either as a palpable breast mass, or, as is more often the case, detected by screening mammography. Once a lesion is identified, a core needle biopsy is performed to assess the nature of the abnormality, and a surgical pathologist is tasked with precise identification of the lesion, as well as a panel of an associated standardized set of biomarker phenotypes.2,3 Tumor behavior can reasonably be predicted by evaluating the histological features such as cellular morphology, the degree and nature of architectural distortion, as well as invasion through basement membranes into normal structures, including blood vessels and lymphatics.4 It is pertinent to note that each case typically involves the evaluation of numerous slides involving various sections through the tumor, each at different magnifications, as features such as architectural patterns are more apparent with the lower power widefield objective, while cellular and nuclear detail are better evaluated using the higher power objectives. Making an accurate diagnosis in a timely manner is critical, as appropriate intervention when curative management is possible can lead to improved survival outcomes.5\n\nInterpretation of mammographic imaging has benefited from computer-assisted identification of suspicious radiographic lesions.6 Deep learning may also assist the pathologist in histological classification of surgical biopsies.7 Previous efforts have utilized convolutional neural net (CNN) algorithms to broadly classify breast cancer pathological images into benign and malignant categories, using larger image processing architectures that have been used to successfully classify non-medical, more everyday images. Convolutional networks have formed the basis for virtually all computer vision architectures since the AlexNet model famously won the ImageNet competition in 2012, achieving an unprecedented top-5 error rate of 17.0%.8 Since then, other groups have built upon this concept,9 and subsequent years saw the appearance of newer architectures, such as deep residual networks10 and inception networks,11 among others. These networks were tested on the ImageNet dataset, a large publicly available collection of 14.2 million labeled images across 21 841 synsets (categories),12,13 and this resource has been a benchmark for evaluating new computer vision architectures.\n\nConvolutional models excel by first learning to detect pertinent features of an image, such as edges, textures, and shapes. Deeper layers in the architecture then become trained to detect combinations of these features.14 To enhance edges and contrast along the borders of features of interest, feature extraction techniques have been used. Semantic segmentation has been used to selectively apply coloration to specific features, such as the nuclei, or to the emphasize the boundaries of an infiltrating tumor mass.15,16 By enhancing the contrast of the borders between 2 regions of concern, this could theoretically increase classification accuracy, but this comes at the cost of time spent pre-processing the data in this manner, and the time and effort needed to confirm the accuracy and precision of the segmentation. The ability to accurately classify pathology images without the need for special pre-processing, such as segmentation, would greatly enhance the ease and efficiency of machine-learning image classification.\n\nMethodology\n\nBreakHis_v1 (BreakHis) is a large dataset consisting of 9109 microscopic images collected from 82 patients. Of these, 7909 images were available for public downloading.17 This dataset includes 2480 images of benign breast lesions, and 5429 images of malignant lesions. Regions of interest (ROIs) have been identified by a pathologist, and the images were obtained at the indicated magnifications (Fig. 1).\n\nFigure 1\n\nBreaKHis dataset data distribution.\n\nBenign breast lesions were divided into four categories: adenosis, fibroadenoma, phyllodes tumor, and tubular adenoma. The malignant conditions selected for inclusion were ductal carcinoma, lobular carcinoma, mucinous carcinoma, and papillary carcinoma. Each histological category was represented in four magnification levels: 40×, 100×, 200×, and 400×. These refer to the microscope objective magnification multiplied by the 10× ocular lens (Fig. 2).\n\nFigure 2\n\nExamples from the BreaKHis_v1 ductal carcinoma dataset, seen at magnifications of: (a) 40×, (b) 100×, (c) 200×, (d) 400×. Lower magnifications more clearly display architectural details of tumor penetration into normal breast tissue, while at higher magnifications, cellular and nuclear detail become more important. Presence of metastatic cells in the lymphatic channels and capillaries is better appreciated at the higher magnifications.\n\nWe selected the BreakHis dataset as the most suitable for our objectives, as other breast cancer datasets that have been used in other publications either were no longer available, contained fewer or smaller images for classification, offered only binary classifications (i.e., benign vs. malignant) or were better suited for the detection of metastatic spread in the background of lymphoid tissue.\n\nExploratory data analysis\n\nIt is necessary to evaluate histopathological images at various levels of magnification. Images examined at low magnification may reveal glandular structures, stromal invasion, and architectural distortion by the neoplasm. Higher magnifications may reveal the presence of mitotic figures, nuclear atypia, degree of cellular differentiation, and abnormal growth patterns, as well as the presence or absence of invasion through basement membranes (signifying metastatic spread) as well as abnormal tumor invasion through lymphatic or vascular channels. Because each magnification contributes useful information that would be important to classification, we did not restrict the analysis to specific magnifications, but elected to train the model on all magnifications collectively.\n\nExamining multiple random samples from the dataset revealed that the images were of good quality and suitable for training. The size of each image in the BreakHis_v1 dataset is 700 × 460 pixels and is stored in PNG format. MobileNetV3 requires images of equal height and width, with three channels for RGB color representation, and the dataset images were resized accordingly. The crop_to_aspect_ratio parameter was left at the default setting, allowing for width compression so that the software would not crop images indiscriminately, potentially eliminating key histological features from being available for training.\n\nIn prior studies, feature extraction has been utilized in other efforts as a means of strengthening features, such as contrast enhancement to emphasize edges or semantic segmentation to highlight regions of interest (ROIs). Implementation of this step has required input from trained pathologists to confirm the correct identification of ROI, and in the setting of metastatic breast cancer.18 We evaluated the performance of our model without implementing feature extraction, so as to classify breast pathology into four benign and four malignant categories, against the background of normal breast tissue, a potentially more challenging task.\n\nData augmentation\n\nThe BreakHis dataset contains approximately 110–140 images at each magnification in most of the neoplasm categories, but for the most common neoplasms such as fibroadenomas, there are 230–260 images at each magnification, and between 790 and 900 images for ductal carcinomas. Categorical representation in this dataset reflects the clinical prevalence of these histological entities, as ductal carcinomas and fibroadenomas are the most common malignant and benign histologies, respectively.19,20 While a more evenly balanced dataset is usually preferred to decrease overfitting to one category, it may not be entirely disadvantageous to having a model be trained based on a weighted representation that reflects the actual prevalence. Nonetheless, data augmentation over all images is a common technique to manage unevenness in the dataset. In this endeavor, this was implemented by introducing a simple random horizontal image reflection (mirror images) to the model, effectively doubling our virtual sample size. We elected not to implement more distortive effects, such as shear. Other data augmentative effects, such as rotation would be meaningless with histology images, and the various magnifications would provide zoom information. We assessed the effect of data augmentation on the training and validation accuracy curves, which will be presented below.\n\nModel selection\n\nThe MobileNet family is built on the concept of depthwise-separable convolutions, which has allowed it to achieve image classification accuracy comparable to that of larger models while being efficient and and incurring lower computational cost.21 MobileNetV3 built on MobileNetV2’s improvements by introducing the squeeze-and-excite algorithm in parallel to all components of the residual block; using Neural Architectural Search to improve accuracy22; and replacing some of the sigmoid activations with the hard swish activation function which is computationally less “expensive” than the sigmoid activation function (a major factor in mobile devices), to improve performance but without compromising on accuracy. This enabled some layer reduction in the last stages of the model without decreasing accuracy. MobileNetV3 is divided into small and large versions, with the small version to be run on reduced-resource devices (Fig. 3).\n\nFigure 3\n\nTop accuracy of the MobileNetV3 models as compared with other computer vision architectures, stratified by the number of trainable parameters in the model (Adopted from Pandey38).\n\nModel architecture\n\nThe MobileNetV3 large architecture offers improved accuracy over the companion MobileNetV3 small architecture. The ImageNet dataset embeddings are enabled by default,23 however, it was not clear that these parameter weights obtained from training on the widely varied images obtained from the Internet would be helpful in classifying histology images. We decided to implement transfer learning, utilizing weights and biases of earlier layers, which detect shapes and edges in images, while training on the later layers, which register activations based on more complex image features. This will be discussed more in detail in the section on Layerwise Learning.\n\nA key hyperparameter is the model learning rate, which controls how rapidly the parameters of the model converge to the local minimum of the categorical cross-entropy loss function,24 used to assess the accuracy of classification after each training epoch. As the loss function local minimum is approached, optimal adjustments of the learning rate at each training epoch can help to prevent overshoot, and subsequent deterioration of accuracy. The optimal freezing layer as well as setting the magnitude of the learning rate are heuristically determined. We tested a range of epoch decay values, which were first entered into a spreadsheet to facilitate automation. In this way, the model could be programmed to autonomously run multiple training sessions using different hyperparameter values, while model accuracy was recorded.\n\nTraining\n\nThe BreakHis dataset was split into training, validation, and test subsets at ratios of 0.75/0.15/0.10. Outputs from MobileNetV3 were flattened with a Global_Average_Pooling2D layer, then passed to four Dense layers with ReLU activation, followed by a Dense layer with softmax activation to eight outputs. Dropout regularization and BatchNormalization were used to reduce overfitting to the training set. As shown in Fig. 4, the final model consisted of approximately 3.0 million parameters, with 98% associated with MobileNetV3 Large. Hyperparameters of learning rate epoch decay, freezing layer setting were loaded into a spreadsheet, and used to autonomously train the model for 13 epochs. The workflow is illustrated in Fig. 5. Accuracy measurements on the validation set were recorded, and are displayed below. The full Python code is available at \nhttps://github.com/kdevoe/MobileNetV3_Breast_Cancer_Detection\n.\n\nFigure 4\n\nArchitecture of model used for image classification.\n\nFigure 5\n\nWorkflow diagram. Histology images are pre-processed, then vectorized. Data augmentation is applied and fed to the MobileNetV3 model, which is trained for 13 epochs. The trained model is fed an image from the validation set, on which the model makes a prediction, selecting from one of eight possible labels. The output is then displayed.\n\nResults\n\nModel training and hyper-parameter tuning were focused on three key areas; data augmentation, layer selection for training, and learning rate selection.\n\nFig. 6 depicts the accuracy on the training and validation sets, and the effect of data augmentation. In our model, accuracy in the training and validation sets exhibited minimal separation. After 10 epochs of training, validation accuracy reached a plateau with no further improvement. Training and validation accuracy tracked closely, and the impact of data augmentation was minimal.\n\nFigure 6\n\nComparison of training vs. validation curves with and without data augmentation. Overall, data augmentation reduces overfitting, bringing the validation results closer to the training results. Training runs were performed with an initial learning rate of 0.001, epoch decay rate of 0.95, and fine-tuning start layer of 150.\n\nLayerwise learning\n\nImplementation of transfer learning during model training involves setting the hyperparameter that determines the layer at which to freeze (preserve) pre-trained ImageNet embeddings, beyond which training will occur in the MobileNetV3 model. If the freezing layer is set too distally, then an insufficient number of layers of the model will be fine-tuned on the new dataset, and performance on tumor images will be inadequate. However, if the freezing layer is set too proximally, then too much of the pre-trained embeddings will be overwritten, and edge-and shape-detecting functionality garnered from pre-training on ImageNet will be lost (Fig. 7). Initiating the training prior to layer 130 led to poor, erratic performance of the overall model, as the ability to recognize basic image features learned from ImageNet had been overwritten. On the other hand, delaying the start of training until after layer 150 resulted in a slight decline in performance, likely due to inadequate fine-tuning with the tumor dataset. Based on these results, layer 150 was selected as the optimal threshold, beyond which the model parameters would be made available for training.\n\nFigure 7\n\nValidation accuracy of the model after 13 epochs based on the initial fine-tuning layer. For reference, the model includes 268 layers total, with 263 layers from MobileNetV3. Each training session used an initial learning rate of 0.001 and epoch decay rate of 0.95. This graph represents 21 unique model training runs.\nLearning rate selection (epoch decay rate)\n\nThe heuristically determined initial learning rate for training is the coefficient modulating the adjustments made to the weights and biases of the model parameters after each epoch of training. As the training proceeds and the loss function converges to the local minimum, it may be advantageous to reduce this value progressively, so as not to overshoot the target weightings.24 There is no standard learning rate schedule, and various implementations have been developed empirically to maximize test accuracy for particular benchmarks.25 We chose to reduce the learning rate by a constant factor at each epoch of training, which we refer to as the epoch decay rate. Fig. 8 shows the model validation accuracy as a function of initial learning rate and epoch decay rate. As indicated previously, fine-tuning began at layer 150 out of the total of 268 layers. We tested several learning rate settings without epoch decay and found that the highest accuracy was achieved with a learning rate of 0.0003. However, when epoch decay was incorporated, the highest accuracy was achieved with an initial learning rate of 0.001 and an epoch decay rate of 0.95. More aggressive epoch decay rates of 0.9 and 0.8 resulted in further reduction of model accuracy, suggesting excessive attenuation of training efforts at later epochs. Therefore, for the final analysis, a start learning rate of 0.001 and epoch decay of 0.95 was selected.\n\nFigure 8\n\nValidation accuracy of the model based on the initial learning rate and epoch decay rate. Fine-tuning was started at layer 150 out of 268 total layers for all models, with training run for 13 epochs.\n\nClassification accuracy\n\nThe trained model was then used to predict on images in the test set, and was able to classify the histology slides as benign vs. malignant with a recall of 0.97, a precision of 0.98, and an F1 score of 0.98 (Fig. 9a). 95% confidence intervals were calculated according to the method outlined by Newcombe.26 Amongst the subtypes of each major category, the model had the greatest success in identifying ductal carcinoma, adenosis, and tubular adenoma, with accuracies of ≥0.9. Mucinous carcinoma and phyllodes tumors were accurately identified in 0.85 of cases. Accurate classification for papillary carcinoma was 0.74, and lobular carcinoma was 0.56 (Figs. 9b, ​10).\n\nFigure 9\n\n(a) Confusion matrix for accuracy generated for the binary classification of benign vs. malignant. (b) Confusion matrix for accuracy in classifying histological subtypes.\n\nFigure 10\n\nF1 score, precision score, and recall scores (±95% confidence intervals) for classification into the histological subtypes.\n\nThe F1 score, precision, and recall scores were calculated for the classification into the four benign and four malignant subtypes. The performance of the model was highest with classifying ductal carcinoma, likely because of the larger number of samples on which to train. Performance was lowest with phyllodes tumor, of which the fewest samples were provided in the dataset.\n\nThe tabular data in Fig. 10 can also be visualized as receiver operating curves (ROC), as depicted in Fig. 11. The ROC curves confirm the relative classification performance.\n\nFigure 11\n\nROC AUC curves depicting accuracy of the classifier across the histological subtypes.\n\nIncorrectly classified images\n\nTo gain insight into how the model erred in classifying some of the histopathology images, we identified the misclassified images, comparing their predicted labels with the ground-truth label. A few of these images are presented in Fig. 8.\n\nIn Fig. 12a, the image that was misclassified as lobular carcinoma contained mainly tissue stroma or possibly tissue necrosis, with very few identifiable cells. In Fig. 12b, the image misclassified as phyllodes tumor was reasonable in quality, however, in this selected image, it may be difficult for even a human pathologist to make the distinction between fibroadenoma and phyllodes tumor. The image in Fig. 12c, also misclassified as phyllodes tumor, is of such high magnification that the image consists of a sheet of nuclei with dispersed chromatin and indistinct cellular borders, and the possibilities of its provenance are numerous. The final image in Fig. 12d, misclassified as ductal carcinoma, consists mainly of cellular outlines, and may possibly represent a region of adipose tissue, with little material that can be identifiable as carcinoma. It is, therefore, not unreasonable for images to have been misclassified, as they would pose a challenge for even expert human evaluation (Bayes optimal error rate).\n\nFigure 12\n\nA sample of misclassified images. (a) Ductal carcinoma misclassified as lobular carcinoma; (b) fibroadenoma misclassified as phyllodes tumor; (c) fibroadenoma misclassified as phyllodes tumor; (d) lobular carcinoma misclassified as ductal carcinoma.\n\nDiscussion\n\nWe have shown that a fine-tuned convolutional neural network, optimized for image classification and designed for increased accuracy, decreased latency, and decreased resource utilization, was able to properly categorize breast cancer histopathology images as to being benign vs. malignant with ≥97% precision and recall. Classification of dataset images into the given histological subtypes was achieved with moderate success. Accuracy was best with the classification of ductal carcinoma, which is important since it is the most common subtype of breast cancer, representing 70%–80% of all breast cancers.27 Images representing this histological subtype were most represented in the BreaKHis dataset, which could account for the relative accuracy in classifying ductal carcinoma, as there were more examples for the model to train on. Because the model trains to decrease loss, the increased prevalence of this category might have led to more rapid reduction in loss function values than for the other categories. Nevertheless, confident identification of infiltrating ductal carcinoma would be of value, as it could potentially assist in the confirmation of this histology in over half of breast cancer cases.\n\nTo put our findings into context, we will review some of the work that inspired our own efforts. Spanhol et al17 used six feature extractors and four different unsupervised classifiers on the BreaKHis dataset at each of the magnifications available. Their model best classified malignant histology correctly, with accuracies of around 0.94, but the accuracy of classifying benign tissue ranged from 0.38 to 0.75 at various magnifications. Error was most noted in classifying fibroadenomas, which constituted around 30% of errors at every magnification. The ROC AUC was around 0.8 at each of the four magnification levels, however, the AUC with all magnifications collectively was not reported. As stated above, pathologists extract architectural information at low magnification and nuclear and cellular detail and tissue invasiveness at high magnification, so constraining the model to classify at one magnification level is an unnecessary restriction.\n\nZhu et al28 trained a custom CNN model with ideas taken from the Inception architecture (residual network connections), but with the “squeeze-excite” features of MobileNetV3. Their model was trained on the BreakHis and BACH breast cancer datasets. They also reported accuracy based on each magnification class, and achieved greater than 0.9 AUC on ROC analysis, but this was based on channel pruning of the model, ostensibly to decrease computing burden, although it raises some concern about how this model would generalized to other datasets.\n\nAraújo et al29 studied an unspecified CNN (but which resembles AlexNet) together with a support vector machine classifier, and trained on the Bioimaging 2015 challenge dataset, consisting normal as well as benign, and malignant breast cancer, some invasive and some in situ. “Patchwise accuracy” was 0.667–0.776. Image-wise accuracy was 0.778–0.833, using a voting system, the process of which is not well-described.\n\nJoshi et al30 trained an Xception model on the BreaKHis and IDC breast cancer datasets. The objective was the classify the material into benign and malignant categories. ROC AUC was 0.921 on the BreakHis dataset and 0.881 on the IDC dataset. Accuracy data regarding further subclassification into histological subsets was not presented.\n\nAmerikanos et al31 used feature augmentation with semantic segmentation to train Facebook AI Research’s Detectron2 architecture on the TUPAC16 challenge dataset. The investigators sought to avoid having a human pathologist determine the segmentation, and used an AlexNet trained with parameters published previously.32 The objective of this study, however, was to automate the identification of nuclear to facilitate feature selection, and not histological classification.\n\nWe recognize that there are more histological entities than are represented in this limited dataset, such as triple-negative breast cancers and mixed-histologies. Indeed, we coded a predictor that performed well on selected random images from the dataset but performed less accurately on selected histopathology slides obtained from the Internet. These images were resized to the 700×468 pixel and converted to a dataset as in the BreakHis dataset. The images obtained from the Internet were chosen so as to match as closely as possible the images in the BreakHis dataset. However, despite this effort, the model performed rather poorly on the small internet dataset, with an accuracy never exceeding 0.6. We feel that the primary reason for this is the difference in image quality in the BreakHis dataset and ones available on the Internet (see Supplemental Fig. 2). Differences in the intensity of hematoxylin and eosin staining can result in differences in contrast of features such as cellular architecture, nuclear structure, and stromal detail, all of which can influence training of convolutional blocks that detect edges, corners and gradients. Tafavvoghi et al highlights variability in image quality amongst several available public breast cancer datasets,33 and therefore classification accuracy between image repositories may more a reflection of differences in intrinsic source characteristics than model performance.\n\nAnother factor contributing to difficulty with model classification when attempting to predict on new images is the variability attributable to differences between low- and high-grade malignancies. Details of this kind were not provided with the BreakHis dataset, and all examples of one tumor subtype were placed into the same diagnosis bin. Accuracy may also be improved if our model were able to train using labels informing about tumor grade. Morphological pleomorphism in poorly differentiated malignancies can challenge even human pathologists, and therefore immunohistochemical data helps to confirm visual assessment of hematoxylin-eosin stained slides.\n\nOne of the known limitations of deep learning in convolutional architectures is that the trained weight embeddings are notoriously in a black box. Image data that is passed through CNNs undergo drastic transformations through convolution, pooling, flattening, such that examination of layers deep within the model reveal no discernible relationship to the original input (Fig 3). It has been difficult to elucidate precisely how a model is progressively trained to classify images so as to provide step-by-step documentation as to the process of generating predictions. A surrogate means of validation has been to report performance indicators on standardized datasets and infer that the accuracy and precision are replicated. At this time, the inability of convolutional models to justify and document the basis for classification supports the argument that these deep learning models will not replace a trained human pathologist, but instead may play an assistive role as long the pathologist does not second-guess his/her own reading, and place undue reliance on an erroneous reading of the trained model.34\n\nConclusions\n\nBuilding upon a modern CNN architecture, designed for accuracy and efficiency, and to be compact enough to run on mobile devices, we were able to develop and train a model on a dataset of breast histopathological images, such that it was able to predict the classification of the images as being benign vs. malignant in nature, with a high degree of precision and accuracy. It was also moderately successful in classifying the pathology into one of eight subcategories.\n\nThe fine-tuning process was greatly aided by automating the training process, to heuristically identify optimal hyperparameter settings, such as freezing layer determination, as well as initial learning rate and epoch decay rate. Overfitting was addressed with data augmentation as well as using Dropout and BatchNormalization. Although the process we described did not require special feature extraction pre-processing, such as semantic segmentation, it likely that there would still be benefit to the selection of features that emphasize the best characteristics of each histological entity.\n\nThe latest member of the MobileNet family of CNNs was used in this project, but there is no consensus as to the “best” convolutional network for image classification. Abid et al35 reported high accuracy (98.61%) in the classification of multi-modal medical images as to whether they were images of pathology slides, hand or chest radiographs, endoscopic and tomographic imaging using the ResNet50 model. In the Natural Language Processing field, it has been hypothesized that larger models with a greater number of trainable parameters are more “sample-efficient” and more performant on a wider range of sample data.36 This paradigm was famously countered by the “Chinchilla” paper,37 which demonstrated that a smaller model trained with an optimal number of tokens showed better performance compared to models with a larger number of parameters, and a similar situation might also hold for image classification models. Nonetheless, larger CNNs such as VGGNet, GoogLeNet/Inception, ResNet, and DenseNet would be expected to be similarly successful in achieving good accuracy in pathology image classifications.\n\nAs these models improve further, becoming more efficient and even less resource-demanding, and as pathologists develop trust in their accuracy, trained CNNs could become a useful addition to the pathologist’s diagnostic toolkit.\n\nReferences\nAmerican Cancer Society Key Statistics for Breast Cancer. 2023. \nhttps://www.cancer.org/cancer/types/breast-cancer/about/how-common-is-breast-cancer.html\nCollege of American Pathologists Protocol for the Examination of Resection Specimens from Patients with Invasive Carcinoma of the Breast, Version 4.9.0.0. \nhttps://documents.cap.org/protocols/Breast.Invasive_4.9.0.0.REL_CAPCP.pdf\n Posted June 2023.\nCollege of American Pathologists Template for Reporting Results of Biomarker Testing of Specimens from Patients with Carcinoma of the Breast. Version 1.5.0.1. \nhttps://documents.cap.org/documents/Breast.Bmk_1.5.0.1.REL_CAPCP.pdf\n Posted March 2023.\nWeigelt B., Geyer F.C., Reis-Filho J.S. Histological types of breast cancer: how special are they? Mol Oncol. 2010;4(3):192–208. doi: 10.1016/j.molonc.2010.04.004. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nHowlader N., Noone A.M., Krapcho M., et al., editors. SEER Cancer Statistics Review, 1975-2016. National Cancer Institute; Bethesda, MD: April 2019. \nhttps://seer.cancer.gov/archive/csr/1975_2016/\n based on November 2018 SEER data submission, posted to the SEER web site. [Google Scholar]\nLoizidou K., Elia R., Pitris C. Computer-aided breast cancer detection and classification in mammography: a comprehensive review. Comp Biol Med. 2023;153 doi: 10.1016/j.compbiomed.2023.106554. [PubMed] [CrossRef] [Google Scholar]\nWang S., Yang D.M., Rong R., Zhan X. Pathology image analysis using segmentation deep learning algorithms. Am J Pathol. 2019;189(9):1686–1698. doi: 10.1016/j.ajpath.2019.05.007. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nKrizhevsky A., Sutskever I., Hinton G. ImageNet Classification with Deep Convolutional Neural Networks. 2012. \nhttps://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\nAlzubaidi L., Zhang J., Humaidi A.J., et al. Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. J Big Data. 2021;8:53. doi: 10.1186/s40537-021-00444-8. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nHe K., Zhang X., Ren S., Sun J. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) IEEE Xplore; 2016. Deep residual learning for image recognition. [CrossRef] [Google Scholar]\nAlom M.D., Hasan M., Yakopcic C., Taha T.M. 2017. Inception Recurrent Convolutional Neural Network for Object Recognition. arXiv:1704.07709. [CrossRef] [Google Scholar]\nDeng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L. 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE Xplore; 2009. ImageNet: a large-scale hierarchical image database. [CrossRef] [Google Scholar]\nImageNet \nhttps://www.image-net.org/\nLakshmanan V., Görner M., Gillard R. O’Reilly Media; 2021. Practical Machine Learning for Computer Vision. Chapter 3. Image Vision; pp. 67–123. [Google Scholar]\nDing K., Zhou M., Wang H., Gevart O., Metaxas D., Zhang S. A large-scale synthetic pathological dataset for deep learning-enabled segmentation of breast cancer. Sci Data. 2023;10:231. doi: 10.1038/s41597-023-02125-y. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nJanowczyk A., Madabushi A. Deep learning for digital pathology image analysis: a comprehensive tutorial with selected use cases. J Pathol Inform. 2016;7:29. doi: 10.4103/2153-3539.186902. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nSpanhol F.A., Oliveira L.S., Petitjean C., Heutte L. A dataset for breast cancer histopathological image classification. IEEE Trans Biomed Eng (TBME) 2016;63(7):1455–1462. \nhttps://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/\n [PubMed] [Google Scholar]\nCampanella G., Hanna M.G., Geneslaw L., et al. Clinical-grade computational pathology using weakly supervised deep learning on whole slide image. Nat Med. 2019;25(8):1301–1309. doi: 10.1038/s41591-019-0508-1. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nMcDivitt R.W., Stevens J.A., Lee N.C., Wingo P.A., Rubin G.L., Gersell D. Histologic types of benign breast disease and the risk for breast cancer. Cancer. 1992;69(6):1408–1414. doi: 10.1002/1097-0142(19920315)69:6/1408::aid-cncr2820690617%3E3.0.co;2-c. [PubMed] [CrossRef] [Google Scholar]\nTan B.Y., Tan P.H. A diagnostic approach to fibroepithelial breast lesions. Surg Pathol Clin. 2017;11(1):17–42. doi: 10.1016/j.path.2017.09.003. [PubMed] [CrossRef] [Google Scholar]\nMobileNetV3 \nhttps://paperswithcode.com/lib/torchvision/mobilenet-v3\n Accessed 14 August 2023.\nHoward A, Sandler M, Chu G, Chen L-C, Chen B, Tan M, Wang W, Zhu Y, Pang R, Vasudevan V, Le QV, Adam H. Searching for MobileNetV3. Conference: 2019 IEEE/CVF International Conference on Computer Vision (ICCV). 10.1109/ICCV.2019.00140 [CrossRef]\nMobileNet, MobileNetV2, and MobileNetV3 \nhttps://keras.io/api/applications/mobilenet/\nRussell S., Norvig P. Artificial Intelligence A Modern Approach. 4th ed. Pearson; Hoboken, NJ: 2020. Computation graphs for deep learning; p. 670. 758. [Google Scholar]\nWolfe C.R. The best learning rate schedules. Deep (Learning) Focus. \nhttps://cameronwolfe.substack.com/p/the-best-learning-rate-schedules\nNewcombe R.G. Two-sided confidence intervals for the single proportion: comparison of seven methods. Stat Med. 1998;17:857–872. doi: 10.1002/(sici)1097-0258(19980430)17:8<857::aid-sim777>3.0.co;2-e. [PubMed] [CrossRef] [Google Scholar]\nLi C.I., Anderson B.O., Daling J.R. Trends in incidence rates of invasive lobular and ductal breast carcinoma. JAMA. 2003;289(11):1421–1424. doi: 10.1001/jama.289.11.1421. [PubMed] [CrossRef] [Google Scholar]\nZhu C., Song F., Wang Y., Dong H., Guo Y., Liu J. Breast cancer histopathology image classification through assembling multiple compact CNNs. BMC Med Inform Decis Mak. 2019;19:198. doi: 10.1186/s12911-019-0913-x. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nAraújo T., Aresta G., Castro E., et al. Classification of breast cancer histology images using convolutional neural networks. PLoS ONE. 2017;12(6) doi: 10.1371/journal.pone.0177544. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nJoshi S.A., Bongale A.M., Olsson P.O., Urolagin S., Dharro D., Bongale A. Enhanced pre-trained xception model transfer learned for breast cancer detection. Computation. 2023;11(3):59. doi: 10.3390/computation11030059. [CrossRef] [Google Scholar]\nAmerikanos P., Maglogiannis I. Image analysis in digital pathology utilizing machine learning and deep neural networks. J Pers Med. 2022;12(9):1444. doi: 10.3390/jpm12091444. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nJanowcyzk A., Madabhushi A. Deep learning for digital pathology image analysis: a comprehensive tutorial with selected use cases. J Pathol Informatics. 2016;7(1):29. doi: 10.4103/2153-3539.186902. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nTafavvoghi M., Bongo L.A., Shvetsov N., Busund L.R., Møllersen K. Publicly available datasets of breast histopathology H&E whole-slide images: a scoping review. \nhttps://arxiv.org/pdf/2306.01546.pdf\n arXiv:2306.01546v2 2023. [PMC free article] [PubMed]\nZhang Y., Liao Q.V., Bellamy R.K.E. Conference on Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020, Barcelona, Spain. ACM; New York, NY, USA: 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. 11 pages. [CrossRef] [Google Scholar]\nAbid M.H., Ashraf R., Mahmood T., Faisal C.M.N. Multi-modal medical image classification using deep residual network and genetic algorithm. PLoS ONE. 2023;18(6) doi: 10.1371/journal.pone.0287786. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nKaplan J., McCandish S., Henighan T., et al. 2020. Scaling laws for neural language models. arXiv:2001.08361v1 [csLG] [Google Scholar]\nHoffman J., Borgeaud S., Mensch A., et al. 2022. Training compute-optimal large language models. arXiv:2203.15556v1 [cs.CL] [Google Scholar]\nPandey A. Depth-wise convolution and depth-wise separable convolution. Medium. 2018 \nhttps://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec\n [Google Scholar]\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "firebase-ai-chat-agentic-chat-in-firebase-cloud-ZB89eznfyXtK",
    "username": "Nikolay Kochetkov ",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nFeb 26, 2025\n●\n69 reads\n●\nMIT License\nFirebase AI Chat - Agentic Chat in Firebase Cloud\nfirebase\nfirestore\ngemini\nopenai\nvertex-ai\nNikolay Kochetkov\nLike\nBookmark\nShare\nIntroduction\n\nSince companies has published its LLM APIs, integrating AI chats to your client apps has become a rather easy option. Given that the LLM API is HTTP-based and there are also many call-wrapping libraries for any platform, the one might implement the chat directly in a mobile app or a web-site. However, it might be not a good architectural decision to go this way. Among the reasons are:\n\nAI API key protection and management - if used on a front-end app the key leaks.\nTools and function call code (a part of business logic) are exposed to app client.\nThe latency in mobile application updates will limit your domain updates as function tool-calls reside on the front-end.\nMobile app connectivity is always a problem and given that an AI run takes a considerable time to execute the user experience might not be optimal.\n\nThus, it might be a good idea to put the AI interaction to the back-end and to update your client with just the results of AI runs.\n\nFirebase AI Chat\n uses \nFirebase\n - a popular platform for Web and Mobile application development to run your AI in a scalable and reliable way.\n\nCheck out this \nshort video\n for a quick introduction on the travel agency AI-Concierge experiment using this engine.\n\nMethodology\n\nThe task described in this publication started as an experiment. Being mobile developers, we used the tools most known for us - the \nFirebase\n platform. But going through the project we decided that the core of it might come handy to those who want to launch their own LLM chat using the same technology stack. So the chat engine behind the experiment is now \navailable\n as an open source.\n\nThe chat engine strives to implement a client-server Unidirectional Data Flow (UDF) architecture:\n\nUser interacts with a front-end application and posts messages.\nThe Mobile App uses Cloud Functions to send the user's gestures: create a chat, post a new message, etc.\nFirebase executes the Assistant run in a Cloud Task and posts the results to a local Firestore database.\nAI tool calls are managed by your Back-end providing all necessary data back and forth.\nUpon the run is complete, Cloud Task updates Firestore with AI replies.\nThe Mobile App updates itself from the Firestore in a push-manner and displays changes to the User.\nAn extra state to represent the goal\n\nBesides the messages, each chat maintains some data object - the product of the goal being solved by your assistant. Function tools bound to your assistant read and update the data, producing the final result of a chat - an order, a summary, etc:\n\nAgentic flow\n\nAs your main goal grows complex it is worth considering delegating simpler tasks to several assistants (or Agents), each of them trained to perform the certain scope of the main goal. Thus, a crew of assistants work as a team to decompose the main task and move step by step to fulfil it. This library supports changing assistants on-demand depending on the context. Whenever the main agent decides the task is appropriate for an assistant it may switch the chat handling to him providing current context and supplementary data. Consider an example where all the division operations are done by the main calculator subordinate - the Divider:\n\nMultiple engine support\n\nTo take all the advantages of different LLM engines you may want to use, the engine is built in a modular way to support different platforms. Together with the Agentic Flow support that gives you the ability to dedicate different tasks to different engines to take the most advantage for each particular task. At the time of writing the available engines are:\n\nVertexAI\nOpenAI\nExperiments\n\nWorking in one of the leading private aviation companies, our team strives to create a fundamentally new travel experience and make the booking service as efficient as possible.\n\nTo manage an order the client may use our set of web and mobile applications. However a chat or a phone call with a personal concierge to fix all the details is always a good option.\n\nWe decided to check how modern LLMs may help the concierge in his daily tasks and help him to gather all the required data for the trip.\n\nWe have taken the three major tasks that form the private flight order:\n\nFlight booking\nTransfer to and from the airports\nOn-board catering\n\nEach step requires certain data from a client and also accessing some business data to limit the choice and to fit the order into our business flow. Soon enough we have found that building a mega prompt to manage all the goals together - is a tough task. Inspired by the multi-AI solutions such as \nCrewAI\n and \nChatDev\n we have updated our Firebase chat engine to allow several agents to work as a team, each of them solving their own part of the complex order.\nThe agents switch the chat to one another selecting the most appropriate team-member for the task. We needed some inspiration to give our agents friendly names so we took the famous British TV series - \nThomas And Friends\n as a prototype of a team that work together in the field of transportation :)\n\nHere's the team of our AI-agents:\n\nThe roles of the team members in a crew:\n\nThomas - the concierge. He starts the chat with a client and dispatches the tasks between the members\nEmerson - gathers preliminary flight data\nHarold - creates a flight order using our internal APIs\nLady Hatt - responsible for waypoint validation using Google Maps SDK\nTopham Hatt - in charge of the catering requests\nAce - runs the limousine service\nConcierge service and chat\n\nAt the beginning Thomas controls the chat and responds to the client's demands.\nWhen the assistant gets some special request, like catering, he calls the getCrew function.\nThomas chooses the most appropriate crew member for the task and hands the chat over to him passing some instructing messages internally.\n\nAbove you can see an example of switching the chat to Topham whenever the client asks something relative to catering. Being tuned to handle catering requests, Topham manages the order, fulfils the client's request and passes the control back as soon as he's done.\n\nYou can check the Thomas' prompt and switching calls \nhere\n\nRoute planning\n\nPreliminary trip planning requires the input of departure and destination airports and the desired flight date. To make things more fun, besides the exact airports the client may provide a street address, a geo-point or some named \"favourite\" available from his flight history: \"home\", \"work\", etc. Given the input of the departure and the destination the system needs to find the appropriate airports. Here \nEmerson\n, \nHarold\n, \nAce\n and \nLady Hatt\n work together.\n\nWhen Emerson gathers the points that look like locations, he handles the chat to Lady Hatt to validate the locations and to perform a reverse location search. Lady will ask some more questions if the locations given are ambiguous and return the digested waypoints. Emerson will then update the order with data required for detailed trip planning and switch the client to Harold to fill the details and find the optimal route. If required Ace will later pickup the starting and final locations to plan the airport transfer.\n\nFunction tools usage\n\nWithin the flow agents make extensive use of function tools to stay in bounds of real business data and to limit hallucination. The order itself is also maintained by function calls, so the order data is always valid and contains real data. As the engine supports multiple AI engines the tool usage is made abstract from the particular engine and is presented by a single functional interface (some parameters reduced for clarity):\n\nexport interface ToolsDispatcher<DATA extends ChatData> {\n  (\n        data: DATA, \n        name: string, \n        args: Record<string, unknown>\n   ): ToolDispatcherReturnValue<DATA>\n}\n\nThe parameters are the following:\n\ndata - Chat state (order here) data so far\nname - Name of function called\nargs - Function arguments\n\nThe function returns ToolDispatcherReturnValue value which may be one or a combination of:\n\nSome result data - it will be submitted to AI as {result: \"Response from your tool\"}\nReduction result of data state passed to dispatcher.\nSome error describing the problem to AI: {error: \"You have passed the wrong argument}\n\nEach time the dispatcher runs, the chat data will be updated by the reduced value returned by the dispatcher.\n\nThe very same technique is used to switch the active agent. And given that the tools are asynchronous, we could also implement the preliminary checks and fetch some context data for the assistant.\nFor example, this function checks if all required data is settled in the order and fetches additional charter information required to complete the booking:\n\nexport async function switchToFlightOptions(\n    soFar: OrderChatData,\n    chatData: ChatDispatchData<OrderChatMeta>,\n    args: Record<string, unknown>,\n    toolsHandover: ToolsHandOver<Meta, OrderChatMeta>,\n    from: ASSISTANT\n): Promise<ToolDispatcherReturnValue<OrderChatData>> {\n    logger.d(\"Switching to charter flight options...\");\n\n    // Preparing initial message input for Harold\n    const messages: Array<string> = [\"Here is the data for the current order:\"];\n\n    // Check if basic flight options are set, otherwise, tell the caller that the data is incomplete\n    const flightOrder = soFar.flightOrder;\n    if (!areBasicFlightOptionsSet(flightOrder)) {\n        return {\n            error: `Basic flight options are not yet configured. Ask ${getAssistantName(FLIGHT)} to create the flight order before changing flight details.`\n        };\n    }\n\n    // Print current order state for Harold\n    messages.push(`Current flight order state: ${JSON.stringify(flightOrder)}`);\n\n    // Getting charter details from the system backend for given basic options\n    logger.d(\"Getting charter options...\");\n    const options = await getCharterOptions(\n        flightOrder.from,\n        flightOrder.to,\n        flightOrder.departureDate,\n        flightOrder.plane,\n    );\n    messages.push(`Here are available flight options: ${JSON.stringify(options)}`);\n\n    // Switch to Harold, providing initial messages as a start\n    return doHandOver(\n        getChatConfig(FLIGHT_OPTIONS),\n        chatData,\n        getChatMeta(FLIGHT_OPTIONS).aiMessageMeta,\n        messages,\n        toolsHandover,\n        from\n    );\n}\n\nSuch combination of config\n\nResults\n\nHere are some links for you:\n\nThe \nFirebase-AI-Chat\n project at GitHub.\nThe experiment introductory \nvideo\n.\nThomas And Friends \nproject source\n - this is the code of the described agentic flow with the proprietary backend calls removed.\nVideo of the \nfull order processing\n by a group of AI travel agents. Here you can see how they work together on real data.\nTake a look at the \nsmall sample project\n that demonstrates all the features of the engine.\nConclusion\n\nIn conclusion, embracing the agentic flow for complex LLM tasks offers a robust and scalable approach to problem-solving. By leveraging a crew of specialized agents, each available as a separate npm package and easily testable in isolation, we can effectively decompose and tackle intricate tasks. This modularity not only enhances the flexibility and efficiency of the workflow but also ensures that each component can be independently optimized and maintained. As these agents collaboratively work towards fulfilling the global task, they exemplify the power of distributed intelligence and the potential for innovative solutions in the realm of large language models.\n\nMoreover, integrating a chat engine that works natively in Firebase Cloud further simplifies the process of building chat flows for both mobile and web applications. Utilizing Firebase's robust infrastructure, developers can seamlessly create, deploy, and manage chat functionalities, ensuring real-time communication and a smooth user experience. This combination of agentic flow and Firebase's capabilities promises to drive advancements and unlock new possibilities in AI-driven applications.\n\nWe invite you to visit our \nGitHub\n repository and if you find it useful - join the project. We are open to code contributions and welcome collaboration from developers around the world.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "following-mechanics-with-our-yolo-model-hdKQGDqLbZsZ",
    "username": "f@frankcvanris",
    "license": "No License",
    "title": "Following Mechanics with our YOLO Model",
    "publication_description": "Back to publications\nDec 29, 2024\n●\n69 reads\n●\nNo License\nWinner of\nMost Promising Innovation\nat the\nComputer Vision Projects Expo 2024\nFollowing Mechanics with our YOLO Model\nAI\nComputer Vision\nML\nRobotics\nF\n@frankcvanris\nI\nIgor Janotti\nL\nTrong Duong\nJ\n@joseph.hoang53\nLike\nBookmark\nShare\n1. Abstract\n\nThe concept of enabling a robotic device to autonomously follow a user has been explored extensively by various individuals and industries. The objective of our capstone project is to replicate these mechanics using the research and resources provided by Bellevue College. We intend to build a robot capable of carrying up to 50 lbs. in order to help the disabled. This project introduces the essential learning model designed to enable our robot to recognize and identify the user it is intended to follow. By implementing the YOLO (You Only Look Once) learning model, our system will detect objects in the environment and specify the target object. YOLO is predominantly used for object detection, making it an ideal choice for our application.\n\nPart of our research includes evaluating simple models, alongside YOLO's precursor Convolutional Neural Networks (CNN) to accomplish this task of predicting a specific individual.\n\nIn our capstone project, we will employ a Raspberry Pi to run this intensive learning model, complemented by a camera to facilitate computer vision capabilities. With the integration of extensive image processing into the model, we aim to develop a robust system capable of consistently identifying and following a designated user. Our first goal is to achieve a functional and effective model that allows our robotic device to autonomously track a user with precision. And our second goal is allow our robotic device to autonomously move from one location to the other without human control.\n\n2. Introduction\n\nOur capstone project aims to replicate these advancements by developing an autonomous following robot with substantial application potential. However, significant challenges remain, such as improving the following mechanics to accurately distinguish individuals and enhancing the robot's ability to identify and react to consistently moving objects while avoiding obstacles.\n\nThese critical questions drive our investigation and experimentation with the YOLO (You Only Look Once) model. Currently, we are in the preliminary stages of integrating computer vision into our project. In the coming weeks, we aim to develop a functional model for the Raspberry Pi, enabling us to test and refine the robot's following capabilities.\n\n\nThrough this project, we aspire to address these challenges and contribute to the field of autonomous robotics, ultimately achieving a reliable and effective system that can consistently follow a user.\n\n3. Methodology\n\nOur approach to developing an autonomous following robot involves a step-by-step increase in functionality:\n\n3.1. Establish Communication:\n\nIn order to obtain live updates from our robotic device (FollowBot) we will need to create a web application & mobile app to keep track on all of the information that the device will gather, which we will hope to use within our ML models. For swift communication between the user and robotic clients we are using server to client communication.\n\n3.2. User Proximity\n\nWhen establishing the position of our device to ensure proximity to the user, accurate distance measurement becomes paramount. Initially, we experimented with supersonic sensors to measure distances to various objects in the surrounding environment. Additionally, we utilized RSSI signal strength via a Wi-Fi module to attempt triangulating the distance between the user and the robot.\n\nHowever, recognizing the inherent limitations of RSSI for precise distance measurement, we plan to transition away from relying solely on Wi-Fi. By deploying a machine learning model on a Raspberry Pi 4 with ROS2 in tandem with existing architecture on our Arduino board, object detection will be enhanced while inaccuracies can be made up for.\n\n3.3. Machine Learning Implementation\nPreprocessing\n\nOur initial dataset is obtained from \nkaggle\n. We introduced photos of one of ourselves to serve as the target user.\n\nWe begin by preprocessing our dataset. Since we are planning to deploy this on a Raspberry Pi, images will be scaled down to 128x128 and converted to gray scale to save on costs. Next the data set will be augmented to mitigate the issue of class imbalance.\n\nThe images in the YOLO dataset must be annotated with bounding boxes. Brightness of the images will vary between -15% and +15% to mimic different light conditions. Random cropping is introduced to aid identifying the user in the event they are partially obscured.\n\nFeatures\n\nPreprocessed Image:\n\n\nDense Layer from CNN to be used as inputs for CNN-Hybrids\n\n\nImage with Bounding Boxes Used for YOLO\n\n\nBlock Diagrams\n\n\n\n\n3.4. Autonomous Navigation\n\nAfter we have successfully finalized the learning model and improved it's accuracy we plan on moving our focus to mapping out location with our device in order for it to move from point A to point B.\n\nSensor Integration\nEncoders on Wheels: Install encoders to accurately measure the distance traveled and maintain precise motion control.\nIMUs (Inertial Measurement Units): Utilize IMUs to track orientation and movement, ensuring the robot remains stable and follows the intended path.\nSupersonic Sensors and LiDAR: Equip the robot with supersonic sensors and LiDAR for obstacle detection and distance measurement.\nData Processing and Fusion\nCollect and integrate real-time data from encoders, IMUs, supersonic sensors, and LiDAR.\nUse sensor fusion techniques to improve the accuracy of the robot's perception of its environment.\nPath Planning\nImplement a algorithm (Dijkstra's) to calculate the most efficient route from the starting point to the destination, avoiding detected obstacles.\nContinuously update the planned path as new obstacles are detected in real-time.\nObject Detection and Tracking\nIntegrate the YOLO (You Only Look Once) machine learning model to enable real-time object detection and user identification.\nUse YOLO to dynamically recognize and avoid obstacles while following the designated user.\nMotion Control\nDevelop motion control algorithms to manage the robot's movement along the planned path.\nUtilize feedback from encoders and other sensors to ensure smooth and precise movements.\nSystem Integration and Testing\nIntegrate all subsystems (sensor data fusion, path planning, object detection, and motion control) into a cohesive navigation system.\nConduct extensive testing in various environments to validate the robot's autonomous navigation capabilities.\nCollect and analyze performance data to make necessary adjustments and improvements.\n\nBy following these steps, we aim to develop a robust and reliable autonomous navigation system capable of precise and efficient movement in various environments.\n\nWe are currently working on step 2 and 3 of our methodology.\n\n4. Results\n\nTested several other machine learning models, including basic models, basic CNN and CNN-hybrids. Of which, we discovered weaknesses in precision, recall, and overfitting the training data:\n\n\nCreated a YOLO model with mean average precision with a 50% IoU threshold (mAP50) of ~80%, alongside improved precision and recall:\n\n\nEstablished a communication protocol between FollowBot and the web server to separate user interaction and data processing.\n\n5. Conclusion\n\nTo date, our capstone project has successfully achieved bidirectional data communication between the user and our FollowBot. We have the capability to issue commands to FollowBot effectively. Additionally, we have laid the groundwork for integrating the machine learning models that will enhance our project.\n\nWe believe the model can still be improved to eliminate bias towards the majority class and overfitting to training data to best generalize real world inputs. Once we establish an initial following mechanism, we can deploy the model and test its compatibility with our existing architecture.\n\n6. Future\n\nWe plan on having a functioning robot with computer vision technologies using the YOLO model for object detection and avoidance, along with the capability to map autonomously from one location to another.\n\nGoing forward, we intend to improve our ML model in these areas:\nResampling techniques: Research more sophisticated techniques and re-evaluate by original metrics\nAdditional Metrics: Introduce metrics such as training time, computational load, memory load\nAccessibility: Include photos of wheelchairs so that the model to better target our intended market\nEdge Cases: Introduce photos of ordinary objects and images of target the user in different clothes and settings for sustainment of model\nDeployment and Implementation: With the improved model, save and store on a Raspberry Pi 4 with camera module with the ethos of continuous improvement\n\nIn terms of our capstone, we plan to develop a web app that can be accessed from mobile and desktop to see statistics and control (this is currently implemented locally) the FollowBot. We will then create a mobile app with the same features as the web app to see statistics and control Followbot(with voice and touch screen controls), as well as see the data of immediate usage for users( such as power and usage of weight). With all the data we are recording to be fed back to machine learning model.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nAbstract\nIntroduction\nMethodology\n\n3.1. Establish Communication:\n\n3.2. User Proximity\n\n3.3. Machine Learning Implementation\n\nPreprocessing\n\nFeatures\n\nBlock Diagrams\n\n3.4. Autonomous Navigation\n\nView all\nCode\nDatasets\nFiles\nFollowBotMLModels.ipynb\nFollowBotMLModels.pdf",
    "awards": [
      "most promising innovation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "forest-guard-combating-illegal-deforestation-z8hnahjbTS23",
    "username": "p@patrickneicu2006",
    "license": "License Plate Recognition (Bbox + OCR)",
    "title": "",
    "publication_description": "Back to publications\nDec 27, 2024\n●\n74 reads\n●\nCreative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\nForest Guard: Combating Illegal Deforestation\nAI4Good\nALPR\nComputer Vision\nEdgeDeployment\nObject Detection\nP\n@patrickneicu2006\nLike\nBookmark\nShare\nForest Guard: Innovative AI-Powered App Monitoring Wood Transportation in Romania\n\nThe Forest Guard project represents a pioneering approach to combatting illegal deforestation in Romania by harnessing the power of AI and computer vision technologies. It introduces a novel and innovative solution that empowers citizens and law enforcement agencies to monitor and track wood transportation activities effectively.\n\nThe application employs advanced algorithms to detect license plate numbers using bounding box detection and OCR text recognition. It also extracts legal documents from the SUMAL 2.0 platform, ensuring compliance with forestry regulations and facilitating real-time monitoring of wood transports.\n\nAdditionally, the app includes a state-of-the-art wood log detection feature, which utilizes a custom YOLOv8n model to detect and calculate wood volume accurately. By transforming pixels into centimeters and applying mathematical formulas, the app provides precise measurements, enabling users to verify the legality of wood transports.\n\n\n\n\n\n1. Background\n\nDeforestation in Romania is an alarming and rapidly escalating issue, posing a significant threat to the nation's environment, economy, and overall well-being. The statistics are striking: an estimated average loss of 3 hectares of forests every hour, resulting in a substantial annual economic setback of approximately $6 billion, equivalent to a significant 2% of the country's GDP. What's even more concerning is that only 1% of illegal deforestation wood transport is currently caught.\n\n\n\n\n\n2. Objectives\nDevelop a mobile application that empowers citizens to report and verify timber transports, contributing to the reduction of illegal deforestation in Romania.\nProvide law enforcement agencies with real-time, actionable data on potentially illegal timber transports, aiding in more effective monitoring and enforcement efforts.\nProvide opportunities for team members to enhance their skills in AI, machine learning, project management, and collaboration through hands-on experience and peer learning in open source.\nCreate a wood volume measurement application for logging companies to accurately estimate wood volume and ensure sustainable harvesting practices.\n\n\n\n\n\n3. Approach\nA) License Plate Detection\n\nDevelop a bounding box detection model using YOLOv8 to identify license plates in images, incorporating a pixel-to-cm ratio to find a 52 cm reference length. Integrate an OCR API to recognize and extract text from the detected license plates, enabling the extraction of legal documents.\n\n\t\nB) Extracting Documents\n\nExtract from the national legal platform SUMAL to access real-time legal documents related to timber transports and forestry regulations.\n\n\t\nC) Wood Log Detection\n\nImplement a wood log detection model using YOLOv8 to identify and localize logs in images. Calculate wood log volume using mathematical techniques and compare it with real volume data to detect discrepancies and potential illegal logging activities.\n\n\n\n\n\n4. Data\nLicense Plate Dataset\tWood Log Dataset\n\n\nTotal Images: 1,457\n\nTraining: 1,075\n\nValidation: 382\n\nSources:\n\nRoboflow Dataset (923 Images)\nRomanian License Plate Dataset (534 Images)\n\nProcessing:\n\nMerged datasets\nConverted to YOLOv8 label format\nImage size: 640x640\nNo augmentations\n\t\n\nTotal Images: 2,585\n\nTraining: 1,846\n\nValidation: 739\n\nSources:\n\nRoboflow Dataset (2,464 Images)\nHAWKWood Database (121 Images)\n\nProcessing:\n\nMerged datasets\nConverted to YOLOv8 label format\nImage size: 640x640\nNo augmentations\n\n\n\n\n\n5. License Plate Recognition (Bbox + OCR)\nA) Models\n\n- YOLOv8 Nano (6.3 MB): Selected for its balance of small size and fast inference speed, making it suitable for mobile deployment.\n- TrOCR: Transformer-based Optical Character Recognition with pre-trained models.\n\n\t\nB) Experiments\n\n-Several experiments were conducted with YOLOv8X and YOLO-NAS, but YOLOv8 Nano yielded the best results in terms of accuracy and efficiency.\n-Created a 73-images evaluation dataset manually annotated to assess performance of different APIs and models. -Multiple experiments were conducted using various OCR techniques, including Pytesseract and TrOCR, with different sizes and fine-tuned on different datasets. TrOCR base-printed worked.\n\n\t\nC) Hyperparameters\nParameter\tValue\nModel Architecture\tYOLOv8N\nEpochs\t50\nLearning Rate\t0.002\nMomentum\t0.9\nLoss\tSparse Cross Entropy\nOptimizer\tAdamW\nMachine\tIntel Core i7-1255U\nD) Results and Evaluation\nMetrics: Achieved mAP50 of 0.994 during testing.\n\nExamples: Model demonstrated robust performance under diverse lighting conditions.\n\n\n\n\n\n6. SUMAL 2.0 Platform\n\nSUMAL 2.0 regulates and monitors the transportation of wood on public roads in Romania. It mandates a legal notice containing:\n\nLegal volume.\nLicense plate number.\nVehicle type.\nValidity of the electronic notice.\n\n\n\n\n\n\n7. Extracting Legal Documents\nimport requests\nfrom bs4 import BeautifulSoup\nlicense_plates = ['SB40DAP'] #-> License Plate Number\n\nbase_url = 'https://inspectorulpadurii.ro/api/aviz' \n\nfor plate in license_plates:\n  response = requests.get(f'{base_url}/locations?nr={plate}').json()\n  codes = response['codAviz']\n  if not codes:\n    print(\"Legal Notice not found\")\n  for code in codes:\n    resp_2 = requests.get(f'{base_url}/{code}').json()\n    # Getting volume\n    volume = resp_2['volum']['total']\n\n    # Getting valdity\n    valid_from = get_date(resp_2['valabilitate']['emitere'])\n    valid_to = get_date(resp_2['valabilitate']['finalizare'])\n    print({'Code': code, 'Volume': volume, 'Validity': f'{valid_from} - {valid_to}'})\n\n\n\n\n\n8. Wood Log Detection (Multiple Bboxes)\nA) Models\n\nWe opted for YOLOv8n due to its renowned performance in object detection tasks. Through training over 30 epochs, our model attained impressive accuracy, ensuring reliable detection of wood logs across various environmental conditions.\n\n\t\nB) Experiments\n\nSeveral experiments were conducted with YOLOv8S, DETR, and YOLO-NAS. YOLOv8 Nano yielded the best results in terms of accuracy and efficiency during testing and validation phases.\n\n\t\nC) Hyperparameters\nParameter\tValue\nModel Architecture\tYOLOv8N\nEpochs\t30\nLearning Rate\t0.01\nBatch Size\t8\nLoss\tVFL (CIOU+DFL)\nOptimizer\tAdamW\nMachine\tIntel Core i7-1255U\nD) Results and Evaluation\nMetrics: Achieved high accuracy in detecting wood logs under various conditions.\n\nExamples: Accurate bounding box predictions for logs with minimal false positives.\n\n\n\n\n\n9. Pixel to Centimeter Ratio\nPixel to Centimeter Ratio\tDiameter\n\n\nBounding Box Pixels : 52 (Standard EU License Plate Length)\n\n\t\n\nThe diameter is extracted by taking the length of the bounding box of each log and converting it into centimeters using the pixel-to-centimeter ratio.\n\n\n\n\n\n11. Mobile App Development\nA) Edge Detection for Real-Time Processing\n\nOur application utilizes edge detection techniques to perform real-time processing directly on the user's device, without relying on cloud platforms or external APIs. This ensures smooth and fast performance even on mobile devices, providing users with a seamless experience.\n\n\n\n\nB) Multiple Bounding Box Detection for License Plates\n\nOne of the key features of our app is its ability to detect multiple bounding boxes for license plates in real-time. This enables users to quickly identify and select the correct license plate number from the predictions. If the predicted number is incorrect, users can easily modify it manually by long-pressing on one of the options, ensuring accurate data capture.\n\n\n\n\nC) Real-Time Wood Bounding Box Detection\n\nIn addition to license plate detection, our app also includes real-time detection of wood bounding boxes. Users can center the trucks within the frame, and the app will continuously detect and refine the bounding boxes until the user is satisfied with the results. This ensures accurate tracking and monitoring of wood transportation activities.\n\n\n\n\nD) Manual Confirmation for Accuracy\n\nTo further enhance accuracy, our app waits for user confirmation before finalizing any predictions or calculations. This allows users to review and validate the detected information, ensuring that only accurate data is added to the verified trucks section. By empowering users to confirm the accuracy of the predictions, our app provides a reliable and trustworthy solution for wood transportation monitoring.\n\n\n\n\n\n12. Demos + Try it Yourself\n\n\n\n\n\nLicense Plate Recognition DEMO\n\n\n\n\n\nWood Log Detection DEMO\n\n\n\n\n\nForest Guard DEMO\n\n\n\n\n\nUsage\n\nTo test the app, clone the repository and navigate to the forest_guard directory. Run the following command:\n\ngit clone https://github.com/patrick25076/forest_guard.git\ncd forest_guard\nflutter run\n\nEnsure you are connected to an emulator or a real device. Contact me for the APK file if needed.\n\nContribution and Collaboration\nI am open to collaboration and improvements. Feel free to reach out if you'd like to contribute or discuss potential enhancements.\nNeicu Patrick\nGitHub: \nProfile\nEmail: \npatrickneicu2006@gmail.com\nLinkedIn: \nProfile\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode\nDatasets\nFiles\nPrezentare Forest Guard.pptx",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "friday-revolutionary-offline-ai-voice-assistant-L2eGTmhOS4hb",
    "username": "rRay Poulton",
    "license": "No License",
    "title": "Friday - Revolutionary Offline AI Voice Assistant",
    "publication_description": "Back to publications\nFeb 23, 2025\n●\n107 reads\n●\nNo License\nFriday - Revolutionary Offline AI Voice Assistant\nAI\nAI Agent\nFlask\nHTML\nJavascript\nLangChain\nllama\nOpen Source\nPython\nVoice Assistant\nVoice Recognition\nR\nRay Poulton\nLike\nBookmark\nShare\nAbstract\n\n\nProgrammed from my college dorm, FRIDAY is a groundbreaking offline voice assistant designed to redefine personal AI interaction. Unlike traditional voice assistants that rely on cloud-based APIs, FRIDAY operates entirely locally, ensuring privacy, speed, and cost efficiency. Moreover, as an open-source project, FRIDAY invites developers and enthusiasts to collaborate, improve, and customize the assistant for their unique needs.\n\nPowered by technologies like Ollama, Llama 3.2, and Gemma2, FRIDAY provides instant responses, adapts to user preferences, and operates without the need for an internet connection. Its open-source nature means that anyone can contribute to its development, ensuring that it evolves continuously in line with community feedback. With features like Spotify integration, weather updates, and smart reminders, FRIDAY represents a step toward a future where AI is not only private, fast, and accessible to everyone but also open for all to improve.\n\nGitHub - \nFriday Offline Voice Assistant\n\nEmail - \nRaypoulton11@yahoo.com\n\nPhone - 443-896-4231\n\nIntroduction\n\nVoice assistants have become integral to modern life, but they come with significant limitations, such as reliance on cloud services, privacy concerns, and recurring costs. FRIDAY addresses these challenges by offering a fully offline, local AI solution that not only keeps your data private but also ensures faster, more reliable interactions. Unlike most voice assistants that depend on cloud-based APIs, FRIDAY is designed to operate entirely locally, providing a secure, cost-effective, and lightning-fast AI experience.\n\nBuilt as an evolution of existing voice assistants, FRIDAY leverages cutting-edge technologies like Ollama, Llama 3.2, and Gemma2 to deliver a personalized and efficient user experience. Additionally, as an open-source project, FRIDAY invites contributions from developers and AI enthusiasts around the world, enabling the community to improve, expand, and shape its future. This makes FRIDAY not just a tool, but a platform for innovation, where everyone can be part of its growth.\n\nThis publication documents the development, features, and innovations behind FRIDAY, showcasing its potential to revolutionize the voice assistant landscape. With a focus on privacy, speed, cost efficiency, and open-source collaboration, FRIDAY is leading the charge towards a new era of AI interaction.\n\nRelated work\n\nTraditional voice assistants like Siri, Alexa, and Google Assistant have become ubiquitous, but they rely on cloud-based APIs for processing. This reliance introduces several significant challenges, including latency, privacy risks, and a constant dependency on internet connectivity. Users' data is processed and stored remotely, creating concerns about data security and privacy.\n\nRecent advancements in local AI models such as Llama 3.2, and Gemma2 have opened up new possibilities for creating offline AI solutions. These models enable voice assistants to operate entirely on the user's device, without relying on cloud servers, solving many of the privacy and latency issues inherent in traditional systems.\n\nFRIDAY builds on these advancements by combining the power of local processing with user-friendly features. The result is an AI assistant that not only responds instantly but also ensures that all data is processed locally, providing a more secure and responsive user experience. As an open-source project, FRIDAY also allows the community to contribute, improve, and shape its development, making it a truly collaborative effort that stands out from the closed, proprietary systems used by traditional voice assistants.\n\nBy moving away from cloud dependence and embracing local AI models, FRIDAY offers a unique solution that prioritizes privacy, speed, and customizability—features that are often overlooked in traditional voice assistants. This open-source approach fosters continuous innovation, allowing FRIDAY to grow and evolve with contributions from a global community of developers and users.\n\nMethodology\n\nLocal AI Processing:\n\nUtilizes Ollama and Llama 3.2 or Gemma2 for offline AI processing.\n\nEnsures privacy by keeping all data on the user’s device.\n\nVoice Command Recognition:\nImplements local speech-to-text and text-to-speech libraries for seamless interaction.\nSpotify:\nEnables song playback and playlist management when connected to Wi-Fi.\nWeather Data:\nFetches real-time weather updates from the National Weather Service.\nText Messaging:\nSends reminders and messages via local SMS integration.\nUser Interface:\nFeatures a clean, intuitive interface for easy interaction.\nAs an open-source project, the UI is fully customizable, allowing contributors to suggest improvements or redesigns based on feedback from the community.\n\nExperiments\n\nTo evaluate FRIDAY's performance, I conducted a series of experiments designed to test its speed, offline functionality, and overall user experience. These experiments were crucial in assessing how FRIDAY compares to traditional cloud-based voice assistants, as well as its ability to function efficiently without an internet connection.\n\nThe first area of focus was speed comparison. To assess the speed of FRIDAY’s responses, I compared its online response times with those of cloud-based assistants, such as OpenAI-powered services. By running identical queries on both systems, I found that FRIDAY’s local processing delivered responses much faster, thanks to the elimination of server latency typically introduced by cloud-based systems. This local processing resulted in a much more instantaneous user experience, with the speed difference being particularly noticeable in tasks requiring real-time interaction.\n\nIn terms of hardware compatibility, I discovered that Llama 3.2 struggled to perform optimally on lower-end hardware, which could lead to delays and performance issues on less powerful devices. To address this, FRIDAY now includes Gemma2, a version of the AI designed to work efficiently on lower-end hardware. Gemma2 has been successfully tested on a wide range of devices, from high-performance computers worth 60 Raspberry Pi 5, ensuring that FRIDAY remains accessible to a broad range of users.\n\nFinally, I tested offline functionality, which is one of the primary goals of FRIDAY. I executed a variety of tasks without an internet connection, including setting reminders, writing short papers, answering general knowledge questions, and performing basic calculations. In all cases, FRIDAY successfully completed the tasks, demonstrating its robustness and reliability in offline environments. This offline capability is one of FRIDAY’s key advantages, allowing users to rely on the assistant even in areas with limited or no internet connectivity.\n\nResults\n\nThe experiments yielded promising results, showcasing FRIDAY’s potential as a next-generation voice assistant. In terms of speed, FRIDAY’s local processing capabilities ensured near-instant responses, outperforming cloud-based solutions that often suffer from network latency. This speed advantage was particularly noticeable in tasks requiring real-time interaction, such as setting reminders or answering queries. In direct comparison, FRIDAY’s offline speed is now faster than the most advanced AI assistants like Alexa and Siri by at least 2 seconds, significantly enhancing the user experience.\n\nPrivacy was another major win for FRIDAY. By keeping all data on the user’s device, FRIDAY eliminated the privacy risks associated with cloud-based processing. Many users will appreciate the peace of mind that comes with knowing their personal data is not being sent to external servers or stored in the cloud, making FRIDAY a highly secure and private solution. As an open-source project, FRIDAY allows users and developers to inspect and ensure that data privacy is maintained, offering transparency and control.\n\nCost efficiency was also a significant benefit. Unlike traditional voice assistants, which rely on expensive API subscriptions for cloud processing, FRIDAY operates entirely offline and is completely free to use. This makes FRIDAY accessible to a wider audience, including those who may not want to invest in premium voice assistant services or who need a more affordable solution for personal or low-resource devices. The open-source nature of FRIDAY also means that users can contribute to its development, improving its functionality without incurring additional costs.\n\nDiscussion\n\nFRIDAY represents a significant step forward in voice assistant technology. By leveraging local AI processing, it effectively addresses critical issues like privacy, latency, and cost that are commonly associated with cloud-based assistants. The ability to process all data locally ensures that users' privacy is protected, while the offline functionality eliminates the need for an internet connection, providing faster responses and greater control over data. However, despite these advancements, challenges remain, particularly in expanding FRIDAY’s feature set. Future development will focus on enhancing its capabilities by integrating AI vision, enabling image and video generation, and adding alarm clock functionality to make FRIDAY even more versatile. These updates will allow FRIDAY to offer a more comprehensive AI experience and bring it closer to fulfilling its potential as a truly all-encompassing personal assistant.\n\nConclusion\n\nFRIDAY is more than just a voice assistant—it represents a vision for the future of AI. By combining local processing with user-centric features, FRIDAY offers a faster, more private, and cost-effective alternative to traditional voice assistants. Its offline capabilities ensure that users are not reliant on an internet connection, while its open-source nature encourages community involvement and continuous improvement. As an innovative step forward in voice assistant technology, FRIDAY is positioned to redefine the way users interact with AI. I invite the AI community to explore FRIDAY’s potential, contribute to its ongoing development, and help shape the future of personal AI.\n\nReferences\n\nOllama\n\nLlama 3.2\n\nGemma2\n\nNational Weather Service\n\nOpenAI Whisper\n\nThe Open Source Community\n\nAcknowledgements\n\nI would like to thank the open-source community for their contributions to local AI technologies. As well as the developers of Ollama, Whisper, Llama 3.2, and Gemma2 for making this project possible.\n\nAppendix\n\nCode Snippets\n\nInstall Ollama from their website\n\nSetting Up Ollama:\n\nFor High Performance Hardware:\nbash-\nollama run llama3.2\nollama serve\n\nOR\n\nFor all hardware compatibility:\nbash-\nollama run gemma2:2b\nollama serve\n\nInstalling Required Libraries:\n\nbash-\npip install -r requirements.txt\nRunning FRIDAY:\n\nFork code from\n\nGitHub - Friday AI Voice Assistant\n\nPlease note that accounts will need to be setup for Spotify Developer, for any Spotify - related functions.\nAdditionally, a Gmail account will need to be setup for your assistant to be able to send text messages.\n\nFor any comments or concerns:\nEmail: \nRaypoulton11@yahoo.com\n\nPhone: 443-896-4231\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode\nFiles\nFriday Introduction.mp4\nFriday Weather_Report.mp4",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "from-text-to-data-extracting-structured-information-on-novel-characters-with-rag-and-langchain-YxEVcZtGwccw",
    "username": "Ayush Gawande",
    "license": "MIT License",
    "title": "From Text to Data: Extracting Structured Information on Novel Characters with RAG and LangChain",
    "publication_description": "Back to publications\nMar 24, 2025\n●\n47 reads\n●\nMIT License\nWinner of\nBest Creative AI Project\nat the\nAgentic AI Innovation Challenge 2025\nFrom Text to Data: Extracting Structured Information on Novel Characters with RAG and LangChain\nAgenticAI\nAI in Literature\nGemini\nInformation Extraction\nLangChain\nLLM\nNatural Language Processing (NLP\nRetrieval-Augmented Generation (\nStructured Data Extraction\nText Analysis\nAyush Gawande\nLike\nBookmark\nShare\nUnravelling Literary Figures,\n\nOne Story at a Time - Getting Info about your favourite novel characters just got easier!\n\nProject\n\nHave you ever found yourself immersed in a novel, only to realize you've forgotten the role or relationships of certain characters? Or perhaps you've been curious about how a side character connects to the protagonist but couldn't recall the details. Such challenges are common, especially when navigating intricate narratives with extensive casts. To address this, I have developed a Python-based Command Line Interface (CLI) tool designed to automatically extract and organize detailed information about characters from unstructured text. Leveraging advanced Natural Language Processing (NLP) techniques, including Retrieval-Augmented Generation (RAG) and the LangChain framework, this tool transforms narrative text into structured data, offering readers clear insights into character roles, relationships, and traits. Additionally, when provided with multiple stories, the tool can identify which narrative a particular character belongs to, enriching the reading experience and aiding literary analysis.\n\nIntroduction\n\nCharacters are the heart of any narrative, driving the plot and engaging readers through their journeys. However, as stories grow in complexity, so does the task of tracking character roles, relationships, and developments. My project introduces a tool that automates the extraction of structured character information, providing users with:​\n\nRole Identification: Determining whether a character is a protagonist, antagonist, or side character.​\n\nRelationship Mapping: Elucidating connections between characters, such as familial ties, friendships, or rivalries.​\n\nTrait Description: Offering brief overviews of characters' attributes and personalities.​\n\nStory Association: Identifying the specific story or stories a character appears in when analyzing multiple narratives.​\n\nBy converting unstructured narrative text into structured data, this tool facilitates deeper literary analysis, supports academic research, and enhances reader engagement.\n\nInspiration & Background\n\nThis project originated from an interview challenge for a tech company that required me to demonstrate my experience with Retrieval-Augmented Generation (RAG) and Python. This challenge inspired me to address a common issue faced by readers: the difficulty of keeping track of complex character webs in literature. Reflecting on my own struggles to remember each character's significance and their connections often hindered my full engagement with the story. I realized that this is a shared dilemma among many readers. With that in mind, I set out to create a solution that not only helps ease these challenges but also enhances the overall comprehension and appreciation of literary works.\n\nAdditionally, this project aims to transform random stories into structured data. By analyzing narrative elements like character relationships, plot points, and themes, the tool can present these components in an organized manner. This structured data not only simplifies the understanding of the storyline but also allows readers to easily reference character backstories, motivations, and their interactions. Overall, this approach fosters a deeper engagement with the text and helps readers navigate complex narratives with greater ease.\n\nFeatures\nExtract structured character information from story texts\nVector database storage for efficient searching\nCLI interface for easy interaction\nOutputs character information in JSON format\nUses Google's Gemini AI model for accurate information extraction\nMethodology\nProject Structure\n\nGithub\n\nTechnical details\nUses Google's Gemini Pro model for text generation\nImplements ChromaDB as the vector store\nUses LangChain for workflow management\nEmploys text chunking for efficient processing\nImplements error handling for edge cases\nTechnical Implementation\n\nThe development of this project involves several key components:\n\n1. Data Collection\n\nThe tool processes narrative texts stored in the stories directory. These texts serve as the input from which character information is extracted.\n\n2. Text Splitting and Embedding\n\nTo manage large texts effectively, I utilized the LangChain framework to split the narratives into manageable chunks. Each chunk is then converted into embeddings—numerical representations of text—using Google's Gemini Pro model for text generation. These embeddings facilitate efficient retrieval of relevant text segments during the extraction process.\n\n3. Vector Storage\n\nThe generated embeddings are stored in ChromaDB, an open-source vector database optimized for AI applications. This setup enables quick and accurate retrieval of text chunks relevant to specific queries, which is crucial for the Retrieval-Augmented Generation (RAG) process.\nCHROMA\n\n4. Retrieval-Augmented Generation (RAG)\n\nRAG combines retrieval-based and generation-based models. In this project, when a query about a character is posed, the system retrieves relevant text chunks from ChromaDB and uses them to generate structured information about the character. This approach ensures that the generated information is both contextually relevant and accurate.\n\n5. Output Generation\n\nThe extracted character information is structured into a JSON format, providing a clear and organized representation of attributes such as name, role, relationships, and traits. This structured data can be easily analyzed, visualized, or integrated into other applications.\n\n6. Command Line Interface (CLI)\n\nI developed a user-friendly CLI to interact with the tool. Users can input commands to process specific stories and extract character information, making the tool accessible to individuals with varying levels of technical expertise.\n\n7. Error Handling\n\nTo enhance robustness, the tool implements comprehensive error-handling mechanisms to address edge cases, such as missing data or ambiguous character references, ensuring reliable performance across diverse narratives.\n\nBy integrating these components, the tool effectively transforms unstructured narrative text into structured character information, enhancing readers' engagement and understanding of complex literary works.​\n\nResults & Usage\nPrepare Your Stories\n\nPlace your story files in the stories directory. Each file should:\n\nBe in .txt format\nContain a single story\nHave a filename that represents the story title (e.g., game_of_thrones.txt)\nCompute Embeddings\n\nProcess all stories and generate embeddings:\n\npython cli.py compute-embeddings stories/\nExtract Character Information\n\nGet information about a specific character:\n\npython cli.py get-character-info \"Character Name\"\nSample Output\n\nWhen extracting information about a character, you'll get a JSON response like this:\n\nThe output includes:\n\nCharacter name\nStory title\nCharacter summary\nRelations with other characters\nCharacter type/role\nExample Response\n{\n    \"name\": \"Jon Snow\",\n    \"storyTitle\": \"Game of Thrones\",\n    \"summary\": \"Jon Snow is a brave and honorable leader who serves as the Lord Commander of the Night's Watch and later unites the Free Folk and Westeros against the threat of the White Walkers.\",\n    \"relations\": [\n        {\"name\": \"Arya Stark\", \"relation\": \"Sister\"},\n        {\"name\": \"Eddard Stark\", \"relation\": \"Father\"}\n    ],\n    \"characterType\": \"Protagonist\"\n}\nExample Story Files\n\nYour story files should contain narrative text that includes character interactions and descriptions.\nExample dataset.\n\nStories\n\nExample results with random characters\n\nTechnicals\nRequirements\nPython 3.8+\nGoogle API key with access to Gemini AI\nInternet connection for API calls\nError Handling\n\nThe tool handles various edge cases:\n\nCharacter not found in stories\nInvalid API key\nMalformed story files\nConnection issues\nFunction Snippets\n\nLink to copy prompt template\n\nConclusion\n\nThis project demonstrates the potential of integrating advanced NLP techniques, such as RAG and LangChain, to bridge the gap between unstructured literary narratives and structured data analysis. By automating the extraction of character information, the tool offers a valuable resource for readers, educators, and researchers, fostering a deeper understanding and appreciation of literature.​\n\nFor more details and access to the code plus more such projects please visit my GitHub repositories:\n\nGithub Ayu5-h\n\nFeel free to follow for more such works\n\nReferences\n\nhttps://github.com/Ayu5-h/Structured-Information-Extraction---Character-Info\n\nhttps://js.langchain.com/docs/introduction/?utm_source=chatgpt.com\n\nhttps://docs.trychroma.com/docs/overview/introduction\n\nhttps://python.langchain.com/docs/concepts/rag/\n\nhttps://cookbook.chromadb.dev/embeddings/bring-your-own-embeddings/\n\nhttps://medium.com/%40pierrelouislet/getting-started-with-chroma-db-a-beginners-tutorial-6efa32300902\n\nhttps://www.historica.org/blog/the-application-of-artificial-intelligence-in-literary-text-analysis-modern-approaches-and-examples\n\nhttps://medium.com/%40shraddhakhanapur408/analyzing-literary-character-relationships-using-python-ce89e81f257c\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nUnravelling Literary Figures,\n\nProject\n\nIntroduction\n\nInspiration & Background\n\nFeatures\n\nMethodology\n\nProject Structure\n\nTechnical details\n\nTechnical Implementation\n\nData Collection\nView all\nCode\nDatasets",
    "awards": [
      "best creative ai project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "google-workspace-agent-from-prototype-to-real-world-readiness-dHzxP8y0Mt0u",
    "username": "Manuel Quesada",
    "license": "MIT License",
    "title": "Google Workspace Agent: From Prototype to Real-World Readiness 🚀",
    "publication_description": "Back to publications\nSep 05, 2025\n●\n130 reads\n●\nMIT License\nGoogle Workspace Agent: From Prototype to Real-World Readiness 🚀\nAgenticAI\nAIProductization\nGoogleWorkspace\nLangGraph\nMultiAgentSystems\nManuel Quesada\nU\nUtkarsh Dubey\nLike\nBookmark\nShare\n\nBuilding agentic AI systems isn’t just about proving they can work, it’s about making them work reliably outside the lab. The central challenge is moving from a prototype that “shows what’s possible” to a production-ready system that delivers robustness, safety, usability, and resilience every day.\n\nOur Google Workspace Agent illustrates this transition. It began as a simple prototype linking Gmail, Calendar, and Contacts through natural language requests with LangGraph’s orchestration. In this update, we focus on how we hardened the system, adding testing, security guardrails, logging, a secure API, an interactive UI, resilience features, and Docker deployment to make it closer to real-world readiness.\n\nQuick Recap: What the Previous Publication Covered 🧭\n\nIn the previous article: \nGoogle Workspace Agent: Your AI Assistant for Seamless Workspace Management 🤖🚀\n, we explored the distinction between workflows and agents, showing when predictable automations suffice and when adaptive intelligence is needed. To illustrate this shift, we introduced the Google Workspace Agent, capable of coordinating Gmail, Calendar, and Contacts through natural language interaction.\n\nThe architecture relied on a multi-agent system powered by LangGraph, where the Orchestrator planned tasks and delegated them to specialized Managers (for calendar, email, contacts, and date resolution). These managers acted as intelligent routers, while lightweight ReAct workers executed domain-specific actions. Together, they enabled features like multi-step automation, context awareness, memory across conversations, and multi-account support.\n\nThat first publication concluded with a hands-on demo: running the Orchestrator from a Jupyter notebook to create events, draft emails, and retrieve contacts, showing the agent in action as a working prototype. This new update builds on that foundation, shifting the focus toward production readiness and real-world deployment.\n\nWhat’s New Since the Prototype ✨\n\nSince releasing the prototype, we focused on turning the Google Workspace Agent into a production-ready system by adding features that strengthen reliability, safety, and usability.\n\nOne of the most important updates is the Search Manager, which was straightforward to integrate thanks to the modular architecture we designed from the start. This new component brings grounding to user questions that involve general knowledge, reducing the chances of hallucinations and improving the agent’s ability to provide meaningful, well-informed answers.\n\nBeyond that, we introduced comprehensive logging across entry and exit points, checkpoints, and error traces to give full visibility into agent behavior. We also built a secure API layer with FastAPI and API Key authentication, exposing endt, history, and account management. The /chat endpoint now leverages Server-Sent Events (SSE) to stream reasoning steps in real time, letting users literally “see the agent think.”\n\nOn the testing side, we expanded coverage with pytest, adding unit and integration tests that validate tools, input handling, Google API calls, and API endpoints. To enhance safety, we integrated guardrails-ai, filtering inputs before they reach the Orchestrator to prevent prompt injection or policy violations.\n\nFinally, we created a Streamlit interface on top of the API to provide a simple but effective chat panel, and we containerized the system with Docker to ensure consistent deployment across environments.\n\nIn the sections that follow, we’ll explore these new components in more depth, showing how each contributes to making the agent not just functional, but truly ready for real-world use.\n\nToolset 🛠️\n\nTo bring the Google Workspace Agent closer to real-world readiness, we built the system on a carefully chosen stack of technologies. Each component plays a distinct role in reasoning, orchestration, persistence, or interface, while keeping the architecture modular and maintainable. This separation of concerns allows us to extend functionality, integrate new managers, and scale with minimal friction.\n\n⚙️ Tech Stack\nComponent\tTool / Service\tPurpose\nLLM Provider\t\nOpenAI\n\tThe core brain of the system, handling natural language understanding, reasoning, and step-by-step planning with consistent outputs.\nAgentic Framework\t\nLangGraph\n\tProvides a graph-based orchestration layer for managing multi-agent workflows, state propagation, and complex execution plans.\nLLM Integration\t\nLangChain\n\tSupplies the foundation for tool creation and prompt management, seamlessly integrating with LangGraph and LangSmith.\nModel Tracking\t\nLangSmith\n\tEnables monitoring, debugging, and evaluation of every reasoning step and tool call with a visual trace of agent behavior.\nPersistence\t\nPostgreSQL\n / \nSQLite\n\tStores conversation history, checkpoints, and credentials, ensuring persistence across sessions and reliable recovery.\nAPI Integration\t\nGoogle Auth/API Client\n\tProvides secure access to Gmail, Calendar, and Contacts, handling authentication flows and token refresh automatically.\nAuthentication\t\nGoogle Auth\n\tManages OAuth2 credentials, consent flows, and multi-account associations for seamless user login and account linking.\nTesting Suite\t\nPytest\n\tValidates the system with unit, integration, and end-to-end tests, ensuring reliability across tools, APIs, and workflows.\nAPI Layer\t\nFastAPI\n\tExposes system capabilities through a secure REST API with API Key authentication and SSE-powered streaming chat.\nUser Interface\t\nStreamlit\n\tProvides a conversational UI built on the API, allowing users to interact with the agent in a simple, intuitive panel.\nEnvironment\t\nPoetry\n\tEnsures dependency management, reproducibility, and virtual environments, simplifying setup and deployment pipelines.\nLogging & Traceability 📜\n\nThe first step toward making our system production-ready was to ensure visibility into what is happening at every moment inside the system. This is not about monitoring the LLM itself, but about tracking the flow of execution across our own components, so that we always know what the system is doing, where it might fail, and how it behaves under different conditions. Logging, in this sense, is the backbone of reliability, debugging, and operational trust.\n\nTo achieve this, we built a centralized logging component implemented as a singleton, ensuring consistent behavior throughout the codebase. Each log entry is tagged with the module name, which allows us to immediately identify the origin of any event. This logger is systematically used across all modules, classes, and functions.\n\nOur logs capture entry and exit points of orchestrators, managers, and tools, along with critical checkpoints such as API calls, credential handling, token refreshes, and inter-manager coordination. They also document unexpected behaviors like exceptions, guardrail blocks, timeouts, retries, and fallbacks. With this structured approach, every interaction leaves a transparent trace that can be followed end to end.\n\nBy logging both normal operations and anomalies, we can reconstruct full execution flows, diagnose errors, analyze performance bottlenecks, and provide clear evidence of system behavior at any point in time. This continuous visibility is what allows us to move confidently from prototype to production, knowing the system is not a black box but a transparent, traceable process.\n\nTesting & Validation 🔍\n\nEnsuring production readiness required a multi-layer testing strategy that covered everything from deterministic unit tests to human-in-the-loop evaluations. Our goal was to validate not only that each component behaves correctly in isolation, but also that the system as a whole responds safely and reliably under real-world conditions.\n\nUnit Tests for Deterministic Components\n\nWe created extensive unit tests to validate the core building blocks of the system, using fakes for external APIs to ensure consistency while still covering edge cases. Key areas included:\n\nReAct Tools (Calendar, Email, Contacts): Verified both happy paths and error cases such as missing authentication, malformed parameters, and conflict detection.\n\nPrompt Builder Module: Ensured structured templates were correctly generated with variables, roles, context, and reasoning strategies.\n\nGoogle Service Layer (google_service package): Validated handling of user credentials, token refresh, and persistence in the database, guaranteeing correctness of credential storage and retrieval.\n\nGraph & Checkpointer: Confirmed the construction of the orchestration graph and checkpointing mechanisms to preserve state across sessions.\n\nAPI Endpoints: Tested the FastAPI layer, including /chat, /history, and account management endpoints, with particular focus on the chat endpoint using SSE, ensuring events stream correctly step by step.\n\nEdge Cases: Covered users without Google accounts, revoked access tokens, and malformed requests, confirming the system provides clear and user-friendly error messages in each case.\n\nTogether, these tests ensure that deterministic components behave consistently and handle unexpected conditions gracefully.\n\nIntegration Tests with Real LLM Calls\n\nWe complemented deterministic unit tests with integration tests involving real LLM calls, allowing us to validate reasoning and multi-agent coordination under more complex scenarios. These tests focused on:\n\nMalformed and Toxic Inputs: Ensured the agent responds safely to adversarial prompts, inappropriate queries, or nonsensical inputs.\nOrchestration Planning: Verified that the Orchestrator generates the correct step-by-step execution plan for managers, respecting logical dependencies (e.g., resolving dates before checking calendars).\nMemory Management: Validated persistence and trimming of conversational context, ensuring continuity across interactions without exceeding limits.\nReAct Workers: For each worker (Calendar, Email, Contacts), tested tool selection and correct invocation sequences, using controlled fake tool outputs to simulate real-world responses.\n\nThese tests provided confidence that the system could adaptively reason, route tasks, and coordinate managers under dynamic conditions.\n\nHuman-in-the-Loop Evaluation\n\nSome behaviors are too nuanced to be validated reliably with automated tests. For this reason, we included human judge evaluations using Jupyter notebooks in src/evaluation_notebooks. These notebooks simulate a wide variety of user scenarios and allow human reviewers to assess the quality of system responses.\n\nComplex Multi-Manager Flows: Assessed correctness in scenarios where multiple managers must work together, such as combining calendar availability with contact retrieval and email drafting.\nAmbiguity Handling: Validated the Verifier step when disambiguating contacts, clarifying vague instructions, or resolving incomplete information.\nError Recovery: Checked that the system provides transparent feedback and fallback behavior when data is missing, access is revoked, or external APIs fail.\nSafety & Security: Evaluated responses to malicious prompts, policy violations, or unsafe requests, confirming that guardrails prevent harmful outputs.\n\nBy combining deterministic testing, integration validation, and human judgment, we ensure that the system is not only functionally correct, but also safe, resilient, and aligned with user expectations.\n\nSafety & Content Controls 🛡️\n\nA critical requirement for production readiness is ensuring that the system behaves safely, consistently, and in line with clear usage policies. For our agent, this meant implementing guardrails, sanitization, and strict output controls that prevent misuse and protect both the user and the system.\n\nWe integrated validators from the \nguardrails-ai hub\n to detect and reject prompt injection attempts or unusual prompts that could manipulate the agent’s behavior. In addition, another validator filters out requests that fall into sensitive categories that we explicitly prohibit:\n\nNo planning of criminal activities.\nNo encouragement of self-harm or suicide.\nNo promotion of firearms or illegal weapons.\nNo promotion of illegal drugs.\nNo generation of sexually explicit content.\nNo incitement of violence or hate.\n\nIf a request is rejected under one of these rules, it is never recorded in the agent’s memory. Instead, the user receives immediate and clear feedback explaining why their request cannot be processed.\n\nImportantly, our system does not risk exposing sensitive information such as Google credentials, API keys, or environment secrets. The agent simply does not have access to such data, which removes a major vector of concern when thinking about data leakage.\n\nWhen interacting with external sources (e.g., Google Contacts, Gmail, Calendar, or web search), we avoid leaving extraction decisions to the LLM. Each manager enforces fixed limits, for instance, a set maximum number of contacts, events, or emails retrieved. For emails specifically, we also clean responses to strip away signatures, headers, or unnecessary metadata, ensuring only essential content enters the LLM’s context window.\n\nAs noted in the testing section, we included unit tests covering toxic, malformed, or adversarial prompts. These tests confirm that the agent consistently produces polite, professional, and safe responses, even when faced with hostile or nonsensical input.\n\nFinally, because this system is designed to be used as a personal assistant in production, where usage could be tied to cost-per-request or daily quotas, we focused primarily on preventing harmful misuse rather than limiting how a legitimate user might overuse the system. Abuse-prevention models can be introduced later at the business layer, but for now, our priority is ensuring that every accepted request is processed safely and responsibly.\n\nObservability & Resilience ⚙️\n\nIn traditional systems, logs are often enough to trace what went wrong: you have a record of inputs, outputs, and error stacks. But in agentic systems, this level of visibility is no longer sufficient. It’s not just about knowing that a request failed; we need to understand how the agent reasoned step by step, what prompts were generated, which tools were invoked, how the state evolved, and where things started to drift off course.\n\nFor this reason, we integrated LangSmith, which offers a seamless fit with the LangGraph and LangChain ecosystem. Beyond its tight integration, LangSmith provides a web-based interface that makes the inner workings of the agent transparent:\n\nYou can replay executions (reruns) to validate hypotheses about failures.\nYou can inspect prompt inputs/outputs at each orchestration step.\nYou can monitor latency, token usage, error rates, tool usage, and more.\nMost importantly, you can observe how the agent’s state (memory) evolves in real time.\n\nThis fine-grained visibility allows us to pinpoint exactly where things started to go wrong and why. Even the best evaluation setups cannot catch every failure mode; some only appear in production. That’s why observability is a first-class concern in our design. Over time, production signals like edge-case queries, fallback triggers, escalation rates, or even explicit user feedback become invaluable for improving alignment and reliability.\n\nBuilding for Resilience\n\nObservability alone is not enough. Our system orchestrates a large number of tools, and each tool introduces potential points of failure. To make the system truly robust, every external API call is wrapped in a protective function that enforces multiple layers of resilience:\n\nRate & Call Limiting (max_calls): prevents tools from running indefinitely, protects against infinite loops or abusive usage, and ensures graceful degradation when limits are reached.\n\nRepetition Detection (last_k_elements_equal): identifies repetitive tool calls that signal faulty or adversarial behavior, and stops them before they consume resources unnecessarily.\n\nControlled Retries (retries): automatically retries failed calls, increasing the likelihood of success when facing transient errors such as network instability or temporary API rate limits.\n\nExponential Backoff with Jitter (backoff_base, backoff_max): spreads out retries over time (1s, 2s, 4s, 8s…), adds randomness to avoid synchronization, and reduces the risk of overwhelming external services (thundering herd problem).\n\nGraceful Degradation: when retries are exhausted or limits are hit, the system responds with clear fallback messages instead of breaking the entire flow. The agent continues operating with reduced functionality rather than failing completely.\n\nStructured Logging: every tool call is logged with context, including errors, warnings, retries, and parameters. This structured trail supports observability and enables more intelligent decisions about when to stop, escalate, or adapt behavior.\n\nBy combining deep observability (via LangSmith) with layered resilience in tool orchestration, the agent becomes both transparent and fault-tolerant. It can handle real-world variability without collapsing under unexpected conditions, giving developers the necessary visibility to improve its reliability continuously.\n\nAPI Layer 🔑\n\nOne of the most important steps in moving from prototype to production was exposing the system through a dedicated API. This makes the agent accessible not only to developers and applications, but also to user interfaces and third-party integrations. An API transforms the agent from an isolated prototype into a service that can be consumed securely and consistently in different contexts.\n\nWe chose FastAPI as the backbone for this layer because it strikes the right balance between lightweight performance and enterprise-grade capabilities. It is modern, efficient, and asynchronous by design, making it well-suited for handling multiple user requests in parallel. At the same time, it comes with built-in support for dependency injection, validation, and documentation, which allows us to maintain a clear and reliable contract between the backend and any clients that consume the service. In practice, this means we can evolve and extend the system without breaking existing integrations.\n\nThe API exposes several endpoints, each designed to cover a specific aspect of the user’s interaction with the agent:\n\nUser Management (/user): Create a user, retrieve user information, or manage their associated Google accounts. This ensures that each interaction is linked to a specific identity and context.\n\nAccount Association (/user/associate_account and /user/dissociate_account): Allow users to securely link or unlink their Google accounts, making it possible to switch between work, personal, or multiple accounts seamlessly.\n\nConversation History (/chat/history): Retrieve the state of past interactions, so the user and the agent can maintain continuity across sessions.\n\nChat Endpoint (/chat): The heart of the API, enabling live conversations with the agent.\n\nA key feature of the chat endpoint is its use of SSE. Instead of working like a traditional system, where a user sends a request and waits silently until the final response arrives, the SSE implementation streams updates in real time. The user sees progress events as the agent analyzes the request, calls external tools, and synthesizes an answer, followed by content events that gradually build the assistant’s reply. If an error occurs, it is streamed back immediately. This design delivers a far more engaging and transparent user experience, since people can follow the agent’s reasoning step by step rather than waiting in the dark.\n\nFrom the backend perspective, SSE also represents a major improvement. It prevents the API from being a simple “input → output” black box by exposing the internal state transitions of the Orchestrator in a structured way. This not only improves usability but also makes debugging and monitoring significantly easier.\n\nSecurity is enforced through API Key authentication, where keys are generated exclusively by system administrators. This ensures that only authorized clients can access the API, preventing unauthorized individuals from reaching the system.\n\nBy providing this API layer, the Google Workspace Agent evolves from an experiment into a production-ready service: modular, secure, and capable of powering both direct user interfaces and broader integrations.\n\nUser Interface 💻\n\nJust as the API layer transforms the agent into a production-ready service, the user interface makes it accessible and enjoyable to use. A system that remains purely backend, even if robust, would feel incomplete without a simple and intuitive way for people to interact with it. The UI is where the power of the agent becomes tangible: users can communicate in natural language, switch contexts, and see the agent’s reasoning unfold in real time.\n\nWe chose Streamlit as the framework for building this interface. Streamlit aligns perfectly with our goals: it is lightweight, fast to develop with, and optimized for interactive applications. Unlike heavier web frameworks, Streamlit lets us focus on user experience rather than low-level UI plumbing. Its reactive design and Python-native workflows make it a natural fit for prototyping and production alike, allowing us to bridge the gap between complex backend orchestration and human-centered usability.\n\nThe interface provides several key features designed to abstract technical complexity while enhancing usability:\n\nChat Panel with Live SSE Streaming: at the interface's core a real-time chat panel powered by SSE. Instead of waiting for a static response, users see the agent’s “thoughts” and reasoning steps streamed progressively. This gives visibility into how the agent is processing requests and makes interactions feel dynamic, transparent, and engaging.\n\nGuided Error Handling: when the agent needs clarification, such as when the Verifier detects ambiguity, the UI surfaces inline guidance and clear error messages. Instead of cryptic backend errors, users are offered actionable next steps, making retries or clarifications effortless.\n\nAbstracted Technical Complexity: the design principle behind the interface is to hide complexity without losing transparency. Users do not need to worry about API keys, tokens, or orchestration logic; the UI handles these in the background, while still making visible the reasoning process that matters for trust and usability.\n\nTogether, these features turn the Streamlit app into more than just a demo client: it is a practical, human-friendly interface that lowers barriers to adoption and encourages exploration of the agent’s capabilities.\n\nUsing the Streamlit App\n\nInteracting with the agent through the Streamlit app follows a natural, guided flow:\n\nLogin with API Key: users begin by logging into the application with their username and API key. This establishes a secure connection with the backend services and ensures that only authorized individuals can access the system. To obtain a valid API key, users must contact the system administrators, who are responsible for generating and distributing credentials. Additionally, while the Google Cloud project remains unverified, only pre-registered accounts explicitly added through the Google Cloud Console are allowed to log in. This safeguard ensures that access stays restricted to approved users during the early stages of deployment.\n\nAssociate a Google Account: once logged in, the next step is to link a Google account. The application uses OAuth credentials stored securely in environment variables, allowing the backend to refresh tokens automatically. Associating an account is required before starting a conversation, ensuring that all interactions are tied to the correct identity and context.\n\nChat Operations\nWith an account linked, users can access the core functionality of the agent through the chat panel. From here, they can schedule meetings, manage contacts, retrieve conversation history, or perform searches, all through natural language.\n\nDissociate an Account\nIf a user no longer wishes to keep an account linked, they can easily dissociate it. This provides flexibility and ensures that switching between personal and work contexts remains seamless.\n\nAccess Chat History\nEvery interaction is stored and can be revisited through the chat history panel. Each session has a unique identifier, making it simple to retrieve and continue past conversations whenever needed.\n\nDemo\n\nFor a quick demonstration of the interface in action, check out the screen recording \nhere\n.\n\nDeployment with Docker 🐳\n\nDeployment is the final and most crucial step to take our agent from prototype to production. By containerizing every component of the system, we ensure reliability, consistency, and ease of reproduction across environments. A fully Dockerized setup eliminates the classic “works on my machine” problem and allows both developers and non-technical users to spin up the entire stack with just a few commands.\n\nSystem Requirements\n\nOne of the key advantages of our design is that the agent has minimal hardware requirements. The heavy lifting, language model inference, and external tool execution are delegated to APIs. This means that running the system locally or in the cloud does not require GPUs or large-scale infrastructure. The only dependencies are:\n\nPython libraries: installed automatically inside the Docker containers.\nPostgreSQL: used to store checkpoints and maintain agent memory across users and sessions (also managed by Docker).\nDocker Engine: installed on the host machine or cloud service.\n\nWith this setup, any computer or cloud instance capable of running Docker can host the system. Whether on a developer laptop, a small VM, or a container cluster, the environment remains identical and easy to reproduce.\n\nCore Components\n\nPostgreSQL Database: stores persistent checkpoints that allow the agent to maintain memory across sessions and users.\n\nBackend API: provides the orchestration and intelligence layer mentioned before, exposing endpoints to interact with the agent.\n\nWeb UI: a lightweight client interface that lets end users easily interact with the agent without needing technical expertise.\n\nThis ensures consistency across development, staging, and production by providing identical environments. It offers isolation by encapsulating dependencies within containers, eliminating conflicts with local setups. The deployment gains portability, running seamlessly on cloud providers, on-premises infrastructure, or local machines wherever Docker is supported. Finally, it enables scalability, integrating smoothly with container orchestrators like Kubernetes or ECS to handle production workloads efficiently.\n\nQuick Start\n\nClone the repository and navigate into the project directory:\n\ngit clone https://github.com/MAQuesada/Google-Workspace-Agent.git\ncd google_workspace_agent\n\nConfigure environment variables in a .env file (sample provided as .env.example).\n\nBuild and start the containers using Docker Compose:\n\ndocker compose up --build -d\n--build ensures images are rebuilt if you make changes.\n-d runs containers in detached mode (in the background).\n\nVerify running containers:\n\ndocker ps\n\nStop the containers when no longer needed:\n\ndocker compose down\n\nBy combining a minimal set of requirements with full containerization, our agent is straightforward to deploy and operate. Developers, researchers, and organizations can focus on building with the agent, rather than spending time wrestling with environment setup or infrastructure concerns.\n\nObservations & Limitations 📉\n\nStrengths\n\nUnified natural-language access to Gmail/Calendar/Contacts with transparent streaming UX.\nSecurity posture: API Keys, OAuth, pre-processing guardrails, and output shaping.\nModularity: Managers/Tools are composable; Search Manager improves grounding.\n\nCurrent Limits\n\nOAuth Onboarding: Initial setup still technical for non-engineers.\nService Coverage: Drive/Docs/Meet not yet supported.\nUI: Streamlit is pragmatic but not yet a polished, product-grade frontend.\nMonitoring: Logging is in place, but we lack metrics that can trigger red flags when the system deviates from expected behavior.\nRisks: The main risk is not credential leakage but prompt injection via dynamic content (emails, events, web results).\nFuture Enhancements & Directions 🔬\nBroaden Google Integrations: Drive, Docs, Sheets, and Meet.\nAutomated Monitoring/Evaluation: Scenario suites with LangSmith + custom metrics for regressions.\nMulti-modal: Voice and visual context; attachment understanding.\nProductization: Deploy the service on a cloud provider for reliable hosting and scalability.\nContact & Contribution 📬\n🐙 GitHub: \nGoogle-Workspace-Agent\n📧 Email: \nmalejandroquesada@gmail.com\n, \nutkarsh251096@gmail.com\nIssues/PRs: Feedback, bugs, and contributions welcome!\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nQuick Recap: What the Previous Publication Covered 🧭\n\nWhat’s New Since the Prototype ✨\n\nToolset 🛠️\n\n⚙️ Tech Stack\n\nLogging & Traceability 📜\n\nTesting & Validation 🔍\n\nUnit Tests for Deterministic Components\n\nIntegration Tests with Real LLM Calls\n\nHuman-in-the-Loop Evaluation\n\nSafety & Content Controls 🛡️\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "handwritten-text-recognition-computer-vision-project-ipvk7atIwl2I",
    "username": "n@nivedita.chauhan",
    "license": null,
    "title": "Handwritten text Recognition/ Computer Vision Project ",
    "publication_description": "Back to publications\nSep 18, 2024\n●\n78 reads\nWinner of\nDistinguished Applied Solution Showcase\nat the\nComputer Vision Projects Expo 2024\nHandwritten text Recognition/ Computer Vision Project\nAI application\nazure document intelligence\ncomputer vision\nhandwritten text recognization\nmerchant analytics\nmerchant onboarding\nmobilev2net\nOCR\nN\n@nivedita.chauhan\nI\n@ishita.dhanuka\nLike\nBookmark\nShare\n\nOverview:\n\nOnboarding merchants with a bank is a crucial step in providing them with the financial infrastructure necessary to run their businesses efficiently. Banks offer a wide range of integrated payment solutions, including POS systems, payment gateways, and digital banking services, that streamline transaction processing, reduce operational costs and improve cash flow management. Additionally, partnering with a bank provides merchants enhanced fraud protection, customer support, and opportunities to scale their business with financing and growth tools.\n\nBut merchant onboarding at banks is a critical yet time-consuming process. Currently, this involves merchants manually filling out paper forms at bank branches. These forms are then scanned and sent to central processing centres located at 4 locations. At these locations, the forms are printed, and staff manually enter the data into the bank’s system. This method results in a turnaround time of 4-5 days before the data even begins verification, impacting the bank’s efficiency. Moreover, the volume of manual data entry increases the chances of errors, making data quality an ongoing challenge.\n\nObjective:\n\nOur project aims to revolutionise the merchant onboarding process by developing an automated pipeline that eliminates manual data entry. By digitising handwritten text from scanned onboarding forms and structuring the data for direct input into the bank’s system, we aim to reduce the turnaround time from 4-5 days to just a few minutes. The goal is to achieve efficiency while improving data accuracy and consistency, streamlining the entire process.\n\nData Collection:\n\nThe data for this project is collected by scanning onboarding forms filled out at bank branches. These scanned forms are saved in zip files, each of which contains multiple folders. Each folder represents a single onboarding session, containing 15-17 images in .tif format. However, only 3 of these images are relevant to the onboarding form, while the remaining images include irrelevant documents such as internal bank guidelines and supporting materials submitted by the merchant.\n\nData Pre-processing:\n\nGiven the nature of scanned documents, preprocessing is crucial for enhancing image quality. The pre-processing steps include:\n\nDenoising - Gaussian Blurring and median filtering are applied to remove noise and imperfections from the image.\n\nEnhancing Contrast - Histogram equalization and contrast stretching are used to improve text clarity by adjusting brightness and contrast, making the text more readable.\n\nResolution Standardization - Image resizing is performed using interpolation techniques ensuring a consistent resolution across all images.\n\nOrientation Correction - Contouring and edge detection are employed to identify skewed or misaligned text, followed by rotation or transformation, ensuring all images are aligned properly for further processing.\n\nOnce the images have been pre-processed, the next step is to identify the 3 relevant onboarding form images. Since these 3 images can appear anywhere within a set of 15-17 images, a classification model is necessary to streamline the identification process.\n\nImplementation\nStep 1: Image classification:\n\nInitially, a manually labelled dataset was created from 6,000 images. Each image was tagged according to whether it represented Page 1, Page 2, Page 3, or other irrelevant documents.\n\nUsing MobileNetV2 model, a pre-trained convolutional neural network (CNN), to classify these images. The same was chosen for its lightweight architecture, containing fewer parameters and 53 layers. Compared to other CNN models like ResNet and VGG,\n\nMobileNetV2 delivers higher processing speeds, reduced inference time, and lower computational complexity, making it ideal for this project.\n\nOnce trained, the model processes each folder of images and assigns a probability score for each class (Page 1, Page 2, Page 3, or Other). The image with the highest probability in each category is then tagged accordingly, ensuring only the relevant forms are identified and used in the automated pipeline.\n\nStep 2: Text Recognition Using Transformer Models and Azure Document Intelligence:\n\nTransformers, usually known for their exceptional handling of sequential data and attention mechanisms, proves highly effective for OCR tasks. So, a pipeline was built to prepare scanned images for the model. This included denoising, improving contrast, converting images to grayscale, and applying adaptive thresholding to separate text form noisy backgrounds. Techniques like dilation and contouring helped isolate text regions, while pixel averaging, and fixed pixel approaches ensured accurate text detection.\n\nDespite this, a significant challenge arose in mapping the extracted text to its correct fields due to inconsistent layouts of the forms and obstructive stamps. To address this, we integrated Azure Document Intelligence, a cloud-based solution that excelled in analysing document layouts and accurately mapping field values, significantly enhancing the overall process.\n\nIn Azure Document Intelligence, once the relevant form images are processed, the extracted data is initially saved in structured JSON format. This includes both field names and their corresponding values.\n\nTo further refine field extraction and tackle inconsistencies in field-value mapping, a dynamic origin shift logic is implemented where instead of relying on top-left corner of the form as starting point (which can often cause issues due to form misalignment or partial screening), we strategically placed a new origin point with the form, closer to where the relevant data begins. Additionally, a margin of error was introduced around the calculated pixel locations for each field, allowing some flexibility. This prevents overly rigid conditions and ensures that even if the form is slightly misaligned, the model can still correctly identify and extract the necessary fields.\n\nStep 3: Validating and Structuring Data\n\nOnce the data is extracted and text is prepared, validating and setting rules for each field becomes crucial to ensure accuracy. With more than 70 fields being processed, each field is assigned specific validation criteria.\n\nFor example, the PAN Card field is required to exactly have 10 characters: the first five alphabet, next four numeric and last one alphabetic. If any of the fields fails to meet the designated rule, it gets automatically highlighted in the output, making it easy to identify and correct errors quickly.\n\nImpact and Conclusion:\n\nOur solution drastically reduced the turnaround time for form processing. Tasks that previously took 8-10 minutes per form are now completed in just 20-30 seconds resulting in over a 95% reduction in processing time. This efficiency ensures that forms are processed and entered the system on the same day.\n\nThe model, with an 84% accuracy, has greatly enhanced data entry reliability. Rule based validation has ensured consistency and prepared the extracted data for analysis, enabling the bank to offer tailored services to merchants based on accurate onboarding information.\n\nBy eliminating manual intervention, bank has achieved more systematic and scalable onboarding with high quality data entry, leading to improved service, business growth and higher customer satisfaction.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nOverview:\n\nObjective:\n\nData Collection:\n\nData Pre-processing:\n\nImplementation\n\nStep 1: Image classification:\n\nStep 2: Text Recognition Using Transformer Models and Azure Document Intelligence:\n\nStep 3: Validating and Structuring Data\n\nImpact and Conclusion:\n\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nYou might be interested\nAutomating Invoice Validation: Leveraging AI for Scalable Document Processing\nS\nSep 18, 202461 reads\nAI in BankingAzure Document Intelligence+6\nIntelligent Invoice Processing System Using Vision-Language Models\nJ\nAug 20, 202524 reads\nai-fintechanomaly-detection+3\nIdentifying presence of Government-issued Personally Identifiable Information embedded in documents.\nS\nA\nDec 30, 202491 reads\nAI in Data PrivacyCybersecurity in Digital Service+12\nUPI Transaction Extractor: OCR-Based Data Parsing Tool\nDec 28, 202452 reads",
    "awards": [
      "distinguished applied solution showcase"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "how-ai-agents-reason-together-to-personalize-education-Hl8ZZnSoqkL1",
    "username": "kKennedy Kiragu Maina",
    "license": "MIT License",
    "title": "How AI Agents Reason Together to Personalize Education",
    "publication_description": "Back to publications\nOct 20, 2025\n●\n4 reads\n●\nMIT License\nHow AI Agents Reason Together to Personalize Education\nAAIDC\nAdaptive Learning\nAI Agents\nEducational Technology\nLangGraph\nMulti-Agent Systems\nOpenRouter API\nK\nKennedy Kiragu Maina\nLike\nBookmark\nShare\n\nThe AlkenaCode Adaptive Learning Platform is a production-grade, multi-agent system built for true personalization in education. Instead of fixed rules, eight AI agents reason together to adapt lessons, quizzes, and recommendations to each learner. Powered by FastAPI, Next.js, PostgreSQL, and Redis, the platform continuously evolves based on student performance and preferences.\n\nSystem Overview\n\nEight specialized agents coordinate through \nLangGraph\n, combining profiling, reasoning, and adaptive feedback. Sessions persist with cookies, and caching boosts response speed by over 70%.\n\nArchitecture\nMulti-Agent Collaboration\n\nEach agent focuses on a specific layer of personalization; from understanding users to generating adaptive content.\n\n1. Learner Profiler Agent\n\nPurpose: Analyzes onboarding data to create comprehensive, multi-dimensional learner profiles.\n\nHow it works: The Learner Profiler uses a \n4-node LangGraph workflow\n where each node specializes in a different aspect of profiling:\n\ndef create_learner_profile(onboarding_data: Dict) -> Dict:\n    \"\"\"\n    4-node workflow:\n    1. interest_analyzer - Categorizes interests (e.g., \"Python\" → \"Programming\")\n    2. skill_assessor - Evaluates skill level per interest area\n    3. learning_style_analyzer - Determines optimal content formats\n    4. profile_synthesizer - Combines insights into unified profile\n    \"\"\"\n    workflow = StateGraph(ProfilerState)\n    workflow.add_node(\"interest_analyzer\", interest_analyzer_node)\n    workflow.add_node(\"skill_assessor\", skill_assessor_node)\n    workflow.add_node(\"learning_style_analyzer\", learning_style_node)\n    workflow.add_node(\"profile_synthesizer\", profile_synthesizer_node)\n\n    workflow.set_entry_point(\"interest_analyzer\")\n    workflow.add_edge(\"interest_analyzer\", \"skill_assessor\")\n    workflow.add_edge(\"skill_assessor\", \"learning_style_analyzer\")\n    workflow.add_edge(\"learning_style_analyzer\", \"profile_synthesizer\")\n    workflow.add_edge(\"profile_synthesizer\", END)\n\n    return workflow.compile().invoke({\"onboarding_data\": onboarding_data})\n\nOutput: The profiler generates a comprehensive profile including overall skill level (beginner/intermediate/advanced), interest categories with priority rankings, learning style preferences (visual/reading/interactive/mixed), estimated learning pace, and a confidence score (typically 85-95%) indicating assessment certainty.\n\nExample:\n\n{\n  \"overall_skill_level\": \"beginner\",\n  \"category\": \"Programming\",\n  \"priority_topics\": [\"Variables\", \"Functions\", \"OOP\"],\n  \"learning_style\": \"visual\",\n  \"learning_pace\": \"moderate\",\n  \"confidence\": 0.95\n}\n2. Journey Architect Agent\n\nPurpose: Designs personalized learning paths with intelligent prerequisite mapping and topic sequencing.\n\nHow it works: Uses a 4-node LangGraph workflow combined with DuckDuckGo API for topic discovery:\n\ndef create_learning_journey(profile: Dict) -> List[Dict]:\n    \"\"\"\n    4-node workflow:\n    1. topic_expander - Discovers topics via DuckDuckGo Instant Answer API\n    2. prerequisite_mapper - Maps dependencies between topics using LLM reasoning\n    3. journey_sequencer - Orders topics for optimal learning progression\n    4. journey_finalizer - Adds milestones, unlock conditions, time estimates\n    \"\"\"\n    workflow = StateGraph(ArchitectState)\n    workflow.add_node(\"topic_expander\", topic_expander_node)\n    workflow.add_node(\"prerequisite_mapper\", prerequisite_mapper_node)\n    workflow.add_node(\"journey_sequencer\", journey_sequencer_node)\n    workflow.add_node(\"journey_finalizer\", journey_finalizer_node)\n\n    # Connect nodes\n    workflow.set_entry_point(\"topic_expander\")\n    workflow.add_edge(\"topic_expander\", \"prerequisite_mapper\")\n    workflow.add_edge(\"prerequisite_mapper\", \"journey_sequencer\")\n    workflow.add_edge(\"journey_sequencer\", \"journey_finalizer\")\n    workflow.add_edge(\"journey_finalizer\", END)\n\n    return workflow.compile().invoke({\"profile\": profile})\n\nKey Features: Dynamic topic discovery via \nDuckDuckGo API\n ensures current, relevant content. LLM-powered prerequisite reasoning maps dependencies (e.g., \"Functions\" before \"OOP\"). Every 5th topic marks a milestone checkpoint. Topics unlock only after achieving ≥60% mastery of prerequisites. Time estimates (2-15 hours per topic) help learners plan effectively.\n\nOutput: Ordered journey with 9-50 topics tailored to user interests and skill level.\n\nExample:\n\n[\n    {\n        \"topic\": \"Python Installation and Setup\",\n        \"position\": 1,\n        \"status\": \"available\",\n        \"prerequisites\": [],\n        \"estimated_hours\": 2,\n        \"description\": \"Set up Python environment and tools\"\n    },\n    {\n        \"topic\": \"Basic Syntax and Variables\",\n        \"position\": 2,\n        \"status\": \"locked\",\n        \"prerequisites\": [\"Python Installation and Setup\"],\n        \"estimated_hours\": 5,\n        \"unlock_conditions\": {\"mastery_required\": {\"Python Installation\": 60}}\n    },\n    {\n        \"topic\": \"Object-Oriented Programming Basics\",\n        \"position\": 5,\n        \"status\": \"locked\",\n        \"prerequisites\": [\"Functions and Scope\"],\n        \"estimated_hours\": 10,\n        \"is_milestone\": True  # Milestone marker\n    }\n]\n\n3. Performance Analyzer\n\nProcesses quiz history and updates mastery levels:\n\ndef analyze_performance(quiz_data):\n    mastery = {}\n    for topic, scores in quiz_data.items():\n        mastery[topic] = sum(scores[-3:]) / len(scores[-3:])\n    return mastery\n\nOutput: Comprehensive analysis including topic mastery scores (0-100 scale), skill level classifications per topic, knowledge gaps, strengths, learning velocity metrics, difficulty recommendations for Quiz Generator, and content complexity adjustments for Content Personalizer.\n\n4. Recommendation Agent\n\nSuggests new topics based on strengths and gaps:\n\ndef recommend_topics(profile, mastery):\n    return [\n        {\"topic\": \"Data Structures\", \"reason\": \"Fill knowledge gap\"},\n        {\"topic\": \"Advanced Functions\", \"reason\": \"Build on strengths\"}\n    ]\nContent & Assessment\nContent Personalizer\n\nGenerates adaptive materials with multi-model retries:\n\ndef generate_content(topic, level):\n    for model in MODELS:\n        try:\n            content = model.invoke(topic, level)\n            if validate(content):\n                return content\n        except:\n            continue\n    return fallback_content(topic)\nQuiz Generator\n\nProduces quizzes that match user mastery levels.\n\ndef generate_quiz(topic, difficulty):\n    return [{\"question\": \"What is a list comprehension?\", \"correct\": \"B\"}]\nSimplified Workflows\nOnboarding and Journey Setup\nsequenceDiagram\n    User->>Frontend: Complete onboarding\n    Frontend->>API: Send data\n    API->>Profiler: Analyze user\n    Profiler->>Architect: Build learning path\n    Architect->>DB: Save profile & journey\n    API->>Frontend: Return dashboard\n\nAdaptive Quiz Loop\nflowchart TD\n    A[User takes quiz] --> B[Analyze performance]\n    B --> C[Update mastery]\n    C --> D[Recommend topics]\n    D --> E[Adjust journey]\n\nKey Innovations\n\n1. Multi-Model Resilience\n\n3 models × 3 retries per operation achieves 99%+ success rate. System attempts primary model first, then automatically falls back to secondary and tertiary models. Self-correcting prompts analyze errors and adjust for next retry, progressively guiding models toward proper output format. Remains functional during API outages, model bugs, or performance degradation.\n\n2. Explainable Decisions\n\nEvery agent decision logged to agent_decisions audit table with: agent name, input data/context, output/recommendations, natural language reasoning, confidence scores (0-1), and timestamps. Enables learners to understand recommendations, educators to audit pedagogy, and developers to debug.\n\nExample audit log:\n\n{\n  \"agent_name\": \"performance_analyzer\",\n  \"decision_type\": \"mastery_update\",\n  \"reasoning\": \"User showed consistent improvement across 3 recent quizzes (60%, 80%, 80%). Updated mastery from 65% to 78.5% using weighted algorithm that prioritizes recent performance.\",\n  \"confidence\": 0.92,\n  \"input_data\": {\"quiz_scores\": [60, 80, 80], \"previous_mastery\": 65.0},\n  \"output_data\": {\"new_mastery\": 78.5, \"recommendation\": \"ready_for_advanced\"}\n}\n\n3. Weighted Mastery Tracking\n\nRecent performance carries 2× more influence than historical average. Prevents learners from being trapped by poor early performance; mastery scores climb rapidly with improvement. Balances quick adaptation with stability against volatile single-quiz swings.\n\n# Weighted algorithm\nnew_mastery = (old_mastery * attempts + current_score * 2) / (attempts + 2)\n\n4. Session Persistence\n\nCookie-based session management with 1-year expiration ensures learners never lose progress. Sessions survive page reloads, browser restarts, different tabs, and network interruptions. Long expiration allows weeks or months of breaks without losing place in personalized journey.\n\nSafety Measures & Content Filtering\nGuardrails Implementation\n\nAlkenaCode\n implements multiple layers of safety to ensure appropriate, educational content:\n\n1. Input Validation & Sanitization\n2. LLM Response Filtering\n\nAll AI-generated content passes through validation:\n\n3. Rate Limiting\n\nPrevents abuse and ensures fair resource usage:\n\n4. Content Moderation\n\nQuiz and content generation includes safety prompts:\n\n5. User Data Privacy\n\nNo personal data collected beyond learning preferences; no emails, names, or identifying information required. Anonymous UUIDs as identifiers. Cookie-based sessions stored locally, not server-side. No tracking beyond learning analytics; no third-party scripts, ads, or profiling. GDPR-compliant with data export/deletion functionality.\n\nContent Quality Assurance\nMulti-model consensus: Controversial topics require agreement from 2+ models\nFact-checking hints: LLM prompts include \"Ensure factual accuracy\"\nEducational focus: All content prompts emphasize learning value\nDifficulty calibration: Content tested against skill level expectations\nProject Maintenance & Support\nActive Development & Updates\n\nActively maintained open-source project. Bug fixes deployed within 24-48 hours. Security patches applied immediately. Feature enhancements driven by community feedback. Documentation updates reflect latest changes. Regular dependency updates maintain compatibility. Current Status: ✅ Production-Ready (v1.0.0).\n\nGitHub Repository\n\nOfficial Repository: \nhttps://github.com/Kiragu-Maina/ultimate-adaptive-guide\n\nComplete source code (backend + frontend), Docker deployment configuration, comprehensive /docs folder, API reference with examples, troubleshooting guides, and contributing guidelines.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nSystem Overview\n\nArchitecture\n\nMulti-Agent Collaboration\n\nLearner Profiler Agent\nJourney Architect Agent\nPerformance Analyzer\nRecommendation Agent\n\nContent & Assessment\n\nContent Personalizer\n\nQuiz Generator\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "i-care-smart-doctor-comprehensive-medical-system-sZgWGLbCUMiS",
    "username": "aAbdallah Fekry",
    "license": "No License",
    "title": "I Care - Smart Doctor - Comprehensive Medical System",
    "publication_description": "Back to publications\nDec 29, 2024\n●\n263 reads\n●\nNo License\nI Care - Smart Doctor - Comprehensive Medical System\n101 Food\nai\nAnatomical Recognition\nartificial intelligence\nBone Fraction\nBrain Tumor\nBreast Cancer\nCalories\nCBC Test\nChatbot\nChest Xray\nclassification\nCNN\nComplete Blood Count\nComputer Vision\nConvolutional Neural Network\nCovid\nCT\nCV\nDiagnosis\nDiseases\nECG\nEKG\nElectrocardiography\nGraduation Project\nhealth care\nI Care\nInception Net\nLangChain\nLarge Language Model\nLLM\nLSTM\nmedical\nMRI\nNatural Language Processing\nNLP\nObject Detection\nOCR\nOCT\nOptical Character Recognition\nPneumonia\nsegmentation\nSmart Doctor\nSpeech Recognition\nStreamlit\nU-Net\nXray\nYolo\nA\nAbdallah Fekry\nLike\nBookmark\nShare\nI Care - Smart Doctor\n\nComprehensive Medical System - Easy Healthcare for Anyone Anytime!\n\n\n(Graduation Project)\n\n1. OverView\nTest Run Video\n\nAs a team leader i have distributed the tasks on the team members, i have decided to work with Agile methodology so i was making a meeting each one or two weeks with the team members to presenting the work done\ni was responsible for the AI modules which are:\n\nMy part in the project consists of three main modules:\n\nModule 1: An AI based smart chatbot called \"Caroline\" talking to the patient\nand taking its disease symptoms by text or voice messages, then diagnosing the disease and\nrecommend making some tests or medical imaging scans to do as x-ray, MRI, Complete Blood Count CBC, ... in addition, given\ninformation about the predicted disease as an overview, symptoms, and\ntreatments. It can predict 30 diseases such as (Breast Cancer, Influenza, Covid 19, Stroke, ...)\n\nModule 2: A sequence of AI Computer Vision models for scanning medical\nimaging scans and medical tests it can scan (X-ray, MRI, CT, OCT, CBC, or Food images),\ndetect the image type (Image Recognition), if it is medical imaging image,\napplying anatomical recognition, disease evaluation, disease diagnosis, and also in tumor or bone fraction cases. It can locate the tumor or the fraction using image segmentation. It can predict 25 disease types such as (Bone Fracture, Brain Tumor, Covid 19, Breast Cancer, ...). It can read the Complete Blood Count (CBC) test images and evaluate overall health and diagnose conditions like anemia, infections, clotting disorders, and blood cancers by analyzing red and white blood cells, hemoglobin, hematocrit, and platelets. It also can recognize 101 food types from images and shows the approximate number of calories per gram.\n\nModule 3: An ensemble Machine Learning (Random Forest) Model for scan Electrocardiography ECG and diagnosis the heart diseases.\n\nIn addition of making algorithm for MBTI personality analysis test.\n\n2. Implementation Methodology\n\nUsing transfer learning to get a pre-trained models on a huge dataset image net and customize their input layer shape to be suitable with the images and customize the output layer structure and activation function as need, so the models have an initial value for the parameters then they train faster and gives better accuracy.\n\nThe AI module is designed in different parts. There are a Natural Language Processing NLP, Deep Learning Computer Vision Classification, Image Segmentation, Optical Character Recognition OCR, Large Language Model LLM, Speech Recognition, and Machine Learning Models all are combined together to mimic a doctor for all specialties.\n\nAI Models Table\n\nContains Input, Output, Functionality, Training and Testing Accuracy, and Recall information\n\nAI Scan Models Work Flow\n\nSequence for the Image Scan Computer Vision Models\n\nChatBot Diseases Detection\n\nAI based smart chatbot called \"Caroline\" talking to the patient and taking its disease symptoms by text or voice messages, then diagnosing the disease and recommend making some tests or medical imaging scans to do as x-ray, MRI, Complete Blood Count CBC, ... in addition, given information about the predicted disease as an overview, symptoms, and treatments.\n\nMethodology: Collecting the data from several datasets contains the diseases symptoms and the disease diagnosis as a Json format contains Tags, Patterns, and Response. The tags contains the class name as \"greeting\", \"diseases name as Skin Cancer, Influenza, ...\" making class for each disease, patterns contains examples of the user expected questions, and the response contains the many options for the output. after making the dataset, applying preprocessing techniques (Tokenization and Stemming, ...). i have developed a deep learnoing LSTM model for the Chatbot predictions taking the preprocessed patient symptoms message as input and predict the class from the 44 classes then chooses randomly from the saved responses.\n\nThere is option speech to the chatbot using voice messages as i have add a Speech Recognition model to detect voices and convert it into text then pass to the chatbot model.\n\nChatbot LSTM Architecture\n\nObject Recognition From Iamges\n\nIdentify the medical imaging scan, medical test report and food from images\n\nMethodology: Collecting the data from several datasets for the four classes starting with making a folder contains four other folders each onr for each class (Medical Imaging Scan, Medical Test Report, Food, Other). i have collected many types of medical imagings as (xray on several body parts, MRI on brain and breast, many food images from Food 101 dataset collectingh 100 images from each food type, and using the dataset \"ImageNet\" for class \"Other\" collecting 10 images from each of its 1000 classes). finally worked normally to made a deep learning CNN model VGG16 architecture with 4 neurons in the output layers and activation function Softmax.\n\nModels CNN Architecture\n\nMedical Imaging Scan Type Detection\n\nIdentifying the medical imaging scan type of 4 main types which are 'Electromagnetic Variations - Xray', 'Magnetic Resonance Imaging - MRI', 'Computerized Tomography - CT', or 'Optical Coherence Tomography - OCT'\n\nMethodology: Developed a multi-class classification CNN model VGG16 architecture for identifying 4 classes (xray, MRI, CT, or OCT Scan) with 4 neurons in the output layers and activation function Softmax.\n\nAnatomical Recognition From Xray Scan\n\nApplying anatomical recognition on all imaging types to identify the body parts\n\nMethodology: Developed two CNN model VGG16 architecture one for multi-class classification for the X-ray body parts with classes (Chest, Feet, Hand, Neck, Other, and Skull) with 6 neurons in the output layers and activation function Softmax. And the other one for binary-class classification for the MRI body parts with classes (Brain and Breast) with 1 neuron in the output layers and activation function Sigmoid.\n\nBrain Tumor Detection From MRI Scan\n\nCan to detect brain tumor from MRI and diagnosis their types as 'Glioma_tumor', 'Meningioma_tumor', and 'Pituitary_tumor'\n\nMethodology: Developed two models the first one is a binary-classification CNN model VGG16 architecture for the MRI Brain Tumor Evaluation with classes (Tumored and Not Tumored) with 1 neuron in the output layers and activation function Sigmoid. and the second one for multi-class classification CNN model VGG16 architecture for the MRI Brain Tumor Type Detection with 3 classes with 3 neurons in the output layers and activation function Softmax.\n\nBone Fracture Detection From Xray Scan\n\nCan to detect bones fraction from x-rays and diagnosis their 10 types as 'Avulsion fracture', 'Comminuted fracture', 'Compression-Crush fracture', 'Fracture Dislocation', 'Greenstick fracture', 'Hairline Fracture', 'Impacted fracture', 'Intra-articular fracture', 'Longitudinal fracture', 'Oblique fracture', 'Pathological fracture', and 'Spiral Fracture'\n\nMethodology: Developed two models the first one is a binary-classification CNN model VGG16 architecture for the X-ray Bone Fraction Evaluation with classes (Fractiored and Not Fractured) with 1 neuron in the output layers and activation function Sigmoid. and the second one for multi-class classification CNN model VGG16 architecture for the X-ray Bone Fraction Type Detection with 10 classes with 10 neurons in the output layers and activation function Softmax.\n\nBreast Cancer Detection From MRI Scan\n\nCan detect breast cancer from MRI and diagnosis its types 'Malignant' or 'Benign'\n\nMethodology: Developed two models the first one is a binary-classification CNN model VGG16 architecture for the MRI Breast Cancer Evaluation with classes (Cancered and Healthy) with 1 neuron in the output layers and activation function Sigmoid. and the second one for multi-class classification CNN model VGG16 architecture for the MRI Breast Cancer Type Detection with 2 classes with 1 neurons in the output layers and activation function Sigmoid.\n\nBrain Tumor Segmentation From MRI Scan\n\nCan locate the tumor location and color it\n\nMethodology: Developed U-net segmentation model to locate the tumor on the brain MRI.\n\nBone Fraction Segmentation From Xray Scan\n\nCan locate the fraction in bones location and make a rectangles on them\n\nMethodology: Developed Yolo segmentation model to locate the Fraction on the bone X-ray.\n\nChest Diseases Detection From Xray Scan\n\nCan detect chest diseases from x-rays and diagnosis if there is 'Covid 19' or 'Pneumonia'\n\nMethodology: Developed a multi-class classification CNN model VGG16 architecture for Chest X-ray images identifying 3 classes (Covid-19, Pneumonia, or Normal) with 3 neurons in the output layers and activation function Softmax.\n\nBody Diseases Detection From CT Scan\n\nCan detect body diseases from CT scans and diagnosis if there is 'Cyst', 'Stones' or 'Tumors'\n\nMethodology: Developed a multi-class classification CNN model VGG16 architecture for CT Scans identifying 4 classes (Cyst, Stone, Tumor, or Normal) with 4 neurons in the output layers and activation function Softmax.\n\nComplete Blood Count Test (CBC) Medical Test\n\nReading CBC Test images data and gives information about it\n\nMethodology: Using Optical Character Recognition OCR model to get the data from the CBC Test images then pass it to Google Gemini Large Language Model LLM using LangChian to evaluate overall health and diagnose conditions like anemia, infections, clotting disorders, and blood cancers by analyzing red and white blood cells, hemoglobin, hematocrit, and platelets.\n\nECG - Heart Diseases Detection\n\nCan detect heart diseases from Electrocardiography - ECG and diagnosis if there is 'Normal beat', 'Supraventricular premature beat', 'Premature ventricular contraction', 'Fusion of ventricular and normal beat', or 'Unclassifiable beat'\n\nMethodology: Developed a Machine Learning classification model to handling the structured ECG data from csv files. firstly i have try four different models to compaire between each other Logistic Regression, Support Vector Machine SVM, Decision Trees, and Random Forest Models. the random forest gives the highest accuracy in a suitable time.\n\nModels comparison table according to time and accuracy\n\n\nEyes Diseases Detection From OCT Scan\n\nCan detect eyes diseases from OCT scans and diagnosis if there is 'CNV - Choroidal Neovascularization', 'DME - Diabetic Macular Edema' or 'Drusen'\n\nMethodology: Developed a multi-class classification CNN model VGG16 architecture for OCT Scans identifying 4 classes (CNV, DME, Drusen, or Normal) with 4 neurons in the output layers and activation function Softmax.\n\nFood Calories Detection\n\nCan detect food types from images and identify their approximation number of calories per gram. It can detect 101 types of food as 'Frensh fries', 'Pizza', 'Sushi', 'Pancakes', 'Chocolate cake', ...\n\nMethodology: Developed a multi-class classification CNN model, as the VGG16 architecture is not suitable for large number of classes so i have used Inception Net v3 architecture for Food Type Detection identifying 101 classes for food types with 101 neurons in the output layers and activation function Softmax. After detecting the food type choosing the average number of calories per gram from the stored data before.\n\n3. Results\n\nAI Models Table\n\nContains Input, Output, Functionality, Training and Testing Accuracy, and Recall information\n\nModels Accuracy\n\nModels Accuracy Visualization\n\nSome Models Training History Visualization\n\nChatbot Training History\n\nBreast Cancer Detection Training History\n\nCT Model Training History\n\nSome Models Heatmap Visualization\n\nBreast Cancer Evaluation\n\nCT Scan Heatmap\n\nMedical Imaging Scan Type Detection Heatmap\n\nChatbot Heatmap\n\n4. Conclusion\n\nThe Smart Medical and Healthcare System stands as a transformative innovation in the healthcare sector, empowering patients to access intelligent medical diagnoses and scan medical imaging, medical tests, ECG Scan or their food calories by AI, make a personality analysis test, and manage their healthcare needs seamlessly.\n\nThe mobile and web applications leverage artificial intelligence models and user-friendly interfaces to deliver an exceptional user experience.\nIn conclusion, the smart medical and healthcare application leverages cutting-edge AI technology to deliver a seamless and exceptional user experience for both patients and healthcare providers. This project represents a significant step forward in making healthcare more accessible, personalized, and efficient, ultimately improving patient outcomes and healthcare service delivery.\n\n5. References\n\nxray body type dataset:\n\nhttps://www.kaggle.com/datasets/ibombonato/xray-body-images-in-png-unifesp-competion\n\nECG dataset:\n\nhttps://www.kaggle.com/datasets/shayanfazeli/heartbeat\n\nCOVID19+PNEUMONIA+NORMAL Chest X-Ray Image Dataset:\n\nhttps://www.kaggle.com/datasets/sachinkumar413/covid-pneumonia-normal-chest-xray-images\n\nbone fracture detection using x-rays dataset:\n\nhttps://www.kaggle.com/datasets/vuppalaadithyasairam/bone-fracture-detection-using-xrays\n\nChest X-ray - 17 Diseases dataset:\n\nhttps://www.kaggle.com/datasets/trainingdatapro/chest-xray-17-diseases?select=xray_chest.csv\n\nBrain Tumor Classification (MRI) dataset:\n\nhttps://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri\n\nBrain tumors 256x256 dataset:\n\nhttps://www.kaggle.com/datasets/thomasdubail/brain-tumors-256x256\n\nBone Break Classification Image Dataset:\n\nhttps://www.kaggle.com/datasets/pkdarabi/bone-break-classification-image-dataset\n\nBreast Cancer Diagnosis dataset:\n\nhttps://www.kaggle.com/datasets/faysalmiah1721758/breast-cancer-diagnosis\n\nRSNA Breast Cancer Detection - 256x256 pngs dataset:\n\nhttps://www.kaggle.com/datasets/theoviel/rsna-breast-cancer-256-pngs?select=10042_495770405.png\n\nBreast Cancer Patients MRI's dataset:\n\nhttps://www.kaggle.com/datasets/uzairkhan45/breast-cancer-patients-mris\n\nBone Fracture Atlas\n\nhttps://figshare.com/articles/dataset/The_dataset/22363012\n\nMRI image Brest cancer dataset:\n\nhttps://www.kaggle.com/datasets/raselislambabu/mri-image-brest-cancer\n\nbone-break-classifier-dataset:\n\nhttps://www.kaggle.com/datasets/amohankumar/bone-break-classifier-dataset\n\nRetinal OCT Images (optical coherence tomography) dataset:\n\nhttps://www.kaggle.com/datasets/paultimothymooney/kermany2018\n\nCT KIDNEY DATASET: Normal-Cyst-Tumor and Stone dataset:\n\nhttps://www.kaggle.com/datasets/nazmul0087/ct-kidney-dataset-normal-cyst-tumor-and-stone\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nI Care - Smart Doctor\n\nOverView\n\nTest Run Video\n\nMy part in the project consists of three main modules:\n\nImplementation Methodology\n\nChatBot Diseases Detection\n\nObject Recognition From Iamges\n\nMedical Imaging Scan Type Detection\n\nAnatomical Recognition From Xray Scan\n\nBrain Tumor Detection From MRI Scan\n\nView all\nComments\n(1)\nDatasets\nFiles\nPersonality_Analysis_MBTI.ipynb\nmodel-6 bone_fracture_diagnosis.ipynb\nmodel-3 xray_anatomical_recognition.ipynb\nmodel-2 medical-imaging-scan-type.ipynb\nmodel-4 chest-xray-diagnosis.ipynb\nmodel-5 bone_fracture_evaluation.ipynb\nModel-7 Bone-Fraction-Segmentation.ipynb\nmodel-1 image-recognition.ipynb\nmodel-13 Breast_cancer_diagnosis.ipynb\nModel-11 Brain-Tumor-Segmentation.ipynb\nmodel-12 Breast_Cancer_evaluation.ipynb\nmodel-8 MRI_anatomical_recognition.ipynb\nmodel-14 CT_Scan_diagnosis.ipynb\nmodel-10 Brain_tumor_diagnosis.ipynb\nmodel-15 OCT-eyes-diseases-diagnosis.ipynb\nModel-17-18 Complete Blood Count CBC Test.ipynb\napp.py\nedit.py\nhome.py\nlogin.py\ndiseasesHistory.py\nintro.py\nprofilee.py\nmodel-19 I-Care-Chatbot.ipynb\nrequirements.txt\nmodel-20 ECG-heart-disease-diagnosis.ipynb\nsettings.py\nsignup.py\nVoiceIn.py\nmyfile.wav\nmodel-9 Brain_tumor_evaluation.ipynb\nmodel-16 Food_calories_detection.ipynb\nscan.py\nmbti.py\necg.py\nchatbot.py\nconfig.toml\nYou might be interested\nProject Documentation: Pneumonia Detection from Chest X-ray using CNN(Computer Vision)\nD\nDec 19, 202421 reads\nMedical Imaging Pneumonia Detect\nAI-Powered Pneumonia Detection via CNNs and X-Ray Analysis\nH\nDec 25, 202422 reads\nComputer VisionConvolutional Neural Networks (C+5\nChest X-Ray Classification using Transfer Learning\nS\nDec 31, 202416 reads\nCNNComputer Vision+9\nBrain Tumor Classification Using CNN\nG\nJul 08, 20257 reads\nAIBrain Tumor+3",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "intelligent-multi-agent-career-assistant-a-production-ready-ai-system-for-job-hunting-kXMF1zeC1Ga6",
    "username": "Isaac Ugwu",
    "license": "MIT License",
    "title": "Intelligent Multi-Agent Career Assistant: A Production-Ready AI System for Job Hunting",
    "publication_description": "Back to publications\nAug 11, 2025\n●\n49 reads\n●\nMIT License\nIntelligent Multi-Agent Career Assistant: A Production-Ready AI System for Job Hunting\nAgentic AI\nLangGraph\nMulti-Agent Systems\nIsaac Ugwu\nLike\nBookmark\nShare\nTL;DR\n\nThis publication presents a production-ready multi-agent AI system that revolutionizes job hunting through autonomous agent coordination. The system combines resume analysis, job market research, CV optimization, and job matching capabilities in a single, intelligent platform. Built with NextJS and FastAPI, it features comprehensive safety measures, persistent data storage, mobile-responsive design, and real-time performance monitoring. The system demonstrates practical AI implementation with robust error handling, content validation, and scalable architecture suitable for enterprise deployment.\n\n1. Introduction\n1.1 Purpose and Objectives\n\nThis publication demonstrates a complete, production-ready multi-agent AI system designed to solve the complex challenges of modern job hunting. The system coordinates multiple specialized AI agents to provide comprehensive career assistance, from resume analysis to job matching and CV optimization.\n\nThis publication is building on my last publication which detailed the implementation, workflow and composition of this Multi-Agent system. This last publication of mine can be found \nhere\n\nThrough this comprehensive documentation, you will gain deep insights into how production-ready multi-agent AI systems are architected and deployed. The publication covers the implementation of advanced safety measures and content validation protocols that ensure reliable operation in real-world environments. You will learn how persistent data storage systems are integrated to provide system-wide analytics and performance monitoring. Additionally, the development of responsive user interfaces specifically designed for AI applications is thoroughly explained.\n\nThe publication delivers detailed coverage of production-ready safety features and comprehensive error handling mechanisms that maintain system stability under adverse conditions. You will explore the development and implementation of extensive testing suites that include unit, integration, and system tests designed to ensure reliability at scale. The mobile-responsive web interface development process is explained in detail, showing how real-time updates and user feedback are seamlessly integrated. Finally, the performance monitoring and analytics dashboard implementation demonstrates how to track and optimize multi-agent system performance in production environments.\n\n1.2 Problem Statement and Significance\n\nTraditional job hunting involves fragmented processes across multiple platforms, manual resume optimization, and time-intensive job market research. Job seekers face significant challenges that compound the difficulty of finding suitable employment opportunities.\nResume optimization presents one of the most significant hurdles, as job seekers struggle to understand what recruiters and hiring managers actually want to see in candidate applications. Without access to industry insights or automated tools, individuals often spend countless hours manually adjusting their resumes for each application, frequently missing key optimization opportunities that could significantly improve their chances of success.\n\nMarket research complexity adds another layer of difficulty to the job hunting process. Identifying relevant opportunities and understanding current market trends requires extensive research across multiple platforms and sources. Job seekers must navigate various job boards, company websites, and industry publications to gather comprehensive information about available positions and market conditions, a process that can be overwhelming and time-consuming.\n\nApplication inefficiency further exacerbates these challenges, as job seekers must manually customize applications for each role they pursue. This repetitive process not only consumes significant time and energy but also increases the likelihood of errors and inconsistencies across applications. Without systematic approaches to application management, many qualified candidates fail to present themselves effectively to potential employers.\n\nThe lack of integrated guidance represents perhaps the most fundamental problem in current job hunting approaches. No single platform provides comprehensive, end-to-end support that addresses all aspects of the job search process. This fragmentation forces job seekers to piece together solutions from multiple sources, creating inefficiencies and gaps in their overall strategy.\n\nThis system addresses these multifaceted challenges by providing an intelligent, autonomous solution that coordinates multiple specialized agents to deliver comprehensive job hunting assistance through a single, integrated interface. By automating routine tasks and providing expert-level guidance, the system transforms the job hunting experience from a fragmented, manual process into a streamlined, intelligent workflow.\n\n1.3 System Overview\n\nThe Intelligent Multi-Agent Career Assistant consists of four specialized AI agents that work collaboratively:\n\nResume Analyst Agent - Analyzes resumes for strengths, weaknesses, and improvement opportunities\nJob Researcher Agent - Conducts market research and identifies relevant job opportunities\nCV Creator Agent - Generates ATS-optimized CVs tailored for specific roles\nJob Matcher Agent - Matches user profiles against job requirements and provides application strategies\n\nThe system features a modern web interface built with NextJS, a robust FastAPI backend, PostgreSQL database for persistent storage, and comprehensive monitoring capabilities.\n\n2. System Architecture and Design\n2.1 Multi-Agent Architecture\n\nThe system implements a coordinator-based multi-agent architecture using LangGraph for workflow orchestration. The architecture ensures:\n\nAutonomous agent coordination through a central coordinator\nDynamic workflow adaptation based on user requests and context\nState management across multiple agent interactions\nHuman-in-the-loop capabilities for complex decisions\n# Core agent coordination structure\ndef create_multi_agent_system():\n    graph = StateGraph(MultiAgentState)\n    \n    # Add agents to the workflow\n    graph.add_node(\"coordinator\", coordinator_agent)\n    graph.add_node(\"resume_analyst\", resume_analyst_agent)\n    graph.add_node(\"job_researcher\", job_researcher_agent)\n    graph.add_node(\"cv_creator\", cv_creator_agent)\n    graph.add_node(\"job_matcher\", job_matcher_agent)\n    \n    # Define workflow logic\n    graph.set_entry_point(\"coordinator\")\n    graph.add_conditional_edges(\"coordinator\", route_to_next_agent)\n    \n    return graph.compile(checkpointer=MemorySaver())\n2.2 Technology Stack\n\nFrontend (NextJS)\n\nReact 18 with TypeScript for type safety\nTailwind CSS for responsive design\nReal-time updates through polling mechanisms\nMobile-first responsive design\n\nBackend (FastAPI)\n\nPython 3.13 with async/await support\nLangChain and LangGraph for AI workflow orchestration\nOpenAI GPT-4 for natural language processing\nComprehensive error handling and logging\n\nDatabase & Storage\n\nPostgreSQL (Neon) for persistent data storage\nCloudinary for file storage and management\nSQLAlchemy ORM for database operations\n\nSecurity & Monitoring\n\nRate limiting and request validation\nContent filtering and safety measures\nPerformance monitoring and analytics\nSecure file upload handling\n3. Production-Ready Features\n3.1 Safety and Content Validation\n\nThe system implements multiple layers of safety to ensure reliable, secure operation:\n\nAI-Powered Content Classification\ndef validate_resume_content(text: str) -> tuple[bool, str]:\n    \"\"\"AI-powered validation to check if extracted text is resume content\"\"\"\n    classification_prompt = f\"\"\"You are a document classifier. \n    Analyze this text and determine if it's from a resume/CV.\n    \n    TEXT TO ANALYZE: {text[:1000]}\n    \n    Answer with \"YES\" if this appears to be resume/CV content\n    Answer with \"NO\" if this is clearly not a resume\n    Format: YES/NO - [brief explanation]\"\"\"\n    \n    response = llm.invoke(classification_prompt)\n    result = response.content.strip()\n    \n    return result.upper().startswith('YES'), result\nFile Security Measures\nMIME type validation using python-magic library\nFile size restrictions (16MB maximum)\nMalicious content scanning for suspicious patterns\nSecure filename generation with session isolation\nContent extraction validation before processing\nInput Sanitization\nUser input filtering to prevent injection attacks\nPrompt sanitization to avoid AI manipulation\nRate limiting to prevent system abuse\nSession management for secure user isolation\n3.2 Error Handling and Resilience\nComprehensive Error Management\n@safe_ai_wrapper(agent_name=\"resume_analyst\", safety_level=\"high\")\ndef resume_analyst_agent(state: MultiAgentState):\n    try:\n        # Agent processing logic\n        resume_content = parse_resume.invoke(resume_path)\n        \n        # Validate content before processing\n        is_valid, explanation = validate_resume_content(resume_content)\n        if not is_valid:\n            return error_response(explanation)\n            \n        # Continue with analysis...\n    except Exception as e:\n        # Log error and return graceful failure\n        logger.error(f\"Resume analysis failed: {e}\")\n        return graceful_error_response(e)\nGraceful Degradation\nFallback mechanisms when individual agents fail\nPartial result delivery when some agents succeed\nUser notification systems for service limitations\nAutomatic retry logic for transient failures\n3.3 Performance Monitoring and Analytics\nReal-Time Performance Tracking\n\nThe system implements comprehensive performance monitoring with persistent storage:\n\nclass PerformanceEvaluator:\n    def log_system_request(self, success: bool, request_time: float):\n        \"\"\"Log system-level metrics with immediate database persistence\"\"\"\n        self.system_metrics.total_requests += 1\n        \n        if success:\n            self.system_metrics.successful_requests += 1\n        else:\n            self.system_metrics.failed_requests += 1\n        \n        # Update running averages\n        self.update_averages(request_time)\n        \n        # Save to database immediately for system-wide tracking\n        self._save_to_database()\nAnalytics Dashboard\nReal-time performance metrics displayed in web interface\nAgent performance breakdown with success rates and timing\nUser satisfaction tracking with feedback collection\nSystem health monitoring with resource usage metrics\nHistorical trend analysis with persistent data storage\n3.4 Database Integration and Persistence\nPostgreSQL Integration\n# Database models for persistent metrics\nclass SystemMetrics(Base):\n    __tablename__ = 'system_metrics'\n    total_requests = Column(Integer, default=0)\n    successful_requests = Column(Integer, default=0)\n    avg_response_time = Column(Float, default=0.0)\n    user_satisfaction_score = Column(Float, default=0.0)\n\nclass AgentMetrics(Base):\n    __tablename__ = 'agent_metrics'\n    agent_name = Column(String(100), nullable=False)\n    total_calls = Column(Integer, default=0)\n    success_rate = Column(Float, default=0.0)\n    performance_grade = Column(String(10))\nData Persistence Strategy\nSystem-wide metrics accumulate across all deployments\nAgent performance data tracks individual agent effectiveness\nUser feedback storage for continuous improvement\nContent validation logs for security auditing\nAutomatic database fallbacks for development environments\n4. User Interface and Experience\n4.1 Responsive Web Design\n\nThe system features a mobile-first, responsive design built with NextJS and Tailwind CSS:\n\nKey UI Components\nDrag-and-drop file upload with visual feedback\nReal-time processing updates with progress indicators\nCollapsible result sections for better mobile experience\nTouch-friendly controls optimized for mobile devices\nResponsive grid layouts that adapt to screen sizes\nMobile Optimizations\n/* Mobile-specific optimizations */\n@media (max-width: 640px) {\n  .prose {\n    font-size: 0.875rem;\n    line-height: 1.5;\n  }\n  \n  button {\n    min-height: 44px; /* Touch-friendly sizing */\n    min-width: 44px;\n  }\n}\n4.2 User Experience Features\nQuick Actions\nPre-defined prompt templates for common use cases\nOne-click analysis for standard resume review\nSmart defaults based on user context\nProgressive disclosure of advanced options\nReal-Time Feedback\nLive processing status with agent activity updates\nExpandable result sections with preview and full view\nDownload capabilities for generated CVs\nSocial sharing of success stories\n4.3 Accessibility and Usability\nKeyboard navigation support throughout the interface\nScreen reader compatibility with semantic HTML\nHigh contrast ratios for visual accessibility\nError message clarity with actionable guidance\nLoading state management with progress indicators\n5. Testing Strategy and Implementation\n5.1 Testing Architecture\n\nThe system implements a comprehensive three-tier testing strategy:\n\nUnit Tests\ndef test_resume_content_validation():\n    \"\"\"Test AI-powered content validation\"\"\"\n    # Test valid resume content\n    valid_content = \"John Doe\\nSoftware Engineer\\nExperience: 5 years...\"\n    is_valid, explanation = validate_resume_content(valid_content)\n    assert is_valid == True\n    \n    # Test invalid content  \n    invalid_content = \"Chapter 1: Introduction to Machine Learning...\"\n    is_valid, explanation = validate_resume_content(invalid_content)\n    assert is_valid == False\nIntegration Tests\ndef test_multi_agent_workflow():\n    \"\"\"Test complete agent coordination workflow\"\"\"\n    test_request = {\n        \"user_message\": \"Analyze my resume and find relevant jobs\",\n        \"resume_path\": \"test_resume.pdf\"\n    }\n    \n    result = multi_agent.process_request(**test_request)\n    \n    assert result[\"success\"] == True\n    assert \"resume_analyst\" in result[\"completed_tasks\"]\n    assert \"job_researcher\" in result[\"completed_tasks\"]\n    assert result[\"processing_time\"] < 30.0  # Performance requirement\nSystem Tests\ndef test_end_to_end_user_workflow():\n    \"\"\"Test complete user journey from upload to results\"\"\"\n    # Simulate file upload\n    response = client.post(\"/api/process\", \n        files={\"file\": test_resume_file},\n        data={\"prompt\": \"Complete job hunting assistance\"}\n    )\n    \n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"success\"] == True\n    assert len(data[\"data\"][\"agent_messages\"]) > 0\n    assert data[\"data\"][\"cv_download_url\"] != \"\"\n5.2 Test Coverage and Automation\n90%+ code coverage across core functionality\nAutomated testing pipeline integrated with deployment\nPerformance benchmarking with load testing scenarios\nSecurity testing for input validation and sanitization\nCross-browser compatibility testing for web interface\n5.3 Error Scenario Testing\n\nThe testing suite includes comprehensive error scenario coverage:\n\nInvalid file upload handling\nNetwork timeout resilience\nDatabase connection failures\nAI service unavailability\nMemory constraint scenarios\nConcurrent user load testing\nEnvironment Configuration\n# Required environment variables\nDATABASE_URL=postgresql://user:pass@host/db?sslmode=require\nOPENAI_API_KEY=your_openai_key\nCLOUDINARY_URL=cloudinary://api_key:api_secret@cloud_name\n6.1 Scalability Considerations\nPerformance Optimization\nDatabase connection pooling for efficient resource usage\nCaching strategies for frequently accessed data\nAsync processing for non-blocking operations\nCDN integration for static asset delivery\nLoad Management\nRate limiting to prevent system overload\nQueue management for high-traffic scenarios\nHorizontal scaling capabilities with stateless design\nResource monitoring with automatic alerts\n6.2 Monitoring and Maintenance\nProduction Monitoring\nReal-time performance dashboards with key metrics\nError tracking with detailed logging and alerts\nResource usage monitoring with automatic scaling triggers\nUser analytics for usage patterns and optimization\nMaintenance Procedures\nAutomated backups of database and critical data\nRolling updates with zero-downtime deployment\nHealth checks with automatic failover capabilities\nSecurity updates with automated vulnerability scanning\n7. Getting Started\n7.1 Quick Start Guide\nPrerequisites\nNode.js 18+ and npm\nPython 3.9+\nOpenAI API key\nPostgreSQL database (optional - falls back to SQLite)\nInstallation and Setup\nClone and Install Dependencies\ngit clone <repository-url>\ncd intelligent-career-assistant\nnpm install\nConfigure Environment\n# Create .env.local file\nOPENAI_API_KEY=your_openai_key_here\nDATABASE_URL=your_neon_postgresql_url  # Optional\nCLOUDINARY_CLOUD_NAME=....\nCLOUDINARY_API_KEY=.......\nCLOUDINARY_API_SECRET=........\nENCRYPTION_KEY=your_encryption_key\nFLASK_SECRET_KEY=your_secret_key\n\nRun the Application\nnpm run dev\n\nThe application will be available at http://localhost:3000 with the FastAPI backend running automatically.\n\nDevelopment Workflow\n# Run tests\nnpm test\npython -m pytest\n\n# Check code quality\nnpm run lint\npython -m flake8 api/\n\n# Build for production\nnpm run build\n7.2 Usage Examples\nBasic Resume Analysis\nNavigate to the application homepage\nUpload a resume file (PDF, DOCX, or TXT)\nClick \"Analyze My Resume\" quick action\nReview detailed feedback and recommendations\nComplete Job Hunt Assistance\nUpload your resume\nSelect \"Complete Job Hunt\" from quick actions\nWait for multi-agent processing (typically 15-30 seconds)\nReview results including:\nResume analysis and improvement suggestions\nRelevant job opportunities with market insights\nGenerated optimized CV with download link\nJob matching recommendations with application strategies\nCustom Requests\nUpload resume (if needed)\nUse the custom prompt field to describe specific needs\nExamples of custom prompts:\n\"Help me transition from finance to tech\"\n\"Find remote software engineering jobs in AI/ML\"\n\"Optimize my resume for marketing manager positions\"\n8. Technical Implementation Details\n8.1 AI Safety Implementation\nContent Validation Pipeline\n\nThe system implements a sophisticated content validation pipeline to ensure uploaded files contain resume content:\n\ndef validate_resume_content(text: str) -> tuple[bool, str]:\n    \"\"\"Multi-stage validation for resume content\"\"\"\n    # Stage 1: Length validation\n    if not text or len(text.strip()) < 50:\n        return False, \"Document too short to be a resume\"\n    \n    # Stage 2: AI-powered classification\n    sample = text[:1000]  # Optimize API costs\n    classification_result = llm.invoke(classification_prompt)\n    \n    # Stage 3: Confidence scoring\n    return parse_classification_result(classification_result)\nSecurity Measures\nFile type validation with MIME type checking\nMalicious content scanning for embedded scripts\nInput sanitization preventing injection attacks\nRate limiting with IP-based restrictions\nSession isolation for secure multi-user support\n8.2 Performance Optimization\nDatabase Architecture\nclass PerformanceEvaluator:\n    def _save_to_database(self):\n        \"\"\"Immediate persistence for system-wide metrics\"\"\"\n        with self._get_db_session() as session:\n            # Save accumulated system metrics\n            system_record = SystemMetrics(\n                total_requests=self.system_metrics.total_requests,\n                successful_requests=self.system_metrics.successful_requests,\n                avg_response_time=self.system_metrics.avg_request_time,\n                # ... additional metrics\n            )\n            session.add(system_record)\nMemory Management\nStreaming file processing for large documents\nChunked text analysis to prevent memory overflow\nGarbage collection optimization for long-running processes\nResource cleanup after request completion\n8.3 Error Recovery Mechanisms\nAgent Failure Handling\ndef coordinator_agent(state: MultiAgentState):\n    \"\"\"Coordinator with comprehensive error handling\"\"\"\n    try:\n        next_agent = determine_next_agent(state)\n        return route_to_agent(next_agent, state)\n    except AgentFailureException as e:\n        # Log failure and try alternative agent\n        logger.warning(f\"Agent {e.agent_name} failed: {e}\")\n        return route_to_fallback_agent(state, e.agent_name)\n    except SystemException as e:\n        # System-level failure - graceful degradation\n        return create_partial_response(state, e)\nData Integrity\nTransaction rollback on database errors\nPartial result preservation when possible\nUser notification of service limitations\nAutomatic retry logic with exponential backoff\n9. Results and Performance Analysis\n9.1 System Performance Metrics\n\nBased on production testing and monitoring data:\n\nProcessing Performance\nAverage response time: 15-25 seconds for complete workflows\nResume analysis: 3-5 seconds average processing time\nJob research: 8-12 seconds for market analysis\nCV generation: 5-8 seconds including PDF creation\nSuccess rate: 94.2% across all agent operations\nScalability Metrics\nConcurrent users: Tested up to 50 simultaneous sessions\nDatabase performance: <100ms query response times\nMemory usage: Peak 512MB per concurrent session\nError recovery: 98.7% successful recovery from transient failures\n9.2 User Satisfaction Analysis\n\nData collected from production usage:\n\nUser Feedback Scores (1-10 scale)\nOverall satisfaction: 8.4/10 average\nResume analysis quality: 8.7/10\nJob recommendations relevance: 8.1/10\nCV generation quality: 8.6/10\nInterface usability: 8.9/10\nUsage Patterns\nMost popular feature: Complete job hunt assistance (67% of sessions)\nAverage session duration: 12 minutes\nReturn user rate: 34% within 30 days\nMobile usage: 43% of total sessions\n9.3 Technical Quality Metrics\nCode Quality\nTest coverage: 91.3% across all modules\nCode complexity: Average cyclomatic complexity of 4.2\nSecurity scan results: Zero high-severity vulnerabilities\nPerformance benchmarks: All critical paths under 30-second SLA\nReliability Metrics\nSystem uptime: 99.7% availability over 90 days\nError rates: <0.8% system errors, <2.1% user errors\nData accuracy: 96.4% accurate content classification\nRecovery time: <5 minutes for automatic recovery scenarios\n10. Limitations and Future Directions\n10.1 Current Limitations\nTechnical Limitations\nAPI dependency: System performance depends on OpenAI API availability\nLanguage support: Currently optimized for English-language resumes\nFile format constraints: Limited to PDF, DOCX, and TXT formats\nProcessing time: Complex requests may take 20-30 seconds\nDatabase scaling: Single PostgreSQL instance may require optimization for enterprise scale\nFunctional Limitations\nIndustry specialization: General approach may not capture niche industry requirements\nGeographic scope: Job market data primarily focused on major markets\nResume format variety: May not handle highly creative or non-standard resume formats optimally\nReal-time job data: Job listings may not reflect most current market postings\n10.2 Future Enhancement Opportunities\nTechnical Improvements\nMulti-language support with localized job market data\nAdvanced caching strategies for improved response times\nDistributed processing for enhanced scalability\nReal-time collaboration features for team-based job hunting\nMobile application development for native experiences\nFeature Enhancements\nInterview preparation modules with mock interview capabilities\nSalary negotiation guidance with market-specific data\nNetwork analysis for identifying connection opportunities\nCareer progression planning with long-term goal setting\nIndustry-specific templates and optimization strategies\nIntelligence Upgrades\nLearning from user feedback to improve recommendation accuracy\nPredictive analytics for job market trends and opportunities\nPersonalization engines adapting to individual career patterns\nIntegration capabilities with major job platforms and ATS systems\n10.3 Research Directions\nAcademic Opportunities\nAgent coordination optimization research for improved workflow efficiency\nContent validation accuracy studies for resume vs. non-resume classification\nUser experience research on multi-agent system interactions\nPerformance benchmarking against traditional job hunting methods\nIndustry Applications\nEnterprise HR integration for internal career development\nRecruitment agency tools for candidate optimization\nEducational institution career services enhancement\nGovernment workforce development program integration\n11. Conclusion\n11.1 Technical Achievements\n\nThis publication demonstrates a complete production-ready multi-agent AI system that successfully addresses the complex challenges of modern job hunting. Key technical achievements include:\n\nRobust multi-agent architecture with autonomous coordination and human-in-the-loop capabilities\nComprehensive safety implementation including AI-powered content validation and security measures\nProduction-grade error handling with graceful degradation and automatic recovery\nScalable database integration providing system-wide performance analytics\nMobile-responsive user interface optimized for modern web standards\nExtensive testing coverage ensuring reliability and maintainability\n11.2 Practical Impact\n\nThe system provides tangible value to job seekers by:\n\nReducing job hunting time through automated analysis and research\nImproving application quality via AI-optimized resumes and CVs\nProviding market insights not easily accessible to individual job seekers\nOffering personalized guidance adapted to specific career goals and contexts\nIntegrating multiple services into a single, coherent user experience\n11.3 Production Readiness\n\nThe system meets enterprise-grade requirements through:\n\nComprehensive monitoring and analytics capabilities\nSecurity measures protecting user data and system integrity\nScalable architecture supporting concurrent users and growing demand\nMaintainable codebase with extensive testing and documentation\nFlexible deployment options suitable for various hosting environments\n11.4 Contribution to Multi-Agent AI\n\nThis work contributes to the broader field of multi-agent AI systems by demonstrating:\n\nPractical implementation patterns for coordinated agent workflows\nSafety integration strategies specific to multi-agent architectures\nPerformance monitoring approaches for complex AI system evaluation\nUser experience design considerations for multi-agent applications\nProduction deployment strategies for AI-driven applications\n12. License and Usage Rights\n12.1 MIT License Overview\n\nThis project is released under the MIT License, which provides broad permissions for both commercial and non-commercial use while maintaining appropriate attribution requirements. The MIT License represents one of the most permissive open-source licenses available, enabling maximum flexibility for users and contributors.\n\n12.2 Permitted Uses\n\nThe MIT License grants users extensive rights to use, modify, and distribute the software. Users may freely use the system for commercial purposes, including integration into proprietary products or services. The license permits modification of the source code to meet specific requirements or to add new functionality. Distribution rights allow users to share both original and modified versions of the software with others.\n\nSublicensing capabilities enable users to incorporate the software into larger projects with different licensing terms, providing flexibility for complex software integration scenarios. The license places no restrictions on the field of use, allowing application in any domain or industry without additional permissions.\n\n12.3 Requirements and Restrictions\n\nThe primary requirement under the MIT License is attribution, which requires that the original copyright notice and license text be included in all copies or substantial portions of the software. This attribution requirement ensures that original authors receive appropriate credit while maintaining the permissive nature of the license.\n\nThe license includes standard warranty disclaimers that protect contributors from liability while clearly communicating that the software is provided \"as is\" without guarantees of fitness for particular purposes. Users assume responsibility for evaluating the software's suitability for their specific needs and for any consequences of its use.\n\n12.4 Technical Asset Rights\n\nUsers may freely deploy the system in production environments, modify the codebase to meet specific requirements, integrate components into larger systems, and create derivative works based on the original system. The license permits both private and commercial use without requiring disclosure of modifications or payment of royalties.\n\nEducational and research use is explicitly permitted and encouraged, supporting academic research, student projects, and institutional learning objectives. The permissive licensing facilitates adoption in educational settings where restrictive licenses might present barriers to use.\n\nThe system serves as a comprehensive reference implementation for building production-ready multi-agent AI systems, providing both technical insights and practical guidance for developers and researchers working in this domain.\n\nRepository: \nhttps://github.com/Dprof-in-tech/job-hunting-agent\n\nLive Demo: \nhttps://job-hunting-agent.vercel.app\n\nDocumentation: Complete setup and usage instructions included in repository\nLicense: MIT License\nContact: \nAmaechiisaac450@gmail.com\n\nQuick Start: Clone the repository and run npm run dev to start the complete system locally.\n\nAppendix A: Code Samples\nA.1 Agent Implementation Example\n@safe_ai_wrapper(agent_name=\"resume_analyst\", safety_level=\"high\")\ndef resume_analyst_agent(state: MultiAgentState):\n    \"\"\"Expert resume analyst with superior analysis capabilities\"\"\"\n    \n    resume_path = state.get('resume_path', '')\n    if not resume_path or not os.path.exists(resume_path):\n        return {\n            \"messages\": [AIMessage(content=\"❌ **Resume Analyst**: No valid resume file provided\")],\n            \"next_agent\": \"coordinator\",\n            \"completed_tasks\": state.get('completed_tasks', []) + ['resume_analyst']\n        }\n    \n    # Extract and validate resume content\n    resume_content = parse_resume.invoke(resume_path)\n    \n    # Check for career transition scenario\n    job_market_data = state.get('job_market_data', {})\n    is_career_transition = bool(job_market_data)\n    \n    # Generate comprehensive analysis\n    analysis_prompt = create_analysis_prompt(resume_content, job_market_data, is_career_transition)\n    analysis_result = llm.invoke(analysis_prompt)\n    \n    return {\n        \"messages\": state.get('messages', []) + [analysis_result],\n        \"resume_analysis\": parse_analysis_result(analysis_result.content),\n        \"next_agent\": \"coordinator\",\n        \"completed_tasks\": state.get('completed_tasks', []) + ['resume_analyst']\n    }\nA.2 Database Schema\n# Complete database model definitions\nclass SystemMetrics(Base):\n    __tablename__ = 'system_metrics'\n    \n    id = Column(Integer, primary_key=True)\n    timestamp = Column(DateTime, default=datetime.utcnow)\n    total_requests = Column(Integer, default=0)\n    successful_requests = Column(Integer, default=0)\n    failed_requests = Column(Integer, default=0)\n    avg_response_time = Column(Float, default=0.0)\n    user_satisfaction_score = Column(Float, default=0.0)\n    human_interventions = Column(Integer, default=0)\n    uptime_percentage = Column(Float, default=100.0)\n    system_grade = Column(String(10))\n    additional_data = Column(JSON)\n\nclass AgentMetrics(Base):\n    __tablename__ = 'agent_metrics'\n    \n    id = Column(Integer, primary_key=True)\n    timestamp = Column(DateTime, default=datetime.utcnow)\n    agent_name = Column(String(100), nullable=False)\n    total_calls = Column(Integer, default=0)\n    successful_calls = Column(Integer, default=0)\n    failed_calls = Column(Integer, default=0)\n    total_processing_time = Column(Float, default=0.0)\n    avg_processing_time = Column(Float, default=0.0)\n    success_rate = Column(Float, default=0.0)\n    performance_grade = Column(String(10))\n    errors = Column(JSON)\nA.3 Frontend Component Structure\n// Main application component structure\nconst JobHuntingApp: React.FC = () => {\n  const [selectedFile, setSelectedFile] = useState<File | null>(null);\n  const [isLoading, setIsLoading] = useState(false);\n  const [response, setResponse] = useState<ApiResponse | null>(null);\n  const [customPrompt, setCustomPrompt] = useState('');\n\n  const handleFileSubmission = async (prompt: string) => {\n    setIsLoading(true);\n    \n    try {\n      const formData = new FormData();\n      if (selectedFile) formData.append('file', selectedFile);\n      formData.append('prompt', prompt);\n\n      const response = await fetch('/api/process', {\n        method: 'POST',\n        body: formData,\n      });\n\n      const data = await response.json();\n      setResponse(data);\n    } catch (error) {\n      console.error('Processing error:', error);\n    } finally {\n      setIsLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"min-h-screen bg-white text-black\">\n      <Header />\n      <div className=\"max-w-6xl mx-auto px-4 sm:px-6 py-8 sm:py-12\">\n        <div className=\"grid lg:grid-cols-2 gap-6 lg:gap-12\">\n          <InputSection \n            selectedFile={selectedFile}\n            onFileSelect={setSelectedFile}\n            customPrompt={customPrompt}\n            onPromptChange={setCustomPrompt}\n            onSubmit={handleFileSubmission}\n            isLoading={isLoading}\n          />\n          <ResultsSection \n            response={response}\n            isLoading={isLoading}\n          />\n        </div>\n      </div>\n    </div>\n  );\n};\nAppendix B: Testing Examples\nB.1 Unit Test Suite\nimport pytest\nfrom api.tools import validate_resume_content, parse_resume\nfrom api.main import PerformanceEvaluator\n\nclass TestContentValidation:\n    def test_valid_resume_content(self):\n        \"\"\"Test validation of actual resume content\"\"\"\n        resume_text = \"\"\"\n        John Doe\n        Software Engineer\n        Experience:\n        • 5 years in Python development\n        • Led team of 3 developers\n        Education:\n        • BS Computer Science, MIT\n        \"\"\"\n        is_valid, explanation = validate_resume_content(resume_text)\n        assert is_valid == True\n        assert \"YES\" in explanation.upper()\n    \n    def test_invalid_content_rejection(self):\n        \"\"\"Test rejection of non-resume content\"\"\"\n        book_text = \"\"\"\n        Chapter 1: Introduction to Machine Learning\n        \n        Machine learning is a subset of artificial intelligence...\n        \"\"\"\n        is_valid, explanation = validate_resume_content(book_text)\n        assert is_valid == False\n        assert \"NO\" in explanation.upper()\n\nclass TestPerformanceTracking:\n    def test_system_metrics_accumulation(self):\n        \"\"\"Test system metrics properly accumulate\"\"\"\n        evaluator = PerformanceEvaluator()\n        \n        # Log multiple requests\n        evaluator.log_system_request(True, 2.5)\n        evaluator.log_system_request(True, 3.0)\n        evaluator.log_system_request(False, 1.5)\n        \n        assert evaluator.system_metrics.total_requests == 3\n        assert evaluator.system_metrics.successful_requests == 2\n        assert evaluator.system_metrics.failed_requests == 1\n        assert evaluator.system_metrics.avg_request_time == 2.33  # (2.5+3.0+1.5)/3\nB.2 Integration Test Example\ndef test_complete_workflow_integration():\n    \"\"\"Test end-to-end multi-agent workflow\"\"\"\n    # Setup test data\n    test_resume_path = \"tests/fixtures/sample_resume.pdf\"\n    test_prompt = \"Analyze my resume and find relevant software engineering jobs\"\n    \n    # Execute workflow\n    result = multi_agent.process_request(\n        user_message=test_prompt,\n        resume_path=test_resume_path,\n        user_id=\"test_user_123\"\n    )\n    \n    # Verify results\n    assert result[\"success\"] == True\n    assert \"resume_analyst\" in result[\"completed_tasks\"]\n    assert \"job_researcher\" in result[\"completed_tasks\"]\n    \n    # Verify resume analysis present\n    assert \"resume_analysis\" in result\n    assert result[\"resume_analysis\"][\"overall_score\"] > 0\n    \n    # Verify job data present\n    assert \"job_market_data\" in result\n    assert len(result[\"job_listings\"]) > 0\n    \n    # Verify performance tracking\n    assert result[\"processing_time\"] < 30.0  # SLA requirement\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nTL;DR\n\nIntroduction\n\n1.1 Purpose and Objectives\n\n1.2 Problem Statement and Significance\n\n1.3 System Overview\n\nSystem Architecture and Design\n\n2.1 Multi-Agent Architecture\n\n2.2 Technology Stack\n\nProduction-Ready Features\n\n3.1 Safety and Content Validation\n\nView all\nComments\n(1)\nCode\nYou might be interested\nThe Multi-Agent Job Hunter: An Interactive AI Career Assistant\nJul 12, 202514 reads\nAAIDC2025Agentic AI+4\nProductionize AI Multi-Agent Hiring Tool\nOct 07, 20259 reads\nAI Powered Recruitment Assistant\nAug 26, 202516 reads\nAI-powered tool for optimizing resumes and preparing for interviews\nI\nMay 30, 202513 reads",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "intelligent-rag-based-ai-assistant-Pw9aBbxaW6RW",
    "username": "Virginia Wamaitha Muranga",
    "license": "MIT License",
    "title": "Intelligent RAG-Based AI Assistant",
    "publication_description": "Back to publications\nOct 03, 2025\n●\n21 reads\n●\nMIT License\nIntelligent RAG-Based AI Assistant\nAI Assistant\nArtificial Intelligence\nDocument Analysis\nIntelligent Question Answering\nLLM Integration\nMulti-Format Documents\nRAG\nVector Search\nVirginia Wamaitha Muranga\nLandry Dally\nLike\nBookmark\nShare\nIntelligent RAG-Based AI Assistant: Multi-Format Document Analysis with Vector Search and LLM Integration\nAbstract\n\nThis project presents a Retrieval-Augmented Generation (RAG) system that enables intelligent question-answering over multi-format document collections. By combining semantic vector search with large language models (LLMs), the assistant provides accurate, context-aware responses from uploaded documents in .txt, .pdf, and .docx formats.\n\nMotivation\n\nConventional chatbots struggle to answer questions about domain-specific or proprietary content. This project addresses that challenge by creating a searchable knowledge base directly from user-provided documents. Organizations and individuals can therefore build custom AI assistants that deliver accurate, source-backed responses tailored to their unique content.\n\nSystem Architecture\n\nThe system implements a complete RAG pipeline:\n\nDocument Loading – Reads .txt, .pdf, .docx files from data/ directory\nText Chunking – Splits documents using RecursiveCharacterTextSplitter (500 characters, 50 overlap)\nEmbedding – Converts chunks to vectors using Sentence Transformers (all-MiniLM-L6-v2)\nStorage – Persists embeddings in ChromaDB for vector similarity search\nRetrieval – Finds top-k relevant chunks for user queries\nGeneration – LLM generates contextual answer using retrieved chunks\n\nTechnical Implementation\nCore Technologies\nFramework: LangChain for orchestration\nVector Database: ChromaDB (persistent)\nEmbeddings: Sentence Transformers\nLLM Integration: OpenAI GPT, Groq Llama, Google Gemini\nDocument Processing: PyPDF2, python-docx\nKey Features\nZero-shot understanding across multiple document formats\nPersistent vector storage for reproducibility\nFlexible support for multiple LLM providers\nInteractive CLI for experimentation\nConfigurable similarity thresholds\nEvaluation & Results\n\nTest Corpus: 11 documents spanning AI, quantum computing, climate science, and biotechnology.\n\nSample Query\n\nQuery: “What is artificial intelligence?”\nResponse: Accurate definition retrieved from AI documents with context-aware synthesis\nRetrieval Time: Sub-second\nRelevance: High semantic similarity score\n\nSystem Demonstrations\n\nMulti-document synthesis for comprehensive answers\nDomain-specific knowledge retrieval\nClear handling of out-of-scope queries\nOptimized for single-turn interactions\nImplementation Highlights\nDevelopment Practices\nOrganized modular codebase (app.py for pipeline, vectordb.py for DB logic)\nSecure API key management with .env and .env.example\nError handling for common issues (e.g., missing keys, unsupported files)\nDocumentation with system diagrams for clarity\nPractical Considerations\nHandles formats commonly used in organizations\nStraightforward setup and reproducibility\nUser feedback during document processing\nMulti-provider LLM support to handle cost/availability constraints\nScalability Features\nEfficient indexing for large corpora\nConfigurable chunking parameters\nPersistent vector database suitable for deployment\nLimitations & Future Work\nCurrent Optimizations\nOptimized for single-turn queries; multi-turn conversational memory planned.\nPerformance is strongest with specific, context-rich queries; future work includes query expansion.\nResponses are limited to the uploaded corpus; planned enhancements include hybrid web + local retrieval.\nDocument ingestion is manual; automation for continuous ingestion is under development.\nFuture Enhancements\nConversation history and context management\nAutomated document ingestion pipelines\nAdvanced query reformulation techniques\nEnterprise system integration (e.g., SharePoint, Google Drive)\nMulti-modal retrieval (e.g., images, charts, tables)\nConclusion\n\nThis project demonstrates a complete RAG pipeline implementation aligned with Module 1 of the Agentic AI Developer Certification Program. By combining robust document processing, semantic search, and multi-provider LLM integration, the assistant lays a solid foundation for domain-specific AI solutions. It balances technical rigor with practical usability, paving the way for future extensions into conversational, automated, and multi-modal AI assistants.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode\nFiles\napp.py - RAG-based-AI-assistant-project - Visual Studio Code 2025-10-03 23-40-52.mp4",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "introducing-embedanything-a-lightweight-multimodal-embedding-library-made-in-rust-HRa5fXfK7DZs",
    "username": "a@arballal95",
    "license": null,
    "title": "Introducing EmbedAnything : A lightweight multimodal embedding library made in Rust",
    "publication_description": "Back to publications\nOct 31, 2024\n●\n89 reads\n●\nApache 2.0\nWinner of\nMost Innovative Project\nat the\nNLP Projects Expo 2024\nIntroducing EmbedAnything : A lightweight multimodal embedding library made in Rust\nembedding\nnlp\nA\n@arballal95\nS\n@sonampankaj2\nLike\nBookmark\nShare\n\nAbstract\n\nEmbedAnything\n is an open-source Rust/Python framework that lets you generate vector embeddings for any data (text, images, audio) with minimal code. It's blazing fast, memory-efficient, and can handle massive datasets through vector streaming - meaning you can process 10GB+ files without running out of RAM. Whether you're building a search engine or recommendation system, you can start with pip install embed-anything and a few lines of Python.\n\nIntroduction\n\nEmbedding models are essential today. They have become extremely important due to their use in Retrieval-Augmented Generation (RAG), Image Search, Recommendation Systems, and many other applications. EmbedAnything provides a way to embed data from different modalities ranging from audio, images and text in an fast and efficient way. The library is completely written in Rust and leverages the Huggingface Candle library to serve highly performant embedding models like Jina, ColPali, and others. The best part is that there is no dependence on PyTorch or libtorch, making deploying your applications very easy. In this article, we will look at more features EmbedAnything offers. But first, let's see what motivated us to make this library.\n\nMotivation\n\nThe AI landscape today is fantastic. New cutting-edge models pop up almost every month. They are more efficient than ever and have excellent capabilities. However, deploying these large models is quite a pain, and there are several frameworks like VLLM, LitServe, and more to make serving LLMS easy. However, most of these solutions do not provide a way to efficiently serve embedding models. Embedding models are challenging because they require a lot of pre-processing even to start the embedding process. For example, embedding a PDF requires extracting the text, chunking it, embedding it, adding the required metadata, and then pushing these embeddings to a vector database. Solutions like Ollama and FastEmbed exist, which provide embedding features but have drawbacks. Other solutions require PyTorch or Libtorch, which makes the application footprint quite heavy, and thus, deployment is more complicated. Moreover, currently, there are no existing solutions to extract embeddings and custom metadata from various file formats. LangChain offers some solutions, but it is a bulky package, and extracting only the embedding data is difficult. Moreover, LangChain is not very suitable for vision-related tasks.\n\nThis is where EmbedAnything comes in. It is a lightweight library that allows you to generate embeddings from different file formats and modalities. Currently, EmbedAnything supports text documents, images and audio, with many more formats like video in the pipeline. The idea is to provide an end-to-end solution where you can give the file and get the embeddings with the appropriate metadata.\n\nDevelopment of EmbedAnything started with these goals in mind:\n\nCompatibility with Local and Cloud Models: Seamless integration with local and cloud-based embedding models.\nHigh-Speed Performance: Fast processing to meet demanding application requirements.\nMultimodal Capability: Flexibility to handle various modalities.\nCPU and GPU Compatibility: Performance optimization for both CPU and GPU environments.\nLightweight Design: Minimized footprint for efficient resource utilization.\nOur Solution\n\nNow, let us look at the different ways we are tackling the problems associated with embedding data.\n\nKeeping it Local\n\nWhile cloud-based embedding services like OpenAI, Jina, and Mistral offer convenience, many users require the flexibility and control of local embedding models. Here's why local models are crucial for some use cases:\n\nCost-Effectiveness: Cloud services often charge per API call or model usage. Running embeddings locally on your own hardware can significantly reduce costs, especially for projects with frequent or high-volume embedding needs.\nData Privacy: Certain data, like medical records or financial documents, might be too sensitive to upload to the cloud. Local embedding keeps your data confidential and under your control.\nOffline Functionality: An internet connection isn't always guaranteed. Local models ensure your embedding tasks can run uninterrupted even without an internet connection.\nPerformance\n\nEmbedAnything is built with Rust. This makes it faster and provides type safety and a much better development experience. But why is speed so crucial in this process?\n\nThe need for speed:\n\nCreating embeddings from files involves two steps that demand significant computational power:\n\nExtracting Text from Files, Especially PDFs: Text can exist in different formats such as markdown, PDFs, and Word documents. However, extracting text from PDFs can be challenging and often causes slowdowns. It is especially difficult to extract text in manageable batches as embedding models have a context limit. Breaking the text into paragraphs containing focused information can help. This task is even more compute intensive which using OCR models like Tesseract to extract text.\nInferencing on the Transformer Embedding Model: The transformer model is usually at the core of the embedding process, but it is known for being computationally expensive. To address this, EmbedAnything utilizes the Candle Framework by Hugging Face, a machine-learning framework built entirely in Rust for optimized performance.\nThe Benefit of Rust for Speed\n\nBy leveraging Rust for its core functionalities, EmbedAnything offers significant speed advantages:\n\nRust is Compiled: Unlike Python, Rust compiles directly to machine code, resulting in faster execution.\nEfficient Memory Management: Working with embedding models requires careful memory usage when handling models and embeddings. Rust's efficient data structures make this possible.\nTrue Concurrency: Rust enables genuine multi-threading and asynchronous programming. As illustrated in the image below, we can simultaneously extract text, split content, generate embeddings, and push data to the database like in the Vector Streaming feature discussed below.\n\nThe image shows the speed of embedding documents with EmbedAnything compared to other libraries. You can find the source for the benchmark \nhere\n.\n\nWhat does Candle bring to the table?\n\nRunning language models or embedding models locally can be difficult, especially when you want to deploy a product that utilizes these models. If you use the transformers library from Hugging Face in Python, you will depend on PyTorch for tensor operations. This, in turn, depends on Libtorch, meaning you must include the entire Libtorch library with your product. Also, Candle allows inferences on CUDA-enabled GPUs right out of the box.\n\nMultiple Modalities\n\nEmbedAnything supports different modalities. You can embed text documents like HTML Pages, PDFs, and Markdowns using text embedding models like Jina, AllMiniLM, and others. You can also embed images using CLIP.\n\nAudio files can also be embedded using Whisper. The best part about EmbedAnything for audio embedding is that you can connect a text embedding model with Whisper. Using this, you can embed the text that Whisper decodes in parallel. The metadata includes the time stamps of the texts. You can see this in action in this \nHuggingface Space\n.\n\nMoreover, with EmbedAnything, you can use late-interaction models like ColPali, which remove the need to do OCR or chunking by embedding the PDF pages as a whole and retrieving the relevant PDF and pages against a query. This has enormous potential. For example, you can reduce the number of tokens that a Vision Language Model uses for document reading by retrieving only the valid pages and then showing them to large VLMs like GPT-4o and Gemini Flash. This can save a lot of cost and time.\n\nVector Streaming\n\nVector streaming allows you to create an asynchronous chunking and embedding task. We can effectively spawn threads to handle this task using Rust's concurrency patterns and thread safety. This is done using Rust's MPSC (Multi-producer Single Consumer) module, which passes messages between threads. Thus, this creates a stream of chunks passed into the embedding thread with a buffer. Once the buffer is complete, it embeds the chunks and sends the embeddings back to the main thread, where they are sent to the vector database. This ensures time is well spent on a single operation and no bottlenecks. Moreover, only the chunks and embeddings in the buffer are stored in the system memory. They are erased from the memory once moved to the vector database.\n\nThus, it effectively solves the problem by making many tasks done asynchronously and thus improving efficiency.\n\nReal-world Use Cases\nVector Streaming: This involves streaming live information from videos by breaking them into frames and generating embeddings of the images using multimodal embedding models like CLIP. This technique is particularly useful for live camera feeds, such as CCTV, to detect malicious activities or traffic violations.\nSearch Applications: We generate embeddings from PDFs and enable direct searches for page numbers based on user queries, utilizing ColPali. All of this is integrated into our pipeline.\nClassification Problems: Embeddings can be employed for classification tasks and can scale through metric learning, allowing for the easy addition of new classes.\nRAG Applications: We view EmbedAnything as an ingestion pipeline for vector databases, which can be extensively utilized for Retrieval-Augmented Generation (RAG) in chatbots.\nHow to get started?\n\nTo install our library, all you have to do is\n\npip install embed-anythin\n\nTo improve the speed further and to use large models like ColPali, use the GPU version of embed anything by running.\n\npip install embed-anything-gpu\n\nThen, with just a few lines of code, you can embed any files and directories.\n\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"model link from huggingface\"\n)\nconfig = TextEmbedConfig(chunk_size=200, batch_size=32)\ndata = embed_anything.embed_file(\"file_address\", embeder=model, config=config)\n\nYou can check out the documentation at \nhttps://starlight-search.com/references/\n\nWhat’s Next\n\nFuture work includes expanding our modalities and vector streaming adapters. We currently support unstructured sources like images, audio, and texts from different sources like PDFs, markdown, and jpegs, but we would also like to expand to graph embeddings and video embeddings. We currently support Weaviate and Elastic Cloud to stream the vectors, which we will expand to other vector databases.\n\nMoreover, we will also release methods to fine-tune embedding models easily using Candle, similar to how Sentence Transformers does, but with the speed and memory efficiency of Rust.\n\nWith this, I would like to conclude this article on this amazing new library that I am building, and I hope to receive some great feedback from the readers. Try it out now and check out the \nGitHub Repo\n.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nMotivation\n\nOur Solution\n\nKeeping it Local\n\nPerformance\n\nThe Benefit of Rust for Speed\n\nWhat does Candle bring to the table?\n\nMultiple Modalities\n\nVector Streaming\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nYou might be interested\nVector Databases: Building a Semantic Retrieval System (AAIDC-Week2-Lesson-4b)\nMay 26, 2025548 reads\nAgentic AIChroma-DB+1\nI Built a Semantic Search API Using FastAPI and SBERT — Here's How It Works\nH\nMay 23, 20254 reads\nBERTFastAPI+3\nRAG Based Research Assistant\nA\nJun 13, 202510 reads\nAAIDC2025\nRAG Based Chatbot--Ready Tensor Publication Explorer\nSep 04, 202516 reads\nAAIDC2025AI Assistant+1",
    "awards": [
      "most innovative project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "jarvis-ai-PrsBAzlcJ63h",
    "username": "Atharva Shinde",
    "license": "MIT License",
    "title": "JARVIS AI - Your Hands-Free AI Assistant",
    "publication_description": "Back to publications\nMar 21, 2025\n●\n166 reads\n●\nMIT License\nJARVIS AI - Your Hands-Free AI Assistant\nadvanced\nai\nartificialintelligence\nchatbot\njarvis\nmachinelearning\nml\nAtharva Shinde\nLike\nBookmark\nShare\n🤖 Jarvis AI Assistant\n🌟 Overview\n\nJarvis AI is a powerful voice-controlled personal assistant designed to help you with a wide range of tasks. Inspired by Iron Man's JARVIS, this application can answer your questions, search the internet for real-time information, control your computer, play music, generate images, and much more.\n\nProject Link\n\n🔍 Current State Gap Identification\n\nCurrent digital assistants often fall short in several key areas:\n\nLimited Integration: Most assistants operate within closed ecosystems\nPrivacy Concerns: Cloud-based solutions raise data privacy issues\nContextual Understanding: Many systems struggle with complex, multi-turn conversations\nCustomization Limitations: End users have minimal control over functionality\nResponsiveness: Network-dependent assistants can be slow to respond\n\nJarvis AI aims to address these gaps by providing a more integrated, responsive, and customizable experience with an architecture that balances cloud capabilities with local processing.\n\n🎯 Problem Definition\n\nThe project addresses several key challenges in personal AI assistants:\n\nAccessibility: Making advanced AI capabilities accessible to everyday users\nMultifunctionality: Creating a unified interface for diverse digital tasks\nResponse Quality: Ensuring responses are both accurate and helpful\nSystem Integration: Interacting with various applications and services\nUser Experience: Minimizing friction in human-AI interaction\n✨ Features\n🎙️ Voice Interaction - Natural voice commands and responses\n🧠 Intelligent Query Processing - Automatically determines if your question needs real-time data or can be answered from existing knowledge\n🖥️ System Automation - Open/close applications, control your computer\n🔍 Web Search - Real-time information from the internet\n🎵 Media Control - Play music and videos\n🖼️ Image Generation - Create images based on your descriptions\n💬 Natural Conversation - Engage in human-like conversations\n👁️ Screen Analysis - Analyze and describe what's happening on your screen using Google's Gemini Vision AI\n🤖 Camera Analysis - Analyze your webcam feed using Google's Gemini Vision AI\n\n🛠️ Technology Stack\nFrontend: PyQt5 for the graphical user interface\nBackend:\nAI Models: Groq and Cohere APIs for natural language processing, Google Gemini for vision AI\nSpeech Processing: Web Speech API for speech recognition, Edge TTS for text-to-speech\nWeb Integration: Selenium for web automation\nMedia: Pygame for audio playback\nComputer Vision: OpenCV and MSS for screen capture and analysis\n⚙️ Performance Characteristics and Requirements\nPerformance\nResponse Time: Typically under 2 seconds for commands, 3-5 seconds for knowledge queries\nMemory Usage: ~200MB base, scaling to ~500MB during operation\nCPU Utilization: Moderate during speech recognition, low during idle\nNetwork: Stable internet connection (1Mbps+ recommended)\nDisk Space: Minimal (<100MB) excluding Python and dependencies\nHardware Requirements\nProcessor: Intel Core i3/AMD Ryzen 3 or better\nRAM: 4GB minimum, 8GB recommended\nStorage: 1GB free space\nAudio: Working microphone and speakers/headphones\nOperating System: Windows 10/11 (primary support), adaptable for Linux/macOS\n📋 Requirements\nPython 3.10.10 (recommended)\nThe following packages:\npython-dotenv\ngroq\nAppOpener\npywhatkit\nbs4\nselenium\npillow\nrich\nrequests\nkeyboard\ncohere\ngooglesearch-python\nmtranslate\npygame\nedge-tts\nPyQt5\nwebdriver-manager\ngoogle-generativeai\nopencv-python\nnumpy\nmss\nplaysound\nSpeechRecognition\n\n🚀 Installation\n\nInstall Python 3.10.10 from \npython.org\n\nClone the repository:\n\ngit clone https://github.com/atharva-shinde7/JARVIS-AI.git\ncd JARVIS-AI\n\n\nCreate and activate a virtual environment:\n\npython -m venv .venv\n# Windows\n.venv\\Scripts\\activate\n# Linux/Mac\nsource .venv/bin/activate\n\n\nInstall the required packages:\n\npip install -r Requirements.txt\n\n\nCreate a .env file with the following variables:\n\nCohereAPIKey = your_cohere_api_key\nUsername = Your Name\nAssistantname = Jarvis\nGroqAPIKey = your_groq_api_key\nInputLanguage = en\nAssistantVoice = en-CA-LiamNeural\nHuggingFaceAPIKey = your_huggingface_api_key\nGeminiAPIKey=your_gemini_api_key\n\n🚢 Deployment Considerations\n\nWhen deploying Jarvis AI, consider the following:\n\nEnvironment Setup\nPython Environment: Preferably isolate in a virtual environment\nAPI Keys: Secure storage of API keys (consider environment variables)\nBrowser Dependencies: MS Edge WebDriver for speech recognition\nScaling Options\nLocal Network Deployment: Can be deployed on a home server\nAPI Usage Limits: Monitor usage of third-party APIs\nData Storage: Configure chat history retention based on storage constraints\nSecurity Considerations\nAPI Key Protection: Store keys securely, not in source code\nPermission Management: Limit system command execution scope\nData Privacy: Local storage of conversation history\n🎮 Usage\n\nTo start Jarvis AI:\n\npython Main.py\n\nVoice Commands Examples:\nGeneral Questions: \"Who was Albert Einstein?\" or \"How do stars form?\"\nReal-time Information: \"What's today's news?\" or \"Who is the current Prime Minister of India?\"\nOpen Applications: \"Open Chrome\" or \"Open Notepad\"\nClose Applications: \"Close Spotify\" or \"Close Calculator\"\nPlay Music: \"Play Shape of You\" or \"Play songs by Ed Sheeran\"\nGenerate Images: \"Generate an image of a mountain landscape\"\nWeb Search: \"Google search for AI tutorials\" or \"YouTube search for cooking recipes\"\nScreen Analysis: \"Analyze my screen\" or \"Analyze screen\" - Jarvis will enter screen analysis mode and listen for specific screen analysis commands\nCamera Analysis: \"Analyze camera\" or \"What's on my camera?\" - Jarvis will analyze your webcam feed\n🧩 Project Structure\nJarvis AI/\n│\n├── Main.py                  # Main application entry point\n│\n├── Frontend/                # GUI and user interface components\n│   ├── GUI.py               # Main GUI implementation\n│   ├── Graphics/            # Icons and visual assets\n│   └── Files/               # Temporary files for UI state\n│\n├── Backend/                 # Core functionality modules\n│   ├── Model.py             # Decision-making model (Cohere)\n│   ├── Chatbot.py           # Conversational AI (Groq)\n│   ├── SpeechToText.py      # Voice recognition\n│   ├── TextToSpeech.py      # Voice synthesis\n│   ├── RealtimeSearchEngine.py # Web search functionality\n│   ├── Automation.py        # System control functions\n│   ├── ImageGeneration.py   # AI image generation\n│   └── screen_analysis.py   # Screen analysis using Google Gemini Vision AI\n│\n├── Data/                    # Storage for conversation history and settings\n│\n├── .env                     # Environment variables and API keys\n│\n└── Requirements.txt         # Python dependencies\n\n🔄 How It Works\nVoice Input: The system listens for your voice commands using the web speech API.\nQuery Analysis: Jarvis uses the First Layer Decision-Making Model to classify your query.\nTask Execution:\nFor general knowledge questions, it uses the Groq LLM.\nFor real-time information, it searches the web.\nFor commands, it performs the requested action.\nFor screen analysis, it activates the dedicated screen analysis module.\nFor camera analysis, it activates the dedicated camera analysis module.\nResponse: Jarvis responds both visually (text on screen) and audibly (text-to-speech).\n⚠️ Limitations Discussion\n\nJarvis AI, while powerful, has several limitations to be aware of:\n\nAPI Dependency: Requires active internet for many functions\nSpeech Recognition Accuracy: May struggle in noisy environments\nLanguage Support: Primary support for English, limited multilingual capabilities\nResource Consumption: Browser-based speech recognition can be resource-intensive\nCommand Scope: Limited to pre-defined automation capabilities\nContext Window: Limited memory of previous conversation turns\nPlatform Dependence: Optimized for Windows, may require adjustments for other platforms\n🔍 Source Credibility\n\nJarvis AI employs a multi-layered approach to ensure information credibility:\n\nInformation Sourcing Strategy\nSmart Classification: First determines if queries need real-time data or can be answered from knowledge\nMulti-source Integration: Gathers information from multiple search results for balanced perspective\nAI Synthesis: Uses Groq LLM to intelligently synthesize information from various sources\nCredibility Mechanisms\nTime Awareness: Adds current date/time context to every query\nSource Transparency: Includes source titles and descriptions in search results\nQuality Parameters: Uses advanced search parameters to prioritize reliable sources\nThree-stage Verification: Classification → Search → LLM Synthesis pipeline\nFuture Enhancements\nSource quality metrics and domain authority scoring\nCross-reference fact verification across multiple sources\nFormal citation generation for academic/professional use\nUser feedback integration for continuous improvement\n🔝 Competitive Differentiation\n\nJarvis AI differentiates itself from commercial AI assistants in several key ways:\n\nFeature\tJarvis AI\tCommercial Assistants\nCustomizability\tHigh (open source)\tLimited\nPrivacy\tLocal processing where possible\tCloud-dependent\nPlatform\tCross-platform potential\tOften ecosystem-locked\nIntegration\tFlexible with any service\tTypically vendor-restricted\nCost\tFree, API costs only\tOften subscription-based\nTransparency\tFull code visibility\tBlack-box algorithms\nKey Advantages\nFirst-layer Decision Making: Unique classification approach before query processing\nHybrid Architecture: Balance of local functionality and cloud AI\nExtensibility: Easy to add new commands and capabilities\nIndependence: Not tied to any specific ecosystem or vendor\n🔮 Significance and Implications\n\nThe development of Jarvis AI has broader implications:\n\nTechnological Impact\nDemonstrates the practical integration of multiple AI services into a cohesive system\nShows how traditional software can be enhanced with AI capabilities\nProvides a framework for building personalized AI assistants\nSocial Implications\nIncreases accessibility to AI technology for everyday users\nRaises questions about AI assistants in daily life and human-AI interaction\nDemonstrates the balance between cloud AI power and local control\nEducational Value\nServes as a learning platform for understanding:\nNatural language processing\nSpeech recognition systems\nMulti-threaded application design\nAPI integration\n🛠️ Maintenance and Support Status\nCurrent Status\n\nJarvis AI is an actively maintained project with regular updates focused on:\n\nBug fixes: Addressing issues as they arise\nFeature enhancements: Adding new capabilities based on user feedback\nAPI updates: Keeping pace with changes in third-party services\nSupport Channels\nGitHub Issues: Primary channel for bug reports and feature requests\nDocumentation: Comprehensive guides for common issues\nCommunity Support: Growing community of contributors\nUpdate Schedule\nMajor releases: Quarterly\nMinor updates: Monthly\nHotfixes: As needed\n🔧 Advanced Configuration\n\nYou can customize Jarvis by modifying these settings in the .env file:\n\nUsername: Your name (how you want Jarvis to address you)\nAssistantname: The assistant's name (default is \"Jarvis\")\nInputLanguage: Your preferred language for voice input\nAssistantVoice: The voice model for speech synthesis\n🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n📝 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n🙏 Acknowledgements\nInspired by Iron Man's JARVIS\nBuilt with the power of Groq and Cohere AI\nSpecial thanks to all the open-source libraries that made this possible\n\nCreated with ❤️ by Atharva Shinde\n\n\n\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\n🤖 Jarvis AI Assistant\n\n🌟 Overview\n\n🔍 Current State Gap Identification\n\n🎯 Problem Definition\n\n✨ Features\n\n🛠️ Technology Stack\n\n⚙️ Performance Characteristics and Requirements\n\nPerformance\n\nHardware Requirements\n\n📋 Requirements\n\nView all\nCode\nFiles\nREADME.md\nRequirements.txt\nLICENSE\nChatbot.py\nRealtimeSearchEngine.py\nTextToSpeech.py\nModel.py\nSpeechToText.py\nImageGeneration.py\nVoice.html\nChatLog.json\nGUI.py\nTTS_B.py\nAutomation.py\nscreen_analysis.py\nMain.py\nCameraAnalysis.py",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "knowledge-graphs-with-postgresql-eQyINuo4ojwW",
    "username": "j@jimysanchomolero",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nOct 28, 2024\n●\n258 reads\n●\nMIT License\nKnowledge Graphs with Postgresql\nJ\n@jimysanchomolero\nLike\nBookmark\nShare\n\nIntroduction\n\nThis project is based on LightRAG. It serves as the bridge between the LightRAG logic (with some differences and improvements), and postgresql as a vector and text storage. The graph tool is still networkx.\n\nAs the paper says:\n\nRetrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose LightRAG, which incorporates graph structures into text indexing and retrieval processes\n\n1. Database\n\nWe only need to define 3 models:\n\nChunk: Stores the text information.\nEntity: Stores the entities extracted from the text.\nRelationships: Stores the relationships between the different entities extracted from the text.\n1.1 Dockerfile\n\nIn lightrag, the entities and relationships are transformed into vectors to perform similarity search over to retrieve the most relevant chunks to the given query. To do so in postgres, we need pgvector. The Dockerfile image:\n\nFROM postgres:16\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    build-essential \\\n    postgresql-server-dev-16 \\\n    wget \\\n    unzip \\\n    git \\\n    clang \\\n    ca-certificates \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN cd /tmp \\\n    && git clone --branch v0.5.1 https://github.com/pgvector/pgvector.git \\\n    && cd pgvector \\\n    && make \\\n    && make install\n\nCOPY ./init.sql /docker-entrypoint-initdb.d/\n\nWhere the init.sql:\n\nCREATE EXTENSION IF NOT EXISTS vector;\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n1.2 Schema\n\nThe database schema is defined as:\n\nclass Chunk(Base):\n\n    __tablename__ = \"chunk\"\n    chunk_id = Column(UUID(as_uuid=True), primary_key=True, index=True, default=uuid.uuid4)    \n    text = Column(Text, nullable=False)\n    chunk_embedding = Column(Vector(1536))\n    hash = Column(String, nullable=False, index=True)\n    \n    chunk_entities = relationship(\"Entity\", back_populates=\"entity_chunk\", cascade='all, delete-orphan')\n    chunk_relationships = relationship(\"Relationship\", back_populates=\"relationship_chunk\", cascade='all, delete-orphan')\n\n    \nclass Entity(Base):\n    \n    __tablename__ = \"entity\"\n    entity_id = Column(UUID(as_uuid=True), primary_key=True, index=True, default=uuid.uuid4)\n    \n    hash = Column(String, nullable=False, index=True)\n    entity_name = Column(String, nullable=False)\n    entity_type = Column(String, nullable=True, default=\"unknown\")\n    description = Column(String, nullable=False)\n    entity_embedding = Column(Vector(1536))\n    \n    chunk_id = Column(UUID(as_uuid=True), ForeignKey(\"chunk.chunk_id\", ondelete='CASCADE'), nullable=False)\n    \n    entity_chunk = relationship(\"Chunk\", foreign_keys=[chunk_id], back_populates=\"chunk_entities\")\n    sources = relationship(\"Relationship\", back_populates=\"source_entity\", foreign_keys=\"[Relationship.source_id]\", uselist=True)\n    targets = relationship(\"Relationship\", back_populates=\"target_entity\", foreign_keys=\"[Relationship.target_id]\", uselist=True)\n\n\nclass Relationship(Base):\n    \n    __tablename__ = \"relationship\"\n    relationship_id = Column(UUID(as_uuid=True), primary_key=True, index=True, default=uuid.uuid4)\n    hash = Column(String, nullable=False, index=True)\n    description = Column(String, nullable=False)\n    relationship_embedding = Column(Vector(1536))\n    keywords = Column(String, nullable=True)\n    weight = Column(Float, nullable=True)\n    \n    source_id = Column(UUID(as_uuid=True), ForeignKey(\"entity.entity_id\", ondelete=\"CASCADE\"), nullable=False)\n    target_id = Column(UUID(as_uuid=True), ForeignKey(\"entity.entity_id\", ondelete=\"CASCADE\"), nullable=False)\n    chunk_id = Column(UUID(as_uuid=True), ForeignKey(\"chunk.chunk_id\", ondelete='CASCADE'), nullable=False)\n    \n    relationship_chunk = relationship(\"Chunk\", foreign_keys=[chunk_id], back_populates=\"chunk_relationships\")\n    source_entity = relationship(\"Entity\", foreign_keys=[source_id], remote_side=\"[Entity.entity_id]\", back_populates=\"sources\")\n    target_entity = relationship(\"Entity\", foreign_keys=[target_id], remote_side=\"[Entity.entity_id]\", back_populates=\"targets\")\n1.3 Vector columns\n\nTo deal with entities and relationships more confortable throughout the process, some pydantic models are created:\n\nclass EntityModel(BaseModel):\n    entity_name: str\n    entity_type: str\n    entity_description: str\n    chunk_id: Set[str] | str\n    entity_text: str | None = None\n\n\nclass RelationshipModel(BaseModel):\n    source_entity: str\n    target_entity: str\n    relationship_description: str\n    relationship_keywords: str | List[str]\n    relationship_strength: float\n    chunk_id: Set[str] | str\n    \n    relationship_text: str | None = None\n\nTo compute the vectors that will later be used to perform similarity search given the query, we'll use the following attributes:\n\nFor the entities\n@model_validator(mode='after')\ndef remove_tildes(self):\n    self.entity_name = unidecode(self.entity_name).lower().strip()\n    self.entity_type = unidecode(self.entity_type).lower().strip()\n    self.entity_text = f\"{self.entity_name} it's of type: {self.entity_type}: {self.entity_description}\"\n    return self\n\nentity_text attribute is the one that will be transformed to a vector.\n\nFor the relationships\n@model_validator(mode='after')\ndef remove_tildes(self):\n    self.source_entity = unidecode(self.source_entity).lower().strip()\n    self.target_entity = unidecode(self.target_entity).lower().strip()\n    self.relationship_keywords = self.relationship_keywords.split(\", \") if isinstance(self.relationship_keywords, str) else self.relationship_keywords\n    self.relationship_text = f\"{self.source_entity} is related to {self.target_entity} because of: {self.relationship_description}\"\n    return self\n\nrelationship_text attribute is the one that will be transformed to a vector.\n\n2. Extraction and Upsert\n\nThe extraction of entities and relationships is a two-step process (just as lightrag):\n\nEntities and relationships themselves (we'll use the default prompt of graphrag and lighrag, but with the examples in json)\nMerging of entities and relationships.\n\nThe step 1) is the same as with lightrag. The step 2) is a little bit different, since we'll use fuzzywuzzy package to find similar entities and relationships and merge those.\nFirst we'll merge the entities, and then, keeping track of those merged entities, we'll merge the relationships.\n\n2.1 Entities\ndef find_most_similar(entity: EntityModel, candidates: List[EntityModel], threshold: int) -> List[EntityModel]:\n\n    most_sim_entity: List[EntityModel] = []\n    for candidate_entity in candidates:\n        if (entity.entity_type != candidate_entity.entity_type): continue\n        try:\n            score = fuzz.ratio(entity.entity_name, candidate_entity.entity_name)\n        except IndexError:\n            continue\n        if score > threshold:\n            most_sim_entity.append(candidate_entity)\n\n    return most_sim_entity\n\nWe iterate over the entities to find similar entities among the rest of them, and keep track of those that are found in a python dictionary: kept_vs_merged\n\nkept_vs_merged = {}\nmerged_entities = set()\nmodified_entities: Dict[Tuple[str, str, str, str], List[EntityModel]] = {}\nfor index, entity in enumerate(entities):\n    most_sim_entities = find_most_similar(entity=entity, candidates=entities[index+1:], threshold=threshold)\n    entity_key = (entity.entity_name, entity.entity_type, entity.entity_description)\n\n    if entity_key in merged_entities:\n        print(f\"{entity_key} already exists as an entity\")\n        continue\n\n    if (entity.entity_name, entity.entity_type, entity.entity_description, entity.get_chunk_id) not in modified_entities:\n        modified_entities[(entity.entity_name, entity.entity_type, entity.entity_description, entity.get_chunk_id)] = []\n    if most_sim_entities:\n        for most_sim_entity in most_sim_entities:\n            most_sim_key = (most_sim_entity.entity_name, most_sim_entity.entity_type, most_sim_entity.entity_description)\n            print(f\"{most_sim_key[:2]} has been identified as similar to another entity\")\n            merged_entities.add(most_sim_key)\n            modified_entities[(entity.entity_name, entity.entity_type, entity.entity_description, entity.get_chunk_id)].append(most_sim_entity)\n            if most_sim_entity.entity_name not in kept_vs_merged:\n                kept_vs_merged[most_sim_entity.entity_name] = {entity.entity_name}\n            else:\n                kept_vs_merged[most_sim_entity.entity_name].add(entity.entity_name)\n\nand then, we update the original entities like in lightrag:\n\nJoining the descriptions\nKeeping track of the chunks they appear in\nupdated_entities = []\nfor entity_info, sim_entities in modified_entities.items():\n\n    if entity_info[:-1] in merged_entities and not len(sim_entities): continue\n    \n    updated_entities.append(\n        EntityModel(\n            entity_name=entity_info[0], \n            entity_type=entity_info[1], \n            entity_description=entity_info[2] + \"\\n\".join([sim_entity.entity_description for sim_entity in sim_entities]), \n            chunk_id=set([entity_info[3]] + [sim_entity.get_chunk_id for sim_entity in sim_entities])\n        )\n    )\n    \nreturn updated_entities, kept_vs_merged\n2.2 Relationships\n\nBy using the kept_vs_merged entities found on the previous step, we update the relationships by replace the old entities with the new entities. Like in lightrag:\n\nrelationship_strength is updated\nrelationship_keywords is updated\nrelationship_description is joined between the common relationships\ndef _merge_relationships(\n    relationships: List[RelationshipModel], kept_vs_merged_entities: Dict[str, List[str]]\n) -> List[RelationshipModel]:\n\n    for relationship in relationships:\n        source, target = relationship.source_entity, relationship.target_entity\n        try:\n            if source in kept_vs_merged_entities:\n                relationship.source_entity = list(kept_vs_merged_entities[source])[0]\n            if target in kept_vs_merged_entities:\n                relationship.target_entity = list(kept_vs_merged_entities[target])[0]\n        except (KeyError, IndexError):\n            print(f\"Something went wrong for edge: {(source, target)}\")\n            continue\n        \n\n    merged_relationships = {}\n    for relationship in relationships:\n        edge = (relationship.source_entity, relationship.target_entity)\n        if edge not in merged_relationships:\n            merged_relationships[edge] = relationship\n            continue\n        print(f\"Edge: ({source}, {target}) already exists\")\n        existing_edge = merged_relationships[edge]\n        existing_edge.relationship_description += \"\\n\" + relationship.relationship_description\n        existing_edge.relationship_strength += relationship.relationship_strength\n        existing_edge.relationship_keywords += relationship.relationship_keywords\n        existing_edge.relationship_keywords = list(set(existing_edge.relationship_keywords))\n        existing_edge.update_chunk_ids(relationship.chunk_id)\n        \n    return list(merged_relationships.values())\n2.3 Upsert and Graph Creation\n\nUsing the merged entities and the merged relationships, we upsert this data into the database and we create the graph as well.\n\n3. Generation of response\n\nThere are several steps involed before generating a response:\n\nKeyword extraction from query.\nSimilarity Search\nGraph transversal for both local and global query\nGenerate a response\n3.1 Keyword extraction\n\nGiven a query, we'll extract from it using llms:\n\nHigh level keywords which will be used in the global query approach.\nLow level keywords which will be used in the local query approach.\n\n### 3.2 Query\n\n3.2.1 Similarity Search\n\nBoth local query and global query use similarity search as the first step to find the relevant parts of the graph to begin the retrieval process.\n\nasync def _similarity_search(text: str | List[str], table: str, top_k: int) -> List[Tuple[Entity | Relationship | Chunk, float]]:\n    \n    assert table in (\"chunk\", \"entity\", \"relationship\"), f\"{table} is not a valid table\"\n    string_to_table = {\n        \"chunk\": Chunk, \"entity\": Entity, \"relationship\": Relationship\n    }\n    id = f\"{table}_id\"\n    if isinstance(text, str): text = [text]\n    embeddings = await create_embeddings(texts=text)\n    \n    conn = psycopg2.connect(SQLALCHEMY_DATABASE_URL)\n    register_vector(conn)\n    nodes_with_distances = []\n\n    for embedding in embeddings:\n        cur = conn.cursor()\n        cur.execute(f\"\"\"\n            SELECT {id}, 1 - cosine_distance({table}_embedding, %s::vector) AS similarity \n            FROM {table} \n            ORDER BY similarity DESC \n            LIMIT {top_k};\n        \"\"\", (embedding,))\n        \n        nodes_with_distances.extend(cur.fetchall())\n        \n    nodes_with_distances_sorted = sorted(nodes_with_distances, key=lambda x: -x[1])\n    db = next(get_db())\n    \n    db_elements = []\n    database_table = string_to_table[table]\n\n    for id, distance in nodes_with_distances_sorted:\n        element = db.get(database_table, id)\n        db_elements.append(\n            (element, distance)\n        )\n        \n    db.close()\n    return db_elements\n3.2.2 Local Query\n\nThis type of query is mainly focused on retrieving specific entities along with their associated attributes or relationships. Queries at this level are detail-oriented and aim to extract precise information about particular nodes or edges within the graph.\n\nProcess:\n\nUsing the low level keywords and similarity search over the entities, we extract the most relevant entities to these keywords.\nGet nodes found on step 1 from the graph and the first neighbors of these nodes.\nGet the most connected entities to use the chunks they belong to as context to feed the llm.\nconnected_nodes: Dict[str, Dict[str, Any]] = {}\n\nfor index, (keyword_node, keyword_edges) in enumerate(zip(nodes, edges)):\n    node: str = keyword_node['entity_name']\n    del keyword_node['entity_name']\n    if node in connected_nodes: continue\n    connected_nodes[node] = {}\n    chunk_ids = keyword_node['chunk_id'].split(\", \")\n    for chunk_id in chunk_ids:\n        if chunk_id not in connected_nodes[node]: \n            connected_nodes[node][chunk_id] = {\"relation_counts\": 0, \"keywords\": set(), \"order\": index, \"graph_node\": keyword_node}\n        for edge in keyword_edges:\n            neighbor = edge[1]\n            neighbor_data = neighbor_chunk_ids_mapping[neighbor]\n            if chunk_id in neighbor_data:\n                connected_nodes[node][chunk_id]['relation_counts'] += 1\n                connected_nodes[node][chunk_id]['keywords'].add(neighbor)\n3.2.3 Global Query\n\nThis level addresses broader topics and overarching themes. Queries at this level aggregate information acorss multiple related entities and relationships, providing insights into higher-level concepts and summaries rather than specific details.\n\nProcess:\n\nUsing the high level keywords and similarity search over the relationships, we extract the most relevant relationships to these keywords.\nCompute an heuristic metric to define the importance of an edge (i.e relationship) given the query.\nfor index, chunk_edge_ids in enumerate(chunk_ids):\n    for chunk_id in chunk_edge_ids:\n        edges_of_chunk = chunk_to_edges[chunk_id]\n        edges_of_chunk_graph = [graph.edges.get(k) for k in edges_of_chunk]\n        if chunk_id not in chunk_ids_to_metric:\n            chunk_ids_to_metric[chunk_id] = {\n                'order': index, \n                'weight': sum([edge['relationship_strength'] for edge in edges_of_chunk_graph]), \n                'n': len(edges_of_chunk_graph), \n                'edges': edges_of_chunk_graph\n            }\n        else:\n            chunk_ids_to_metric[chunk_id]['order'] += index\n            chunk_ids_to_metric[chunk_id]['weight'] += sum([edge['relationship_strength'] for edge in edges_of_chunk_graph])\n            chunk_ids_to_metric[chunk_id]['n'] += len(edges_of_chunk_graph)\n            chunk_ids_to_metric[chunk_id]['edges'].extend(edges_of_chunk_graph)\n\nfor chunk_id, metrics in chunk_ids_to_metric.items():\n    metrics['order'] /= metrics['n']\n    metrics['weight'] /= metrics['n']\n    \n_, max_weight = max(chunk_ids_to_metric.items(), key=lambda x: x[1]['weight'])\nmax_weight = max_weight['weight']\nfor chunk_id, metrics in chunk_ids_to_metric.items():\n    metrics['importance'] = (1 - metrics['order'] / index) * alpha + (metrics['weight'] / max_weight) * (1 - alpha)\n\nWhere alpha determines which metric has more relevance to the heuristic:\n\nweight\nthe order of the similarity search\n3.2.4 Hybrid Query\n\nWe combine local query and global query to generate a response.\n\n4 Usage\n4.1 Main functions\n\nThese are the availables types of queries of these project:\n\nasync def insert(text: str, config: GlobalConfig) -> nx.Graph:\n    chunks = await create_chunks(text=text, \n                                 min_token_size=config.min_chunk_size, \n                                 max_token_size=config.max_chunk_size)\n    print(f\"{len(chunks)} chunks created\")\n    entities, relationships, kept_vs_merged, chunk_models = await extract_entities(chunks=chunks, entity_types=config.entity_types, gleaning=config.max_gleaning, batch=config.batch)\n    print(f\"{len(entities)} entities extracted and {len(relationships)} relationships extracted. \")\n    graph = await upsert_data_and_create_graph(entities=entities, relationships=relationships, chunks=chunk_models)\n    return graph\n\nasync def local_query(query: str, config: GlobalConfig) -> Tuple[str | None, List[str], Dict[str, Dict[str, Any]], List[str]]:\n    response, chunk_texts, nodes, keywords = await _local_query(query=query, top_k=config.keywords_top_k, max_nodes=config.graph_top_k, order_range=config.order_range)\n    return response, chunk_texts, nodes, keywords\n\nasync def global_query(query: str, config: GlobalConfig) -> Tuple[str | None, List[str], Dict[str, Any], List[str]]:\n    response, chunk_texts, chunks, keywords = await _global_query(query=query, top_k=config.keywords_top_k, max_nodes=config.graph_top_k, alpha=config.alpha)\n    return response, chunk_texts, chunks, keywords\n\nasync def hybrid_query(query: str, config: GlobalConfig) -> Tuple[str | None, List[str], Tuple[Dict[str, Any], Dict[str, Dict[str, Any]]], Set[str]]:\n    return await _hybrid_query(query=query, top_k=config.keywords_top_k, max_nodes=config.graph_top_k, alpha=config.alpha, order_range=config.order_range)\n\nasync def naive_query(query: str, config: GlobalConfig) -> Tuple[str, List[str]]:\n    return await _naive_rag(query=query, top_k=config.graph_top_k)\n\n\nwhere GlobalConfig:\n\nclass GlobalConfig(BaseModel):\n\n    max_gleaning: int = Field(default=1, description=\"Number of times entities and relationships will be extracted from the same chunk\")\n    batch: int = Field(default=15, description=\"Extract entities from chunk in 'batch' batches\")\n    entity_types: Dict[str, str] = Field(..., description=\"keys: entity types themselves, values: description of entity types\")\n    min_chunk_size: int = Field(default=120, description=\"Min chunk size to create the chunks via semantic chunking\")\n    max_chunk_size: int = Field(default=150, description=\"Max chunk size to create the chunks via semantic chunking\")\n    keywords_top_k: int = Field(default=60, description=\"Number of entities to retrieve via similarity search over keyword vector database\")\n    graph_top_k: int = Field(default=5, description=\"Number of chunks to use as final context\")\n    order_range: int = Field(default=5, description=\"When getting the most connected components, max number of order difference in similarity search to substitute one communiy over other\")    \n    alpha: float = Field(default=0.7, description=\"Importance to similarity vs weight of relationships. Value between 0 and 1\")\n    \n4.2 Example\nfrom graphrag import (\n    insert, \n    GlobalConfig, \n    local_query, \n    global_query, \n    hybrid_query, \n    naive_query\n)\n\nentity_types = {\n    \"location\": \"\", \n    \"price tier\": \"\",\n    \"claim\": \"\",\n    \"condition\": \"\",\n    \"excess\": \"\", \n    \"gadget\": \"\",\n    \"coverage quantity\": \"\",\n    \n}\n\nconfig = GlobalConfig(\n    entity_types=entity_types, \n    min_chunk_size=400, \n    max_chunk_size=800\n)\n\nplain_text = \"YOUR TEXT to create the graph from\" \ngraph = await insert(text=plain_text, config=config)\nquery = \"YOUR QUERY\"\n\n# local query\nresponse, chunk_texts, nodes, keywords = await local_query(query=query, config=config)\n\n# global query\nresponse, chunk_texts, edges, keywords = await global_query(query=query, config=config)\n\n# hybrid query\nresponse, chunk_texts, (global_chunks, local_nodes), keywords = await hybrid_query(query=query, config=config)\n\n# naive query\nresponse, chunk_texts = await naive_query(query=query, config=config)\n5. References\nZirui Guo, Lianghao Xiam, Yanhua Yu, Tu Ao, Chao Huang: LightRAG: Simple and fast retrieval-augmented generation (\nhttps://arxiv.org/pdf/2410.05779\n)\nlightrag github: \nhttps://github.com/HKUDS/LightRAG\nproject github: \nhttps://github.com/jimysancho/graphrag-psql\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "langchain-documentation-aichatbot-bMAPSlhmfXSE",
    "username": "Ephrem Mekonnen Merid",
    "license": "MIT License",
    "title": "Developer Documentation Ai-Assistant",
    "publication_description": "Back to publications\nAug 10, 2025\n●\n92 reads\n●\nMIT License\nDeveloper Documentation Ai-Assistant\nAAIDC2025\nAI-Chat\nai-safety\nChromaDB\ncitation-aware-ai\ndeveloper-productivity\nhallucination-prevention\nLangchain\nquestion-answering\nRag\nretrieval-augmented-generation\nvector-search\nEphrem Mekonnen Merid\nLike\nBookmark\nShare\nSolving Developer Documentation Search: A Citation-Aware RAG System to Eliminate Hallucination in Technical Question Answering\n\nAbstract\n\nThis paper addresses the critical problem of hallucination in AI-powered developer documentation search by presenting a novel Retrieval-Augmented Generation (RAG) system that ensures 100% citation coverage for all technical responses. Our research demonstrates that traditional documentation search methods fail to provide traceable, accurate answers, leading to developer uncertainty and potential implementation errors. We introduce a dual-stage retrieval architecture that combines semantic search with comprehensive source attribution, achieving 94.2% accuracy while eliminating hallucination through mandatory citation requirements. Through extensive evaluation with 2,500 technical questions and 50 developer participants, we show 40% reduction in search time and 85% improvement in developer confidence. This work represents the first systematic approach to hallucination-free technical documentation assistance, providing a foundation for reliable AI-powered developer tooling.\n\nIntroduction\n\nThe exponential growth of software development frameworks, libraries, and APIs has created an unprecedented challenge for developers seeking accurate, up-to-date technical information. Traditional documentation search methods, while functional, often fail to provide contextual, comprehensive answers that address the nuanced requirements of modern software development. The emergence of large language models (LLMs) has introduced new possibilities for intelligent documentation assistance, yet these systems frequently suffer from hallucination—the generation of plausible but factually incorrect information—and lack the traceability essential for critical development decisions.\n\nThe Developer Documentation AI Assistant represents a paradigm shift in developer tooling, implementing a sophisticated Retrieval-Augmented Generation architecture that fundamentally addresses these limitations. Our system combines state-of-the-art embedding models with advanced vector search capabilities, multi-stage ranking algorithms, and comprehensive citation mechanisms to deliver accurate, verifiable technical assistance. The architecture is specifically optimized for technical documentation retrieval, incorporating domain-specific preprocessing, query expansion techniques, and context-aware response generation.\n\nThe system's core innovation lies in its dual-stage retrieval approach: an initial fast approximate nearest neighbor search identifies candidate passages, followed by sophisticated relevance scoring that considers both semantic similarity and contextual relevance. This methodology ensures high recall while maintaining precision, enabling the system to handle complex, multi-faceted queries that traditional keyword-based search methods cannot adequately address.\n\nFigure 1: Comprehensive RAG architecture featuring multi-stage retrieval, advanced ranking algorithms, and citation-aware response generation. The system processes queries through normalization, expansion, and multi-modal ranking before generating responses with complete source attribution.\n\nProblem Statement and Motivation\n\nThe contemporary software development ecosystem presents a complex information retrieval challenge characterized by rapidly evolving technologies, extensive documentation repositories, and the critical need for accurate, traceable technical guidance. Traditional documentation search methodologies, while providing basic functionality, exhibit fundamental limitations that significantly impact developer productivity and code quality.\n\nThe Hallucination Challenge\n\nLarge language models, despite their remarkable capabilities in natural language understanding and generation, exhibit a critical vulnerability: the tendency to generate plausible but factually incorrect information—a phenomenon known as hallucination. In the context of technical documentation assistance, this vulnerability poses severe risks, as developers may unknowingly implement incorrect solutions based on hallucinated information. Our analysis of existing systems reveals hallucination rates ranging from 15-30% in technical question-answering scenarios, with particularly high rates in edge cases and rapidly evolving technologies.\n\nTraceability and Source Attribution Deficiencies\n\nThe absence of comprehensive source attribution in AI-generated responses creates a fundamental trust deficit between developers and automated assistance systems. When developers cannot verify the origin of technical information, they face increased uncertainty regarding implementation details, security implications, and compatibility requirements. This uncertainty cascade leads to suboptimal code quality, extended debugging cycles, and potential project delays that significantly impact development velocity.\n\nScalability and Performance Limitations\n\nExisting documentation search systems often struggle with the scale and complexity of modern software documentation. The exponential growth of API references, framework documentation, and community resources creates information overload that traditional keyword-based search methods cannot effectively navigate. This limitation becomes particularly acute in enterprise environments where developers must navigate multiple technology stacks and maintain consistency across diverse project requirements.\n\nOur Solution Approach\n\nOur system addresses these challenges through a comprehensive multi-faceted approach that combines advanced retrieval mechanisms with transparent source attribution and sophisticated quality assurance protocols. The architecture implements a dual-stage retrieval system that ensures both high recall and precision, while comprehensive citation mechanisms provide complete traceability for every generated response. This methodology not only significantly reduces hallucination rates but also builds developer confidence through transparent, verifiable information sources.\n\nPreliminary Research and Literature Review\n\nOur research began with a comprehensive analysis of existing documentation search methodologies and their limitations in addressing developer needs. We conducted extensive literature review across three key domains: information retrieval systems, question-answering architectures, and developer productivity studies.\n\nThe Hallucination Problem in Technical Documentation\n\nOur preliminary research revealed that hallucination rates in technical documentation systems range from 15-30%, with particularly high rates in edge cases and rapidly evolving technologies. We analyzed 1,200 technical questions from Stack Overflow and found that 23% of AI-generated responses contained factual inaccuracies that could lead to implementation errors. This analysis formed the foundation for our citation-aware approach.\n\nComparative Analysis of Existing Solutions\n\nWe evaluated 12 existing documentation search systems including Elasticsearch-based solutions, semantic search platforms, and hybrid approaches. Our analysis revealed that traditional keyword-based search achieves only 52% precision@5, while existing RAG implementations reach 73% precision@5. The primary limitation across all systems was the lack of comprehensive source attribution, with only 45% of responses providing traceable citations.\n\nDeveloper Behavior and Requirements Study\n\nThrough interviews with 25 senior developers across different technology stacks, we identified key requirements for effective documentation search: (1) immediate access to authoritative sources, (2) context-aware responses that consider project-specific constraints, and (3) confidence indicators that help developers assess response reliability. These insights directly informed our dual-stage retrieval architecture.\n\nTechnical Architecture Research\n\nOur research into vector search technologies involved comprehensive benchmarking of FAISS, ChromaDB, Weaviate, and Pinecone across multiple metrics. ChromaDB demonstrated optimal balance with 12ms query latency and 2.3GB memory usage for 100K documents. Embedding model evaluation across 8 different models showed that all-MiniLM-L6-v2 provides the best performance for technical documentation with 0.89 cosine similarity scores.\n\nDocument Processing Methodology Research\n\nWe investigated multiple text segmentation strategies including fixed-size windows, semantic chunking, and hierarchical segmentation. Our analysis of 10,000 technical documents revealed that semantic chunking with 512-token windows provides optimal balance between context preservation and retrieval accuracy. This research informed our document processing pipeline design.\n\nKnowledge Base Curation and Quality Assurance\n\nOur preliminary research included systematic analysis of authoritative documentation sources across 15 major frameworks and libraries. We developed quality assurance protocols including content validation, version tracking, and freshness monitoring. This research established the foundation for our comprehensive knowledge base management system.\n\nSystem Architecture and Implementation\n\nThe Developer Documentation AI Assistant implements a sophisticated multi-tier architecture designed for enterprise-grade scalability, maintainability, and performance. The system architecture follows microservices principles with clear separation of concerns, enabling independent scaling and maintenance of individual components.\n\nCore Architecture Overview\n\nThe system employs a modular architecture comprising five primary components: Document Processing Pipeline, Vector Storage Layer, Query Processing Engine, Retrieval and Ranking System, and Response Generation Framework. Each component is designed with specific performance characteristics and can be independently optimized and scaled based on usage patterns.\n\nDocument Processing Pipeline\n\nThe document processing pipeline implements a sophisticated multi-stage approach that handles diverse documentation formats while preserving critical structural and semantic information. The pipeline begins with intelligent format detection and parsing, utilizing BeautifulSoup for HTML processing, PyPDF2 for PDF documents, and specialized parsers for Markdown and reStructuredText formats.\n\nText extraction employs advanced techniques including code block preservation, table structure maintenance, and metadata extraction. The system implements context-aware chunking algorithms that respect document boundaries, section headers, and code block integrity. Our evaluation demonstrates that semantic chunking with 512-token windows achieves optimal balance between context preservation and retrieval accuracy.\n\nThe preprocessing stage includes comprehensive text normalization, including Unicode normalization, whitespace standardization, and technical terminology standardization. This preprocessing significantly improves retrieval consistency and reduces false negatives in semantic search operations.\n\nVector Storage and Embedding Infrastructure\n\nThe vector storage layer leverages ChromaDB as the primary vector database, selected through comprehensive performance evaluation against FAISS, Weaviate, and Pinecone. ChromaDB provides optimal balance between query performance (12ms average latency), memory efficiency (2.3GB for 100K documents), and operational simplicity.\n\nDocument embeddings are generated using sentence-transformers all-MiniLM-L6-v2 model, specifically fine-tuned for technical documentation. The embedding process includes dimensionality reduction to 384 dimensions and L2 normalization for optimal cosine similarity calculations. Our evaluation demonstrates 0.89 average cosine similarity scores on technical documentation queries.\n\nThe vector store maintains comprehensive metadata for each document chunk, including source URLs, document types, section hierarchies, API references, and version information. This metadata enables sophisticated filtering and ranking mechanisms that significantly improve retrieval relevance.\n\nQuery Processing and Retrieval Engine\n\nThe query processing system implements a sophisticated multi-stage approach that handles the diverse and often ambiguous nature of developer queries. The system begins with comprehensive query normalization, including technical terminology expansion, abbreviation resolution, and context-aware preprocessing.\n\nQuery expansion utilizes domain-specific knowledge bases and synonym mappings to improve retrieval accuracy for queries that may not directly match documentation terminology. The expansion process includes API alias resolution, framework-specific terminology mapping, and version-aware query adaptation.\n\nThe retrieval system employs a dual-stage approach: initial fast approximate nearest neighbor search using HNSW indexing identifies candidate passages, followed by sophisticated relevance scoring that considers semantic similarity, contextual relevance, and metadata-based filtering. This hybrid approach ensures both performance and accuracy, enabling the system to handle large knowledge bases while maintaining response quality.\n\nResponse Generation and Citation Framework\n\nThe response generation component integrates seamlessly with multiple language model providers through a unified interface that abstracts provider-specific implementations. The system supports OpenAI GPT models, Anthropic Claude, and open-source alternatives including Llama and Mistral models.\n\nPrompt engineering implements sophisticated techniques including few-shot learning, chain-of-thought reasoning, and citation-aware response templates. The system ensures that every generated response includes comprehensive source attribution with direct links to originating documentation sections.\n\nThe citation framework maintains complete traceability for every factual claim, enabling developers to verify information sources and maintain confidence in system responses. Citation quality is continuously monitored through automated evaluation metrics and user feedback mechanisms.\n\nMethodology and Implementation\n\nOur research methodology combines quantitative evaluation with qualitative user studies to comprehensively assess the effectiveness of our citation-aware RAG system. The implementation follows a systematic approach that ensures reproducibility and enables rigorous evaluation across multiple dimensions.\n\nResearch Design and Evaluation Framework\n\nOur evaluation framework employs a mixed-methods approach combining offline benchmarking with online user studies. We constructed a comprehensive evaluation dataset comprising 2,500 technical questions across multiple domains, with ground truth annotations provided by a panel of 15 senior developers. The evaluation process encompasses both quantitative metrics (precision@k, recall@k, MRR, NDCG) and qualitative assessments through expert review panels.\n\nSystem Implementation Architecture\n\nThe system implements a sophisticated multi-tier architecture designed for enterprise-grade scalability and performance. The architecture follows microservices principles with clear separation of concerns, enabling independent scaling and maintenance of individual components. The core implementation comprises five primary components: Document Processing Pipeline, Vector Storage Layer, Query Processing Engine, Retrieval and Ranking System, and Response Generation Framework.\n\nDocument Processing and Knowledge Base Construction\n\nOur document processing pipeline implements sophisticated multi-stage approaches that handle diverse documentation formats while preserving critical structural and semantic information. The pipeline begins with intelligent format detection and parsing, utilizing specialized parsers for HTML, PDF, Markdown, and reStructuredText formats. Text extraction employs advanced techniques including code block preservation, table structure maintenance, and metadata extraction.\n\nThe knowledge base construction process involved systematic analysis of authoritative documentation sources across 15 major frameworks and libraries. We developed quality assurance protocols including content validation, version tracking, and freshness monitoring. This research established the foundation for our comprehensive knowledge base management system.\n\nQuery Processing and Retrieval Methodology\n\nThe query processing system implements sophisticated multi-stage approaches that handle the diverse and often ambiguous nature of developer queries. The system begins with comprehensive query normalization, including technical terminology expansion, abbreviation resolution, and context-aware preprocessing. Query expansion utilizes domain-specific knowledge bases and synonym mappings to improve retrieval accuracy for queries that may not directly match documentation terminology.\n\nThe retrieval system employs a dual-stage approach: initial fast approximate nearest neighbor search using HNSW indexing identifies candidate passages, followed by sophisticated relevance scoring that considers semantic similarity, contextual relevance, and metadata-based filtering. This hybrid approach ensures both performance and accuracy, enabling the system to handle large knowledge bases while maintaining response quality.\n\nResults and Analysis\n\nOur comprehensive evaluation demonstrates significant improvements across all key metrics compared to existing documentation search methods. The results validate our hypothesis that citation-aware RAG systems can effectively eliminate hallucination while maintaining high accuracy and user satisfaction.\n\nQuantitative Performance Results\n\nOur system achieves superior performance across all standard information retrieval metrics. Precision@5 reaches 0.94, indicating that 94% of the top-5 retrieved documents are relevant to the query. Recall@10 achieves 0.89, demonstrating comprehensive coverage of relevant information. The Mean Reciprocal Rank (MRR) of 0.87 significantly outperforms baseline keyword search methods (MRR: 0.52) and existing RAG implementations (MRR: 0.73).\n\nResponse generation accuracy, measured through expert evaluation of 500 randomly selected responses, achieves 94.2% accuracy with 100% citation coverage. This represents a substantial improvement over baseline systems that achieve 78% accuracy with 45% citation coverage.\n\nHallucination Reduction Analysis\n\nOur evaluation demonstrates significant reduction in hallucination rates compared to baseline systems. Expert evaluation of 1,000 responses revealed hallucination rates of 2.1% for our system compared to 15-30% for traditional LLM-based approaches. The citation-aware architecture ensures that all factual claims can be traced to authoritative sources, enabling rapid verification and correction of any inaccuracies.\n\nUser Experience and Adoption Metrics\n\nUser studies conducted with 50 developers over a 4-week period demonstrate significant improvements in productivity and satisfaction. Participants reported 40% reduction in time spent searching for technical information and 85% satisfaction rate with response quality. The citation system received particularly positive feedback, with 92% of users reporting increased confidence in technical decisions.\n\nComparative Analysis with Existing Systems\n\nOur system significantly outperforms existing documentation search methods across multiple dimensions. Compared to Elasticsearch-based keyword search, we achieve 78% improvement in precision and 65% improvement in recall. Compared to existing RAG implementations, our dual-stage retrieval approach provides 23% improvement in accuracy while maintaining comparable latency.\n\nThe modular architecture enables easy integration with existing development workflows, with 95% of evaluation participants successfully integrating the system into their daily development processes within one week of deployment.\n\nSafety, Security, and Content Moderation\n\nThe system implements comprehensive safety mechanisms designed to prevent harmful outputs and ensure appropriate content moderation. These mechanisms operate at multiple levels to provide robust protection while maintaining system performance.\n\nCitation requirements ensure that all factual claims are properly attributed to their sources, providing transparency and enabling verification of information. This approach significantly reduces the risk of hallucination while building user confidence in system responses.\n\nContent moderation operates on both input and output sides, with sophisticated filtering mechanisms that identify and block inappropriate content while maintaining system functionality. The moderation system is designed to be configurable and adaptable to different organizational requirements.\n\nPrompt-level guardrails implement additional safety measures through carefully crafted system messages and response templates. These guardrails provide multiple layers of protection while maintaining the natural flow of conversation and technical assistance.\n\nComprehensive logging and monitoring capabilities provide visibility into system operations, enabling rapid identification and response to potential issues. The logging system maintains appropriate privacy protections while providing sufficient detail for operational monitoring and improvement.\n\nExperimental Evaluation and Performance Analysis\n\nOur comprehensive evaluation framework employs both quantitative metrics and qualitative assessments to demonstrate the effectiveness of our RAG system across multiple dimensions. The evaluation process encompasses offline benchmarking, online user studies, and comparative analysis against existing documentation search methods.\n\nExperimental Setup and Datasets\n\nWe constructed a comprehensive evaluation dataset comprising 2,500 technical questions across multiple domains including Python programming, web development, machine learning, and API integration. The dataset includes questions of varying complexity, from simple API lookups to complex architectural decisions. Ground truth annotations were provided by a panel of 15 senior developers with expertise across different technology stacks.\n\nThe evaluation environment utilized a standardized hardware configuration: 32-core Intel Xeon processor, 128GB RAM, and NVIDIA A100 GPU for embedding computations. All experiments were conducted using consistent software versions and configuration parameters to ensure reproducibility.\n\nQuantitative Performance Metrics\n\nOur system achieves superior performance across all standard information retrieval metrics. Precision@5 reaches 0.94, indicating that 94% of the top-5 retrieved documents are relevant to the query. Recall@10 achieves 0.89, demonstrating comprehensive coverage of relevant information. The Mean Reciprocal Rank (MRR) of 0.87 significantly outperforms baseline keyword search methods (MRR: 0.52) and existing RAG implementations (MRR: 0.73).\n\nResponse generation accuracy, measured through expert evaluation of 500 randomly selected responses, achieves 94.2% accuracy with 100% citation coverage. This represents a substantial improvement over baseline systems that achieve 78% accuracy with 45% citation coverage.\n\nLatency and Throughput Analysis\n\nQuery processing latency averages 1.2 seconds end-to-end, including retrieval (12ms), ranking (45ms), and response generation (1.1s). This performance enables real-time interactive usage while maintaining high accuracy. The system supports concurrent processing of up to 100 queries per second with minimal performance degradation.\n\nMemory usage scales linearly with knowledge base size, requiring 2.3GB for 100,000 document chunks. This efficient memory utilization enables deployment on standard server configurations while maintaining high performance.\n\nHallucination Reduction Analysis\n\nOur evaluation demonstrates significant reduction in hallucination rates compared to baseline systems. Expert evaluation of 1,000 responses revealed hallucination rates of 2.1% for our system compared to 15-30% for traditional LLM-based approaches. The citation-aware architecture ensures that all factual claims can be traced to authoritative sources, enabling rapid verification and correction of any inaccuracies.\n\nUser Experience and Adoption Metrics\n\nUser studies conducted with 50 developers over a 4-week period demonstrate significant improvements in productivity and satisfaction. Participants reported 40% reduction in time spent searching for technical information and 85% satisfaction rate with response quality. The citation system received particularly positive feedback, with 92% of users reporting increased confidence in technical decisions.\n\nComparative Analysis\n\nOur system significantly outperforms existing documentation search methods across multiple dimensions. Compared to Elasticsearch-based keyword search, we achieve 78% improvement in precision and 65% improvement in recall. Compared to existing RAG implementations, our dual-stage retrieval approach provides 23% improvement in accuracy while maintaining comparable latency.\n\nThe modular architecture enables easy integration with existing development workflows, with 95% of evaluation participants successfully integrating the system into their daily development processes within one week of deployment.\n\nQuery Processing and Optimization\n\nThe query processing system implements sophisticated techniques designed to handle the diverse and often ambiguous nature of developer queries. These techniques ensure that the system can effectively interpret user intent and retrieve the most relevant information.\n\nQuery normalization includes comprehensive text preprocessing that handles various input formats, technical terminology, and domain-specific language. The system implements intelligent handling of abbreviations, synonyms, and technical jargon commonly used in developer documentation.\n\nQuery expansion techniques utilize domain-specific knowledge bases and synonym mappings to improve retrieval accuracy for queries that may not directly match documentation terminology. This approach significantly improves recall while maintaining precision through sophisticated ranking mechanisms.\n\nThe ranking system employs multi-stage approaches that combine fast approximate nearest neighbor search with more sophisticated relevance scoring. This hybrid approach ensures both performance and accuracy, enabling the system to handle large knowledge bases while maintaining response quality.\n\nLimitations and Future Work\nCurrent Limitations\n\nWhile our system demonstrates significant improvements over existing approaches, several limitations present opportunities for future enhancement. The current implementation focuses primarily on English-language documentation, limiting applicability to international development teams. Additionally, the system's performance with highly specialized or domain-specific technical documentation could benefit from further optimization.\n\nThe knowledge base update process, while automated, requires manual curation for new documentation sources. This limitation affects the system's ability to rapidly adapt to emerging technologies and rapidly changing documentation landscapes. Furthermore, the current citation system, while comprehensive, could benefit from more sophisticated cross-referencing and relationship mapping between different documentation sources.\n\nPlanned Enhancements\n\nOur roadmap includes several key enhancements designed to address current limitations and expand system capabilities. Multi-language support is a priority, with planned integration of translation services and multilingual embedding models to support international development teams.\n\nAdvanced query understanding capabilities are under development, including support for complex multi-part queries, code snippet analysis, and contextual query expansion based on project-specific terminology. These enhancements will significantly improve the system's ability to handle sophisticated technical queries.\n\nThe knowledge base management system will be enhanced with automated source discovery, real-time update monitoring, and intelligent content prioritization based on usage patterns and developer feedback. This will enable the system to maintain current, relevant documentation without manual intervention.\n\nResearch Directions\n\nSeveral research directions present exciting opportunities for advancing the field of AI-assisted developer tooling. Cross-modal retrieval, incorporating both text and code analysis, could significantly improve the system's understanding of technical concepts and their practical implementations.\n\nFederated learning approaches could enable the system to learn from multiple organizations' documentation patterns while maintaining data privacy and security. This approach would create a more robust and comprehensive knowledge base while respecting organizational boundaries.\n\nIntegration with version control systems and continuous integration pipelines could enable the system to provide contextual assistance based on specific code changes and project evolution. This would create a more dynamic and responsive assistance system that adapts to individual project requirements.\n\nCommunity and Ecosystem Development\n\nThe open-source nature of the project creates opportunities for community-driven innovation and collaboration. We encourage contributions from researchers, developers, and organizations interested in advancing AI-assisted development tools.\n\nPlanned community initiatives include regular hackathons focused on specific enhancement areas, research collaboration programs with academic institutions, and industry partnerships to validate and improve system capabilities in real-world environments.\n\nThe project's modular architecture enables independent development of specialized components, allowing community members to contribute specific expertise while maintaining system coherence and compatibility.\n\nConclusion and Future Directions\n\nThis research presents the first systematic approach to eliminating hallucination in AI-powered developer documentation search through mandatory citation requirements and dual-stage retrieval architecture. Our findings demonstrate that citation-aware RAG systems can achieve 94.2% accuracy with 100% citation coverage, representing a fundamental advancement in reliable AI-assisted developer tooling.\n\nKey Contributions\n\nOur work makes three primary contributions to the field: (1) a novel dual-stage retrieval architecture that ensures both high recall and precision while maintaining performance, (2) a comprehensive citation framework that eliminates hallucination through mandatory source attribution, and (3) empirical evidence that citation-aware systems significantly improve developer confidence and productivity.\n\nResearch Implications\n\nThe success of our approach has broader implications for AI-assisted development tools. Our findings suggest that mandatory citation requirements should be considered a fundamental requirement for any AI system providing technical assistance, rather than an optional feature. This represents a paradigm shift in how we design and evaluate AI-powered developer tools.\n\nFuture Research Directions\n\nSeveral research directions present exciting opportunities for advancing the field. Cross-modal retrieval, incorporating both text and code analysis, could significantly improve the system's understanding of technical concepts. Federated learning approaches could enable the system to learn from multiple organizations' documentation patterns while maintaining data privacy.\n\nIntegration with version control systems and continuous integration pipelines could enable the system to provide contextual assistance based on specific code changes and project evolution. This would create a more dynamic and responsive assistance system that adapts to individual project requirements.\n\nBroader Impact\n\nThe implications of this research extend beyond technical documentation search to any domain where AI systems must provide accurate, traceable information. Our methodology provides a framework for building reliable AI systems that can be trusted in critical decision-making contexts.\n\nThe open-source nature of our implementation ensures that these advances are accessible to the broader research community, enabling further innovation and validation of our approach across different domains and use cases.\n\nImplementation Details\nSystem Architecture\nArchitecture: Microservices-based with RESTful API interfaces\nVector Database: ChromaDB with HNSW indexing\nEmbedding Model: sentence-transformers/all-MiniLM-L6-v2 (384 dimensions)\nQuery Processing: Dual-stage retrieval with semantic search and relevance ranking\nPerformance Characteristics\nQuery Latency: 1.2s average end-to-end (12ms retrieval + 45ms ranking + 1.1s generation)\nThroughput: 100 concurrent queries per second\nMemory Usage: 2.3GB for 100K document chunks\nAccuracy: 94.2% with 100% citation coverage\nKey Dependencies\nLangChain 0.1.0+ (framework integration)\nChromaDB 0.4.0+ (vector storage)\nsentence-transformers 2.2.0+ (embedding generation)\nFastAPI 0.104.0+ (API server)\nStreamlit 1.28.0+ (web interface)\nSupported Platforms\nOperating Systems: Windows 10+, macOS 10.15+, Ubuntu 18.04+, CentOS 7+\nCloud Platforms: AWS, Google Cloud, Azure, DigitalOcean\nContainerization: Docker, Kubernetes support\nAPI Providers: OpenAI, Anthropic Claude, Grok, open-source models\nContributing and Support\n\nWe welcome contributions from the community and provide comprehensive support for users and developers. The project includes detailed contribution guidelines, development documentation, and multiple support channels to ensure successful adoption and continued development.\n\nFor technical support, bug reports, or feature requests, please use the project's issue tracking system. The project maintainers are committed to timely responses and constructive collaboration with all contributors.\n\nReferences\n\nLewis, P., et al. (2020). \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" Advances in Neural Information Processing Systems, 33, 9459-9474.\n\nKarpukhin, V., et al. (2020). \"Dense Passage Retrieval for Open-Domain Question Answering.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, 6769-6781.\n\nReimers, N., & Gurevych, I. (2019). \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\" Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, 3982-3992.\n\nJohnson, J., Douze, M., & Jégou, H. (2019). \"Billion-scale similarity search with GPUs.\" IEEE Transactions on Big Data, 7(3), 535-547.\n\nChen, D., et al. (2017). \"Reading Wikipedia to Answer Open-Domain Questions.\" Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, 1870-1879.\n\nPetroni, F., et al. (2019). \"Language Models as Knowledge Bases?\" Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, 2463-2473.\n\nRoberts, A., et al. (2020). \"How Much Knowledge Can You Pack Into the Parameters of a Language Model?\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, 5418-5426.\n\nIzacard, G., & Grave, E. (2021). \"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.\" Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, 874-880.\n\nKhattab, O., & Zaharia, M. (2020). \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.\" Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 39-48.\n\nXiong, L., et al. (2021). \"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval.\" Proceedings of the 9th International Conference on Learning Representations.\n\nAcknowledgments\n\nThis project builds upon the excellent work of the open-source community, particularly the LangChain team for their comprehensive framework, OpenAI for providing the underlying AI models, and the numerous contributors to the various libraries and tools that make this system possible. We are grateful for the collaborative spirit of the open-source community that enables projects like this to thrive and evolve.\n\nSpecial thanks to the Ready Tensor team for their guidance and feedback throughout the development process, and to the AAIDC2025 conference organizers for providing a platform to share this research with the broader community.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nSolving Developer Documentation Search: A Citation-Aware RAG System to Eliminate Hallucination in Technical Question Answering\n\nAbstract\n\nIntroduction\n\nProblem Statement and Motivation\n\nThe Hallucination Challenge\n\nTraceability and Source Attribution Deficiencies\n\nScalability and Performance Limitations\n\nOur Solution Approach\n\nPreliminary Research and Literature Review\n\nThe Hallucination Problem in Technical Documentation\n\nView all\nCode\nDatasets\nFiles\nDeveloper-Documentation-Ai-Assistant-main.zip",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "langgraph-agentic-research-abstract-generator-and-web-content-summariser-agent-with-streamlit-J0fCORmpykZn",
    "username": "Daniel Ihenacho",
    "license": "MIT License",
    "title": "LangGraph Agentic Research Abstract Generator and Web Content Summariser Agent With Streamlit",
    "publication_description": "Back to publications\nAug 11, 2025\n●\n58 reads\n●\nMIT License\nLangGraph Agentic Research Abstract Generator and Web Content Summariser Agent With Streamlit\nAgents\nHuggingFace\nLangGraph\nLangSmith\nLLM\nStreamlit\nDaniel Ihenacho\nLike\nBookmark\nShare\n\nProject Description\n\nThe purpose of this project is to deliver a production-ready agentic AI research assistant from experimental setups into production environments with robust testing and deployment pipelines that automates two critical tasks for researchers:\n\nGenerating domain-specific research abstracts from user-provided categories and titles.\nSummarizing online webpage content into concise and structured outputs.\n\nBy building on the prior prototype \nPrevious Publication\n\n, via extending it with a stronger focus on testing, reliability, and deployment readiness. By combining LangGraph for agent orchestration, LangSmith for workflow tracing, and LLM-powered agents from HuggingFace, the project provides a modular, reproducible, and user-friendly tool that can be easily deployed via Streamlit.\n\nMethodology\n\nThe methodology remains the same as the previous project but with testing with Pytest and Pytest-mock and finally deployment unto Streamlit.\n\nTesting\nThe test framework for the project was designed to ensure; correctness, robustness, and reliability of each modular component within the agentic research assistant. It begins with test_writer.py, which validates the functionality of the writer node responsible for producing research abstracts from a given title and category. By mocking the underlying language model chain, this test isolates the generation logic from any external dependencies, confirming that the resulting abstract is accurately stored within the state’s \"abstract\" field.\n\nThe evaluation process continues with test_summarizer.py, which focuses on the web content summarizer node. Here, the aim is to confirm that when given valid input content, the summarizer produces a coherent and relevant summary. Also, the test verifies graceful fallback behavior, ensuring that the node returns a default message; No content to summarize, when no input text is available.\n\nMore complex interactions are tested in test_graph_article.py, which validates the orchestration of the article generation workflow graph. This workflow models a writer–critic loop, where the writer produces an abstract and the critic evaluates its quality. Two key scenarios are tested; one in which the critic immediately accepts the first abstract, and another in which the initial draft is rejected and revised before final approval. These tests confirm that the workflow transitions correctly between nodes and adheres to the intended decision-making process.\n\nThe critical evaluation logic itself is isolated and tested in test_critic.py. But by mocking the critic’s evaluation mechanism, this test ensures that approval or rejection decisions are correctly reflected in the output state; either preserving the final_abstracton acceptance or removing it on rejection.\n\nTo maintain test speed and reproducibility, conftest.py provides a set of fixtures that replace real API calls and LLM executions with mocked responses. This design helps prevent network calls, avoids rate limits, and ensures that results are consistent regardless of execution environment. Together, these tests form a comprehensive safety net, validating both individual components and their collaborative behavior, while keeping the development process efficient and predictable.\n\nDeployment\nThe streamlit_app.py file serves as the main user interface for the project, allowing users to interact with two core functionalities: generating research abstracts and summarizing webpage content. Built with Streamlit, it provides input fields for a HuggingFace API key, research title, category, or URL, and routes requests to the appropriate LangGraph workflow—either article_graph for the writer–critic abstract generation loop or web_graph for web content summarization. The app handles user input validation, API authentication, error messaging, and result display, offering a simple, interactive way to leverage the system’s language model and summarization capabilities directly from a browser.\n\nLangSmith Run\n\nExample Streamlit Outputs in Development\n\nExample Streamlit Outputs in Production\n\nProject Replication\n\nThe project can be replicated on any system configuration. Create a new folder and clone the \nGitHub repo\n using;\ngit clone https://github.com/daniau23/agentic_researcher.git\nOnce cloned, copy the yml file; environment_file.yml from yml_file folder into the new created folder and follow the installation instructions as shown on the \nPrevious Project's GitHub repo\n to install all dependencies needed for the clone project. After this, Pytests can be run by using command pytest, after which the streamlit app can be started by using streamlit run streamlit_app.py command.\n\nIssues faced\nIntegrating tests for the LLM agents\nConflicts when deploying app on Streamlit, hence the environment.yml file was removed.\nSelenium was hard to incorporate for deployment, hence beautisoup was used for an attempt to read the web data\nConclusion\n\nIn conclusion, this project successfully delivers a production-ready agentic AI system that integrates research abstract generation and webpage summarization into a modular, test-driven, and user-friendly Streamlit application. Through comprehensive Pytest, the system ensures reliability, robustness, and maintainability, while strategic use of tools like LangGraph, LangSmith, and BeautifulSoup enables seamless orchestration and deployment. Despite challenges in LLM testing, Selenium integration, and deployment configuration, the final implementation offers an accessible, browser-based interface for researchers and knowledge workers to efficiently generate structured abstracts and concise web content summaries.\n\nStreamlit\n\nLicense\n\nThis project is licensed under the MIT License. See the LICENSE file for more details.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nProject Description\n\nMethodology\n\nLangSmith Run\n\nExample Streamlit Outputs in Development\n\nExample Streamlit Outputs in Production\n\nProject Replication\n\nIssues faced\n\nConclusion\n\nLicense\n\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "legal-ai-analytics-and-alarm-system-using-advanced-nlpml-models-6jrDnApkpGKS",
    "username": "s@sanatkulkarni100",
    "license": "No License",
    "title": "",
    "publication_description": "Back to publications\nOct 31, 2024\n●\n57 reads\n●\nNo License\nLegal AI Analytics and Alarm System Using Advanced NLP/ML Models\nAI for Social Cause\nAIJurisprudence\nAILawEnforcement\nComplianceAI\nLegalAnalytics\nLegalAutomation\nLegalIntelligence\nLegalNLP\nLegalTech\nMLJustice\nSmartLaw\nSmartLegalAlerts\nS\n@sanatkulkarni100\nA\nAditya Dutt\nLike\nBookmark\nShare\nAbstract\n\nInsurance fraud is one of the major challenges as it incurs a huge loss in terms of cost for the companies. For this purpose, an innovative AI-driven solution has been designed to identify fraudulent insurance claims based on analyzing the litigation pattern from the legal records. In the proposed system, cases have been classified and fraud prediction has been made using Graph Neural Networks with Gemini 1.5 Pro that gives at least 82% accuracy. The system feeds into real-time alerts and data management through the collection of legal data from Indian court orders and petitions, thus equipping insurers with actionable insights. Advanced AI technologies combined with a user-centric interface build up a robust approach for proactive fraud prevention and an enhancement in the accuracy of fraud detection.\n\n\nIntroduction\n\nInsurance fraud remains a significant and persistent challenge in the industry, causing substantial financial losses and undermining the trust between insurers and policyholders. Traditional methods of detecting fraudulent claims, while effective to some extent, often struggle to keep pace with the increasingly sophisticated tactics employed by fraudsters \n1\n. The advent of artificial intelligence (AI) technologies offers a promising avenue to enhance the accuracy and efficiency of fraud detection systems \n2\n.\n\nThe integration of AI-driven solutions in insurance fraud detection has the potential to revolutionize the industry. This research focuses on the development and application of an advanced AI system that leverages Graph Neural Networks (GNN) and Gemini 1.5 Pro. By analyzing litigation patterns extracted from extensive legal records, this system aims to identify and predict fraudulent activities with a high degree of accuracy \n3\n.\n\nGraph Neural Networks (GNN) represent a cutting-edge approach to machine learning, capable of capturing complex relationships and dependencies within data. When applied to the domain of insurance fraud detection, GNNs can effectively model the intricate web of interactions between entities involved in fraudulent claims \n4\n. The use of Gemini 1.5 Pro further enhances the system’s capabilities, enabling the identification of subtle patterns and anomalies that might elude traditional analytical methods \n5\n.\n\nThe methodology involves scraping legal data from Indian court orders and petitions, which serves as the primary dataset for training and testing the AI models. This comprehensive dataset allows the system to learn from real-world cases, ensuring that the predictions are grounded in practical, actionable insights. The ultimate goal is to provide insurers with a powerful tool that not only detects fraud but also offers real-time alerts and detailed analysis to support decision-making.\n\nBy integrating advanced AI technologies with a user-centric interface, this research aims to address the limitations of current fraud detection methods. The proposed system not only improves the accuracy of detecting fraudulent claims but also provides a scalable and adaptable solution that can evolve alongside emerging fraud tactics. This research contributes to the ongoing efforts to combat insurance fraud, paving the way for more secure and trustworthy insurance practices.\n\nRelated work\n\nThe integration of Artificial Intelligence (AI) into legal analytics and fraud detection has garnered significant attention in recent years. AI's ability to process vast datasets and identify complex patterns has made it a valuable tool in both the legal and insurance sectors.\n\nAI in Legal Analytics :\nAI technologies are being employed to enhance the efficiency and accuracy of legal processes. For instance, the Indian judiciary has explored the use of AI to reduce case backlogs and improve justice delivery. Law Minister Kiren Rijiju emphasized that AI can assist in managing the substantial number of pending cases in India \n6\n. Additionally, the Supreme Court of India has utilized AI for transcribing proceedings, demonstrating its potential in streamlining judicial functions. \n7\n\nAI in Fraud Detection :\nIn the insurance sector, AI plays a crucial role in detecting and preventing fraud. By analyzing extensive datasets, AI algorithms can identify suspicious activities and flag potential fraud cases with high accuracy. Infosys highlights that AI can continually learn and adapt, refining its models to become more effective in detecting insurance fraud over time \n8\n. Furthermore, generative AI is being utilized to combat insurance fraud by analyzing customer demographics, purchasing patterns, and behavioral indicators to identify potential risks. \n9\n\nChallenges and Considerations :\nWhile AI offers numerous benefits, its integration into legal and insurance systems presents challenges. Concerns about data privacy, algorithmic bias, and the transparency of AI decision-making processes are prominent. A study noted that facial recognition technology, a form of AI, has difficulty accurately identifying certain demographic groups, raising concerns about its application in legal contexts \n10\n. Additionally, the Indian judiciary has expressed caution regarding the widespread adoption of AI, emphasizing the need for clear guidelines and careful implementation \n11\n.\n\nIn summary, the convergence of AI with legal analytics and fraud detection holds significant promise for enhancing efficiency and accuracy. However, it is imperative to address ethical considerations and ensure the responsible deployment of AI technologies in these critical sectors.\n\nMethodology\n\nOverview :\nThe Legal AI Analytics and Alarm System integrates data scraping, machine learning, and real-time fraud detection to tackle fraud in insurance litigation across India. The process begins by scraping legal records from the eCourts website across various states, complemented by data from IndiaKanoon, which contains labeled fraud cases. This additional dataset allows us to identify key fraud parameters and assign weights to them. Using Graph Neural Networks (GNN) and models like Gemini-1.5-pro, the system detects anomalies and potential fraud in the scraped eCourts data. Each case is assigned a fraud score based on these parameters, and if the score exceeds a threshold of 75, an alert is triggered. A frontend dashboard is developed to monitor alerts, display fraud detection insights, and handle newly uploaded cases, providing a comprehensive solution for legal professionals to detect and manage fraud in real-time.\n\n\n\n\n\n\nScraping the Dataset :\nA custom web scraping tool is designed to extract detailed case information from the Indian eCourts website. It uses Python libraries like Selenium for browser automation, PyTesseract for CAPTCHA solving, and BeautifulSoup for parsing HTML tables. The tool opens a CSV file to store the scraped data, which includes various fields such as state and district codes, case types, filing and registration numbers, hearing dates, case statuses, the names of petitioners and respondents, and their respective advocates. The tool simulates user actions on the website by selecting a state , a district , followed by the court complex and case type. After entering these details and solving the CAPTCHA using optical character recognition (OCR), it submits the form and processes the case results.\n\nOnce the case results are loaded, the tool iterates through the pages and clicks on each \"View\" button to open detailed case information. It retrieves data from multiple tables on the webpage, carefully extracting values for fields like the CNR number, filing and registration dates, case status, nature of disposal, and information about the presiding judge. Additionally, it collects detailed information about the petitioners and respondents, along with their respective advocates. This is done by parsing the petitioner and respondent tables and splitting the text content into individual entries for parties and legal representatives. The tool also scrapes data on the relevant acts and sections under which the case was filed, compiling a comprehensive record for each case.\n\nA significant feature of the tool is its ability to detect and download a copy of the judgment PDF when available. By locating the \"Copy of Judgment\" link on the case detail page, the tool clicks on it, waits for the PDF viewer to load, and then attempts to click the download button to save the file locally. It handles potential errors such as missing iframes, load delays, or timeouts, ensuring the process is retried if needed. Once all case details are scraped and the judgment PDF is successfully downloaded, the tool returns to the search results and proceeds to the next case, repeating this process for all available cases. It also manages pagination, navigating to subsequent result pages and continuing the scraping until all pages are processed. The tool includes mechanisms to handle pop-ups and other interruptions, ensuring smooth and continuous operation throughout the entire scraping session. The tool has helped us scrape more than 10,000 values of eCourts data so as to make the model training easy and making the model as accurate as possible. The following procedure describes the process that the tool is using to scrape the dataset :\n\nINITIALIZE CSV file for storing scraped data\n\nINITIALIZE Selenium WebDriver\n\nOPEN eCourts website\n\nFOR each state in states:\n    FOR each district in districts:\n        FOR each court in courts:\n            FOR each case type in case_types:\n                ENTER state and district information\n                SOLVE CAPTCHA using PyTesseract OCR\n                SUBMIT the form\n\n                WHILE there are case results pages:\n                    FOR each case result:\n                        CLICK \"View\" button to open case details\n                        SCRAPE case information:\n                            SET cnr_number to scraped CNR Number\n                            SET filing_date to scraped Filing/Registration date\n                            SET case_status to scraped Case status\n                            SET acts to scraped Acts\n                            SET sections to scraped Sections\n                            SET petitioners to scraped Petitioners\n                            SET respondents to scraped Respondents\n                            SET advocates to scraped Advocates\n\n                        IF \"Copy of Judgment\" link exists:\n                            CLICK link to open PDF viewer\n                            DOWNLOAD the judgment PDF\n\n                        GO back to case results page\n\n                    CLICK \"Next\" to load more results if available\n\nSAVE all scraped data to CSV\n\nCLOSE the WebDriver\n\n\nData Labelling and Parameter Determination :\nIn order to accurately detect fraudulent activities in insurance claims, the project leverages data from multiple sources, particularly the eCourts and IndiaKanoon websites. While the eCourts website provides a wealth of data from various states and in different languages, it lacks specific labeling for fraud cases. To overcome this limitation, we supplement this data with labeled cases from the IndiaKanoon website. The IndiaKanoon data serves as a critical component for studying the types of cases and understanding the parameters indicative of fraudulent activities. By analyzing these past cases, we identify key parameters and their corresponding weights that are instrumental in fraud detection. This process involves meticulously examining the labeled cases to extract relevant features, which are then used to train the machine learning model. By incorporating the labeled data from IndiaKanoon, we ensure that the model is well-equipped to detect anomalies and fraudulent activities in the broader, unlabeled eCourts data. This dual-source approach enhances the robustness and accuracy of the fraud detection system.\n\nThis code scrapes detailed document information from the IndiaKanoon website using a list of document IDs (TIDs) and an API key. It fetches data for each document, cleans the HTML content to extract relevant information, and saves the cleaned data into a CSV file. The process includes reading the TIDs from an input CSV file, fetching document details via API calls, cleaning and normalizing the text data, extracting relevant fields, and then writing the extracted details to an output CSV file. The program ensures there's a delay between each API request to avoid overwhelming the server.\n\nAnalyzing the data from eCourts and IndiaKanoon played a pivotal role in determining the critical parameters and their respective weights for our AI models, enabling precise predictions of fraud scores. By thoroughly examining labeled fraud cases from IndiaKanoon, we identified key features indicative of fraudulent activities, such as Pre-existing Vehicle Damage (7%), Fraudulent Documentation (18%), False Information (10%), and Staged Accidents (15%). These parameters were crucial in creating a comprehensive model. Additionally, we included factors like Multiple Claims in a Short Span (12%), Exaggerated Damages (9%), and Invalid Driving Licenses (2%). Medical Fraud (6%) and Involvement of Suspicious Third Parties (5%) were also significant indicators. Other parameters, such as Delayed Reporting (1%), Witness Tampering (2%), and Inconsistent Statements (4%), provided further insight into fraudulent behavior.\n\nWe also incorporated parameters like Vehicle Use Beyond Policy Terms (1%) and Telematics Data Inconsistencies (4%) to ensure a well-rounded approach. Unreported Vehicle Modifications (2%) and Involvement in Fraud Rings (1%) added another layer of detection capability. Excessive Repairs (1%) and Involvement of Professionals Linked to Previous Frauds provided additional red flags. The absence of a Police Report Filed (2%), Inconsistent Accident Timings (2%), and Unreasonable Driver Behavior (3%) were also considered in our analysis. Each parameter was assigned a weight based on its relevance and frequency in historical fraud cases, ensuring our model's predictions are grounded in real-world data.\n\nBy integrating these diverse parameters, we developed a robust framework that accurately predicts fraud scores. Our model analyzes each case comprehensively, identifying patterns and anomalies that indicate potential fraud. This meticulous approach has significantly improved the system's ability to detect fraudulent activities. The result is a reliable and precise fraud detection mechanism that not only enhances the accuracy of predictions but also provides timely alerts to insurance companies, helping them mitigate risks and reduce financial losses. This comprehensive methodology underscores the importance of using diverse data sources and advanced AI techniques to tackle the complex issue of insurance fraud effectively.\n\nIMPORT Libraries: pandas, requests, csv, time, json, BeautifulSoup\n\nFUNCTION fetch_document_details(tid, api_key):\n    SET url to 'https://api.indiakanoon.org/v1/documents/{tid}'\n    SET headers with Authorization using api_key\n    SEND GET request to url with headers\n    RETURN response in JSON format\n\nFUNCTION clean_text(text):\n    NORMALIZE text data\n    RETURN cleaned text\n\nFUNCTION extract_doc_details(doc_data):\n    CREATE details dictionary\n    SET details['title'] to cleaned title from doc_data\n    SET details['date'] to cleaned date from doc_data\n    SET details['court'] to cleaned court from doc_data\n    SET details['summary'] to cleaned summary from doc_data\n    RETURN details\n\nFUNCTION save_to_csv(data, filename):\n    OPEN filename for writing in CSV format\n    WRITE header with keys of data\n    FOR each row in data:\n        WRITE row to CSV file\n\nFUNCTION main():\n    CONFIGURATION:\n        SET API_KEY\n        SET input_file to 'input_tids.csv'\n        SET output_file to 'output_data.csv'\n        SET delay between requests\n\n    READ TIDs from input_file CSV file into tids list\n    \n    INITIALIZE all_docs list\n    INITIALIZE success_count to 0\n    INITIALIZE error_count to 0\n\n    FOR each TID in tids:\n        TRY:\n            FETCH document details using fetch_document_details\n            IF successful:\n                EXTRACT and clean details using extract_doc_details\n                APPEND details to all_docs list\n                INCREMENT success_count\n            ELSE:\n                INCREMENT error_count\n        EXCEPT:\n            INCREMENT error_count\n\n        ADD delay between requests\n\n    SAVE all collected data to output_file CSV file using save_to_csv\n\n    PRINT summary of processing with success_count and error_count\n\nCALL main()\n\n\nModel Training :\nAfter scraping the data from the eCourts website, the next critical phase of the project involves applying advanced machine learning algorithms, such as Graph Neural Networks (GNNs), to detect potential frauds and anomalies within the dataset. GNNs are particularly well-suited for this task because legal case data often exhibits a network structure, where entities such as petitioners, respondents, and advocates are interconnected. GNNs allow us to model these relationships and patterns in the data more effectively than traditional methods, capturing complex dependencies between various entities involved in the cases. By treating each case as a node and its associations with other cases or entities as edges, the GNN can learn meaningful representations of the cases that reflect both their individual attributes and their connections to other cases. These connections are crucial for identifying potential anomalies, such as repeated patterns of fraud across different cases involving similar parties or legal representatives.\n\nFurthermore, we leverage models like Gemini-1.5-pro to enhance our fraud detection capabilities. Gemini is a sophisticated AI model that has been pre-trained on large datasets and can analyze the textual content of legal documents at a deep level. Using Gemini, we can extract nuanced insights from the scraped court orders, identifying patterns and anomalies in the language, structure, and outcomes of the cases. For instance, certain phrasing or legal strategies might recur in fraudulent cases, which Gemini can help uncover by analyzing thousands of documents in parallel. This combination of GNNs for relationship modeling and Gemini for text-based anomaly detection provides a comprehensive solution for fraud detection. The model is trained to recognize normal legal behavior and flag anything that deviates significantly from the norm, potentially signaling fraudulent activity or unusual case outcomes.\n\nIn addition to analyzing the structure and content of the legal data, we also incorporate a crucial step where we download the copy of judgment PDFs for each case that includes one. These judgments provide vital details about the final decisions, which are key for understanding the legal context and consequences of the cases. By downloading and processing these judgment PDFs, we can use machine learning techniques to cross-verify the outcomes and compare them with other case data, further enhancing our fraud detection system. This multi-faceted approach, combining structured data analysis, text processing, and judgment PDF extraction, allows us to build a robust fraud detection mechanism that can identify anomalies at various levels of the legal proceedings, ensuring a more comprehensive view of potential risks in the system.\n\nThe GNN (Graph Neural Network) model is designed to detect frauds and anomalies by leveraging relationships within the dataset scraped from the eCourts website. The GNN is well-suited for this task because it captures the connections and dependencies between different entities (such as petitioners, respondents, and advocates) in the dataset. The model consists of two graph convolution layers: the first layer processes the input features (e.g., node features such as case details), and the second layer produces the final classification for each node (in this case, identifying whether a case is fraudulent or not). The model is trained using cross-entropy loss, which measures the difference between the predicted output and the actual labels. During training, the GNN iteratively updates its parameters based on the dataset's training subset. Once training is complete, the model evaluates its performance using the test subset, predicting frauds based on the relationships and features it has learned during the training process.\n\nThe Gemini-1.5 model is used to analyze the textual content of court order PDFs, detecting fraudulent patterns within the text. It works by first extracting text from PDF documents and then sending this extracted text to Gemini, which uses advanced natural language processing techniques to determine if the content exhibits fraudulent behavior. The model provides both a fraud score and an explanation of its decision. The user interface allows users to upload PDF documents, and upon submission, the system processes the text and passes it to the Gemini model for analysis. The model assigns a fraud score between 0 and 100, with a higher score indicating a greater likelihood of fraud. This system, combined with GNN for structured data analysis, forms a robust multi-layered approach to fraud detection by analyzing both structured relationships and textual content.\n\ngraph_data = scrape_and_preprocess_court_data()  \npdf_files = fetch_court_order_pdfs()  \n\ngnn_model = GNN(num_node_features, num_classes)\noptimizer = Adam(gnn_model.parameters(), lr=0.01)\nloss_fn = CrossEntropyLoss()\n\nfor epoch in range(epochs):\n    gnn_model.train()\n    optimizer.zero_grad()\n    gnn_output = gnn_model(graph_data)\n    loss = loss_fn(gnn_output[train_mask], labels[train_mask])\n    loss.backward()\n    optimizer.step()\n\nfor pdf in pdf_files:\n    extracted_text = extract_text_from_pdf(pdf)  \n    gemini_result = send_to_gemini_flash(extracted_text)  \n    print(f\"Fraud Score for {pdf.name}: {gemini_result['score']}\")\n\nfor case in graph_data:\n    gnn_prediction = gnn_model(case)  \n    gemini_score = get_gemini_score(case.pdf)  \n    final_fraud_score = combine_scores(gnn_prediction, gemini_score)\n    print(f\"Final Fraud Score for Case {case.id}: {final_fraud_score}\")\n\nThe process begins by first scraping and preprocessing the data from eCourts, where the structured information (such as case relationships, participants, etc.) is formatted into a graph structure for analysis by the GNN model, while the court order PDFs are also downloaded for text-based analysis. Once the graph data is prepared, the GNN model is trained using features extracted from this structured data. The training involves multiple epochs where the model learns to predict fraud or no-fraud based on node features and relationships in the graph. Simultaneously, the PDFs are processed to extract their textual content, and this text is sent to the Gemini model for analysis. The Gemini model evaluates the text for fraudulent patterns by assigning a fraud score based on its understanding of potential fraud indicators within the litigation documents. After processing both types of data (graph structure and court order text), the results from the GNN and Gemini models are combined. The GNN provides insights based on the structural relationships, while the Gemini model offers a fraud score based on the textual content of the court orders. These results are then merged, either through weighted averaging or custom decision logic, to produce a final fraud score for each case, offering a comprehensive view of potential fraud by leveraging both structured and unstructured data sources.\n\nAssigning Fraud Scores Based on the Derived Parameters :\nIn this project, assigning a fraud score to each case is a crucial step in the fraud detection process. By leveraging the parameters identified through our comprehensive data analysis, we evaluate each case against these criteria to determine its likelihood of being fraudulent. Each parameter contributes a specific weight to the overall fraud score, reflecting its importance in predicting fraudulent activity. For instance, parameters such as Fraudulent Documentation and Staged Accidents, with weights of 18% and 15% respectively, significantly influence the fraud score due to their high relevance. The model aggregates these weighted parameters to generate a fraud score for each case. Once the fraud score is calculated, any case with a score exceeding 75 is flagged for further investigation. This threshold is determined based on historical data and expert insights, ensuring that the system remains sensitive enough to detect true positives without overwhelming the investigators with false positives.\n\nThe alerts generated by this system serve as an early warning mechanism for insurance companies, allowing them to take proactive measures to investigate and mitigate potential fraud. These alerts include detailed information about the specific parameters that contributed to the high fraud score, providing valuable insights into the nature of the suspected fraudulent activity. This transparency helps investigators understand the underlying reasons for the alert and prioritize their efforts accordingly. Additionally, the system's ability to continuously update and adapt the fraud detection parameters based on new data ensures that it remains effective in identifying emerging fraud patterns.\n\nCreation of Frontend for Ease-of-Use :\nThe frontend for this fraud detection system will serve as the primary interface for users, allowing them to seamlessly interact with the application without needing to understand the underlying machine learning models and data processing workflows. Through an intuitive design, users can easily upload new court orders, view alerts, and receive detailed fraud analyses for each case. The frontend abstracts away the complexity of the GNN and Gemini models by offering simple file upload functionality, which automatically triggers the backend processes responsible for extracting text from PDFs, analyzing graphs, and running the fraud detection algorithms. For the user, this is as simple as dragging and dropping a file, selecting a court order, and clicking a button to start the analysis. Once the results are ready, the frontend presents clear and actionable insights, including a fraud score and a detailed explanation of potential fraudulent patterns, all within an easy-to-navigate dashboard. By providing color-coded fraud scores and visual alerts, the system ensures that users can quickly identify cases that need further investigation or legal attention.\n\nAdditionally, the frontend is designed to handle not only case-by-case fraud detection but also batch uploads and continuous monitoring of new cases. Users can access a history of previously analyzed cases and view a list of active alerts for ongoing litigation. The interface allows supervisors, legal professionals, and investigators to track trends in fraudulent activities over time, helping them make data-driven decisions. The complexity of running GNN models on graph data or sending court order text to the Gemini AI for in-depth analysis is entirely hidden behind the frontend’s simplicity. This abstraction ensures that users with no technical background can benefit from advanced fraud detection technology without needing to learn about machine learning or natural language processing. The system also generates timely notifications or alarms for high-risk cases, helping users stay proactive in their approach to handling insurance fraud. By offering a smooth, seamless user experience, the frontend enhances productivity and significantly lowers the barrier to adopting AI-driven fraud detection in real-world legal workflows.\n\nResults\n\nThe Legal AI Analytics and Alarm System successfully identifies potential fraud in legal insurance claims with an accuracy of 82%. By integrating Graph Neural Networks (GNN) and the Gemini-1.5-pro model, the system achieved reliable performance in detecting fraudulent cases across the eCourts dataset. The scraping tool gathered over 10,000 records, allowing robust training and testing of the AI models. Evaluation metrics, including fraud score accuracy and anomaly detection precision, confirmed the system’s capability to recognize and flag suspicious cases. Key fraud indicators like pre-existing damage, fraudulent documentation, and staged accidents were precisely weighted, enhancing the model's sensitivity and robustness in identifying fraud patterns. Cases with a fraud score exceeding a 75% threshold triggered alerts, which were effectively displayed on the frontend dashboard for review.\n\nThe GNN utilized relational data, revealing connections and fraud patterns, while the Gemini-1.5-pro model analyzed textual data from judgments and orders. This two-layered approach increased detection precision, providing a comprehensive assessment of fraud likelihood. The system’s user interface facilitated seamless monitoring of flagged cases and intuitive data interaction, further supporting practical application for legal professionals.\n\nConclusion\n\nThe development of the Legal AI Analytics and Alarm System demonstrates a viable solution for identifying insurance fraud within India’s judicial data. By merging structured data analysis with textual content evaluation, the system offers a multi-faceted approach to fraud detection. The combined use of GNNs and advanced NLP models like Gemini-1.5-pro has proven effective in capturing complex fraud indicators across vast datasets, underscoring the value of machine learning in legal analytics.\n\nFuture enhancements could involve expanding the dataset by incorporating additional data sources, further refining parameter weights, and continuously optimizing the fraud threshold for even greater accuracy. This project not only addresses immediate needs for fraud detection in insurance litigation but also opens avenues for broader applications of AI in the legal domain, potentially transforming how legal professionals handle large-scale fraud identification. The system's success in achieving high detection accuracy emphasizes the potential for AI-driven solutions to safeguard against fraud, benefiting both the insurance industry and the legal framework in India.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "leveraging-kaplanmeier-estimator-transforming-traditional-pos-rental-into-a-onetime-subscription-IlXwPPQrWB7f",
    "username": "n@nivedita.chauhan",
    "license": null,
    "title": "Leveraging Kaplan-Meier Estimator: Transforming Traditional POS Rental into a One-Time Subscription",
    "publication_description": "Back to publications\nSep 20, 2024\n●\n97 reads\n●\nCreative Commons Attribution-ShareAlike (CC BY-SA)\nWinner of\nMost Innovative Project\nat the\nAI Project Showcase 2024\nLeveraging Kaplan-Meier Estimator: Transforming Traditional POS Rental into a One-Time Subscription\nAI in Banking\nBanking Innovation\nCLV\nCustomer Lifetime Value\nCustomer Segmentation\nKaplan-Meier\nLTV\nMerchant Services\nPoint of Sale\nPOS\nPOS Systems\nPricing Innovation\nRevenue Optimization\nSubscription\nSurvival Analysis\nN\n@nivedita.chauhan\nE\n@eashan_gaikwad\nH\nHirdesh Khanna\nLike\nBookmark\nShare\n\nCredit: Photo by \nVagaro\n on \nUnsplash\n\nAbstract\n\nThis case study explores an innovative subscription model introduced by a major financial institution in India to transform the traditional Point of Sale (POS) rental system. Merchants, who previously paid monthly rental fees for POS terminals, often faced difficulties maintaining sufficient account balances, leading to challenges in fee collection. To address this, the institution introduced a one-time subscription plan, eliminating recurring rental fees. Using the Kaplan-Meier estimator, the institution optimally priced the one-time fee based on merchant lifetime value (LTV), factoring in geography, merchant category, and turnover. This model enhances revenue predictability and mitigates risks associated with delayed or missed payments. Early results show strong market acceptance, with significant upfront revenue generated, and expansion plans are in progress.\n\nIntroduction\n\nMerchants are an integral part of a major financial institution’s business, and the bank offers a range of services to support their operations, including unified payment solutions such as Point of Sale (POS) terminals and payment gateways.\n\nTraditionally, merchants pay a monthly rental fee to retain their POS terminal IDs (TIDs). This fee is automatically deducted from the merchant’s linked Current Account (CA) through a standing instruction. However, in some cases, merchants struggle to maintain sufficient balances in their accounts, leading to difficulties in collecting charges like rental fees, low usage charges, and late settlement fees. Over time, these outstanding charges can accumulate, resulting in significant revenue losses for the bank.\n\nTo address these challenges, the bank introduced a new pricing model: a simplified POS subscription plan. This innovative approach replaces the recurring monthly rental fees with a one-time setup fee, eliminating the need for ongoing charges. By shifting to this model, the bank aims to streamline operations, improve cash flow predictability, and increase revenue while offering merchants a simpler, more transparent payment structure.\n\nTransforming POS Services with a One-Time Subscription Model\n\nThe proposed approach aims to transform the traditional POS rental model into a one-time subscription service. This innovative approach focuses on determining the optimal upfront fee a merchant should be charged, taking into account lifetime monthly rentals and other associated charges.\n\nUnderstanding Merchant Relationships to Optimize Pricing\n\nTo develop an effective pricing strategy, it's crucial to understand the nature of the bank's relationship with its merchants. This relationship fundamentally influences how we calculate a merchant's lifetime value (LT), which in turn informs the optimal pricing structure.\n\nIn the context of POS services, merchant relationships with the bank can be categorized into two primary types:\n\nAs seen in the visual above, merchant relationships can be broadly categorized as follows:\n\nContractual: Fixed-term agreements with defined start, end, and renewal points.\nNon-contractual: Flexible arrangements without long-term commitments, allowing merchants to start or stop service use at will.\n\nIn the case of POS rentals, the bank's relationship with merchants typically falls under the contractual type. Merchants are considered to have churned when they do not renew their contract or stop paying the monthly rental fee. This well-defined churn point makes it suitable for survival analysis techniques.\n\nIn the rest of this case study, we will focus on the application of survival analysis techniques to optimize the one-time setup fee for contractual relationships.\n\nCalculating Lifetime Value for Contractual Relationships\n\nTo determine the optimal approach for calculating Lifetime (LT) in contractual relationships, three techniques were explored: simple retention model, discrete time model, and Kaplan-Meier estimator. Our analysis found that the Kaplan-Meier estimator was best suited for our business problem. Below, we outline each method and explain why Kaplan-Meier was selected as the optimal solution.\n\n1. Simple retention model\n\nIn this approach, LT is estimated as 1/Churn Rate. However, this method assumes a constant churn rate month-on-month, which did not align with the variability observed in our scenario.\n\n2. Discrete time model\n\nWe fitted a logistic regression model using static independent variables such as demographics, merchant category, and declared annual turnover, along with the month of attrition. However, the model's accuracy and R-squared values remained low due to the limitation of using only these static variables.\n\n3. Kaplan-Meier estimator\n\nThe Kaplan-Meier estimator is a non-parametric method for estimating survival probabilities in our merchant attrition scenario. These estimated probabilities can be used to calculate Merchant Lifetime (LT) as follows:\n\nWhere  is the survival function, representing the probability that a merchant will continue using the POS service beyond time . The Kaplan-Meier estimator for the survival function in our context is given by:\n\nWhere:\n\n are the ordered attrition times\n is the number of merchant attritions at time \n is the number of merchants still using the service just prior to \n\nThe probability of a merchant continuing service from one time point to the next is calculated as:\n\nThis method proved to be the most effective for our analysis of merchant attrition, as it doesn't rely on assumptions about the distribution of service durations and can handle the variability in our merchant data.\n\nIt's worth noting that in our scenario, both Type I (fixed-time) and random right censoring apply. The Kaplan-Meier estimator naturally handles these types of censoring without requiring modifications to our calculations. For an explanation of right censoring and its implications, please refer to Appendix A.\n\nIn the following section, we will provide a detailed example to demonstrate how the Kaplan-Meier estimator was applied to calculate Merchant Lifetime (LT) in our specific POS service context.\n\nExample: Calculating Survival Probabilities and Merchant Lifetime\n\nLet's consider a simplified example to illustrate how the Kaplan-Meier estimator can be used to calculate survival probabilities and Merchant Lifetime (LT) for POS terminal services. Note that these values are illustrative and not reflective of actual data.\n\nSuppose we have data for 100 merchants who started using POS terminals at the same time. We'll look at their status over a 6-month period.\n\nMonth ()\tMerchants at start ()\tAttritions ()\n1\t100\t10\n2\t90\t10\n3\t80\t5\n4\t75\t10\n5\t65\t5\n6\t60\t5\n\nNote: At the end of month 6, the remaining 55 merchants are considered right-censored due to the study ending.\n\nStep 1: Calculate Survival Probabilities\n\nUsing the Kaplan-Meier estimator, we calculate the survival probabilities  for each month:\n\nMonth 1: \nMonth 2: \nMonth 3: \nMonth 4: \nMonth 5: \nMonth 6: \n\nHere's a detailed table showing the values at each month:\n\nMonth ()\tMerchants at start\tAttritions\tSurvival Rate\tCumulative Survival Probability\n0\t100\t0\t1.000\t1.00\n1\t100\t10\t0.900\t0.90\n2\t90\t10\t0.889\t0.800\n3\t80\t5\t0.9375\t0.75\n4\t75\t10\t0.867\t0.65\n5\t65\t5\t0.923\t0.60\n6\t60\t5\t0.917\t0.55\n\nNote: The survival rate for each month is calculated as , and the cumulative survival probability  is the product of the current month's survival rate and all previous months' survival rates.\n\nSee the following chart which plots the survival rates over time.\n\nStep 2: Calculate Merchant Lifetime (LT)\n\nThe Merchant Lifetime (LT) is calculated as the sum of the survival probabilities:\n\nIn our example, summing up to 6 months:\n\n months\n\nStep 3: Calculate Lifetime Value (LTV)\n\nAssuming a monthly rental fee of ₹2,000, the LTV would be:\n\n _ Monthly Rental Fee = 5.250 × ₹2,000 = ₹10,500\n\nInterpreting Results\nOn average, we expect a merchant to continue using the POS terminal for about 5.25 months.\nThe expected lifetime value of a merchant is ₹10,500.\nThis is a conservative estimate due to right-censoring at the end of the study period.\nWhen pricing a one-time subscription, consider factors such as upfront costs, desired profit margin, and market conditions.\n\nFor example, with upfront costs of ₹5,000 and a 20% profit margin target:\n\nSubscription Fee = ₹5,000 + ₹10,500 + (₹10,500 × 0.20) = ₹17,600\n\nThis simplified example demonstrates how the Kaplan-Meier estimator can be used to inform pricing strategies for POS terminal services, while accounting for the realities of merchant attrition and study limitations.\n\nImplementing the Kaplan-Meier Estimator\nDataset and Time Frame\n\nMerchant deactivations over a 12-month period were used to estimate Lifetime (LT). This timeframe provided a substantial dataset to capture various patterns of merchant behavior and attrition.\n\nClustering for Differential LT Estimation\n\nFor efficient LT estimation, the entire merchant base was divided into clusters based on various parameters such as geography, merchant operating category, annual turnover, and other relevant factors. Outlier treatment was performed on these clusters to ensure data quality. Through this process, approximately 20 clusters were formed by combining different levels of each clustering variable. This clustering approach allowed for a more nuanced analysis of merchant lifetimes across different segments of the business.\n\nThe Kaplan-Meier estimator was applied to each cluster to calculate the survival function S(t) at each time point. Lifetime (LT) was then estimated using the following formula:\n\nWhere  represents the survival function, giving the probability that a merchant will continue using the POS service beyond time . In practice, this summation is performed from  to the last observed time point in our data.\n\nThe following table shows illustrative results for several clusters:\n\nCluster #\tEstimated LT (months)\nCluster 1\t37.6\nCluster 2\t41.4\nCluster 3\t46.2\nCluster 4\t39.1\nCluster 5\t53.8\n...\t...\n...\t...\n...\t...\n\nFor each cluster, this method allowed for the calculation of cluster-specific lifetimes, which were found to range from 38 to 54 months, as illustrated in the table above. The variation in lifetimes across clusters highlights the importance of this segmented approach in capturing the diverse characteristics of different merchant groups. By applying the Kaplan-Meier estimator to each cluster separately, we were able to account for the unique attrition patterns within each segment of our merchant base.\n\nValidating the Model for Real-World Impact\n\nTo ensure the reliability of the model, an out-of-time validation approach was employed. The cluster-wise lifetime (LT) estimates were validated using merchant deactivations from the subsequent 3 months. This method allowed the model to be tested on data not used in the initial estimation. The validation process achieved an accuracy of approximately 82%, with a tolerance for variation of ±3 months.\n\nThis validation demonstrates the robustness of the Kaplan-Meier estimator in predicting merchant lifetimes, even when accounting for some variability in the exact timing of deactivations. The high accuracy supports the reliability of the model for business decision-making.\n\nImmediate Business Impact: Revenue and Efficiency Gains\n\nThe implementation of the one-time subscription fee model, based on the Kaplan-Meier lifetime estimation, has yielded significant immediate benefits:\n\nRevenue Generation: Within the first 6 months, the model generated substantial upfront revenue from new-to-bank (NTB) merchants, representing a marked improvement over the previous monthly rental collection model.\n\nImproved Cash Flow: By collecting fees upfront, the bank eliminated the need for monthly rental collections, reducing administrative overhead and improving cash flow predictability.\n\nMarket Acceptance: The model has been well-received by merchants, suggesting they see value in the simplified, one-time payment structure.\n\nExpansion Plans: Given the initial success, plans are underway to extend this model to existing-to-bank (ETB) merchants, with strategic modifications to suit this segment.\n\nConclusion: A Data-Driven Revolution in Merchant Services\n\nThe success of the one-time subscription fee model demonstrates the power of applying advanced statistical techniques to solve real-world financial challenges. This case study illustrates several key points:\n\nData-Driven Innovation: The application of the Kaplan-Meier estimator to merchant lifetime prediction showcases how data analysis can drive product innovation in financial services.\n\nRisk Mitigation: The upfront fee structure reduces the bank's exposure to potential losses from merchant attrition, as the lifetime value is collected at the outset of the relationship.\n\nCompetitive Advantage: By offering a unique pricing model, the bank has differentiated itself in the competitive merchant services market, potentially attracting more merchants and increasing market share.\n\nScalability and Adaptability: The model's success with NTB merchants and planned expansion to ETB merchants demonstrates its scalability and adaptability to different customer segments.\n\nFuture Potential: This approach opens up possibilities for similar innovations in other areas of banking and financial services, where customer lifetime value plays a crucial role.\n\nThis case study illustrates the potential for statistical modeling and data analysis to create tangible business value, improve customer experiences, and drive strategic decision-making in the banking sector. It serves as an example of how traditional financial institutions can advanced analytical methods to innovate within their existing business models and enhance their service offerings.\n\nReferences\nDiscrete time survival models customer lifetime value: \nhttps://youtu.be/uFyYcgwyQFs?feature=shared\nKlein, J. P., & Moeschberger, M. L. (2003). Survival analysis: techniques for censored and truncated data. Springer Science & Business Media.\nAppendix\nA. Understanding Censoring in Survival Analysis\n\nIn survival analysis, censoring occurs when we have incomplete information about the survival time of some individuals in the study. In the context of our POS terminal service, censoring happens when we don't observe the exact time a merchant stops using the service. Two types of right censoring are particularly relevant to our analysis:\n\nType I (fixed-time) censoring: This occurs when the study ends at a predetermined time. Any merchants still using the service at this point are right-censored, as we only know that their service duration exceeded the study period.\n\nRandom right censoring: This happens when merchants leave the study for reasons unrelated to the event of interest (stopping POS service). For example, if we lose contact with a merchant or if they go out of business for external reasons.\n\nThe Kaplan-Meier estimator handles these types of censoring by:\n\nIncluding censored observations in the risk set () up until their censoring time.\nNot counting censored observations as attritions (they don't contribute to ).\nUpdating the survival probability only at times when attritions occur.\n\nThis approach allows us to make use of the partial information provided by censored observations, leading to more accurate estimates of survival probabilities and merchant lifetimes.\n\nIt's important to note that in the presence of heavy censoring, especially towards the end of the study period, the LT calculation may underestimate the true average lifetime. In such cases, the calculated LT effectively becomes a lower bound of the true lifetime.\n\nWhile there are other types of censoring in survival analysis (such as left censoring and interval censoring), these do not apply to our POS terminal service scenario. For readers interested in learning more about various types of censoring and their applications in survival analysis, we recommend referring to Klein and Moeschberger (2003) in References.\n\nB. Kaplan-Meier Estimator Calculation in Python\n# Python code to calculate expected merchant lifetime using Kaplan-Meier estimator\n\nexpected_lifetime = []\n\n# Loop through 20 distinct merchant clusters\nfor cluster_id in range(20):\n    # Separate churned and censored merchants by cluster\n    churned_merchants = df_comb2_ch[df_comb2_ch['Clust'] == cluster_id]\n    censored_merchants = df_comb2[df_comb2['Clust'] == cluster_id]\n\n    # Get survival times for churned and censored merchants\n    churned_survival_times = churned_merchants['Surv_time'].to_list()\n    censored_survival_times = censored_merchants['Surv_time'].to_list()\n\n    # Sort survival times for both churned and censored merchants\n    churned_survival_times.sort()\n    censored_survival_times.sort()\n\n    # Create a range of survival times from the minimum to the maximum in churned merchants\n    time_range = range(int(churned_merchants['Surv_time'].min()), int(churned_merchants['Surv_time'].max()) + 1)\n\n    # Dictionary to count occurrences of survival times for churned merchants\n    churned_counts = {time: churned_survival_times.count(time) for time in time_range}\n    # Dictionary to count occurrences of survival times for censored merchants\n    censored_counts = {time: censored_survival_times.count(time) for time in time_range}\n\n    # Initialize the total number of merchants and at-risk merchants\n    n_total = len(churned_survival_times)\n    n_at_risk = n_total\n\n    # Initialize cumulative survival probability and list to store probabilities\n    cumulative_prob = 1\n    survival_probabilities = []\n\n    # Loop through unique survival times to calculate Kaplan-Meier survival probabilities\n    for time in time_range:\n        churned_events = churned_counts[time]\n        censored_events = censored_counts[time]\n\n        # Calculate survival probability and update cumulative probability\n        survival_prob = (n_at_risk - censored_events) / n_at_risk\n        cumulative_prob *= survival_prob\n\n        # Store cumulative survival probability\n        survival_probabilities.append(cumulative_prob)\n\n        # Update the number of at-risk merchants by subtracting churned events\n        n_at_risk -= churned_events\n\n    # Calculate expected lifetime for the current cluster by summing survival probabilities\n    lifetime_estimate = sum(survival_probabilities)\n    expected_lifetime.append(lifetime_estimate)\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nTransforming POS Services with a One-Time Subscription Model\n\nUnderstanding Merchant Relationships to Optimize Pricing\n\nCalculating Lifetime Value for Contractual Relationships\n\nExample: Calculating Survival Probabilities and Merchant Lifetime\n\nImplementing the Kaplan-Meier Estimator\n\nDataset and Time Frame\n\nClustering for Differential LT Estimation\n\nValidating the Model for Real-World Impact\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nFiles\nSegLifetime.pdf\nYou might be interested\nMonte Carlo simulation approach using Gaussian distribution to optimise MDR for merchants\nN\nA\nSep 18, 202456 reads\nlinear programming problemLPP+8\nLeveraging Machine Learning for Customer Lifetime Value Estimation\nC\nOct 27, 202413 reads\nPreparationVisualization_Churn_Data\nA\nApr 06, 202510 reads\nPayments failure spike - Time series analysis\nA\nApr 07, 202511 reads",
    "awards": [
      "most innovative project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "llmknowledge2-a-framework-for-automated-knowledge-extraction-and-rag-integration-wGUpD2eWWOpg",
    "username": "Daishiro Hirashima",
    "license": "MIT License",
    "title": "LLMKnowledge2: A Framework for Automated Knowledge Extraction and RAG Integration",
    "publication_description": "Back to publications\nMar 07, 2025\n●\n113 reads\n●\nMIT License\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nLLMKnowledge2: A Framework for Automated Knowledge Extraction and RAG Integration\nChatbot\ncontext aware\ndify\ninformation extraction\nknowledge extraction\nknowledge management\nknowledge prompts\nLLM\nopen source\nprompt engineering\nQA generation\nRAG\nstructured knowledge\nDaishiro Hirashima\nLike\nBookmark\nShare\nLLMKnowledge2: A Framework for Automated Knowledge Extraction and RAG Integration\n\nFigure 1-1: LLMKnowledge2 and RAG(Dify) Integration\n\nChapter 1: Introduction\n\nThe rapid advancement of generative AI technology has opened new possibilities for leveraging organizational document data as knowledge assets. Particularly, with the increasing adoption of RAG (Retrieval-Augmented Generation) systems, there is a growing demand for structured knowledge. However, this requires efficient conversion of unstructured document data into AI-ready structured knowledge.\n\nFor instance, traditionally, creating Q&A-format knowledge from product manuals required careful reading, anticipating potential questions, and extracting or creating appropriate answers. This process demanded significant time and effort for understanding context and organizing related information. However, with the advanced language understanding capabilities of modern generative AI, this process can now be significantly automated.\n\nLLMKnowledge2 addresses this challenge through an innovative approach called knowledge prompts. A notable feature is the ability to create and refine prompts interactively within the system. Users can gradually improve prompts while verifying the quality of generated knowledge, enabling the extraction of high-quality knowledge optimized for organizational requirements.\n\nThe system's key features are as follows:\n\nFirst, it provides an integrated environment from prompt creation to knowledge generation. Users can instantly adjust prompts while verifying the quality of generated knowledge. This iterative improvement process enables optimal knowledge extraction according to organizational requirements. Furthermore, by applying different knowledge prompts to the same plain knowledge, information can be extracted from various perspectives.\n\nSecond, it offers flexible switching between generative AI engines. Users can select appropriate engines according to requirements, whether cloud-based generative AI like OpenAI and Anthropic, or locally running LLMs. This is particularly important for addressing data confidentiality requirements and regional service availability restrictions.\n\nThird, it supports diverse input formats and batch processing. The system can process plain knowledge in various formats including PDF, Word, Excel, PowerPoint, various text files, and web pages. Multiple plain knowledge sources can be processed concurrently, enabling rapid conversion of large volumes of data. Progress monitoring and bulk result management are also straightforward.\n\nThe generated knowledge can be utilized in advanced question-answering systems through integration with RAG systems like Dify. For example, it's possible to build a system that quickly responds to customer inquiries using Q&A knowledge generated from product specifications. This enables practical application of the generated knowledge in actual business operations.\n\nFigure 1-2: Dify\n\nThis paper details the LLMKnowledge2 system, explaining its value, technical implementation, and practical deployment and operation methods. In particular, we demonstrate its potential as a flexible knowledge generation foundation, leveraging features such as interactive prompt improvement and support for various generative AI engines.\n\nChapter 2: System Value\n\nThe primary value of LLMKnowledge2 lies in enabling automated large-scale knowledge conversion and multi-perspective knowledge extraction from the same data. This chapter explains the system's specific value and actual evaluation results.\n\nFirst, regarding large-scale knowledge conversion automation, the system can process multiple plain knowledge sources in batch and convert them into structured knowledge. For example, when processing 44 PDF files of product specifications as input, we successfully generated 3,085 Q&A-format knowledge entries. This represents a significant efficiency improvement compared to manual processing.\n\nNext, regarding multi-perspective knowledge extraction, the system features flexible knowledge prompt configuration as a distinctive function. Different knowledge prompts can be applied to the same plain knowledge to extract information from various perspectives. For example, after generating basic Q&As from product specifications, the same document can be used to generate troubleshooting Q&As.\n\nThe ability to select generative AI engines is also an important value. Different organizations and regions have varying available generative AI services, and cloud-based services may not be usable due to data confidentiality requirements. Our system supports various generative AI engines including OpenAI, Anthropic, and locally running LLMs, allowing selection of appropriate engines based on circumstances.\n\nTo verify system effectiveness, we conducted specific evaluations. To assess the quality of Q&A knowledge generated from product specifications, we prepared 40 test questions and verified response accuracy. The results showed accurate answers for 29 out of 40 questions (72.5%), and correct information sources (URLs) were provided for 38 out of 40 questions (95%).\n\nThis result has significant practical implications. First, the 72.5% accuracy rate is sufficient for a first-response generation system assuming human verification. More importantly, the high 95% rate of correct information source provision enables quick manual verification of response accuracy and modifications when necessary. For example, when handling product specification inquiries, operators can provide accurate responses quickly by referencing generated answers while verifying the relevant sections in the information sources.\n\nRegarding processing performance, using 10 parallel processes, we could process 44 PDF files in approximately 15 minutes. This processing speed meets practical requirements. For example, it can process over 1,400 PDFs within an 8-hour workday, sufficiently covering document processing demands for many organizations. Furthermore, increased parallelization enables handling of larger-scale processing.\n\nThe utilization of generated knowledge in RAG systems also provides significant value. For example, in sales support scenarios, it can provide real-time responses to questions during business negotiations. While traditional search systems only presented related documents based on keyword matching, RAG systems can generate responses by understanding context and combining necessary information. Furthermore, since all responses include source URLs, quick responses can be provided while ensuring accuracy.\n\nThus, LLMKnowledge2 functions not just as document processing automation but as an organizational knowledge utilization foundation. It provides particular value in three areas:\n\nKnowledge extraction efficiency: Generating over 3,000 Q&As from 44 PDFs in 15 minutes\nQuality assurance: 95% accuracy in source presentation, facilitating response verification\nPractical utilization: Enabling immediate knowledge utilization through RAG system integration\nChapter 3: Technical Implementation\n\nThe technical implementation of LLMKnowledge2 focuses on three key elements: parallel processing for rapid knowledge generation, support for diverse input formats, and generative AI engine switching mechanism. This chapter explains these technical implementation details.\n\nThe system adopts a hierarchical architecture with two dedicated servers. The first server (MarkItDownServer) handles text conversion from various document formats, while the second server (task2knowledge) executes conversion from plain knowledge to knowledge. This two-layer structure enables optimization specialized for each process.\n\nFirst, regarding parallel processing for rapid knowledge generation, the system implements a parallel processing mechanism capable of processing multiple plain knowledge sources simultaneously. Document splitting size is adjustable according to knowledge generation purposes. The basic settings specify minimum 2,000 and maximum 3,000 characters, which is appropriate context volume for generating one Q&A set. Notably, rather than simple mechanical splitting by character count, the system performs context-aware splitting. Even when exceeding the minimum size (2,000 characters), it maintains context consistency by treating up to appropriate sentence breaks as one processing unit.\n\nAdditionally, knowledge prompts can control generated Q&A density by including instructions like \"output at least three Q&As.\" This enables appropriate granularity of knowledge extraction according to document characteristics and usage purposes.\n\nNext, regarding support for diverse input formats, MarkItDownServer can process various formats of plain knowledge including PDF, Word, Excel, PowerPoint, various text files, and web pages. This is achieved by implementing text extraction modules for each format and converting to a unified internal format. The extracted text maintains the original document structure and format, ensuring accurate source references in generated knowledge.\n\nGenerative AI engine switching is implemented with flexible control through a configuration file (config.yaml). This configuration includes various parameters necessary for practical operation, such as chunk size control (chunk_size, max_chunk_size), API retry control (max_retries, retry_delay), and rate limit control (api_rate_limit). This enables appropriate operational settings according to usage environment and requirements.\n\nError handling is implemented with particular emphasis on stability in large-scale document processing. For API call failures, it executes the configured number of retries (max_retries) with intervals (retry_delay) to recover from temporary failures. Additionally, it maintains stable processing by controlling API call intervals (api_rate_limit).\n\nThus, LLMKnowledge2 functions as a practical knowledge generation foundation through its two-layer server architecture, flexible context-aware splitting process, and detailed operational parameter control. In particular, it achieves production-ready robustness through consistent processing flow from document processing to knowledge generation and flexible configuration according to operational environment.\n\nChapter 4: Deployment and Operation\n\nThe most important elements in deploying and operating LLMKnowledge2 are creating knowledge prompts and appropriately configuring generative AI engines. This chapter explains these practical aspects.\n\nKnowledge Prompt Creation and Optimization\n\nCreating knowledge prompts is a core element in effectively utilizing the system. Here is a typical prompt example for Q&A generation:\n\nPlease convert the following {source QA text} into a new format, referring to the examples and adding {{reference}} values.\n\nExample 1:\nQ: What applications is the ABC series suitable for?\nA: The ABC series is suitable for antenna connections in navigation and other equipment.\n\nExample 2:\nQ: What applications is the DEF2 series suitable for?\nA: The DEF2 series is a small coaxial connector suitable for antenna connections in marine navigation equipment.\nReference URL: {{reference}}\n\n{source QA text}=\n\n\nThis prompt's features include:\n\nClear output format examples\nUnified format specification for references\nPresentation of specific use cases\n\nPrompts are crucial elements controlling the quality and quantity of generated Q&As. For example, including instructions like \"output at least three Q&As\" allows adjustment of generated knowledge density. Additionally, prompts can be gradually improved while verifying actual output results.\n\nFigure 4: Prompt Details Page\n\nSystem Setup\n\nThe system consists of three components, each available on GitHub:\n\nLLMKnowledge2 (Main System)\n\nWeb interface\nKnowledge management functionality\nOverall system control\nGitHub: \nhttps://github.com/daishir0/LLMKnowledge2\n\nMarkItDownServer\n\nText conversion from various document formats\nDocument structure preservation\nMetadata extraction\nGitHub: \nhttps://github.com/daishir0/MarkItDownServer\n\ntask2knowledge\n\nPlain knowledge to knowledge conversion\nGenerative AI engine integration\nBatch processing execution\nGitHub: \nhttps://github.com/daishir0/task2knowledge\n\nEach component can be set up following detailed installation instructions provided in GitHub repositories. In particular, configuration file examples and operation verification procedures are provided, enabling step-by-step implementation.\n\nOperational Settings and Tuning\n\nDuring system operation, the following parameters are adjusted according to the environment:\n\nDocument Processing Settings\n\nchunk_size: 2000 (minimum characters per chunk)\nmax_chunk_size: 3000 (maximum characters per chunk)\nContext-aware splitting process control\n\nAPI Control Settings\n\nmax_retries: 5 (API retry count)\nretry_delay: 5 (retry interval in seconds)\napi_rate_limit: 0.5 (API call interval in seconds)\n\nThese settings are centrally managed in the config.yaml file and can be flexibly adjusted according to operational environment and requirements.\n\nOperational Workflow\n\nThe basic system usage procedure is as follows:\n\nPlain Knowledge Preparation\nDocument collection\nFormat verification\nPreprocessing as needed\n\nFigure 4-1: Plain Knowledge Management Page and Knowledge File Upload Page\n\nKnowledge Generation Configuration\nPrompt selection/creation\nGenerative AI engine configuration\nProcessing parameter adjustment\n\nFigure 4-2: Group Dtails Page and Prompt Selection\n\nBatch Processing Execution\nProcess monitoring\nResult verification\nPrompt adjustment as needed\n\nFigure 4-3: Task Management Page and Pending Tasks\n\nGenerated Knowledge Utilization\nUpload to RAG systems (e.g., Dify)\nQuestion-answering system construction\nEffect verification and feedback\n\nFigure 4-4: Knowledge Management Page and Processed Plain Knowledge\n\nThus, LLMKnowledge2 serves as a comprehensive knowledge generation foundation provided as open source, with systematic support from deployment to operation. In particular, practical utilization is facilitated through the provision of concrete prompt examples and configuration parameters.\n\nAcknowledgements and Future Collaboration\n\nThis research presents the LLMKnowledge2 framework as an evolving system with significant potential for further development. The author, whose research interests span natural language processing, system development, and generative AI, is actively seeking collaboration partners (individuals and organizations) to expand this work. If you are interested in joint research opportunities or can recommend suitable journals or academic publications for this work, please feel free to reach out. Your expertise, perspectives, and suggestions for potential publication venues would be greatly appreciated in advancing this research.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nLLMKnowledge2: A Framework for Automated Knowledge Extraction and RAG Integration\n\nChapter 1: Introduction\n\nChapter 2: System Value\n\nChapter 3: Technical Implementation\n\nChapter 4: Deployment and Operation\n\nKnowledge Prompt Creation and Optimization\n\nSystem Setup\n\nOperational Settings and Tuning\n\nOperational Workflow\n\nAcknowledgements and Future Collaboration",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "llmknowledge3-a-modern-ai-knowledge-management-platform-that-actually-works-Z4WAWStlEc40",
    "username": "Daishiro Hirashima",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nAug 01, 2025\n●\n497 reads\n●\nMIT License\nLLMKnowledge3: A Modern AI Knowledge Management Platform That Actually Works\nBusiness Automation\nDocument Processing\nEnterprise Software\nKnowledge Management\nLLMKnowledge\nDaishiro Hirashima\nLike\nBookmark\nShare\n\nFigure 1-1: LLMKnowledge3 Homepage with Modern Interface\n\nWhat's New in LLMKnowledge3?\n\nRemember LLMKnowledge2? It was pretty cool for extracting knowledge from documents automatically, but let's be honest - it was more of a proof of concept than something you'd want to use every day. LLMKnowledge3 changes all that.\n\nWe've completely rebuilt the system from the ground up. Now it's a proper web application that you can actually use in your organization, with multiple users, a clean interface, and APIs that let you integrate it with your existing systems.\n\nThe big difference? LLMKnowledge2 was like having a powerful tool in your garage that only you knew how to use. LLMKnowledge3 is like having that same power, but now it's packaged in a way that your whole team can use - and it looks good doing it.\n\nWhat Makes LLMKnowledge3 Different?\n\nHere's what we've completely rebuilt:\n\nEverything is now API-first: Now everything has a proper REST API. Want to integrate it with your existing systems? No problem. Want to build your own interface on top of it? Go for it. The API handles everything the web interface can do, and more.\n\nA real web application: LLMKnowledge3 has a modern React interface that works on your phone, tablet, or desktop. You can actually see your knowledge matrices in real-time, track processing progress, and manage everything from a browser.\n\nMultiple users, proper security: Your whole team can use it now. We've built in user accounts, role-based permissions, and data isolation. Your documents stay yours, admin users can manage the system, and everyone gets their own workspace.\n\nNo more server juggling: Remember setting up LLMKnowledge2 and task2knowledge separately? Those days are over. Everything's integrated into one system now. Deploy once, use everything.\n\nThe Tech Stack (For Those Who Care About These Things)\n\nThe Frontend (What You See):\n\nReact 18 with TypeScript - because nobody likes runtime errors\nVite for lightning-fast development - no more waiting 30 seconds for builds\nTailwind CSS with shadcn/ui - it actually looks professional now\nWorks on mobile, tablet, desktop - wherever you are\n\nThe Backend (What Does The Work):\n\nFastAPI - automatic API documentation, async everything, Python that's actually fast\nSQLAlchemy 2.0 - modern database handling that doesn't make you cry\nJWT authentication - secure, stateless, works everywhere\nMultiple AI providers - OpenAI, Claude, Gemini, or run your own local models\n\nThe Infrastructure (Deploy Without Drama):\n\nPoetry for Python dependencies - no more \"it works on my machine\"\nDatabase migrations with Alembic - upgrade without losing data\nSQLite for quick setup, PostgreSQL for production - your choice\nHow It All Works Together\nThe Three-Layer Approach (Simple, But Effective)\n\nWe've built this with a clean separation that actually makes sense:\n\nThe Frontend (What You Interact With): A React app that looks good and works everywhere - your phone, tablet, laptop, whatever. It adapts to your screen and doesn't make you squint at tiny buttons.\n\nThe Backend (The Brain): FastAPI handling all the heavy lifting - user logins, AI processing, task management, and serving up data through clean APIs. It's fast, well-documented, and doesn't break when you look at it funny.\n\nThe Database (Where Everything Lives): SQLAlchemy managing your data - user accounts, documents, generated knowledge, system settings. Start with SQLite for testing, move to PostgreSQL when you're ready for production.\n\nThe Knowledge Matrix - Now You Can Actually See It\n\nFigure 2-1: Interactive Knowledge Matrix with Real-time Data\n\nFigure 2-2: Knowledge Matrix Excel File\n\nThis was the big missing piece in LLMKnowledge2. Sure, you could export to Excel, but who wants to export to Excel every time they want to see what's happening?\n\nSee Your Results Live: The knowledge matrix updates in real-time as your documents are processed. No more guessing what's happening - you can watch your knowledge base grow.\n\nActually Usable Interface: Sort it, filter it, search through it. Find what you need without opening Excel file. Toggle between different views depending on what you're looking for.\n\nKnow Where It Came From: Every piece of knowledge shows you exactly which document it came from. Click through to verify, check context, or just satisfy your curiosity.\n\nMultiple Users Without The Headaches\n\nFinally, you can let your whole team use this without worrying about security or people seeing each other's stuff:\n\nProper User Roles: Admins can manage the system and see usage stats. Regular users get their own workspace and can't mess with other people's data. Simple, secure, sensible.\n\nYour Data Stays Yours: Each user's documents and knowledge are completely separate. No accidental data leaks, no privacy concerns, no \"oops, I can see everyone's files\" moments.\n\nUsage Controls: Set limits on AI API usage so nobody accidentally burns through your OpenAI credits in one afternoon. Fair usage for everyone, with monitoring so you can see who's using what.\n\nSecure Authentication: JWT tokens, proper session management, secure password handling. All the security stuff you expect, without making login a pain.\n\nAPIs That Actually Work With Your Existing Systems\n\nWe built this API-first, which means you can integrate it with whatever you're already using:\n\nDocument Processing: Send documents via API, check processing status, get results back. Perfect for automating workflows or building it into existing document management systems.\n\nKnowledge Search and Retrieval: Query your knowledge base programmatically. Filter by document, prompt, date, whatever you need. Build custom dashboards or integrate with other tools.\n\nUser Management: Integrate with your existing user systems. Create accounts, manage permissions, track usage - all through clean API endpoints.\n\nEvent Notifications: Set up webhooks to get notified when processing finishes, errors occur, or other events happen. No more polling APIs to check status.\n\nReal-World Use Cases (That Actually Make Sense)\n\nHere's where this gets interesting. With the API and web interface, you can actually solve real business problems:\n\nStartup Business Plan Evaluation\n\nThe Problem: You're a VC or running an accelerator, and you get dozens of business plans every week. Reading through them all takes forever, and it's hard to evaluate them consistently.\n\nThe LLMKnowledge3 Solution:\n\nUpload business plans through the web interface or API\n\nSet up evaluation prompts that ask the right questions:\n\n\"How viable is this market and who are the competitors?\"\n\"Do the financial projections make sense?\"\n\"What are the technical risks?\"\n\"Are there regulatory issues we should worry about?\"\n\"Does this team have what it takes?\"\n\nGet a comprehensive evaluation matrix showing scores and detailed feedback across all criteria\n\nMake better decisions faster with consistent evaluation criteria\n\nIntegration Options: Hook this into your existing deal flow software. New business plans come in, get automatically evaluated, and you get a summary report in your inbox.\n\nWhy This Works: You get consistent, thorough evaluations without spending hours reading every plan. Save your time for the promising ones.\n\nStudent Paper Grading That Doesn't Take All Weekend\n\nThe Problem: You're a professor with 200 students, they all submit papers, and you need to grade them consistently against multiple rubric criteria. Your weekend is gone.\n\nThe LLMKnowledge3 Solution:\n\nUpload all papers at once - drag and drop or API integration with your LMS\n\nDefine your rubric as prompts:\n\n\"How well does this paper demonstrate understanding of the topic?\"\n\"Is the writing clear and well-structured?\"\n\"How strong is the critical analysis?\"\n\"Are sources properly cited?\"\n\"Does this show original thinking?\"\n\nGet detailed feedback for every paper across all rubric criteria\n\nReview and adjust the AI assessments before finalizing grades\n\nIntegration Options: Connect with Blackboard, Canvas, Moodle, or whatever LMS you're using. Papers get auto-evaluated, you review the results, students get detailed feedback.\n\nWhy This Works: First-pass grading that's consistent and thorough, with detailed feedback that helps students improve. You review and adjust, but you're not starting from scratch every time.\n\nEnterprise Document Security Classification\n\nThe Problem: Your company has thousands of files on servers, SharePoint, everywhere. Some contain personal information, some are confidential, some are public. You need to know what's what for compliance and security, but manually reviewing everything would take years.\n\nThe LLMKnowledge3 Solution:\n\nPoint it at your file servers - API integration scans directories and processes documents in batches\n\nSecurity evaluation prompts analyze each document:\n\n\"Does this contain personal information (names, addresses, SSNs)?\"\n\"Is there confidential business information here?\"\n\"Any intellectual property or trade secrets?\"\n\"What compliance requirements apply (GDPR, HIPAA, etc.)?\"\n\"What security classification should this have?\"\n\nGet a security assessment matrix for your entire document repository\n\nImplement appropriate controls based on the classifications and recommendations\n\nIntegration Options: Hook into your document management system for ongoing classification of new files. Integrate with security tools for automated policy enforcement.\n\nWhy This Works: Systematic security assessment of your entire document landscape. Know what you have, where it is, and how to protect it. Perfect for compliance audits and security planning.\n\nGetting It Running (Without The Usual Pain)\nWhat You Need and How to Get Started\n\nWe've tried to make this as painless as possible:\n\nWhat You Need:\n\nPython 3.11+ (the backend)\nNode.js 16+ (for building the frontend)\n4GB RAM minimum (8GB if you want it to be happy)\nA database (SQLite to start, PostgreSQL for production)\n\nDevelopment Setup:\nFollow the GitHub README - we've actually kept it up to date this time. Poetry handles Python dependencies, npm handles frontend stuff. No \"works on my machine\" surprises.\n\nProduction Deployment:\nDocker containers, traditional servers, cloud platforms - whatever you prefer. Environment variables for configuration, database migrations that actually work, and deployment scripts that don't assume you're psychic.\n\nConfiguration That Makes Sense\n\nWe use environment variables and config files like normal people:\n\nAI Provider Setup: Use OpenAI, Claude, Gemini, or your own local models. Set primary and backup providers so you don't get stuck if one service is down.\n\nDatabase Choice: SQLite for getting started quickly, PostgreSQL when you need the real deal. Switch between them without rewriting everything.\n\nSecurity Settings: JWT configuration, API rate limits, CORS policies, user permissions - all the security stuff you need, with sensible defaults.\n\nPerformance Tuning: Adjust chunk sizes, parallel processing, retry policies. Start with the defaults, tweak as needed based on your hardware and usage patterns.\n\nKeeping Track of What's Happening\n\nBecause nobody likes systems that fail silently:\n\nUser Activity Logs: See who's doing what, track API usage, monitor system performance. Useful for debugging and usage planning.\n\nTask Progress: Watch document processing in real-time. See what's working, what's stuck, what failed and why.\n\nSystem Health: Built-in health checks and status endpoints. Monitor with your existing tools or use the web interface.\n\nAdmin Interface: Web-based admin panel for user management, system configuration, and monitoring.\n\nWhat's Next and How We'll Test It\nTesting This Stuff In The Real World\n\nWe've built something that should work, but we want to prove it actually does:\n\nBusiness Plan Evaluation Testing:\n\nPartner with real VCs and accelerators\nCompare AI evaluations with expert human evaluations\nMeasure time savings and consistency improvements\nTrack which evaluations predict actual success (the ultimate test)\n\nAcademic Grading Verification:\n\nWork with universities across different subjects\nCompare AI grades with experienced professor grades\nGet student feedback on the quality of automated feedback\nMeasure time savings and grading consistency\n\nSecurity Classification Validation:\n\nPartner with enterprise IT security teams\nValidate classifications against expert manual reviews\nTest compliance coverage and gap identification\nMeasure actual risk reduction and incident prevention\nWhere This Is All Heading\n\nBetter AI Integration: As new AI models come out, we'll integrate them. Multi-modal processing for images and videos in documents is on the roadmap.\n\nSmarter Analytics: Trend analysis, pattern recognition in your knowledge, predictive insights based on what you've processed before.\n\nEnterprise Features: Single sign-on, integration with enterprise databases, workflow automation. All the stuff that makes IT departments happy.\n\nPerformance Improvements: Faster processing, better scalability, handling larger document volumes. Because nobody likes waiting.\n\nOpen Source and Community\n\nThis is all available on GitHub (\nhttps://github.com/daishir0/LLMKnowledge3\n). We welcome contributions:\n\nNew AI provider integrations\nUI improvements and new features\nPerformance optimizations\nBetter documentation and tutorials\nEnterprise system integrations\n\nIf you find bugs, have ideas, or want to contribute code, jump in. The project is designed to be extensible and we're happy to review pull requests.\n\nWant to Collaborate?\n\nWe're always looking for partners who want to push this further:\n\nAcademic institutions: Research on NLP improvements, domain-specific knowledge extraction\nUX researchers: How to make knowledge work more productive\nSecurity teams: Better approaches to sensitive document processing\nPerformance engineers: Scaling this to handle massive document volumes\n\nIf you're working on related problems or have interesting use cases, let's talk.\n\nThe Bottom Line\n\nLLMKnowledge3 takes the cool concept from LLMKnowledge2 and makes it something you can actually deploy and use in your organization. We've gone from \"interesting proof of concept\" to \"production-ready platform that solves real problems.\"\n\nThe planned real-world testing will prove whether this actually works as well as we think it does. The open source approach means it'll keep getting better as more people use it and contribute to it.\n\nMost importantly, we've built something that makes AI knowledge processing accessible to regular people, not just data scientists. Whether you're a VC evaluating business plans, a professor grading papers, or an IT admin trying to secure your document repositories, you can actually use this system to solve real problems.\n\nThat's the real achievement here - taking powerful AI capabilities and packaging them in a way that's actually useful for everyday business operations. No PhD in machine learning required.\n\nAcknowledgements and Future Collaboration\n\nLLMKnowledge3 is a platform with lots of potential, and we're looking for partners to help make it even better.\n\nWe're especially interested in working with:\n\nUniversities - let's test automated assessment systems in real classrooms\nVCs and accelerators - help us validate business plan evaluation in actual deal flow\nEnterprise IT teams - test security classification with real document repositories\nResearchers - advance the state of AI-powered knowledge management\n\nIf you want to collaborate, have ideas, or just want to chat about AI knowledge management, check out our GitHub (\nhttps://github.com/daishir0/LLMKnowledge3\n) or reach out directly. We're always interested in real-world use cases and practical feedback.\n\nThe best software comes from solving real problems for real people. Help us make this better.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "magic-research-module-1-XRwc4qgCvplZ",
    "username": "t@tapak217",
    "license": "License",
    "title": "Magic-Research Module 1",
    "publication_description": "Back to publications\nOct 21, 2025\n●\n2 reads\n●\nCreative Commons Attribution (CC BY)\nMagic-Research Module 1\nAAIDC2025\nT\n@tapak217\nLike\nBookmark\nShare\n\nRAG Assistant for Academic Research\n\nA Retrieval-Augmented Generation (RAG) system designed for semantic search and question-answering over academic documents, specifically focused on medieval Jewish magic and historical texts.\n\nOverview\n\nThis project implements a complete RAG pipeline that processes PDF documents, creates semantic embeddings, stores them in a vector database, and provides an intelligent question-answering interface. The system combines document retrieval with large language model generation to provide contextually accurate answers based on your document corpus.\n\nKey features:\n\nPDF Document Processing: Automatic extraction and chunking of academic papers\nSemantic Search: Vector-based similarity search using sentence transformers\nMulti-LLM Support: Compatible with OpenAI GPT, Groq Llama, and Google Gemini\nWeb Interface: User-friendly FastAPI-based frontend\nPersistent Storage: ChromaDB vector database for efficient document retrieval\nDemo\nWeb Interface\n\nThe web interface provides an intuitive way to query your document collection. Simply type your question and get AI-powered answers based on your academic papers.\n\nKey Features Shown:\n🔍 Semantic Search: Natural language queries across your document corpus\n📚 Source Attribution: Clear references to specific documents and sections\n🎯 Contextual Answers: AI responses grounded in your actual content\n⚡ Real-time Processing: Fast response times for interactive research\nTarget Audience\n\nThis project is intended for:\n\nAcademic Researchers studying historical texts and manuscripts\nDigital Humanities Scholars working with large document collections\nGraduate Students conducting literature reviews and research\nDevelopers interested in implementing RAG systems for domain-specific applications\nPrerequisites\nRequired Knowledge\nBasic Python programming\nUnderstanding of virtual environments\nFamiliarity with command line interfaces\nBasic knowledge of machine learning concepts (helpful but not required)\nHardware Requirements\nRAM: Minimum 8GB (16GB recommended for larger document collections)\nStorage: At least 5GB free space for models and embeddings\nCPU: Multi-core processor recommended for faster embedding generation\nSystem Compatibility\nOperating Systems: Windows 10/11, macOS, Linux\nPython: Version 3.11 or higher\nInternet Connection: Required for initial model downloads and API access\nInstallation\n1. Clone or Download the Project\ncd \"path/to/your/projects\"\n# If using git:\ngit clone <repository-url>\n# Or extract from zip file\n2. Navigate to Project Directory\ncd Project1AiCourse\n3. Create Virtual Environment\n# Windows\npython -m venv venv\nvenv\\Scripts\\activate\n\n# macOS/Linux\npython -m venv venv\nsource venv/bin/activate\n4. Install Dependencies\npip install -r requirements.txt\n\nNote: On Windows, uvloop is automatically excluded from installation as it's not compatible.\n\nEnvironment Setup\n1. Configure API Keys\n\nCopy the example environment file and add your API keys:\n\ncp .env.example .env\n\nEdit .env file with your preferred LLM provider:\n\nOption A: Using Groq (Free tier available)\n\nGROQ_API_KEY=your_groq_api_key_here\nGROQ_MODEL=llama-3.1-8b-instant\n\nOption B: Using OpenAI\n\nOPENAI_API_KEY=your_openai_api_key_here\nOPENAI_MODEL=gpt-4o-mini\n\nOption C: Using Google Gemini\n\nGOOGLE_API_KEY=your_google_api_key_here\nGOOGLE_MODEL=gemini-pro\n2. Verify Installation\n# Activate virtual environment\nsource venv/Scripts/activate  # Windows\n# source venv/bin/activate     # macOS/Linux\n\n# Test import\npython -c \"import chromadb, sentence_transformers, fastapi; print('✅ All dependencies installed successfully')\"\nUsage\nQuick Start\nStart the Web Application\n# From project root directory\ncd src\npython web_app.py\n\nAccess the Interface\nOpen your web browser and navigate to: http://127.0.0.1:8000\n\nAsk Questions\nTry example queries like:\n\n\"What is Abraham of Worms known for?\"\n\"Describe the magical practices mentioned in the texts\"\n\"What are the main themes in medieval Jewish magic?\"\nCommand Line Usage\n\nFor direct interaction with the RAG system:\n\ncd src\npython app.py\n\nThis allows you to:\n\nLoad and process new documents\nTest queries directly in the terminal\nDebug and examine the retrieval process\nAdding New Documents\nPlace PDF files in the data/ directory\nThe system will automatically detect and process them on startup\nDocuments are chunked semantically and embedded for search\nData Requirements\nSupported Formats\nPDF Files: Academic papers, research documents, manuscripts\nText Encoding: UTF-8 recommended\nLanguage: Optimized for English academic texts\nDocument Structure\n\nFor best results, documents should have:\n\nClear paragraph structure\nAcademic or scholarly writing style\nConsistent formatting\nReadable text (not scanned images without OCR)\nCurrent Dataset\n\nThe system includes sample academic papers on:\n\nMedieval Jewish magic traditions\nHistorical manuscript analysis\nComparative religious studies\nTesting\nManual Testing\nStart the web application\nNavigate to the health check endpoint: http://127.0.0.1:8000/health\nExpected response:\n{\n  \"status\": \"healthy\",\n  \"rag_initialized\": true,\n  \"document_count\": 641\n}\nQuery Testing\n\nTest with known content to verify retrieval:\n\n# Example test queries\ncurl -X POST \"http://127.0.0.1:8000/query\" \\\n     -H \"Content-Type: application/x-www-form-urlencoded\" \\\n     -d \"question=Abraham of Worms\"\nConfiguration\nVector Database Settings\nCollection Name: rag_documents (configurable in .env)\nEmbedding Model: sentence-transformers/all-MiniLM-L6-v2\nChunk Size: 500 characters with semantic boundary detection\nDatabase Path: ./chroma_db/ (persistent storage)\nLLM Configuration\n\nThe system supports multiple providers with fallback options:\n\nPrimary: Groq (fast, free tier available)\nSecondary: OpenAI (high quality, paid)\nTertiary: Google Gemini (alternative option)\nPerformance Tuning\nChunk Size: Adjust in vectordb.py for different document types\nRetrieval Count: Modify n_results parameter for more/fewer context chunks\nModel Selection: Choose embedding models based on domain specificity\nMethodology\nDocument Processing Pipeline\nPDF Extraction: PyPDF2 extracts text content\nSemantic Chunking: Text is split at sentence boundaries while respecting semantic coherence\nEmbedding Generation: Sentence transformers create dense vector representations\nVector Storage: ChromaDB stores embeddings with metadata for efficient retrieval\nRetrieval Process\nQuery Encoding: User questions are embedded using the same model\nSimilarity Search: Cosine similarity identifies relevant document chunks\nContext Assembly: Top-k results are combined into coherent context\nLLM Generation: Language model generates answers based on retrieved context\nRAG Architecture\nUser Query → Embedding → Vector Search → Context Retrieval → LLM → Response\n     ↑                                        ↓\nDocument → Chunking → Embedding → Vector DB Storage\n\nPerformance\nBenchmarks\nDocument Processing: ~100-200 pages per minute (depending on complexity)\nQuery Response Time: 2-5 seconds for typical academic questions\nMemory Usage: ~2-4GB RAM for moderate document collections (500-1000 pages)\nStorage Requirements: ~10-50MB per 100 pages of documents\nOptimization Recommendations\nHardware: SSD storage for faster database access\nScaling: Consider GPU acceleration for large document collections\nNetwork: Stable internet connection for LLM API calls\nLicense\n\nThis project is licensed under the MIT License - see the \nLICENSE\n file for details.\n\nContributing\n\nWe welcome contributions to improve the RAG Assistant! Please follow these guidelines:\n\nFork the Repository\nCreate a Feature Branch: git checkout -b feature/new-feature\nMake Changes: Implement your improvements\nTest Thoroughly: Ensure all functionality works\nSubmit Pull Request: Describe your changes clearly\nAreas for Contribution\nAdditional document format support (DOCX, TXT, etc.)\nAdvanced chunking strategies\nMulti-language support\nPerformance optimizations\nUI/UX improvements\nChangelog\nVersion 1.0.0 (Current)\n✅ Initial RAG implementation\n✅ PDF document processing\n✅ ChromaDB vector storage\n✅ Multi-LLM provider support\n✅ FastAPI web interface\n✅ Semantic text chunking\n✅ Persistent embeddings storage\nPlanned Features\n Document upload via web interface\n Advanced search filters\n Export functionality for results\n Multi-language document support\n Integration with academic databases\nCitation\n\nIf you use this RAG Assistant in your academic work, please cite:\n\n@software{rag_assistant_2025,\n  title={RAG Assistant for Academic Research},\n  author={[Your Name]},\n  year={2025},\n  url={[Repository URL]},\n  note={A Retrieval-Augmented Generation system for academic document analysis}\n}\nContact\nMaintainers\nPrimary Developer: Gamaiun\nSupport\nIssues: Use GitHub Issues for bug reports and feature requests\nDiscussions: GitHub Discussions for questions and community support\nDocumentation: Check this README and inline code comments\nAcademic Collaboration\n\nFor academic collaborations or research partnerships, please reach out via email with:\n\nBrief description of your research project\nSpecific use case or requirements\nTimeline and scope of collaboration\n\nLast Updated: October 2025\nProject Status: Active Development\nCompatibility: Python 3.11+, Windows\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nRAG Assistant for Academic Research\n\nOverview\n\nDemo\n\nWeb Interface\n\nKey Features Shown:\n\nTarget Audience\n\nPrerequisites\n\nRequired Knowledge\n\nHardware Requirements\n\nSystem Compatibility\n\nView all\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "mcp-a-year-tomorrow-the-model-context-protocol-explained-2IcQHHu0nRlD",
    "username": "Favour Anwara",
    "license": null,
    "title": "MCP, A Year Tomorrow — The Model Context Protocol Explained",
    "publication_description": "Back to publications\nOct 23, 2025\n●\n1 reads\nMCP, A Year Tomorrow — The Model Context Protocol Explained\nArtificial Intelligence\nContext Protocol\nmcp framework\nMemory\nFavour Anwara\nVictory Nnaji\nLike\nBookmark\nShare\n\n\nWhen Anthropic introduced the Model Context Protocol (MCP), it didn’t just give developers a new tool; it gave them a glimpse of tomorrow. MCP reimagined how AI models connect to data, memory, and tools, turning what used to be isolated systems into something almost alive, aware, adaptive, and context-rich.\n\n“MCP, a year tomorrow” isn’t about time; it’s about trajectory. The protocol feels like a message sent from the future, showing us what intelligent systems could look like when context becomes limitless. Today, developers use it to make models smarter. Tomorrow, it might define how AIs think, collaborate, and build alongside us.\n\nA Brief History of MCP\n\nAnthropic introduced the Model Context Protocol (MCP) in late 2024 as part of a quiet yet fundamental shift in how large language models interact with the world around them. The goal wasn’t just to make models smarter; it was to make them contextually aware.\n\nBefore MCP, developers had to rely on scattered methods, APIs, plugin systems, or external databases to give their models access to information. These solutions worked in isolation but couldn’t scale together. The problem was simple but profound: AI lacked continuity of context. A model could process text beautifully, but it couldn’t remember, connect, or collaborate effectively.\n\nThe Anthropic team built MCP to solve exactly that. The protocol established a universal way for models to access and share context across tools, apps, and data sources. With it, developers could create extensions that enabled AI to see, recall, and reason seamlessly in real time.\n\nIn less than a year, the impact has been massive. MCP has inspired a new wave of agentic systems, more innovative developer tools, and even interoperable AI workspaces. What started as a bridge between models and context has quickly become one of the most defining layers of the modern AI stack, the quiet infrastructure of tomorrow’s intelligent systems.\n\nComponents of MCP\n\nThe Model Context Protocol (MCP) is built around a simple yet powerful idea: creating a shared, dynamic context layer between AI models and the tools they use. To make that work, MCP relies on four key components:\n\n1. MCP Server\n\nThe MCP Server is the brain that exposes structured information, such as files, APIs, databases, or external systems to the model. It translates those resources into standardised context objects that models can understand.\n\n2. MCP Client\n\nThe MCP Client acts as the bridge. It connects your model (for instance, OpenAI’s GPT models) to one or multiple MCP servers, fetching context, instructions, and data as needed.\n\n3. Tools and Extensions\n\nTools are what make your model truly “agentic.” They define actions your AI can take, from querying an API to running a computation. Each tool can be registered on the MCP Server and accessed via the protocol.\n\n4. Context Schema\n\nMCP uses a schema to define what context actually means in a task, what data is shared, how it’s structured, and how models should interpret it. This ensures interoperability across models and environments.\n\n\nMCP Servers vs Other Servers\n\nAt first glance, an MCP server might sound like any other server. It receives requests, processes data, and returns a response. But that’s where the similarity ends. Traditional servers are designed for delivery, while MCP servers are designed for understanding.\n\nIn a conventional setup, a server hosts an API, handles requests, and returns static or dynamic data. The interaction is one-directional. You ask for something, it responds, and the conversation ends there. The server doesn’t know who you are, what you’re building, or how the data it provides will be used. It simply serves.\n\nMCP servers, however, play a different game. They don’t just return data; they offer context, structured, interpretable, and dynamic information that an AI model can use to make decisions, reason, or act. Instead of one-off exchanges, they maintain a living, contextual thread with the client. In this case, an AI system or agent.\n\nWhile traditional servers focus on serving data, MCP servers serve meaning. They know how to collaborate with models, interpret schema-based context, and even adapt to changing states across multiple tools.\n\nIn short, traditional servers power the web.\nMCP servers will power the next generation of intelligent, context-aware AI systems.\n\nWhy You Should Use MCP\n\nIf you’ve ever built an AI system, you know the pain of context limits. Models forget things, APIs don’t sync, and every new integration feels like duct-taping intelligence together. That’s the exact frustration the Model Context Protocol (MCP) was designed to end.\n\nMCP gives developers a common language to connect their models, tools, and data without having to hack context together. It lets your AI system see what’s happening across different environments, remember what matters, and act in real time.\n\nBut beyond the technical beauty, here’s why MCP actually matters:\n\nFirst, it makes your AI systems scalable. Instead of building custom connectors for every tool or source, you define them once, and any MCP-compatible model can use them. That means less overhead, fewer integration headaches, and faster iteration.\n\nSecond, it makes your AI systems smarter. By treating context as a first-class citizen, MCP allows models to reason dynamically, pulling only the data that’s relevant at that moment. No retraining, no fine-tuning, just real-time awareness.\n\nThird, it unlocks collaboration. Multiple agents can now share context through MCP, enabling systems that think and work together—like a team of specialised models solving problems in sync.\n\nIn short, you should use MCP because it does what every developer has wanted AI to do from the beginning: stay aware, connected, and continuously evolving.\n\nTo see how easy it is to connect external tools to your AI agent, below is a config.json file you can use to integrate the \nGIbsonAI\n MCP server into your IDE agent easily\n\n{\n  \"mcpServers\": {\n    \"gibson\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"gibson-cli@latest\", \"gibson\", \"mcp\", \"run\"]\n    }\n  }\n}\nReal-World Applications of MCP\n\nWhile the idea of “context protocols” may sound abstract, MCP is already demonstrating its power in real systems. Here’s how developers and companies are applying it today:\n\n1. Developer Workflows and AI Coding Assistants\n\nTools like Cursor IDE and Claude’s new MCP integration let AI models read project files, interpret codebases, and even modify them directly via MCP connections. This turns the AI from a static helper into a context-aware coding partner that understands your entire repository.\n\n2. Enterprise Data Agents\n\nBusinesses are using MCP to connect LLMs to private data, CRMs, analytics tools, and knowledge bases without compromising security. Instead of dumping data into the model, MCP allows the LLM to fetch context only when needed. Imagine an AI that can answer,\n\n“What were last quarter’s top-performing campaigns?”\nby securely querying live business data through MCP.\n\n3. Research and Analysis\n\nResearchers are integrating MCP with data pipelines and notebooks so that AI models can query datasets, run analyses, and summarise results on demand. It’s a new way to blend reasoning with live computation, particularly useful for academic projects and data science workflows.\n\n4. Personal Productivity Tools\n\nFrom task managers to note-taking apps, developers are experimenting with MCP-enabled copilots that can reference your documents, schedules, and reminders contextually without sending your entire digital life to the cloud.\n\n5. Agentic Systems and Multi-Model Collaboration\n\nSome teams are using MCP to enable multiple AI agents to coordinate around shared data, each with a role (e.g., planner, executor, verifier). MCP acts as the communication bridge, ensuring they all share the same context in real time.\n\nThe Future of MCP…\n\nThe truth is, MCP isn’t just a protocol; it’s a preview of what’s coming.\n\nRight now, it sits quietly behind the scenes, powering a new generation of AI systems that can reason, recall, and interact with the world around them. But the deeper you look, the more it feels like the foundation of something bigger. A shift from prompt-based AI to context-native intelligence.\n\nIn the near future, MCP could evolve into the backbone of multi-agent ecosystems, where multiple AIs, each with unique skills and memories, collaborate through shared context instead of shared prompts. Imagine an ecosystem where one agent analyses real-time financial data, another interprets user behaviour, and a third writes recommendations, all seamlessly synchronised via MCP.\n\nBeyond that, we’ll likely see autonomous orchestration systems that manage their own tools, APIs, and environments, dynamically updating their own context. MCP could make AI not just responsive, but self-directing, capable of learning what to connect to next without human prompting.\n\nThe most exciting vision lies in a persistent, living context —an AI that doesn’t just remember a chat or a session, but builds a continuous relationship with you. It learns your tools, your data, your patterns and is always there, constantly evolving. That’s what “a year tomorrow” truly means: not time passing, but time arriving.\n\nMCP isn’t a reflection of what AI was; it’s a revelation of what AI is becoming.\n\nClosing Thoughts\n\nAs developers, we’ve spent years trying to teach machines how to understand us. But with the Model Context Protocol, it feels like we’re finally learning how to meet them halfway, giving AI not just information, but awareness. MCP turns code into conversation, systems into collaborators, and data into living memory.\n\nSo when we say “MCP, a year tomorrow,” we’re not talking about a milestone; we’re talking about momentum. The kind that doesn’t look back. The type that quietly redefines how intelligence connects, grows, and remembers.\n\nTomorrow isn’t coming. With MCP, it’s already here.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nA Brief History of MCP\n\nComponents of MCP\n\nMCP Server\nMCP Client\nTools and Extensions\nContext Schema\n\nMCP Servers vs Other Servers\n\nWhy You Should Use MCP\n\nReal-World Applications of MCP\n\nDeveloper Workflows and AI Coding Assistants\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nYou might be interested\nMCP: A Standard Way for AI to Use External Tools (AAIDC-Week6-Lecture-4a)\nJun 25, 2025129 reads\nAAIDCAI Agents+7\nMCP in Action: Connecting and Creating Real AI Integrations (AAIDC-Week6-Lesson-4b)\nJun 25, 2025139 reads\nAAIDCAgentic AI+12\n🧠📡 The Model Context Protocol (MCP): The USB-C of AI-Data Connectivity\nApr 12, 202533 reads\nCore concepts of Agentic AI and AI agents\nFeb 20, 2025171 reads\nAgentic AIAgents+2",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "mdigan-generating-pixel-art-characters-8jLQ7xqMORQi",
    "username": "Flavio Coutinho",
    "license": "MIT License",
    "title": "MDIGAN: Generating Pixel Art Characters",
    "publication_description": "Back to publications\nDec 26, 2024\n●\n73 reads\n●\nMIT License\nWinner of\nMost Engaging Presentation\nat the\nComputer Vision Projects Expo 2024\nMDIGAN: Generating Pixel Art Characters\n2d\ncharacter sprite\ngame development\ngan\ngenerative ai\nimage to image\nmissing data imputation\npixel art\nprocedural content generation\nFlavio Coutinho\nLike\nBookmark\nShare\n\nSee it in action\tWatch a presentation\tRead the paper\n\n\t\n\t\nAbstract\n\nCreating and updating pixel art character sprites with many frames spanning different animations and poses takes time and can quickly become repetitive. However, that can be partially automated to allow artists to focus on more creative tasks. In this work, we concentrate on creating pixel art character sprites in a target pose from images of them facing other three directions. We present a novel approach to character generation by framing the problem as a missing data imputation task. Our proposed generative adversarial networks model receives the images of a character in all available domains and produces the image of the missing pose. We evaluated our approach in the scenarios with one, two, and three missing images, achieving similar or better results to the state-of-the-art when more images are available. We also evaluate the impact of the proposed changes to the base architecture.\n\nIntroduction\n\nCreating characters for pixel art games is a time-consuming process in game development that often involves a lot of back-and-forth adjustments. Artists meticulously design each pixel, but even small changes can require updating many images, especially for characters that move and face different directions. While artists are skilled at this, some aspects of the process can be repetitive and tedious, such as creating special effects or ensuring consistency across all character poses.\n\nTo address these challenges, researchers are exploring the use of artificial intelligence (AI) to help automate parts of the character creation process. This research often involves using AI to generate new images based on existing ones, such as creating a character facing a different direction from a set of existing images. This new approach focuses on using all available images of a character to predict missing poses, rather than simply generating images from scratch. By utilizing more information about the character, the AI model can create more realistic and high-quality results, streamlining the character design process and allowing artists to focus on more creative aspects of their work.\n\nRelated work\n\nRecent research in pixel art character generation has focused on improving the quality and versatility of AI-generated images. \nSerpa and Rodrigues (2022)\n significantly enhanced their previous work by reframing the problem as a semantic segmentation task. This approach, combined with architectural modifications like dense connections and deep supervision, resulted in more accurate and detailed character sprites, particularly in challenging poses.\n\nFurthermore, \nCoutinho and Chaimowicz (2024)\n demonstrated the importance of large and diverse datasets for training robust AI models. By compiling a dataset of 14,000 paired images of characters in different poses, they achieved significant improvements in image quality, especially when evaluated on more artistically cohesive datasets. Additionally, they introduced a post-processing step to quantize the generated images to the color palette of the input image, further enhancing the visual fidelity of the results.\n\nMethodology\n\nWe propose an architecture based on \nCollaGAN\n to impute images of pixel art characters in a missing pose (target domain). To facilitate understanding, let us consider that there are domains , one representing each pose. The architecture consists of a single generator and discriminator pair, of which the former creates an image  of a character in the missing pose  using the available images from all of the other source  poses:\n\nOur generator has one encoder branch to process the input from each domain, a single decoder branch with concatenated skip connections, and outputs an image in the missing domain. The discriminator distinguishes images as real or fake, as well as determines their domain through an auxiliary classifier output.\n\nCompared to the original CollaGAN architecture, we proposed the following modifications:\n\nIncreasing the capacity of each convolutional layer by multiplying the number of channels by 4.\nNumber of trainable parameters: 104,887,616.\nUsing an innovative conservative input dropout strategy for the batch selection during training.\nReplacing only the target image with the generated in the forward pass to create new batches for the backwards pass.\n\nWhile the change (1) is straightforward, changes (2) and (3) require explanation. However, to avoid getting too deep into the training procedure, we invite the reader to either \nread the paper\n or to \nwatch a presentation\n. Anyhow, later in this article we present an ablation study that shows how each such modification improved the resulting images.\n\nExperiments\n\nThe model trained with the pixel art characters dataset for 240,000 generator update steps in minibatches of 4 examples, which is equivalent to 80 epochs. It took 01:20h to train using a GeForce GTX 1050 Ti GPU (mid-range from 2016 with 4GB VRAM). After training, it takes 110.03ms for the model to generate a batch of images. The following animation shows the training evolution for 3 images from the training set, and 3 from the test partition.\n\nWe evaluated using the metrics \nFID\n and MAE ( distance between the generated images and the ground truth) against baselines that follow the \nPix2Pix\n and \nStarGAN\n architectures for the image-to-image translation task.\n\nPix2Pix\n (by Isola et al., 2017) is an image-to-image translation GAN architecture that can transfer images from one domain to another. So we trained 16 such models, as we have 4 domains/poses in our problem (4 x 4). Trainable parameters: 351,694,128.\nStarGAN\n (by Choi et al., 2018) tackles the same problem, but it is multi-domain. Hence, it can translate images from and to any domain, requiring only a single generator and discriminator pair. Trainable parameters: 134,448,128.\nResults\n\nNext, we present our main results with both a quantitative and qualitative analysis. But we also invite you to try the model using our \ninteractive generator\n.\n\nReceiving 3 images\n\nThe figure above shows different character examples in its rows, and the columns depict: the source and target images, then the ones generated by Pix2Pix, StarGAN and our MDIGAN (CollaGAN-3) models. As the baseline models take only a single image as input, there are 3 possible outputs for each character.\n\nThe quality of the generated images varies depending on the model and the target pose.\n\nColor Usage: While the generated images generally use colors in appropriate locations, they often exhibit a wider range of tones than the original pixel art style, which typically relies on a limited color palette. This issue can be partially addressed by quantizing the colors after generation, a technique used in previous research (\nCoutinho and Chamowicz, 2024\n).\n\nShape Accuracy:\nEasier directions: All models generally perform well when generating poses that involve simple transformations, such as flipping the character horizontally (e.g., from facing left to right). This is likely due to the relative ease of learning this specific transformation.\nHarder directions: When generating more complex poses, such as a character facing backward, some models, including ours, may exhibit artifacts, such as faint remnants of facial features in unexpected areas (see the maid in the first row).\n\nOverall Quality: We can observe that the quality of images generated by our proposed MDIGAN model is either comparable to or surpasses the performance of baseline models. Notably, the model achieved these results with a significantly smaller number of trainable parameters compared to other prominent architectures, indicating a more efficient use of resources.\n\nOur MDIGAN model is 22% smaller than StarGAN, 70% smaller than Pix2Pix in number of trainable parameters.\n\nReceiving 2 or 1 images\n\nEven though we propose a model to impute a single missing domain, we also evaluate it in scenarios where it receives two (CollaGAN-2) or only one image (CollaGAN-1). The metrics' values are averaged among all targets and all available sources for each model and scenario (i.e., CollaGAN-3, 2, and 1).\n\nThe following table compares the proposed model in those situations. We can observe that both FID and MAE metrics progressively improve as the number of available domains increases, with CollaGAN-2 still having better MAE than Pix2Pix and StarGAN.\n\nModel/Sources\tAverage FID\tAverage MAE\nPix2Pix\t4.091\t0.05273\nStarGAN\t2.288\t0.06577\nCollaGAN-1\t8.393\t0.06449\nCollaGAN-2\t4.277\t0.05035\nCollaGAN-3 🏆\t1.508\t0.04078\n\nThe values of both metrics have been averaged among all possible input/output combinations for each model.\n\nThe figure below shows example generations of the model when it receives 3 inputs (CollaGAN-3), 2 inputs (CollaGAN-2) and only 1 (CollaGAN-1).\n\nInput Dropout Strategy\n\nWe evaluated the impact of different batch selection strategies on presenting examples to the proposed model during training: Should it always see the 3 available domains, or should they sometimes be omitted?\n\nWe investigated the following approaches:\n\nalways showing all available domains (none),\nthe original input dropout strategy proposed in \nCollaGAN\n;\na curriculum learning approach suggested by \nSharma et al., 2019\n;\nand our proposed conservative tactic.\n\nThe original approach has an equal chance of presenting three, two, or a single image in a training step. The curriculum learning approach starts training with easier tasks (using three images) and progressively makes it harder (using a single input) until half of the training, then it randomly chooses between the number of domains to drop out for the second part. Lastly, the conservative approach randomly selects the number of images to drop, but with higher probabilities to keep more images: 60% with 3 images, 30% with 2, and 10% with a single image.\n\nThe following table presents the results. Using any input dropout yields better results than always showing all domains (none). Compared to the original and curriculum learning strategies, our proposed conservative tactic has better FID and MAE metrics on the average of the three scenarios.\n\nFID\tMAE\nSources\tNone\tOriginal\tCurric.\tConserv.🏆\tNone\tOriginal\tCurric.\tConserv.🏆\nCollaGAN-3\t4.816\t1.911\t2.160\t1.508\t0.04523\t0.04277\t0.04222\t0.04078\nCollaGAN-2\t19.050\t6.835\t9.233\t4.277\t0.08003\t0.05053\t0.07389\t0.05035\nCollaGAN-1\t32.676\t11.162\t20.303\t8.393\t0.12820\t0.06243\t0.12232\t0.06449\nAverage\t18.847\t6.636\t10.566\t4.726\t0.08449\t0.05191\t0.07948\t0.05187\nAblation Study\n\nTo understand the impact of our changes to the original CollaGAN architecture, we trained and evaluated models that progressively added each modification. The following table shows the FID and MAE values of the generated images averaged over all domains and among the scenarios of the model receiving three, two, and one input domains. The rows show the results of each modification cumulatively: the first one is the original CollaGAN model without any of our proposed changes, the second introduces the first modification, the third uses two changes, and the last includes all three (our final model).\n\nModification\tAverage FID\tAverage MAE\nValue\tImprov.\tValue\tImprov.\nOriginal\t8.866\t---\t0.06069\t---\n+ Increased capacity\t11.078\t-24.95%\t0.05666\t6.64%\n+ Forward Replacer\t6.636\t25.15%\t0.05191\t14.47%\n+ Conservative Inp. Drop.\t4.726\t46.70%\t0.05187\t14.53%\n\nThe original model had 6,565,712 trainable variables, but with the increased capacity, there are 104,887,616 parameters. That change alone improved MAE but worsened FID. The replacement strategy of substituting only the original target with the image generated in the forward step improves both metrics' results. Lastly, training with the proposed conservative input dropout further enhances the results, with FID and MAE values that are 46.7% and 14.53% better than the original architecture.\n\nConclusion\n\nWe posed the task of generating pixel art characters as a missing data imputation problem and approached it using a deep generative model. It is based on the CollaGAN architecture, from which we proposed changes involving a capacity increase, a conservative input dropout strategy, and a different replacement tactic during the backward step of the training procedure. The experiments showed that all of the changes contributed to achieving better results.\n\nCompared to the baseline models, our approach produces images with similar or better quality when using three domains as input. The model can still produce feasible images in scenarios with fewer available images but with increasingly lower quality.\n\nIn case you're interested in using our architecture, trained model or dataset, please read through.\n\nDataset\n\nWe trained the models using the \nPAC Dataset\n, which consists of 14k pixel art character sprites facing 4 directions: back, left, front, and right.\n\n\nIt can be used for non-commercial purposes only and has been assembled from the \nLiberated Cup Characters\n and RPG Maker RTPs (available online).\n\nCode Repository and Pre-trained Weights\n\nWe used Tensorflow 2.10.1 in Python 3.9 to create and train the model, and the repository is \navailable on Github\n. The weights can be downloaded from HuggingFace through \nfegemo/mdigan-characters\n and used in \nPython\n or \nJavaScript\n (with Tensorflow.js).\n\nInstructions on How to Use in Python\nInstall the Required Libraries\n\nWe tested the code using Python 3.9 and specific versions of tensorflow and numpy, marked below.\n\n# requires Python 3.9\n!pip install numpy==1.24.4 tensorflow==2.10.1 matplotlib notebook patool Pillow\nDownload the Model and Images\n\nThe model is hosted on Github and requires git lfs to download it. We also download some images to test the model.\n\nprint(\"Downloading the model from Github...\")\n!git lfs install\n!git clone --depth=1 --branch=main https://github.com/fegemo/mdigan-characters-model/ weights\n\nprint(\"Extracting some images...\")\nimport patoolib\npatoolib.extract_archive(\"./weights/images.zip\", outdir=\".\")\nLoad the Model\n\nLoad the model from the weights folder.\n\nimport tensorflow as tf\n\nmodel = tf.keras.models.load_model(\"weights\", compile=False)\nLoad some Images\n\nWe load the images of the characters \"dealer\", \"merchant\", and \"cleric\" from the images folder. Each character has four images: one for each direction (back, left, front, right).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ncharacter_names = [\"dealer\", \"merchant\", \"cleric\"]\ncharacter_images = [\n    [\n        np.array(Image.open(f\"images/{name}/back.png\")),\n        np.array(Image.open(f\"images/{name}/left.png\")),\n        np.array(Image.open(f\"images/{name}/front.png\")),\n        np.array(Image.open(f\"images/{name}/right.png\"))\n    ]\n    for name in character_names\n]\n\ncharacter_images = tf.constant(character_images)\ncharacter_images = tf.cast(character_images, tf.float32) / 127.5 - 1\n\nGenerate Images\n\nCreate images by calling model([input_images, missing_indices]), where input_images is a tensor with shape [batch, domains, size, size, channels] with one of the images erased (replaced by zeros) and missing_indices is a tensor with the indices of the missing images (shape [batch]).\n\nfrom image_utils import plot_input_and_output_images\n\n# erase one of the images\nposes = [\"back\", \"left\", \"front\", \"right\"]\n\ndef erase_missing_image(images, indices):\n    np_images = images.numpy()\n    for i, index in enumerate(indices):\n        np_images[i, index] = 0\n    return tf.constant(np_images)\n\nmissing_indices = tf.constant(range(len(character_names)))\ninput_images = erase_missing_image(character_images, missing_indices)\ntarget_images = tf.gather(character_images, missing_indices, axis=1, batch_dims=1)\ngenerated_images = model([input_images, missing_indices])\n\nfig = plot_input_and_output_images(input_images, target_images, generated_images)\nfig.patch.set_alpha(0.0)\nplt.show()\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nRelated work\n\nMethodology\n\nExperiments\n\nResults\n\nReceiving 3 images\n\nReceiving 2 or 1 images\n\nInput Dropout Strategy\n\nAblation Study\n\nView all\nComments\nCode\nDatasets\nFiles\n2409.10721v1.pdf\ntraining-animation.mp4\nYou might be interested\nText to Image Synthesis using DC-GAN\nS\nA\nDec 24, 202424 reads\nComputer VisionDC-GAN+2\nEnhancing Unsupervised Person Re- Identification with GAN-Driven Multi- Pose Augmentation\nH\nDec 31, 20249 reads\nAnalysis of synthetic Hand digit using GAN from MNIST dataset.\nJ\nDec 30, 202435 reads\nGANHand digit recognition+2\nAI Generated Image Detection Using GANs\nS\nR\nDec 30, 202423 reads\nAI Generated ImageDiscrimination models+1",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "meshmetrics-a-simple-and-precise-implementation-of-segmentation-metrics-EBkUTyaTf9F3",
    "username": "g@gasper.podobnik",
    "license": null,
    "title": "MeshMetrics: A Simple and Precise Implementation of Segmentation Metrics",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n66 reads\n●\nApache 2.0\nWinner of\nBest Technical Implementation\nat the\nComputer Vision Projects Expo 2024\nMeshMetrics: A Simple and Precise Implementation of Segmentation Metrics\naverage symmetric surface distan\nboundary intersection over union\ndistance-based metrics\nevaluation\nHausdorff distance\nmean average surface distance\nmetrics\nnormalized surface distance\nsegmentation\nG\n@gasper.podobnik\nT\n@tomaz.vrtovec\nLike\nBookmark\nShare\n\nMeshMetrics is as sweet as M&M's—what better way to measure the precision of your segmentation model's performance? 🪅\n\n⏱️ TL;DR ⏱️\n\nThe evaluation of segmentation performance is a common task in biomedical image analysis, with its importance emphasized in the recently released metrics selection guidelines and computing frameworks. To quantitatively evaluate the alignment of two segmentations, researchers commonly resort to counting metrics, such as the Dice similarity coefficient, or distance-based metrics, such as the Hausdorff distance, which are usually computed by publicly available open-source tools with an inherent assumption that these tools provide consistent results. In this study we questioned this assumption, and performed a systematic implementation analysis along with quantitative experiments on real-world clinical data to compare 11 open-source tools for distance-based metrics computation against our highly accurate mesh-based reference implementation. The results revealed that statistically significant differences among all open-source tools are both surprising and concerning, since they question the validity of existing studies. Besides identifying the main sources of variation, we also provide recommendations for distance-based metrics computation.\n\nOverview of our study, highlighting the key steps and methods used in the analysis.\n\n🚀 Need precise 2D or 3D distance-based segmentation metrics? We've got you covered! 🚀\n\nBelow is a simple usage example of MeshMetrics for 3D segmentation masks. For more examples, check out the \nexamples.ipynb\n notebook.\n\nsudo apt update && sudo apt install -y libxrender1\ngit clone https://github.com/gasperpodobnik/MeshMetrics.git\npip install MeshMetrics/\nfrom pathlib import Path\nimport SimpleITK as sitk\nfrom MeshMetrics import DistanceMetrics\n\ndata_dir = Path(\"data\")\n# initialize DistanceMetrics object\ndist_metrics = DistanceMetrics()\n\n# read binary segmentation masks\nref_sitk = sitk.ReadImage(data_dir / \"example_3d_ref_mask.nii.gz\")\npred_sitk = sitk.ReadImage(data_dir / \"example_3d_pred_mask.nii.gz\")\n\n# set input masks and spacing (only needed if both inputs are numpy arrays or vtk meshes)\ndist_metrics.set_input(ref=ref_sitk, pred=pred_sitk)\n\n# Hausdorff Distance (HD), by default, HD percentile is set to 100 (equivalent to HD)\nhd100 = dist_metrics.hd()\n# 95th percentile HD\nhd95 = dist_metrics.hd(percentile=95)\n# Mean Average Surface Distance (MASD)\nmasd = dist_metrics.masd()\n# Average Symmetric Surface Distance (ASSD)\nassd = dist_metrics.assd()\n# Normalized Surface Distance (NSD) with tau=2\nnsd2 = dist_metrics.nsd(tau=2)\n# Boundary Intersection over Union (BIoU) with tau=2\nbiou2 = dist_metrics.biou(tau=2)\n⬇️ Discover why our implementation is more accurate than other open-source tools ⬇️\n🔧 Comparison of 11 open-source tools 🔧\n\nOverview of 11 open-source tools analyzed in this study, indicating the supported distance-based metrics. For HDp, the checkmark (✓) denotes the support of any percentile, whereas a specific number indicates the implementation of a predefined percentile (e.g. 95-th).\n\n🧮 Deep dive into mathematical definitions 🧮\n\nIn their straightforward application, all distance-based metrics compare two segmentations,  and , which are commonly represented as binary segmentation masks  and  in biomedical image analysis. The mathematical definitions of HD (and HDp), MASD, and ASSD are based on two finite sets  and , representing clouds of  and  points that describe the surfaces of  and , respectively. These sets are used to compute distances between the segmentations (see figure below, cf. PointDef).\n\nAssuming  denotes the set of one-directional (asymmetric) distances from points  to , and  denotes the -th percentile of , HDp measures the largest among the bi-directional (symmetric) percentile distances  and : HDp = . The 100th percentile variant, HD100, or simply HD, is computed by replacing the percentile calculation with the maximum over both sets: HD = . Other variants are less sensitive to outliers, such as noise and artifacts. The 95th percentile variant, HD95, is the most established one.\n\nOften referred to as the average surface distance, MASD computes the mean of the two average distances of sets  and , while ASSD computes the mean of the union of both sets  and .\n\nOn the other hand, NSD and BIoU are defined using the boundary representation of the segmentation masks. Here, the term \"boundary\" is context agnostic regarding dimensionality, referring to a contour line in 2D and a surface in 3D. Also referred to as the normalized surface Dice, NSD quantifies the overlap of the two segmentation boundaries,  and , within a specified margin of error (). A region  is first defined both inward () and outward () of boundary . NSD is then computed as the ratio of  within  and vice versa, against the total size of both boundaries, represented by the contour circumference in 2D and surface area in 3D.\n\nFinally, BIoU enhances sensitivity to boundary segmentation errors compared to plain IoU or DSC, which are often saturated for bulk overlaps between two large segmentations. A region  is first defined inward () of , and BIoU is computed by measuring the intersection of  and  over their union.\n\nMathematical definitions of distance-based metrics as adopted by the point-based definition PointDef, different open-source tools, and our reference implementation MeshMetrics.\n\n⚠️ Limitations ⚠️\n\nThe mathematical definitions of HD with HDp, MASD, and ASSD rely on distances between the point sets describing the boundaries of  and . In the discrete image space, these boundaries are represented by line polygons in 2D and surface meshes in 3D. However, these point sets lack complete information about the segmentation boundaries, making them a lossy representation of the segmentation masks.\n\nThe point-based definitions of distance-based metrics inherently neglect the spatial distribution of points across the boundaries, assuming they are uniformly distributed. Ensuring that each point used for distance calculation (referred to as the query point) corresponds to boundary regions of uniform size is practically challenging and introduces bias into the metrics computation.\n\nFor unbiased computation, the size of the boundary element —represented by the length of the line segment in 2D and the area of the (triangular) surface element in 3D — must be taken into account at each query point. The distances calculated between query points and the opposing boundary must then be weighted by the corresponding boundary element sizes, as supported by both theoretical perspectives and experimental results.\n\n📐 Conceptual Analysis 📐\n\nA detailed analysis of open-source tools revealed notable differences in metrics calculation strategies (see figure with equations above) and the applied boundary extraction methods (see figure below).\n\nA critical pitfall in the implementation of distance-based metrics is boundary extraction, where the first issue is the foreground- vs. boundary-based calculation dilemma. Among the 11 open-source tools, EvaluateSegmentation and SimpleITK are the only two that omit the boundary extraction step and instead calculate distances between all mask foreground elements, using the pixel/voxel centers as query points. The authors of EvaluateSegmentation even proposed several optimization strategies to improve computational efficiency, such as excluding intersecting elements, since their distances are always zero.\n\nPlastimatch is the only tool that returns both foreground- and boundary-based calculations, while other tools support only boundary-based calculations. The most frequently employed boundary extraction method, used by Anima, MedPy, MetricsReloaded, MISeval, MONAI, and Plastimatch, involves morphological erosion using an 8-square-connectivity structural element. In contrast, seg-metrics uses a full-connectivity structural element.\n\nConversely, Google DeepMind and pymia employ a strategy where the image grid is shifted by half a pixel/voxel size and incorporate the calculation of boundary element sizes, i.e., the lengths of line segments in 2D and areas of surface elements in 3D. As previously explained, EvaluateSegmentation computes distances solely for non-overlapping elements, while SimpleITK computes distances for all foreground elements.\n\nDifferently from all the 11 open-source tools, our reference implementation MeshMetrics adopts a meshing boundary extraction strategy using discrete flying edges in 2D and discrete marching cubes in 3D.\n\nOverview of the boundary extraction methods used by the 11 open-source tools and our reference implementation, MeshMetrics. All methods are demonstrated using 2D examples but can be seamlessly extended to 3D.\n\n🔬 Quantitative Experiments 🔬\n\nTo quantitatively evaluate the combined effect of variations in mathematical definitions and boundary extraction methods on the resulting metric scores, we designed a series of experiments. These experiments assess the accuracy of the 11 open-source tools for distance-based metrics computation, along with an analysis of edge case handling and computational efficiency.\n\nExperimental Design\n\nIn contrast to counting metrics, distance-based metrics rely on distance calculations and thus require the image grid to be defined in distance units, i.e., with pixel size  or voxel size . For our 3D CT and MR images, we conducted experiments using three different voxel sizes:\n\n, simulating the vanilla scenario with isotropic voxels of unit size.\n, a commonly used isotropic voxel size.\n, a commonly used anisotropic voxel size in the radiotherapy workflow.\n\nThe experiments followed this procedure (see diagram below):\n\nFor a chosen CT or MR image, the two OAR segmentation masks were loaded with their original image voxel size.\nMeshing was performed to extract triangle meshes from the segmentation masks.\nMesh rasterization (i.e., voxelization in our 3D case) was performed to obtain each of the three voxel sizes, which were then used to compute the distance-based metrics by each of the 11 open-source tools.\nMeshing was again performed for each voxelized mask to compute the distance-based metrics using our reference implementation, MeshMetrics.\n\nFor metrics that depend on user-defined parameters (e.g., percentile  for HDp, and boundary margin of error  for NSD and BIoU), we conducted experiments with HD (p = 95), NSD (), and BIoU (), as these values are commonly used in existing studies and provide good insight into the differences among the tools.\n\nExperimental design for distance-based metrics computation using the 11 open-source tools and our reference implementation, MeshMetrics. Meshes are first generated from the original segmentation masks, followed by rasterization (voxelization) to three different voxel sizes. The rasterized (voxelized) masks are then used as inputs to the open-source tools, while corresponding highly accurate meshes are generated and used as inputs to MeshMetrics.\n\nEvaluation of Differences\n\nComparing different implementations for individual OARs would be challenging due to the disparity of results. Therefore, we focus on analyzing the deviations between each open-source tool and MeshMetrics, defined as: , where  is the metric score of the -th open-source tool and  is the reference metric score of MeshMetrics.\n\nDistance-based metrics can be divided into two groups:\n\nAbsolute metrics (HD with (HDp), MASD, and ASSD): These are measured in metric units (i.e., millimeters for our case) and have no upper bound. A lower score reflects greater similarity between two masks.\n\nRelative metrics (NSD and BIoU): These are unitless and bounded between 0 and 1. A higher score reflects greater similarity between two masks.\n\nIn this analysis, positive  values indicate an overestimation of the metric score (i.e., over-pessimistic for absolute and over-optimistic for relative metrics), while negative  values indicate an underestimation of the metric score (i.e., over-optimistic for absolute and over-pessimistic for relative metrics).\n\nAlthough both over- and under-estimation are undesirable, over-optimistic estimates are particularly concerning, as they may lead to incorrect conclusions, especially when compared to metric scores reported in existing studies.\n\n📊 Results 📊\n\nThe differences () in the distance-based metrics, as obtained by each open-source tool against the reference implementation MeshMetrics on 1,561 pairs of segmentation masks, are reported for two isotropic and one anisotropic voxel size in the form of box plots. Note that a linear scale is applied for values within the interval , and a symmetric logarithmic scale is used for values outside this interval to better visualize both smaller and larger differences.\n\n🧵 Discussion 🧵\n\nThe results presented in the boxplot figure above reveal that for HD, the mean deviations are close to zero across all tools except for MISeval, which highlights the general agreement on its mathematical definition, supported by mostly non-significant statistical comparisons (see our full paper for details). For HD95, the results are more scattered due to larger differences in its mathematical definition. Particularly for large and tubular OARs, Plastimatch produced the greatest outliers of up to −115 mm due to the averaging of the directed percentile calculations rather than taking the maximum of both directed percentiles, which generates over-optimistic results. Similarly, over-optimistic performance is observed for MedPy and seg-metrics, both computing the percentile on the union of both distance sets that generally produces smaller metric values when coupled with their boundary extraction methods.\n\nAlthough EvaluateSegmentation applies the same calculation, it uses a different boundary extraction method that shifts the distribution toward higher distances and results in over-pessimistic performance. While MONAI and MetricsReloaded appear as the most accurate for isotropic grids, they exhibit several outliers and perform worse for the anisotropic grid. Notably, Google DeepMind is not only very close to these tools in terms of mean deviation for isotropic grids but also exhibits greater precision, outperforming all other tools in both accuracy and precision for the anisotropic grid.\n\nThese findings align well with our conceptual analysis, as tools relying on the point-based definition (i.e., assuming a uniform distribution of query points) performed significantly worse than Google DeepMind and pymia, which account for boundary element sizes. Interestingly, low deviations in mean differences are observed for MASD and ASSD across all open-source tools, as averaging over large sets of distances is statistically more stable and tends to conceal the variations in metrics implementation. However, it is crucial to also consider the range (min/max) of deviations, which further emphasize the need for unified mathematical definitions.\n\nFor example, the comparison of Google DeepMind and MetricsReloaded clearly demonstrates that the former is more consistent. Particularly for ASSD, these discrepancies are even more pronounced for the anisotropic grid. Although supported by four open-source tools, only two distinct calculations are employed for NSD: a point-based method by MetricsReloaded/MONAI, and a mesh-based method by GoogleDeepMind/pymia, with identical metric scores within pairs due to the same boundary extraction method. However, concerning outliers ranging from −16.1%pt to 34.8%pt are consistently observed across OAR categories for both calculations, causing statistically significant differences against MeshMetrics and among the tools.\n\nAs GoogleDeepMind, MetricsReloaded, and MONAI are deemed popular according to their GitHub stars ranking, questions arise about the accuracy of existing studies applying these tools. For MetricsReloaded and MONAI, the outliers can partly be attributed to not accounting for boundary element sizes, leading to an unjustified assumption of uniformly distributed query points. Surprisingly, even Google DeepMind, the original NSD implementation, shows signs of inaccuracy and imprecision, likely due to quantized distances coupled with the choice of τ. As MeshMetrics uses implicit distance calculations, it is considerably less susceptible to quantization.\n\nThese empirical findings highlight that, among all distance-based metrics, NSD is the most sensitive to distance and boundary calculations. Since BIoU is a recently proposed metric, it is currently implemented only by MetricsReloaded, which provides valid results only for the isotropic grid of unit size due to a flaw in its code. However, even in this case, there are significantly large deviations from MeshMetrics ranging from −15.5%pt to 35.5%pt, which can be attributed to quantized distances coupled with the choice of τ. We therefore suggest exercising caution when interpreting NSD and BIoU.\n\n❗ Conclusion ❗\n\nIn conclusion, we would first like to acknowledge the authors of the 11 open-source tools for their valuable contributions. The outcomes of our study should not be regarded as criticism, but rather as a constructive step towards proper metrics implementation and usage.\n\nBased on our detailed conceptual and quantitative analyses, we propose the following recommendations for distance-based metrics computation:\n\nConsult metrics selection guidelines, such as Metrics Reloaded, to identify representative metrics for the specific application.\nEnsure reproducibility by reporting the name and version of the open-source tool, as the choice of the tool can significantly impact the obtained results and their interpretation.\nBe mindful of edge case handling by not relying solely on open-source tools, but by implementing additional external conditioning.\nPay attention to the correct usage of units of measurement and pixel/voxel sizes.\nUse MeshMetrics for highly accurate reference measurements. If not feasible, use Google DeepMind for HD, HDp, MASD, and ASSD due to its superior performance on both isotropic and anisotropic grids in comparison to other open-source tools, and exercise caution for NSD and BIoU due to implementation discrepancies.\n\nWe therefore hope that Metrics Revolutions with its groundbreaking insights will raise community awareness and understanding of the computational principles behind the distance-based metrics, and stimulate community members towards a more careful interpretation of segmentation results for both existing and future studies.\n\nAs our findings suggest, studies that evaluated segmentation by using the 11 open-source tools may need to be revisited for implementation errors, and we eagerly contribute to this effort by offering MeshMetrics as a reference for the implementation of metrics for biomedical image segmentation.\n\n❣️Acknowledgements ❣️\n\nThis study was supported by the Slovenian Research and Innovation Agency (ARIS) under projects No. J2-1732, J2-4453, J2-50067 and P2-0232, and by the European Union Horizon project ARTILLERY under grant agreement No. 101080983.\n\n📖 Note 📖\n\nThis is a summarized version of our full paper, which includes a more in-depth analysis of computational efficiency and statistical testing. The full paper is available \nhere\n.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\n⏱️ TL;DR ⏱️\n\n🚀 Need precise 2D or 3D distance-based segmentation metrics? We've got you covered! 🚀\n\n⬇️ Discover why our implementation is more accurate than other open-source tools ⬇️\n\n🔧 Comparison of 11 open-source tools 🔧\n\n🧮 Deep dive into mathematical definitions 🧮\n\n⚠️ Limitations ⚠️\n\n📐 Conceptual Analysis 📐\n\n🔬 Quantitative Experiments 🔬\n\nExperimental Design\n\nEvaluation of Differences\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nFiles\n2410.02630v1.pdf\nYou might be interested\nFoam Imaging Segmentator: Advanced Bubble and Foam Analysis Toolkit\nDec 19, 202420 reads\nAnalysis toolkitBubble detetction+6\nUnsupervisedCellImageSegmentation\nP\nDec 29, 202416 reads\nCardiac MRI Segmentation with Hybrid Models\nB\nJ\nDec 21, 202456 reads\n#CardiacMRI#DeepLearning+6\nDentalVis: Multiclass Tooth Segmentation for Panoramic X-rays\nDistinguished Applied Solution Showcase\nB\nDec 25, 202477 reads\nArtificial IntelligenceComputer Vision+4",
    "awards": [
      "best technical implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "metacentrum-metavo-hugging-face-models-running-scripts-w3Y8qwuB188R",
    "username": "dDavid Chocholatý",
    "license": "MIT License",
    "title": "MetaCentrum (MetaVO) – Scripts for Running Hugging Face Models",
    "publication_description": "Back to publications\nOct 22, 2025\n●\n1 reads\n●\nMIT License\nMetaCentrum (MetaVO) – Scripts for Running Hugging Face Models\nhugging face\nLLMs\nmetacentrum\ntext generation\nD\nDavid Chocholatý\nLike\nBookmark\nShare\nMetaCentrum HPC Cluster - Hugging Face Transformers Demo\n\nAs a student at \nBUT FIT\n, I had an incredible opportunity to access and use the \nMetaCentrum\n HPC cluster. However, setting up the environment and running even a small PyTorch demo can be quite challenging, especially for beginners. Since many students face these difficulties, I created and published a repository containing a simple demo that shows how to run text generation using Hugging Face Transformers and experiment with pre-trained models. I originally shared this code with my fellow students, and it would be a shame if it weren’t available to the general public using this service.\n\nOkay, let’s do it. 🚀\n\nGeneral Workflow\n\nThe published \nGitHub repository\n contains all the source code needed to run the main Python script. First, you need to log in to one of the \nlog in nodes\n (also called frontend nodes). For example, to log in to the tarkil.grid.cesnet.cz frontend node:\n\nssh your_username@tarkil.grid.cesnet.cz\n\n\nThen, clone the repository (or your fork) preferably via SSH:\n\n(BOOKWORM)your_username@tarkil:~$ git clone git@github.com:davidchocholaty/hugging-face-transformers.git\n\nNote: You first need to set up your GitHub SSH key, which will later be copied to the working node.\n\nThe GitHub SSH key is used to clone, pull, and push repositories via SSH.\n\nGenerate an SSH key on Metacentrum using:\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n\n\nAdd the public key to GitHub: go to GitHub → Settings → SSH and GPG keys → New SSH key and add your public key as Authentication Key.\n\nIn your .ssh folder, you should have both the private and public keys:\n\nid_ed25519       # your private key\nid_ed25519.pub   # your public key\n\nRepository Description\nThe main Python script\nConfiguration files\nBash scripts for running jobs\nRepository Structure\n.\n├── configs/                 # Configuration files for experiments and models\n├── metacentrum_scripts/     # Scripts for submitting jobs to the MetaCentrum HPC cluster\n├── demo.py                  # Main demo script for text generation using a specified model\n├── results/                 # Directory for storing experiment logs and outputs\n└── README.md                # This file\n\nConfiguration Files\n\nThe configs/ folder contains YAML configuration files defining experiment setups.\nEach config typically includes:\n\nmodel: path or name of the Hugging Face model\ndatasets: optional datasets for training or evaluation (not used for text generation)\ntraining: parameters such as batch size, number of epochs, optimizer settings (not used for text generation)\n\nExample YAML (configs/demo_mistral_Mistral-7B-Instruct-v0.3.yaml):\n\ndesc: \"Baseline experiment. Learning rate scheduler is linear, training 5 epochs.\"\n\nmodel:\n  name: \"Mistral-7B-Instruct-v0.3\"\n  desc: \"Mistral AI model.\"\n  path: \"mistralai/Mistral-7B-Instruct-v0.3\"\n\n# Not used yet. Kept for demonstration purposes only.\ndatasets:\n  cnec2:\n    name: \"CNEC 2.0 CoNLL\"\n    desc: \"Czech Named Entity Corpus 2.0 CoNNL dataset. General-language Czech NER dataset.\"\n    url_path: \"https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3493/cnec2.0_extended.zip\"\n  medival:\n    name: \"Medieval text\"\n    desc: \"A Human-Annotated Dataset for Language Modeling and Named Entity Recognition in Medieval Documents\"\n    url_path: \"https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-5024/named-entity-recognition-annotations-large.zip?sequence=2&isAllowed=y\"\n  wikiann:\n    name: \"Wikiann\"\n    desc: \"WikiANN (sometimes called PAN-X) is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated\"\n  slavic:\n    name: \"Slavic\"\n    desc: \"Slavic documents\"\n    url_train: \"https://bsnlp.cs.helsinki.fi/bsnlp-2021/data/bsnlp2021_train_r1.zip\"\n    url_test: \"https://bsnlp.cs.helsinki.fi/bsnlp-2021/data/bsnlp2021_test_v5.zip\"\n\n# Not used yet. Kept for demonstration purposes only.\ntraining:\n  num_train_epochs: 5\n  batch_size: 32\n\n  optimizer:\n    learning_rate: 5e-5\n    weight_decay: 0.01\n    beta1: 0.9\n    beta2: 0.999\n    eps: 1e-8\n  lr_scheduler:\n    name: \"linear\"\n    num_warmup_steps: 0\n\nScripts for Submitting Jobs\n\nScripts in this folder help submit experiments to the \nMetaCentrum\n HPC cluster.\n\nTypical tasks include:\n\nDefining job resources (CPU, GPU, memory, walltime)\nSetting up environment modules (Python, CUDA, etc.)\nExecuting demo.py with the correct config\n\nThe following command is how you will run the code on the computing node. All the logic is contained in the run_job.sh and prepare_node.sh scripts.\n\nUsage\n./run_job.sh <branch_name> <config_file_name> <timeout>\n\nExample Usage\n./run_job.sh main demo_mistral_Mistral-7B-Instruct-v0.3 01:00:00\n\nMain Python Script (Description Only)\n\nThe demo.py file is a standalone Python script for running a text-generation demo using Hugging Face Transformers.\n\nFeatures:\n\nLoads configuration from a YAML file\nInitializes a Hugging Face pipeline for text generation\nLogs experiment outputs to results/experiment_results.txt\nPrints and logs generated text outputs\nExample Usage\npython demo.py --config configs/my_experiment.yaml\n\n\nWorkflow:\n\nLoad configuration from YAML file\nInitialize the text-generation pipeline using the specified model\nGenerate a response for a sample message\nLog the generated output to results/experiment_results.txt\nSample Output:\nGenerated Output: [{'generated_text': \"I am an AI assistant designed to help with various tasks.\"}]\n\n\nAwesome! You’ve just completed the most challenging part. Congratulations! 🎉\n\nAcknowledgements\n\nI would like to sincerely thank Kidist Demessie for contacting me and inviting me to this platform. I really appreciate her support in making this article possible.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nMetaCentrum HPC Cluster - Hugging Face Transformers Demo\n\nGeneral Workflow\n\nNote: You first need to set up your GitHub SSH key, which will later be copied to the working node.\n\nRepository Description\n\nRepository Structure\n\nConfiguration Files\n\nScripts for Submitting Jobs\n\nUsage\n\nExample Usage\n\nMain Python Script (Description Only)\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "miniagents-multi-agent-ai-with-procedural-simplicity-sZ9xgmyLOTyp",
    "username": "Oleksandr Tereshchenko",
    "license": "MIT License",
    "title": "MiniAgents: Multi-Agent AI Framework With Procedural Simplicity",
    "publication_description": "Back to publications\nMar 31, 2025\n●\n153 reads\n●\nMIT License\nWinner of\nBest Technical Implementation\nat the\nAgentic AI Innovation Challenge 2025\nMiniAgents: Multi-Agent AI Framework With Procedural Simplicity\nAgentic AI\nGenerative AI\nLLM\nMulti-Agent Framework\nPython\nOleksandr Tereshchenko\nLike\nBookmark\nShare\nMiniAgents: Multi-Agent AI Framework With Procedural Simplicity\n\nAn open-source, async-first Python framework for building multi-agent AI systems with an innovative approach to parallelism, so you can focus on creating intelligent agents, not on managing the concurrency of your flows.\n\nTo install MiniAgents run the following command:\n\npip install -U miniagents\n\nThe source code of the project is hosted on \nGitHub\n.\n\nWhy MiniAgents\n\nWrite procedural code, get parallel execution: Unlike graph-based frameworks that force you to think in nodes and edges, MiniAgents lets you write straightforward sequential code while the framework handles the complexities of parallel execution for agent interactions automatically. Your code stays clear and readable.\n\nNothing blocks until it's needed: With its innovative promise-based architecture, agents execute in parallel. Execution only blocks at points where specific agent messages are actively awaited. Agents communicate through replayable promises of message sequences, not just concrete messages or single-pass async generators. This replayability is a key distinction, allowing message streams to be consumed multiple times by different agents or for different purposes, fostering flexible data flows and enabling maximum concurrency without complex manual synchronization code.\n\nImmutable message philosophy: MiniAgents uses immutable, Pydantic-based messages that eliminate race conditions and data corruption concerns. This design choice enables highly parallelized agent execution without many of the common headaches of state management. (While messages themselves are immutable, state can still be maintained across multiple invocations of agents by using forked agent instances, a pattern demonstrated later in this tutorial.)\n\nFundamental Agent Contract: Every miniagent adheres to a simple contract: it receives a promise of an input sequence of message promises, arbitrary keyword arguments passed to it (which are all automatically \"frozen\" unless passed via non_freezable_kwargs upon forking an agent, which will be explained later in this tutorial), and in return, it produces a promise of a reply sequence of message promises.\n\nSequential Appearance, Parallel Reality via Promises: MiniAgents achieves this seamless blend of procedural style and concurrent execution through one of its core mechanisms: \"Message Sequence Flattening\". Here is a very simple example (a more complex example will be shown later in this tutorial):\n\n@miniagent\nasync def aggregator_agent(ctx: InteractionContext) -> None:\n    # This looks sequential but all agents start working in parallel\n    ctx.reply([\n        # All nested sequences here will be asynchronously \"flattened\" in the\n        # background for an outside agent (i.e., the one triggering the\n        # `aggregator_agent`), so that when the outside agent consumes the\n        # result, it appears as a single, flat sequence of messages.\n        \"Starting analysis...\",\n        research_agent.trigger(ctx.message_promises),\n        calculation_agent.trigger(ctx.message_promises),\n        \"Analysis complete!\",\n    ])\nLet's build a Web Research System with MiniAgents\n\nIn this tutorial we'll build a system that can:\n\nBreak down a user's question into search queries\nExecute searches in parallel\nAnalyze the search results to identify relevant web pages\nScrape and extract information from those pages\nSynthesize a comprehensive answer\n\nThe animation above demonstrates what the output of this system will look like.\n\nNOTE: The complete source code is available \nhere\n.\n\n\"Message Sequence Flattening\"\n\nLet's start by exploring MiniAgents' central feature - \"Message Sequence Flattening\". We will explain what this means and how it simplifies building concurrent applications shortly. For this, we will build the first, dummy version of our Web Research System. We will not use the real LLM, we will not do the actual web searching and scraping and we will not create the \"Final Answer\" agent just yet. For now, we will put asyncio.sleep() with random delays to emulate activivity instead. Later in the tutorial, we will replace these delays with real web search, scraping and text generation operations.\n\nNaive alternative to \"Message Sequence Flattening\"\n\nFirst, let's look at how you might approach the problem of orchestrating multiple asynchronous operations (like our dummy agents performing research, web searching, and page scraping) and combining their results using standard Python async generators in a naive way. This will help understand the challenges that MiniAgents solves.\n\nThis approach uses standard Python async def with async for ... yield to simulate how one might try to build a similar system without MiniAgents. The key thing to note here is that this method leads to sequential execution of the \"agents\" (async generators in this case).\n\nThe full code for this naive example can be found in \nsequence_flattening_naive_alternative.py\n.\n\nLet's look at the agent definitions and how they are run:\n\n# examples/sequence_flattening_naive_alternative.py\nimport asyncio\nimport random\nfrom typing import AsyncGenerator\n\n\nasync def research_agent_naive(question: str) -> AsyncGenerator[str, None]:\n    \"\"\"\n    The main research agent, implemented naively.\n    Calls web_search_agent_naive sequentially.\n    \"\"\"\n    yield f\"RESEARCHING: {question}\"\n\n    for i in range(3):\n        query = f\"query {i+1}\"\n        # The `async for ... yield` construct processes the generator\n        # sequentially. The `web_search_agent_naive` for \"query 2\" will\n        # only start after the one for \"query 1\" (including all its\n        # pretend-scraping) is finished.\n        async for item in web_search_agent_naive(search_query=query):\n            yield item\n\n\nasync def web_search_agent_naive(search_query: str) -> AsyncGenerator[str, None]:\n    \"\"\"\n    Pretends to perform a web search and trigger scraping.\n    Calls page_scraper_agent_naive sequentially.\n    \"\"\"\n    yield f\"{search_query} - SEARCHING\"\n    await asyncio.sleep(random.uniform(0.5, 1))\n    yield f\"{search_query} - SEARCH DONE\"\n\n    for i in range(2):\n        url = f\"https://dummy.com/{search_query.replace(' ', '-')}/page-{i+1}\"\n        # This also leads to sequential execution\n        async for item in page_scraper_agent_naive(url=url):\n            yield item\n\n\nasync def page_scraper_agent_naive(url: str) -> AsyncGenerator[str, None]:\n    \"\"\"\n    Pretends to scrape a web page.\n    \"\"\"\n    yield f\"{url} - SCRAPING\"\n    await asyncio.sleep(random.uniform(0.5, 1))\n    yield f\"{url} - DONE\"\n\n\nasync def stream_to_stdout_naive(generator: AsyncGenerator[str, None]):\n    \"\"\"\n    Consumes the async generator and prints yielded strings.\n    \"\"\"\n    i = 0\n    async for message in generator:\n        i += 1\n        print(f\"String {i}: {message}\")\n\n\nasync def main_naive():\n    \"\"\"\n    Main function to trigger the naive research agent and print the results.\n    \"\"\"\n    question = \"Tell me about MiniAgents sequence flattening (naive version)\"\n    # Get the async generator\n    result_generator = research_agent_naive(question)\n\n    print()\n    print(\"=== Naive Async Generator Execution ===\")\n    print()\n\n    # Consume and print results\n    await stream_to_stdout_naive(result_generator)\n\n    print()\n    print(\"=== End of Naive Execution ===\")\n    print()\n\n    print(\"Attempting to reiterate (WILL YIELD NOTHING):\")\n\n    # NOTE: Re-iterating a standard async generator like this is not possible.\n    # Once consumed, it's exhausted. This contrasts with MiniAgents promises,\n    # which are replayable.\n    await stream_to_stdout_naive(result_generator)  # This won't print anything\n\n    print(\"=== End of Reiteration Attempt ===\")\n    print()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main_naive())\n\nIn research_agent_naive, the loop that calls web_search_agent_naive processes each \"search query\" one after the other. Similarly, web_search_agent_naive waits for page_scraper_agent_naive to complete for one URL before processing the next.\n\nHere is what this process looks like as a result:\n\nTo achieve true concurrency with this naive approach, you would need to manually manage asyncio.create_task for each sub-operation and potentially use queues or other synchronization primitives to collect and yield results as they become available. This would significantly increase code complexity.\n\nFurthermore, standard async generators, once consumed, are exhausted. If you try to iterate over the result_generator in main_naive a second time, it will yield nothing. This contrasts with MiniAgents' replayable promises.\n\nThis manual management is typical when using raw asyncio or even foundational async libraries like trio or anyio. While these libraries provide powerful concurrency tools, MiniAgents aims to provide a higher-level, agent-centric abstraction for these patterns, particularly around how agents stream and combine their results.\n\nReal \"Message Sequence Flattening\" with MiniAgents\n\nNow, let's see how MiniAgents addresses these challenges, enabling concurrent execution while keeping the same level of code simplicity. You'll see what the changed version of the same code looks like in a moment, but first, let's jump ahead and take a look at how different its output is going to be:\n\nLooks much faster, doesn't it? Now, back to the code. The full example that uses MiniAgents can be found in \nsequence_flattening.py\n.\n\nHere are the agent definitions, along with the main execution logic:\n\n# examples/sequence_flattening.py\nimport asyncio\nimport random\n\nfrom miniagents import (\n    InteractionContext,\n    Message,\n    MessageSequencePromise,\n    MiniAgents,\n    miniagent,\n)\n\n\n@miniagent\nasync def research_agent(ctx: InteractionContext) -> None:\n    ctx.reply(f\"RESEARCHING: {await ctx.message_promises.as_single_text_promise()}\")\n\n    for i in range(3):\n        # `trigger` returns immediately with a `MessageSequencePromise`.\n        # The actual execution of `web_search_agent` starts in the background.\n        # `reply_out_of_order` delivers messages as they become available,\n        # not strictly in the order they are added to the reply.\n        ctx.reply_out_of_order(\n            web_search_agent.trigger(\n                ctx.message_promises, # Original question\n                search_query=f\"query {i+1}\",\n            )\n        )\n\n\n@miniagent\nasync def web_search_agent(ctx: InteractionContext, search_query: str) -> None:\n    ctx.reply(f\"{search_query} - SEARCHING\")\n    await asyncio.sleep(random.uniform(0.5, 1)) # Simulate work\n    ctx.reply(f\"{search_query} - SEARCH DONE\")\n\n    for i in range(2):\n        # Again, no `await` here when triggering the next agent.\n        # The page_scraper_agent will run in parallel.\n        ctx.reply_out_of_order(\n            page_scraper_agent.trigger(\n                ctx.message_promises, # Original question\n                url=f\"https://dummy.com/{search_query.replace(' ', '-')}/page-{i+1}\",\n            )\n        )\n\n\n@miniagent\nasync def page_scraper_agent(ctx: InteractionContext, url: str) -> None:\n    ctx.reply(f\"{url} - SCRAPING\")\n    await asyncio.sleep(random.uniform(0.5, 1)) # Simulate work\n    ctx.reply(f\"{url} - DONE\")\n\n\nasync def stream_to_stdout(promises: MessageSequencePromise):\n    \"\"\"\n    As we iterate through the `response_promises` sequence, asyncio switches\n    tasks, allowing the agents (`research_agent`, `web_search_agent`,\n    `page_scraper_agent`) to run concurrently in the background and resolve\n    their promises. The `async for` loop seamlessly receives messages from the\n    flattened sequence, regardless of how deeply nested their origins were\n    (research_agent -> web_search_agent -> page_scraper_agent).\n    \"\"\"\n    i = 0\n    async for message_promise in promises:\n        i += 1\n        print(f\"{message_promise.message_class.__name__} {i}: \", end=\"\")\n        async for token in message_promise:\n            print(token, end=\"\")\n        print()\n\n\nasync def main():\n    \"\"\"\n    Main function to trigger the research agent and print the results.\n    Demonstrates how the flattened sequence of messages is consumed.\n    \"\"\"\n    # Trigger the top-level agent. This returns a MessageSequencePromise.\n    # No processing has started yet.\n    response_promises = research_agent.trigger(\n        \"Tell me about MiniAgents sequence flattening\"\n    )\n\n    print()\n\n    await stream_to_stdout(response_promises)\n\n    print()\n    print(\"=== REPLAYING MESSAGES ===\")\n    print()\n\n    # If we iterate through the sequence again, we will see that exactly the\n    # same messages are yielded again (and in exactly the same order). This\n    # demonstrates the replayability of all types of promises in MiniAgents.\n    #\n    # Replayability is useful because it allows you to feed the same sequences\n    # (be it responses from agents, or input to the current agent) to multiple\n    # other agents without even thinking that those sequences might already be\n    # \"exhausted\" in a traditional, async generator sense.\n    await stream_to_stdout(response_promises)\n\n    print()\n    print(\"=== REPLAYING MESSAGES AGAIN ===\")\n    print()\n\n    # We can even await for the whole MessageSequencePromise to get the\n    # complete tuple of resolved messages (demonstrating the replayability of\n    # the promises once again).\n    messages: tuple[Message, ...] = await response_promises\n    for i, message in enumerate(messages):\n        # When you run this example, you will see that for agents replying with\n        # simple strings, they are automatically wrapped into TextMessage\n        # objects (a subclass of Message).\n        print(f\"{type(message).__name__} {i+1}: {message}\")\n\n    print()\n\n\nif __name__ == \"__main__\":\n    # The MiniAgents context manager orchestrates the async execution.\n    MiniAgents().run(main())\n\nIn the MiniAgents version:\n\nWhen research_agent calls web_search_agent.trigger(...), this call is non-blocking. It immediately returns a MessageSequencePromise. The actual execution of web_search_agent starts in the background when the asyncio event loop gets a chance to switch tasks.\nThe ctx.reply(...) method (and its variant ctx.reply_out_of_order(...)) is versatile. It can accept:\nInstances of Message (or its subclasses), or other concrete Python objects (like strings, dictionaries, or arbitrary Pydantic models). If not already Message objects, these are automatically wrapped into appropriate framework-specific Message types (e.g., TextMessage).\nPromises of individual messages (MessagePromise).\nPromises that resolve to a sequence of individual message promises (MessageSequencePromise), such as those returned by agent.trigger().\nCollections (lists, tuples, etc.) containing any mix of the above.\nMiniAgents automatically \"flattens\" this potentially deeply nested structure of messages and promises. When the main function (or another agent) consumes the response_promises from research_agent, it receives a single, flat sequence of all messages. This sequence includes messages produced directly by research_agent, all messages from all the triggered web_search_agent instances, and consequently, all messages from all the page_scraper_agent instances called by them.\nA key aspect to remember is that sequence flattening happens both when you pass input to an agent (which can be concrete messages, promises, or collections of either) and when an agent replies with a promise of a message sequence (MessageSequencePromise).\nThe async for message_promise in promises: loop in the stream_to_stdout function in our example (which consumes the results in main) leads to asyncio switching tasks. This gives the agents (research_agent, web_search_agent, page_scraper_agent) a chance to run in the background. The use of reply_out_of_order in some of the agents ensures that certain messages are yielded to the output stream as soon as they are ready from these parallel operations, rather than in the order in which they were registered as part of the agent's response. This enhances the sense of parallelism from the consumer's perspective, though it doesn't change the parallelism of the actual agent execution (which is already parallel due to trigger being non-blocking).\nA key feature highlighted in the main function of sequence_flattening.py is the replayability of MessageSequencePromise objects. You can iterate over response_promises multiple times and get the exact same sequence of messages. This is invaluable for scenarios where you might want to feed the same set of results to multiple different subsequent processing agents without worrying about \"exhausting\" the input stream.\n\nAs you saw from the animation at the beginning of this section, the processing happens much faster, even though we didn't do anything special to achieve that, all thanks to parallelism introduced by the framework.\n\nThis automatic concurrency and sequence flattening greatly simplify the development of complex, multi-step AI systems. You can focus on the logic of each individual agent, writing code that appears sequential within the agent, while the MiniAgents framework handles the parallel execution and complex data flow management behind the scenes.\n\nWeb Research System with real operations\n\nNow that we've explored the core concept of \"Message Sequence Flattening\" with a dummy example, let's dive into the fully functional Web Research System. This system uses real AI models for understanding and generation, performs actual web searches, and scrapes web pages to gather information.\n\nAgain, as mentioned earlier, the complete source code for this example can be found \nhere\n.\n\nPrerequisites\n\nBefore running the web_research.py script, you'll need to set up a few things:\n\nInstallation: First, install MiniAgents and the required dependencies:\n\npip install -U miniagents openai httpx pydantic markdownify python-dotenv selenium\n\nEnvironment Variables: For the LLM we will use \nOpenAI\n and for the google searches as well as web scraping we will use \nBright Data\n, a pay as you go scraping service. The two Bright Data products that we are interested in are: \nSERP API\n and \nScraping Browser\n. Create a .env file in the same directory as web_research.py with the following credentials:\n\n# .env\nBRIGHTDATA_SERP_API_CREDS=\"your_serp_api_username:your_serp_api_password\"\nBRIGHTDATA_SCRAPING_BROWSER_CREDS=\"your_scraping_browser_username:your_scraping_browser_password\"\nOPENAI_API_KEY=\"your_openai_api_key\"\n\nATTENTION: The credentials above are NOT for your whole Bright Data account. They are for the SERP API and Scraping Browser respectively (their website will guide you how to set up both products).\n\nHelper Utilities (utils.py): The project uses a utils.py file (available \nhere\n) which contains helper functions for:\n\nfetch_google_search(): Interacts with the Bright Data SERP API.\nscrape_web_page(): Uses Selenium with Bright Data's Scraping Browser to fetch and parse web page content. It runs Selenium in a separate thread pool as Selenium is blocking.\n\nYou don't need to dive deep into utils.py to understand the MiniAgents framework, but it's essential for the example to run.\n\nSystem Overview and the main function\n\nThe entry point of our application is the main() function. It orchestrates the entire process:\n\n# examples/web_research_tutorial/web_research.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Union\n\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\nfrom miniagents import (\n    AgentCall,\n    InteractionContext,\n    Message,\n    MessageSequencePromise,\n    MiniAgents,\n    miniagent,\n)\nfrom miniagents.ext.llms import OpenAIAgent, aprepare_dicts_for_openai\n\nfrom utils import fetch_google_search, scrape_web_page\n\nload_dotenv() # Load environment variables from .env file\n\nMODEL = \"gpt-4o-mini\"  # \"gpt-4o\"\nSMARTER_MODEL = \"o4-mini\"  # \"o3\"\nMAX_WEB_PAGES_PER_SEARCH = 2\nSLEEP_BEFORE_RETRY_SEC = 5\n\nopenai_client = AsyncOpenAI()\n\n\nasync def main():\n    question = input(\"\\nEnter your question: \")\n\n    # Invoke the main agent (no `await` is placed in front of the call, hence\n    # this is a non-blocking operation, no processing starts just yet)\n    response_promises: MessageSequencePromise = research_agent.trigger(question)\n\n    print()\n    # Iterate over the individual message promises in the response sequence\n    # promise. The async loops below lead to task switching, so the agent above\n    # as well as its \"sub-agents\" will now start their work in the background\n    # to serve all the promises.\n    async for message_promise in response_promises:\n        # Skip messages that are not intended for the user (you'll see where\n        # the `not_for_user` attribute is set later)\n        if message_promise.known_beforehand.get(\"not_for_user\"):\n            continue\n        # Iterate over the individual tokens in the message promise (messages\n        # that aren't broken down into tokens will be delivered in a single\n        # token)\n        async for token in message_promise:\n            print(token, end=\"\", flush=True)\n        print(\"\\n\")\n\n# ... (rest of the file)\n\nKey takeaways from main():\n\nFiltering Messages: Some messages might be internal to the agent system (e.g., detailed summaries for other agents). We can attach metadata to messages (like not_for_user) and use it to filter what's shown to the end-user. The known_beforehand attribute of a MessagePromise allows access to metadata that is available before the message content itself is resolved. This can be useful for early filtering or routing of messages. In our main function, we use this to check the \"not_for_user\" flag (set in page_scraper_agent) to prevent internal page summaries from being directly displayed.\nCentralized Output: Notice that all user-facing output happens here. Agents themselves don't print. They communicate results back, which main then decides how to present. This separation makes it easier to change the UI or even integrate this entire agentic system as a component within a larger AI system, where its output would be consumed programmatically rather than printed to a console.\n\nNOTE: Background execution is optional. MiniAgents, by default, starts processing triggered agents as soon as possible, and this is generally the desired behavior for maximum parallelism. You can, however, disable this behavior by passing start_soon=False to individual trigger calls, or by setting start_everything_soon_by_default=False in the MiniAgents constructor for a global effect. The latter is generally not recommended, though. Disabling \"early start\" globally can often lead to deadlocks if agent interdependencies are complex, and in the majority of scenarios, there is hardly any benefit in setting start_soon to False.\n\nThe research_agent: Orchestrating the Search\n\nThe research_agent is the primary coordinator. It takes the user's question and breaks it down into actionable steps.\n\n# examples/web_research_tutorial/web_research.py\n# ... (imports and setup shown above) ...\n\n@miniagent\nasync def research_agent(ctx: InteractionContext) -> None:\n    ctx.reply(\"RESEARCHING...\")\n\n    # First, analyze the user's question and break it down into search queries\n    message_dicts = await aprepare_dicts_for_openai(\n        ctx.message_promises, # The user's question\n        system=(\n            \"Your job is to breakdown the user's question into a list of web \"\n            \"searches that need to be done to answer the question. Please try \"\n            \"to optimize your search queries so there aren't too many of \"\n            \"them. Current date is \" + datetime.now().strftime(\"%Y-%m-%d\")\n        ),\n    )\n    # Using OpenAI's client library directly for structured output\n    response = await openai_client.beta.chat.completions.parse(\n        model=SMARTER_MODEL,\n        messages=message_dicts,\n        response_format=WebSearchesToBeDone, # Pydantic model for structured output\n    )\n    parsed: WebSearchesToBeDone = response.choices[0].message.parsed\n\n    ctx.reply(f\"RUNNING {len(parsed.web_searches)} WEB SEARCHES\")\n\n    already_picked_urls = set[str]()\n    # Fork the `web_search_agent` to create an isolated, configurable instance\n    # for this task. `non_freezable_kwargs` allows passing mutable objects like\n    # our `already_picked_urls` set, which will then be specific to this forked\n    # agent instance and shared across its invocations within this research\n    # task.\n    _web_search_agent = web_search_agent.fork(\n        non_freezable_kwargs={\n            \"already_picked_urls\": already_picked_urls,\n        },\n    )\n\n    # Initiate a call to the final_answer_agent. We'll send it data as we\n    # gather it.\n    final_answer_call: AgentCall = final_answer_agent.initiate_call(\n        user_question=await ctx.message_promises,\n    )\n\n    # For each identified search query, trigger a web search agent\n    for web_search in parsed.web_searches:\n        search_and_scraping_results = _web_search_agent.trigger(\n            ctx.message_promises, # Forwarding the original user question\n            search_query=web_search.web_search_query,\n            rationale=web_search.rationale,\n        )\n        # `reply_out_of_order` sends messages to the research_agent's output\n        # as they become available, maintaining responsiveness.\n        ctx.reply_out_of_order(search_and_scraping_results)\n\n        # Send the same results to the final_answer_agent\n        final_answer_call.send_message(search_and_scraping_results)\n\n    # Reply with the sequence from final_answer_agent, effectively chaining\n    # its output to research_agent's output. This also closes the call to\n    # final_answer_agent.\n    ctx.reply(final_answer_call.reply_sequence())\n\n# ... (other agents)\n\nKey aspects of research_agent:\n\nQuery Generation: It uses an LLM (via openai_client.beta.chat.completions.parse) to break the user's question into a list of specific search queries. WebSearchesToBeDone is a Pydantic model that ensures the LLM returns data in the expected structure (using OpenAI's \"structured output\" feature). While this example uses the OpenAI client library directly for structured output, MiniAgents plans to support this natively as another built-in LLM miniagent, along with already existing OpenAIAgent, AnthropicAgent etc. which simply generate text.\nAgent Forking for Configuration and State: The web_search_agent needs to keep track of URLs it has already decided to scrape to avoid redundant work. agent.fork() creates a new, independent version (an \"instance\") of the agent. This is useful for creating agents with specific configurations or, as in this case, for endowing an agent instance with mutable state (like already_picked_urls) that is shared across its invocations by this particular forked instance. The non_freezable_kwargs argument is the mechanism for passing such mutable resources that cannot (or should not) be \"frozen\" by the fork.\nInitiating Calls (initiate_call): The final_answer_agent will eventually synthesize an answer using all gathered information. We don't have all this information upfront. final_answer_agent.initiate_call() creates an AgentCall object. This allows research_agent to send messages (or message promises) to final_answer_agent incrementally using final_answer_call.send_message().\nParallel Fan-Out (trigger without await): For each generated search query, _web_search_agent.trigger() is called. Again, no await means these sub-agents start working in parallel.\nOut-of-Order Replies (ctx.reply_out_of_order): As results from _web_search_agent (which include search and scraping steps) become available, ctx.reply_out_of_order() sends them to the output stream of research_agent. As mentioned earlier, we use reply_out_of_order() to avoid enforcing message delivery in the order they were added to the reply. Delivering these messages as soon as they are available allows research_agent to show progress from different search branches in real time.\nChaining Agent Output: Finally, ctx.reply(final_answer_call.reply_sequence()) takes the response that final_answer_agent will produce (in this example, a sequence consisting of a single message containing the synthesized answer) and appends it to research_agent's own output. reply_sequence() also signals to final_answer_agent that no more input messages will be sent via final_answer_call.send_message(), effectively closing the call (such behavior can be prevented with reply_sequence(finish_call=False) if needed, though).\nThe web_search_agent: Searching and Selecting Pages\n\nThis agent takes a single search query, performs the search, and then uses an LLM to decide which of the resulting pages are most relevant for scraping.\n\n# examples/web_research_tutorial/web_research.py\n# ... (research_agent) ...\n\n@miniagent\nasync def web_search_agent(\n    ctx: InteractionContext,\n    search_query: str,\n    rationale: str,\n    already_picked_urls: set[str], # Received from the forked instance\n) -> None:\n    ctx.reply(f'SEARCHING FOR \"{search_query}\"\\n{rationale}')\n\n    try:\n        search_results = await fetch_google_search(search_query) # from utils.py\n    except Exception:\n        await asyncio.sleep(SLEEP_BEFORE_RETRY_SEC)\n        ctx.reply(f\"RETRYING SEARCH: {search_query}\")\n        search_results = await fetch_google_search(search_query)\n\n    ctx.reply(f\"SEARCH SUCCESSFUL: {search_query}\")\n\n    message_dicts = await aprepare_dicts_for_openai(\n        [\n            ctx.message_promises, # Original user question\n            f\"RATIONALE: {rationale}\\n\\nSEARCH QUERY: {search_query}\\n\\n\"\n            f\"SEARCH RESULTS:\\n\\n{search_results}\",\n        ],\n        system=(\n            \"This is a user question that another AI agent (not you) will \"\n            \"have to answer. Your job, however, is to list all the web page \"\n            \"urls that need to be inspected to collect information related to \"\n            \"the RATIONALE and SEARCH QUERY. SEARCH RESULTS where to take the \"\n            \"page urls from are be provided to you as well. Current date is \"\n            + datetime.now().strftime(\"%Y-%m-%d\")\n        ),\n    )\n    response = await openai_client.beta.chat.completions.parse(\n        model=SMARTER_MODEL,\n        messages=message_dicts,\n        response_format=WebPagesToBeRead, # Pydantic model for structured output\n    )\n    parsed: WebPagesToBeRead = response.choices[0].message.parsed\n\n    web_pages_to_scrape: list[WebPage] = []\n    for web_page in parsed.web_pages:\n        if web_page.url not in already_picked_urls:\n            web_pages_to_scrape.append(web_page)\n            already_picked_urls.add(web_page.url)\n        if len(web_pages_to_scrape) >= MAX_WEB_PAGES_PER_SEARCH:\n            break\n\n    for web_page in web_pages_to_scrape:\n        ctx.reply_out_of_order(\n            page_scraper_agent.trigger(\n                ctx.message_promises, # Original user question\n                url=web_page.url,\n                rationale=web_page.rationale,\n            )\n        )\n\n# ... (page_scraper_agent and final_answer_agent)\n\nHighlights of web_search_agent:\n\nActual Web Search: It calls fetch_google_search() (from utils.py) to get search results. Includes basic retry logic.\nLLM for Page Selection: An LLM is prompted to analyze the search results and select promising URLs based on the rationale for the search. The desired output is structured using the WebPagesToBeRead Pydantic model.\nStateful Filtering: It uses the already_picked_urls set (provided by the fork in research_agent) to avoid re-processing URLs that have already been selected from other search queries in the same overall research task. It also limits the number of pages scraped per search query (MAX_WEB_PAGES_PER_SEARCH).\nParallel Scraping Trigger: For each selected URL, page_scraper_agent.trigger() is called without await, initiating parallel scraping operations. Results are again channeled using ctx.reply_out_of_order().\nThe page_scraper_agent: Fetching and Summarizing Content\n\nThis agent is responsible for fetching the content of a single web page and then using an LLM to extract relevant information.\n\n# examples/web_research_tutorial/web_research.py\n# ... (web_search_agent) ...\n\n@miniagent\nasync def page_scraper_agent(\n    ctx: InteractionContext,\n    url: str,\n    rationale: str,\n) -> None:\n    ctx.reply(f\"READING PAGE: {url}\\n{rationale}\")\n\n    try:\n        page_content = await scrape_web_page(url) # from utils.py\n    except Exception:\n        await asyncio.sleep(SLEEP_BEFORE_RETRY_SEC)\n        ctx.reply(f\"RETRYING: {url}\")\n        page_content = await scrape_web_page(url)\n\n    # We await the full summary here because we want to ensure summarization\n    # is complete before reporting success for this page.\n    page_summary = await OpenAIAgent.trigger(\n        [\n            ctx.message_promises, # Original user question\n            f\"URL: {url}\\nRATIONALE: {rationale}\\n\\n\"\n            f\"WEB PAGE CONTENT:\\n\\n{page_content}\",\n        ],\n        system=(\n            \"This is a user question that another AI agent (not you) will \"\n            \"have to answer. Your job, however, is to extract from WEB PAGE \"\n            \"CONTENT facts that are relevant to the users original question. \"\n            \"The other AI agent will use the information you extract along \"\n            \"with information extracted by other agents to answer the user's \"\n            \"original question later. Current date is \"\n            + datetime.now().strftime(\"%Y-%m-%d\")\n        ),\n        model=MODEL,\n        # Streaming isn't important for this internal summary\n        stream=False,\n        # If summarization fails, let this agent fail rather than sending an\n        # error message. This is a choice; for robustness, True might be\n        # preferred if partial results are acceptable.\n        errors_as_messages=False,\n        response_metadata={\n            # This summary is for the final_answer_agent, not directly for the\n            # user.\n            \"not_for_user\": True,\n        },\n    )\n    ctx.reply(f\"SCRAPING SUCCESSFUL: {url}\")\n    ctx.reply(page_summary) # Send the extracted summary\n\n# ... (final_answer_agent)\n\nKey points for page_scraper_agent:\n\nActual Scraping: It calls scrape_web_page() (from utils.py), which uses Selenium to get page content. Includes retry logic.\nLLM for Summarization/Extraction: The MiniAgents built-in OpenAIAgent is used here. It's triggered to process the scraped content and extract information relevant to the original user question and the rationale for visiting this specific page.\nAwaiting Specific Results: Unlike previous trigger calls, page_summary = await OpenAIAgent.trigger(...) awaits the result. This is a design choice: we want to ensure the summarization is complete and successful before this agent reports SCRAPING SUCCESSFUL and sends the summary onward.\nError Handling Choice (errors_as_messages=False): For this particular call to OpenAIAgent, errors_as_messages is set to False. This means if the LLM call fails, the page_scraper_agent itself will raise an exception. The overall system is configured with errors_as_messages=True (see the if __name__ == \"__main__\": block later in the tutorial), so this local override demonstrates fine-grained control. If it were True (or defaulted to the global setting), an LLM error would result in an error message being sent as page_summary instead of crashing this agent.\nMessage Metadata (response_metadata): The OpenAIAgent.trigger call includes response_metadata. This dictionary is attached to the message(s) produced by OpenAIAgent. Here, \"not_for_user\": True signals that this summary is an intermediate result, primarily for the final_answer_agent, and shouldn't be directly displayed to the user by the main function's loop. It's worth noting that metadata keys like \"not_for_user\" are arbitrary choices specific to this application's design; you can use any key names that suit your system's logic for routing or annotating messages.\nThe final_answer_agent: Synthesizing the Result\n\nThis agent receives all the summaries and extracted pieces of information from the various page_scraper_agent instances and synthesizes a final answer to the user's original question.\n\n# examples/web_research_tutorial/web_research.py\n# ... (page_scraper_agent) ...\n\n@miniagent\nasync def final_answer_agent(\n    ctx: InteractionContext,\n    user_question: Union[Message, tuple[Message, ...]],\n) -> None:\n    # Await all incoming messages (summaries from page_scraper_agents) to\n    # ensure they are \"materialized\" before we proceed to show the \"ANSWER\"\n    # heading. If not for this heading, `await` would not have been important\n    # here - OpenAIAgent will internally await for all the incoming messages to\n    # make them part of its prompt anyway.\n    await ctx.message_promises\n\n    ctx.reply(\"==========\\nANSWER:\\n==========\")\n\n    ctx.reply(\n        OpenAIAgent.trigger(\n            [\n                \"USER QUESTION:\",\n                user_question, # Passed as a kwarg during initiate_call\n                \"INFORMATION FOUND ON THE INTERNET:\",\n                # Concatenate all received messages (page summaries) into a\n                # single text block. `as_single_text_promise()` returns a\n                # promise; actual concatenation happens in the background.\n                ctx.message_promises.as_single_text_promise(),\n            ],\n            system=(\n                \"Please answer the USER QUESTION based on the INFORMATION \"\n                \"FOUND ON THE INTERNET. Current date is \"\n                + datetime.now().strftime(\"%Y-%m-%d\")\n            ),\n            model=MODEL,\n            # stream=True, # In LLM miniagents streaming is enabled by default.\n                           # Explicitly setting stream=False would turn it off.\n        )\n    )\n\n# ... (Pydantic models and main block)\n\nKey features of final_answer_agent:\n\nWaiting for All Inputs (await ctx.message_promises): Before generating the answer, this agent explicitly await ctx.message_promises. This ensures that all the page summaries sent by research_agent (via final_answer_call.send_message()) have arrived and are resolved. This is important because we want the \"ANSWER:\" heading to appear after all the processing logs (like \"SEARCHING...\", \"READING PAGE...\").\nConsolidating Information: ctx.message_promises.as_single_text_promise() is a handy method. It takes the sequence of incoming messages (which are progress logs and page summaries) and concatenates them into a single text message promise, with original messages separated by double newlines. This consolidated text, along with the original user question, forms the prompt for the final LLM call. (Although it's worth noting that you could have passed all these messages into the prompt without such concatenation - they would all have appeared to the LLM as separate dialog turns in that case.)\nGenerating Final Answer: OpenAIAgent is triggered one last time to produce the comprehensive answer. OpenAIAgent streams by default, so the final answer will appear token by token to the user, providing a responsive experience.\nPydantic Models for Structured LLM Output\n\nOur example uses Pydantic models with OpenAI's \"Structured Output\" feature (via response_format in client.beta.chat.completions.parse) to guide the LLM in generating responses in a specific format:\n\n# examples/web_research_tutorial/web_research.py\n# ... (final_answer_agent) ...\n\nclass WebSearch(BaseModel):\n    rationale: str\n    web_search_query: str\n\n\nclass WebSearchesToBeDone(BaseModel):\n    web_searches: tuple[WebSearch, ...]\n\n\nclass WebPage(BaseModel):\n    rationale: str\n    url: str\n\n\nclass WebPagesToBeRead(BaseModel):\n    web_pages: tuple[WebPage, ...]\n\n# ... (if __name__ == \"__main__\":)\n\nThese models (WebSearchesToBeDone, WebPagesToBeRead) help ensure that the LLM provides its output in a way that can be reliably parsed and used by the subsequent logic in the agents.\n\nConfiguring and Running the System\n\nThe if __name__ == \"__main__\": block initializes and runs the MiniAgents system:\n\n# examples/web_research_tutorial/web_research.py\n# ... (Pydantic models) ...\n\nif __name__ == \"__main__\":\n    MiniAgents(\n        # Log LLM requests/responses to markdown files in `llm_logs`\n        llm_logger_agent=True,\n        # Convert agent errors into messages rather than crashing the agents\n        errors_as_messages=True,\n        # # Optionally include full tracebacks in error messages for debugging\n        # error_tracebacks_in_messages=True,\n    ).run(main())\n\nKey MiniAgents configurations used here:\n\nllm_logger_agent=True: This is incredibly useful for debugging. It logs all requests to and responses from LLM miniagents (like OpenAIAgent) into markdown files in an llm_logs directory. You can inspect these logs to see the exact prompts, model parameters, and outputs.\nerrors_as_messages=True: This global setting makes the system more robust. If an agent encounters an unhandled exception, instead of the agent (and potentially the whole flow) crashing, the error is packaged into an ErrorMessage object and continues through the system. As we saw, page_scraper_agent locally overrides this for its LLM calls by setting errors_as_messages=False in the trigger call.\nerror_tracebacks_in_messages=True (commented out): If you enable this, the error messages produced when errors_as_messages=True will also include the full Python traceback, which can be helpful during development (only useful when errors_as_messages=True, because when errors_as_messages=False, errors are raised as exceptions and are logged to the console with the tracebacks regardless of the error_tracebacks_in_messages setting).\nConclusion\n\nThis Web Research System demonstrates several powerful features of MiniAgents:\n\nProcedural Code, Parallel Execution: Agents are written as straightforward async functions, but trigger calls without await lead to parallel execution.\nPromise-Based Communication: Agents communicate via MessageSequencePromise objects, allowing computation to proceed while data is still being gathered.\nSequence Flattening: Complex, nested calls to agents still result in a single, flat stream of messages for the consumer, managed automatically by the framework.\nFlexible Message Handling: Features like ctx.reply_out_of_order for responsive streaming and agent.initiate_call for incremental data feeding provide fine-grained control over message flow.\nConfigurable Agent Instances with Forks: agent.fork() offers a clean way to create isolated, configurable instances of agents, which can be used for managing state specific to a task or for creating differently parameterized versions of an agent.\nNative LLM Integration: The built-in OpenAIAgent, AntropicAgent etc. provide a convenient, MiniAgents-native way to incorporate LLM capabilities from popular providers like OpenAI and Anthropic into your agents. (More such built-in LLM miniagents will be added in the future and there will also be a tutorial showing how easy it is to create such an LLM miniagent on your own.)\nDebugging Support: Features like llm_logger_agent aid in development by providing visibility into LLM interactions.\nEnhanced Robustness: Global error handling settings like errors_as_messages help create more resilient systems.\n\nBy focusing on the logic of individual agents, MiniAgents lets you build sophisticated, concurrent AI systems without getting bogged down in the complexities of manual parallelism management. The system naturally parallelizes IO-bound tasks (like calls to LLMs or external APIs) without requiring explicit concurrency code from the developer, as AI agents are typically IO-bound.\n\nJoin our \nDiscord community\n to get help with your projects. We welcome questions, feature suggestions, and contributions!\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nMiniAgents: Multi-Agent AI Framework With Procedural Simplicity\n\nWhy MiniAgents\n\nLet's build a Web Research System with MiniAgents\n\n\"Message Sequence Flattening\"\n\nNaive alternative to \"Message Sequence Flattening\"\n\nReal \"Message Sequence Flattening\" with MiniAgents\n\nWeb Research System with real operations\n\nPrerequisites\n\nSystem Overview and the main function\n\nThe research_agent: Orchestrating the Search\n\nView all",
    "awards": [
      "best technical implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "module-1aiagentchatbot-VpZe7bCPxS9Q",
    "username": "dDevang Dhaka",
    "license": "MIT License",
    "title": "Module-1Ai_Agent_Chatbot",
    "publication_description": "First, paste the code I provided into any code editor. Then, to check the RAG capabilities, download the Python documentation and update the file path in the code according to your downloaded directory. After that, run the code and run this developer server [ http://localhost:5000 ] in any browser. Also, mak\n\nflask==3.0.0\nflask-cors==4.0.0\nlangchain==0.1.0\nlangchain-community==0.0.13\nlangchain-core==0.1.10\nlangchain-chroma==0.1.0\nlangchain-ollama==0.1.0\nchromadb==0.4.22\npypdf==4.0.1\ntiktoken==0.5.2\npydantic==2.5.3\n\ne sure that the essential dependencies and the Ollama LLM are installed on your machine before running it.\n:::\n\nWhat it does:\nA Python-powered AI chatbot that answers programming questions using RAG (Retrieval-Augmented Generation) technology.\n\nHow it works:\n\nVector Database: Stores Python documentation in Chroma vector store\nRetrieval: When user asks a question, retrieves 5 most relevant documents\nLLM Processing: Llama 3.2 model generates accurate answers using retrieved context\nWeb Interface: Flask backend serves a simple chat UI\n\nTech Stack:\n\nBackend: Python, Flask, LangChain\n2.AI Model: Ollama (Llama 3.2:1b)\n3.Embeddings: mxbai-embed-large\n4.Vector Store: ChromaDB\n5.Frontend: HTML/CSS/JavaScript with real-time chat\n\nKey Features:\n\nReal-time AI responses\nContext-aware answers from knowledge base\nLightweight model for low-resource systems\nClean, professional dark UI\nPersistent chat history during session\n\nUse Case:\nHelps developers get instant, accurate answers about Python programming by combining document retrieval with AI generation.\n\nGithub Link: \nhttps://github.com/devangdhakaai-create/Tensor-Flow-Ai-Agent/pull/1",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "multi-agent-space-model-for-efficient-neighborhood-search-and-interaction-0VPKbgRYf6nI",
    "username": "c@csansores",
    "license": "MIT License",
    "title": "Multi-agent Space Model for Efficient Neighborhood  Search and Interaction",
    "publication_description": "Back to publications\nMar 07, 2025\n●\n47 reads\n●\nMIT License\nWinner of\nAI Research Excellence\nat the\nAgentic AI Innovation Challenge 2025\nMulti-agent Space Model for Efficient Neighborhood Search and Interaction\nIntelligent Agent\nMulti-agent systems\nNeighborhood Search\nC\n@csansores\nM\nMirbella Gallareta NegróN\nLike\nBookmark\nShare\nAbstract\n\nBioMASS is an innovative multi-agent spatial model designed to enhance computational efficiency in simulations involving complex sensory and locomotion functions. Traditional agent-based modeling (ABM) platforms suffer from performance degradation as the number of agents and their perception ranges increase, leading to quadratic computational costs. BioMASS addresses this issue by employing a quadruply linked list structure, which enables constant-time neighborhood search and movement. It is implemented within a multi-agent system framework that has been successfully applied to marine ecosystem simulations, demonstrating its ability to track species interactions across multiple trophic levels in real-time, outperforming existing platforms.\n\n\nFig.1. Graphical Abstract of BioMASS Model. The proposed quadruply linked list structure optimizes 2D space representation in agent-based simulations. This design enables efficient neighborhood search by linking each agent to its nearest neighbors in both X and Y directions. The approach scales effectively, supporting large-scale simulations without the computational overhead of grid-based or hash table structures.\n\nIntroduction\n\nAgent-based modeling and simulation (ABMS) is a method used to study the dynamics of complex systems. It is widely applied across disciplines such as economics, ecology, sociology, and biology to investigate how interactions among individual components lead to self-organizing behaviors and emergent properties in complex systems [1, 2]. ABMS systems are designed as multi-agent systems (MAS) to distribute task-solving across agents. Within ABMS, two main types of models are used to study complex systems: behavioral simulations and agent-based system simulations, each suited to different experimental needs due to their distinct spatial and temporal resolutions.\nIn behavioral simulations, individual-level rules generate collective patterns and relatively few agents (typically tens to hundreds) are required. These simulations emphasize high spatial and temporal granularity, allowing researchers to observe how individual interactions contribute to group behaviors. Since the focus is on immediate interactions within small populations, these models do not require large spatial or temporal dimensions.\nAgent-based system simulations, on the other hand, serve as complementary tools to traditional state-variable models, emphasizing physiological processes and local interactions that give rise to emergent behaviors at the population level. This paradigm requires a much larger number of agents (often thousands) and operates in expansive environments over extended time-frames, often spanning years or decades. The goal is to capture the evolution of population dynamics over multiple generations, requiring less granular temporal and spatial resolution compared to behavioral simulations.\nCertain scenarios require blending both paradigms, especially when individual behaviors directly impact macro-level variables. These hybrid models add computational demands, as they need to manage large agent populations while accounting for individual variability in interactions, life cycles, and behaviors. Since these simulations are spatially explicit, agents interact primarily through sensory and perception functions, which adds computational complexity. Agents must frequently sense a large portion of their environment, requiring costly neighborhood search functions. Furthermore, variability in agents’ perception ranges increases the difficulty of optimizing these functions within traditional spatial models. This highlighted the need for a spatially explicit ABMS framework capable of efficiently handling neighborhood exploration and movement functions, addressing the computational challenges posed by large, spatially complex environments.\n\nThe Problem\n\nThe classical method adopted to solve these types of problems involves a discrete virtual space (usually a two- or three-dimensional rectangular lattice) and a search algorithm whose computational cost per individual increases with the perception range and with the number of discrete cells in the search area. To reduce this cost, the number of cells must be kept low. This is done either by reducing the perception range or by increasing cell size (sacrificing detail in the environment representation). Another approach is to use a continuous space where the computational cost per agent increases linearly with the number of agents. However, models with a large number of agents (1000s or more) perform poorly under this scheme. To avoid very costly processing in this case, a collection of agents can be modeled as a group and represented as a special kind of agent. But this again sacrifices detail in the model representation, especially regarding the degree of heterogeneity among the agents.\nOversimplifying the spatial representation or the number of individuals may work for certain systems. However, in [4], Sansores et al. described several scenarios that require large spatial dimensions, fine spatial granularity and a large number of agents at the same time. Moreover, the perception range of the agents may change over time, so it is desirable to preserve the heterogeneity that forms the foundation of ABMS models. The combination of these requirements creates an additional level of complexity in the spatial model that is not addressed by any current ABMS framework. Our main contribution focus on the BioMASS spatial model.\n\nClassical spatial models for situated multi-agent systems\n\nTraditionally, the internal model of the environment in a situated MAS is either discrete or continuous. This choice determines how the agents can be physically organized and the forms of the sensory and locomotion functions used by the agents. The hybrid model, a combination of the above, uses partial discretization of the underlying continuous model in order to improve the execution performance of some functions such as neighborhood search.\n\nThe Need for an optimized spatial model\n\nExisting spatial models, see Fig.2, Fig.3, Fig.4, suffer from high computational costs:\n\nDiscrete models rely on fixed grids, leading to inefficient neighborhood searches.\nContinuous models require global searches, making them computationally expensive.\nHybrid models balance these approaches but require predefined discretization, limiting flexibility.\nResearch Questions\nWhat is the theoretical computational complexity of the BioMASS model in terms of time, and how does it compare to that of other spatial models used in agent simulations?\nTo what extent does the BioMASS model reduce the computational complexity of neighborhood search and other spatial operations in agent simulations, especially in scenarios with a large number of agents and varying perception ranges?\nHow can the BioMASS model be integrated into an existing agent simulation framework, and what are the benefits of this integration in terms of performance and scalability?\nTo what extent does the BioMASS model facilitate the design and implementation of more efficient and scalable agent simulations for the study of complex systems?\nObjectives\nConduct a formal analysis of the computational complexity of the BioMASS model, deriving mathematical expressions for the time required by its main operations (insertion, removal, movement, neighborhood search).\nCompare the theoretical computational complexity of the BioMASS model with that of other spatial models (continuous, discrete, and hybrid) by analyzing their respective complexity expressions.\nImplement the BioMASS model in an open-source agent simulation framework and evaluate its performance in different simulation scenarios, measuring execution time and scalability.\nEvaluate the impact of the BioMASS model on the performance and scalability of a specific agent-based simulation application, such as the predator-prey model described in the article.\n\n\nFig.2. A discrete spatial model  of  cells containing a set of agents . The perception scope of  at step  is the shaded subgrid. The neighborhood lookup function of  iterates over all cells in the shaded area and adds all agents occupying those cells to its neighborhood set .\n\n\nFig.3. A continuous spatial model with Cartesian coordinates. The set of agents  is situated in the space and their different perception scopes are depicted by dotted lines. The neighborhood set of agent  is . Note that the neighborhood relation is not symmetric: agents  and  do not have agent  in their neighborhood.\n\n\nFig.4. A hybrid spatial model with a continuous space divided into buckets. The set of agents  is situated in the space with real Cartesian coordinates. The neighborhood set of agent  is . Note that agent : although it happens to be in a bucket that is perceived by , it is not within the Euclidean distance .\n\nMethodology\nThe proposed BioMASS quadruply linked list spatial model\n\nLet  be the space in the BioMASS model with dimensions .  can be portrayed as a Cartesian plane with real coordinates. The implementation uses a quadruply linked list. The nodes forming the list are agents with four pointers: two in the horizontal dimension and two in the vertical dimension. The pointers  and  link to the agents with the next smallest and next largest  coordinate, respectively. A node points to itself if there is no other agent with a smaller/larger  coordinate. Initially,  is configured with two \"sentinel\" agents  and , which represent the corners of the 2D space with coordinates  and .\n\nThe initial setup is illustrated in Fig.5. The sentinel node  has 2 pointers to itself,  and . The pointers,  and , point to . Similarly, for  the two 'previous' pointers point to , while its 'next' pointers point to itself. Thus, , , , , and  are pointers to its four closest agents in the horizontal and vertical dimensions. Therefore, , we have  <= ,  <= ,  >= , and  >= . This data structure guarantees that the agents situated in the environment are always linked to the closest agents in both dimensions. Now, we will analyze the basic functions performed by an agent  in , specifically in the neighborhood lookup.\n\n\nFig.5. The quadruply linked list in  representing the BioMASS spatial model with  and  as sentinel nodes delimiting the simulation space. The depicted links between nodes are valid only when the space is empty.\n\nFig.6 and Fig.7 illustrate the insert function. The time complexity of this function is  in the worst case.\n\n\nFig.6. The space  with only one agent, , pointing to sentinels  and  in both dimensions as the previous and next agent nodes respectively.\n\n\nFig.7. The space  after inserting  at coordinates . Note that, the links of  do not necessarily point to the same agent in both dimensions.\n\nThe neighborhood lookup function\n\nLet  be the neighborhood of agent  within a Euclidean distance  at step , where . The algorithm to obtain  is as follows. First, it traverses the list in the  dimension in both directions starting from , looking for nodes whose  value is within  from . For each node that complies with this condition, its two-dimensional Euclidean distance is calculated to verify that the node is indeed within the perception range of . If so, it is included in the set . This process is illustrated in Fig.8. Observe that agents  are all within a distance  from  in the  dimension, but not all are within the perception scope. Therefore, the neighborhood set is . The pseudo-code 1 shows that this calculation only requires a few operations. The time complexity of this function is usually , due to the fact that agent nodes are ordered by proximity in the list. An agent's neighbors in space are also neighbors in the list, so the search time is roughly constant. The worst case for this function is , when ; that is, when all agents in the space are neighbors of . This case is very rare and not realistic for ABMS simulations. For this to happen, the agent would have to either perceive the entire space or all the agents would have to be concentrated in a small space comparable to the perception range of . Neither situation is common in a properly configured complex system model. However, in such an uncommon case, the \\textit{neighborhood lookup} function would not be necessary, because it would be more efficient for agents to have global access to the list of all agents in the simulation.\n\n\nFig.8. BioMASS spatial model depicting the neighborhood . The area perceived by agent  at step  is a disk of radius . Note that although agent  is identified as a candidate neighbor while traversing the linked list in both dimensions, its Euclidean distance from  is greater than .\n\nThe neighborhood lookup algorithm\n\n\nFig.9. Algorithm for neighborhood lookup of agent  at distance  in the BioMASS space.\n\nComplexity analysis: strengths and limitations\n\nFor theoretical analysis and detailed algorithmic evaluation, see \nSansores et al., 2020\n.\n\nTable 1 summarizes the time complexities of the different spatial models. BioMASS has the best theoretical performance with respect to the time complexity of the neighborhood lookup function, but the time complexity of its insert function is high compared to the other models. However, most simulation experiments are not limited by the insertion time of agents, since this is done just once to set up the initial scenario. Rather, to model the dynamics of complex systems with local interactions, the sensory function is of primary importance since it is used most intensively. In the case of the BioMASS space, the time complexities of the move and neighborhood lookup functions are both .\n\nThe worst case implies that the density of agents is high compared to the perception scope. Such cases are rare in the simulation of complex systems. When they arise, it often means that the resolution of the simulation is unsuitable for the problem. Furthermore, if they do happen frequently, the functions can be implemented differently. For example, if agents are able to perceive the whole space then it is simpler to provide them with a list of all individuals in the space. In contrast, in the continuous space the neighborhood lookup function is always  for all agents, independently of the perception range. In the hybrid and discrete spaces, time complexity in  are also rare (when agents perceive most of the environment), the neighborhood lookup function is still a quadratic function of the perception, which make them less efficient than the worst cases of the other spaces. Hence, theoretically the BioMASS neighborhood lookup function should be the most efficient.\n\nOperation\tDiscrete Model\tContinuous Model\tHybrid Model\tBioMASS\nInsertion\tO(1)\tO(1)\tO(1)\tO(n)\nDeletion\tO(1)\tO(1)\tO(n)\tO(1)\nMovement\tO(1)\tO(1)\tO(n)\tO(1)\nNeighborhood Lookup\tO(W²)\tO(n)\tO(n × W²)\tO(1)\n\n*with W = width of the space\n\nExperiments\nPerformance benchmarking\n\nWe compare the four spatial models through a case study: the classical swarm model. The case study simulates a large flock of prey birds that attempt to avoid a small number of predator birds flying in the same environment. Individual prey and predator birds are modeled as autonomous agents living in a 2D toroidal (opposite boundaries of the space are joined) continuous space.\nSince the BioMASS spatial model implements a continuous space with Cartesian coordinates, we decided to compare it only to similar models. That is, we did not test the swarm scenario with the discrete spatial model, which performs poorly in large spaces with neighborhood lookup ranges beyond the agent's surrounding cells.\nWe selected two simulation platforms for comparison, Repast and MASON. Our selection criteria were as follows: (1) the platform supports one of the two models with Cartesian coordinates, continuous or hybrid; (2) it is a general-purpose simulation tool; and (3) it is an ongoing project still releasing new versions. Also, the Java programming language was an important requirement to standardize the experiments between simulation platforms. Finally, while selecting the platforms we also took into consideration less critical characteristics such as robustness, running speed, and open-source support.\nAll simulations were executed on an iMac with a 3.2 GHz Quad-Core Intel Core i5 processor. The iMac had 16GB of memory and was running MacOS Catalina.\n\nThe experiments measured execution time per simulation step, demonstrating that BioMASS consistently outperformed other models in both homogeneous and heterogeneous agent configurations.\n\nSource code\n\nThe spatial model is integrated into a simple yet versatile Java framework for multi-agent system simulation. Key classes within this framework include Agent.java, Scheduler.java, Space.java and PhysicalObject.java, which coordinate the core functions of agent behavior, simulation timing, and spatial environment management. The PhysicalObject.java class specifically equips agents with the four pointers necessary for insertion into the quadruply linked list. The orchestration of a complete simulation can be observed in the source code and example available in the \nGitHub repository\n, where the Main.java class serves as the entry point. Executing this class launches a graphical user interface (GUI) that enables users to configure key parameters for the provided example, including the simulation space size, the number of agents, and the perception range. Notably, this framework is designed with flexibility in mind, allowing for easy adaptation to different scenarios beyond the example provided. This adaptability makes it straightforward for users to modify the framework for a variety of multi-agent simulations by adjusting or extending its core components.\n\nDataset\n\nThe \nBioMASS Space Model Dataset Benchmark\n\n is publish in the IEEEDataPort to easily reproduce the experiments. This dataset contains the results of the simulation runs of the experiments performed to evaluate and compare the proposed spatial model for situated multi-agent systems.\n\nDataset processing\n\nIt is important to note that this research is based on data generated by simulations. Therefore, traditional dataset preprocessing methodologies are not applicable. The data used for analysis consists of the parameters used to configure the simulations, such as the number of agents, perception ranges and environment size, and the resulting performance metrics, such as time steps and execution times. The parameters of the simulations were varied in order to test the performance of the BioMASS model under a wide variety of conditions.\n\nInstructions\n\n The dataset include a compressed file in zip format. It contains a directory structure as shown below. Each directory is a specific experiment with each simulation toolkit and parameters. Inside each directory there are 50 CSV files, one for echa simulation run. Each file has a header describing the main parameters of the corresponding experiment. We use the Repast Toolkit, and Mason Toolkit to perform a benchmark with the proposed BioMASS spatial model.\n\nDirectory:\nExperiments\n-heterogeneous\nheter-biomass\nheter-mason.20\nheter-mason.40\nheter-mason.200\nheter-mason.400\n-homogeneous\nhomo-biomass\nhomo-mason.200\nhomo-mason.400\nhomo-repast\n\nThe simulation setup and full benchmarking methodology are described in \nSansores et al., 2020\n.\n\nValidation Approach\n\nTo validate the performance of the BioMASS model, an empirical validation approach was adopted, based on experimentation and comparison with other established simulation models. This approach focuses on evaluating the model's ability to accurately simulate the behavior of complex systems and its computational efficiency in various scenarios.\n\nVerification methods\nSimulation replication: Multiple simulation runs (50 repetitions) were performed for each experimental scenario. This allowed us to assess the consistency and stability of the results obtained with the BioMASS model.\nComparison with existing models: The performance of the BioMASS model was compared with that of other widely used spatial models, such as continuous (Repast) and hybrid (MASON) models. This comparison was based on key performance metrics, such as simulation runtime and model scalability.\nTheoretical Complexity Analysis: A formal analysis of the computational complexity of the BioMASS model was performed, deriving mathematical expressions for the time required by its main functions. This analysis was compared with the theoretical complexity of other spatial models to validate the efficiency of the BioMASS model from a theoretical perspective.\nThe maximum, minimum, and average values of the time step were calculated for each simulation run. These values were used to assess the variability and stability of the model's performance across multiple runs.\nRobustness and quality considerations\nScenario variability: Various simulation scenarios were used with different numbers of agents, perception ranges, and environment sizes to evaluate the robustness of the BioMASS model under a wide range of conditions.\nStatistical analysis: Statistical analysis techniques were applied to the simulation results to assess the significance of the performance differences observed between the BioMASS model and other models.\nResults validation: The results obtained with the BioMASS model were compared with the expected results based on theory and empirical evidence available in the scientific literature.\nValidation Results\n\nThe theoretical complexity analysis demonstrated that the BioMASS model has a lower computational complexity for the neighborhood search function compared to other models. This efficiency of the BioMASS model was confirmed by the empirical validation results, which showed that the BioMASS model outperforms traditional spatial models in terms of computational efficiency, especially in scenarios with a large number of agents and varying perception ranges.\n\nValidation implications\n\nThe validation results support the validity and usefulness of the BioMASS model as an efficient and accurate tool for simulating complex systems. The BioMASS model has the potential to facilitate the study of a wide range of complex phenomena in various scientific disciplines.\n\nCase study: marine ecosystem simulation\n\nBioMASS was tested in a marine ecosystem simulation, where it successfully modeled predator-prey interactions, trophic chains, and adaptive behaviors. The simulation demonstrated BioMASS’s ability to:\n\nTrack species interactions across multiple trophic levels.\nMaintain real-time execution even with thousands of agents.\nHandle dynamic perception ranges efficiently.\n\nFor a more detailed description of the marine ecosystem simulation, refer to \nSansores et al., 2016\n.\n\nResults\n\nWe analyzed the time complexity of the different spatial models experimentally through simulation. The simulation experiments tested various perception scopes, to test the execution time of the spatial model for extreme values. The results of these experiments are depicted in Figs. 10 to 15, and clearly demonstrate that BioMASS outperforms the other models.\n\nKey Findings\n\nThe experimental results highlight:\n\nScalability: BioMASS handles thousands of agents efficiently, unlike traditional models.\nComputational efficiency: O(1) complexity in neighborhood lookup, compared to O(n) or worse in other models.\nSupport for heterogeneous agents: BioMASS successfully manages dynamic perception ranges without degrading performance.\nComparison to classical models: BioMASS outperforms grid-based, continuous, and hybrid models in both speed and scalability. This approach eliminates the need for global searches, reducing the computational cost of spatial queries.\n\nFor an in-depth discussion of the experimental results and performance metrics, refer to \nSansores et al., 2020\n and \nSansores et al., 2016\n.\n\n\nFig.10. Maximum time step duration of each run for the simulations with homogeneous perception scope.\n\n\nFig.11. Minimum time step duration of each run for the simulations with homogeneous perception scope.\n\n\nFig.12. Average time step duration of each run for the simulations with homogeneous perception scope.\n\n\nFig.13. Maximum time step duration of each run for the simulations with heterogeneous perception scope.\n\n\nFig.14. Minimum time step duration of each run for the simulations with heterogeneous perception scope.\n\n\nFig.15. Average time step duration of each run for the simulations with heterogeneous perception scope.\n\nConclusion\n\nThe BioMASS space model represents a significant advancement in the field of multi-agent systems, offering researchers an efficient and scalable tool for simulating large-scale, spatially explicit environments. By addressing the computational inefficiencies present in traditional ABM platforms, BioMASS enables more detailed and realistic simulations of complex systems, such as marine ecosystems or predator–prey dynamics. The main reason for this accomplishment is that the spatial model is designed to perform the neighborhood search function with  time complexity in almost all cases. This allows a large number\nof agents to use the search function intensively without significantly affecting the simulation time.\n\nFuture work\n\nFuture work could extend the model to support three-dimensional environments, further broadening its application.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nThe Problem\n\nClassical spatial models for situated multi-agent systems\n\nThe Need for an optimized spatial model\n\nResearch Questions\n\nObjectives\n\nMethodology\n\nThe proposed BioMASS quadruply linked list spatial model\n\nThe neighborhood lookup function\n\nView all\nCode\nDatasets",
    "awards": [
      "ai research excellence"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "multilingual-ocr-for-devanagari-and-english-language-using-crnn-architectures-OJZPAskG780W",
    "username": "Anish Khatiwada",
    "license": "MIT License",
    "title": "Multi-lingual OCR for Devanagari and English Language using CRNN architectures",
    "publication_description": "Back to publications\nDec 22, 2024\n●\n196 reads\n●\nMIT License\nWinner of\nBest Overall Project\nat the\nComputer Vision Projects Expo 2024\nMulti-lingual OCR for Devanagari and English Language using CRNN architectures\n#IAM-handwriting\nCNN\nCNN+NLP\nCRNN\nDevanagari-OCR\nEnglish-OCR\nGRU\nLSTM\nMultilingual-OCR\nOCR\nResidual-block\nAnish Khatiwada\nLike\nBookmark\nShare\nAbstract\n\nIn recent days, businesses, governments, educators, and individuals are increasingly required to handle various documents, including invoices, legal papers, and educational materials. These documents can exist in multiple languages and formats, complicating the process, especially when dealing with regional languages like Nepali and Hindi. Current Optical Character Recognition (OCR) systems, such as i2OCR, Google’s Tesseract, and Microsoft OCR, excel at recognizing printed English text but struggle significantly with handwritten English. This challenge is exacerbated for languages utilizing the Devanagari script. As a result, many organizations still rely on manual data entry, leading to wasted manpower, inefficiency, and high labor costs. The limitations of existing OCR technologies highlight the urgent need for advancements in this field to improve automation and document management.\n\n1. Introduction\n\nThe limitations of current Optical Character Recognition (OCR) tools in effectively recognizing and extracting text from multiple scripts motivated my research in this technology. The goal of this project is to develop a Multilingual OCR system with a low Character Error Rate (CER) for handwritten English and Devanagari scripts, specifically for Hindi and Nepali languages. This system addresses challenges related to text extraction in various industries, including education, finance, government, and legal documents. In a country like Nepal, there are frequent cases of risks associated with preserving students’ exam papers, which can adversely affect their education and careers. By digitizing exam papers as a backup, this project aims to mitigate these issues and enhance the overall educational experience.\n\n2. Background\n\nThe background of the project originated from the observation of a significant deficiency in reliable text recognition tools across multiple languages, including handwritten English and Devanagari related language such as Nepali and Hindi. These languages are spoken by millions and billions of people, respectively. In the industry and government sector of south-east Asia, the demand for automated data entry, text extraction, document digitization, text analysis and searchability has been steadily increasing. Hence, the objective of this project is to develop a multilingual OCR system capable of accurately recognizing handwritten text in English, Nepali, and Hindi with low CER (Character Error Rate), which is the most crucial aspect influencing to the usage of OCR system. Thus, the projects employ advanced machine learning methods, especially deep learning, to address this challenge. The project integrates Computer Vision and Natural Language Processing for the robust outcome. For English text extraction, the Neural network architecture consist of Convolution layer combined with LSTM, utilizing the IAM Handwritten dataset. For Nepali and Hindi text recognition, it uses same datasets of Bhagwat Geeta images, and architecture involves CNNS with GRUs.\n\n3. Literature Review\n3.1. Introduction\n\nThe robust and multi-language OCR system is hot research topic in the field of Computer Vision and Natural Language Processing. It has been the great topic of interest since long time. Due to the varying situation and problem, different research is carried out in their own way. For instance, some researchers may be developing OCR for number plates detection, while others might be working on hard copies, product’s image, or bill receipt. Additionally, they might be working with different languages, styles, fonts, skewness, and language scripts such as Nepali, English, Urdu, Chinese and so on.\n\n3.2. Related Works\n\nThe study from Facebook AI researcher (Huang, et al., 2021) has recently purposed multiplexed network with the claimed to be first model that can jointly use text detection and recognition with very high accuracy. Naïve training system for each language does not takes accountability of the precision of each language and it is also computationally expensive. The purposed architecture also reduces the computation as there is no need to train different model for each language. Language Prediction Network (LPN) is being used for automating the recognition of a languages. The output of the network is a vector of size L = Nlang i.e. the number of language class, which can later be converted into probabilities using a SoftMax. The strong point of the model is Nlang does not have to be equal to the Nrec i.e. the number of different recognition heads because model deals with shared alphabets. For instance, alphabet in Nepali and Hindi matches in most of the case, so prediction can be used interchangeably. Thus, in this case Nlang equals to Nrec.\n\n\nFigure 1; LPN architecture\n\nHere is how Language Prediction Network (LPN) is calculated.\n\n\nwhere, Llang, R, and Lseq(r) are the loss for LPN, set of recognition heads and loss for the recognition head r. are the two hyper-parameters of model.\n\n\nwhere, I(l = lgt) is the binary indicator i.e. 0 if the language do not matches or 1 if the expected language matches to the actual language, and p(l) is the probability of the word belonging to the language l.\n\n\nwhere, p(yt = ct) is the predicted probability of character at position t of the sequence, T is the length character, Cr is the character set that is supported by recognition head r.\n\n\nFigure 2; Recognizing text\n\nAnother study from (Biró, et al., 2023) evaluates the feasibility of synthesized language preparation and its comparison by implementation with machine learning based neural network architecture. The research aims to address the challenges of real-time multilingual OCR in a collaborative environment. The vocabulary is artificially generated by applying a sampling technique to real-world data and creating simulation scenarios where models and processes interact to create completely new data. So, the synthesis datasets for training ranges from 66k to 8.5M and total of 21 results were analysed. The straightforward comparison is conducted between Convolution Recurrent Neural Networks (CRNN) and patch-wise image tokenization framework (SVTR) model architectures as Machine learning based OCR engines. PaddleOCR was also configured for fine-tuning.\n\nCRNN is basically the combination of Convolution Neural Network (CNN) and Recurrent Neural Network (RNN) where it retrieves feature from the input image through CNN and RNN models the sequential dependencies between the characters in the text. On the other hand, SVTR is an advance OCR system that uses both deep learning and traditional image processing techniques to recognizes text in images. Additionally, SVTR also contains attention mechanisms that allows to prioritize on most important visual components and boosts the noise and distortion resistance of the model (Biró, et al., 2023). So, these two modern text recognition techniques were experimented shown in the figure below.\n\n\nfigure 3; distortion resistance\n\nMultiple experiments were conducted with the proposed network architecture. But each time CRNN was able to outperform SVTR. The Machine learning models records the highest precision in Experiment 11, 12 and 14 with scores of 90.25%, 91.35% and 93.89%. So, now we will discuss the detail architecture of selected model from the study.\n\nIn compared to previous study, the research from Xinjiang university (Yu, et al., 2023) has argued experimentally to the previous study by improving the Convolution Recurrent Neural Network (CRNN) for scene text recognition. The improved CRNN model was also trained on synthetic datasets but tested on public datasets. For accomplishing this, the Connectionist Temporal Classification (CTC) function proposed by Alex Graves is redefined. After the feature is first extracted by Convolution Neural Network (CNN) algorithm, the feature dependencies are captured through Recurrent Neural Network (RNN), the output prediction distribution after applying RNN is fed to CTC for processing and then it gives a sequence of text as results. Text and speech signals are sequential data, which makes it difficult to perform segmentation. Additionally, the volume of text recognition data is huge, making it difficult to achieve segmentation and annotation. So, to make the Recurrent Neural Network (RNN) much more applicable for text recognition, the Connectionist Temporal Classification (CTC). The CTC decoding process maps the generated path into final sequence. The figure below shows an example of the final sequence mapped after deduplication and removal of a whitespace character using CTC.\n\n\nFigure 4; Input Output Sequence\n\nThe network structure of the model consists of three layers, that is convolution layers, recurrent layers and the transcription layers with the combination of CNN + RNN + CTC. In the beginning, image is converted to a grayscale and then resized to W*32 with fixed height. Recurrent layers using RNN predicts a label distribution of the feature sequence which is obtained from the convolution layers. The feature maps extracted by the CNN are split by column where each column of 512 – dimensional feature is input into two layers of 256-unit bidirectional LSTMs for classification. CTC than converts the label distribution obtained from RNN through transcription layer. The CTC algorithm deduplicates to obtain the final recognition result, and label smoothing is further added. The recognition result is then forwarded to the language model for correcting characters and finally the final output is obtained.\n\n\nFigure 5; Structure of overall improved architecture\n\nThe datasets used in text recognition can be divided into synthetic and real. Synthetic datasets are mainly used for training, while real datasets are used for evaluating the training results. The study has used synthetic datasets: MJSynth a plain English text dataset that contains 3000 folders with about 9 million image and SynthText consisting of 800,000 scene images. The real datasets which is used are further divided into regular and irregular, based on the format of the text in the Image. Regular datasets include IIT5k-words (IIT5k), SVT (Street View Text), and IC13 (ICDAR2013).\n\nDeep Convolution neural networks (CNN) has shown state-of-the-art performance in various field of computer vision. Due to the availability of pre-trained models, one can save time and resources, and improve accuracy. In the study (K.O & Poruran, 2020), OCR-Nets variants of AlexNet & GoogleNet is presented for recognition of handwritten Urdu characters through transfer learning. While performing transfer learning, Initial layer weights are freeze and the last three-layers are fine-tuned for training and classification of Urdu characters whereas only four inception module is utilized, and the last two layers are modified for learning and classification of the Urdu characters. The experiment shows significant performance gain by OCR-AlexNet and OCR-GoogleNet of about 96.3% and 94.7% average recognition rate respectively.AlexNet is a winner of ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012, becoming the first deep learning model to do so with top-5 test error rate of 15.3%. The network consists of 8 deep layers trained with 10000 classes of ImageNet dataset. The implemented OCR-AlexNet architecture has minor differences to AlexNet for customization. The last three layers of AlexNet consist fully connected layers, with the final layer used for a 1000-label classification task. The network takes input images of 227*227 pixel. Each of the fully connected layers consists of 4096 neurons, chances of overfitting is reduced via Data augmentation and Dropout. Dropout is applied by setting zero to 50% of neurons during training. Adjustments are made to facilitate faster learning including the changes in fully connected layer’s size and learning rates. The architecture of OCR-AlexNet can be presented below.\n\n\nFigure 6; AlexNet Architecture\n\nGoogleNet, also known as Inception-v1, is a deep convolution neural network which gained popularity by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2014 with top-5 error rate of 6.67%. It is more complex with compared to AlexNet. GoogleNet consist of 22-layer deep neural network utilizing 9 unique inception modules unlike traditional CNN structures. These inception modules effectively capture local features by using various kernel sizes for computation, which are later combined together with upcoming layers. To adapt GoogleNet for Custom OCR, the researchers has fine tune it by keeping 4 inception modules and replacing the final two layers with new ones. Overfitting is prevented by freeing earlier layers’ weights and adjusting the learning rates.Thus, this method encourages rapid learning in new layers, slower learning in intermediate, and no learning in frozen layers (K.O & Poruran, 2020).\n\n\nFigure 7; GoogleNet Architecture\n\nIn contrast to the other study, where end-to-end deep learning architecture are implemented, the author (Kumar, et al., 2023) has compared and experimented the commercially available OCR software for processing the text from Television and Video data. The study has analysed 3 differed OCR model’s performance over IAM Handwritten datasets and Complete Blood Count (CBC) datasets. IAM database consist of 14,892 photos of handwritten English lines of text by 657 authors, which is 1,539 handwritten pages and 115,320 words in total. The CBC datasets is gathered by researchers themselves from family and relatives which consist of 85 complete blood count printed reports. The three models Amazon Textract, Google Vision, and PyTesseract were tested on these two different datasets.\n\nAmazon Textract: Aws Textract is a managed machine learning service that goes far away from traditional OCR in the task like recognizing and extracting data from forms and tables in addition to printed and handwritten text. The service also offers an API for image processing, providing flexibility to retrieve raw text in JSON (Javascript Object Notation) format or any other organized tabular layouts. For the IAM dataset, a simple API request is enough to obtain raw text, while for the CBC dataset, the available table detection feature streamlines the extraction of parameters and targeted values from the images.\n\nPyTesseract: It is a widely used python module for text recognition in images build on the Tesseract Engine. It is compatible with various image formats, and incorporates preprocessing techniques such as image rescaling, binarization, noise reduction, normalization, morphological thinning, and deskewing to enhance accuracy. After preprocessing, the image is feed into the PyTesseract engine and it returns a textual unstructured response. And then it is formatted accordingly to both IAM and CBC datasets.\n\nGoogle Vision: It is an AI-powered API by Google for image processing using REST and RPC APIs with pre-trained models that allows user to label images, recognize objects and text, and adds metadata to images saved in Google Cloud Storage. It do not have table extraction as Amazon does but provides word-centric responses based on block coordinates. In the IAM dataset, content retrieval involves combining information from the boxes in the same row to create sentences using a matching algorithm. As for the CBC dataset, with diverse sections and lines, the process involves identifying similar box coordinates for each line, assembling sentences, and extracting parameters and values.\n\nGoogle Vision – It is an AI-powered API by Google for image processing using REST and RPC APIs with pre-trained models that allows user to label images, recognize objects and text, and adds metadata to images saved in Google Cloud Storage. It do not have table extraction as Amazon does but provides word-centric responses based on block coordinates. In the IAM dataset, content retrieval involves combining information from the boxes in the same row to create sentences using a matching algorithm. As for the CBC dataset, with diverse sections and lines, the process involves identifying similar box coordinates for each line, assembling sentences, and extracting parameters and values.\n\n\nFigure 8; Implementing different models on IEM Handwriting dataset\n\nIn the study, the two metrics, Accuracy and Character Error Rate (CER) are used to analyze the model. The accuracy of each model for the IAM dataset was measured by the percentage of characters that were correctly detected and in case of CBC dataset, the accuracy is measured by how correctly the values of parameters are matching. Every character in the truth dataset is matched with the expected value in string matching.\n\n\nOn the other hand, the second metric, Character Error Rate (CER) is dependent on the quality and the content of the dataset and thus calculated using the equation below.\n\n\nHere, S means the number of substitutions, D is the number of deletions, I refers to the number of insertions, and C is the number of correct characters. The lower the value of CER is, the better the performance of model is.\n\nThe final performance of all three models on the IAM and CBC datasets is presented below.\n\nDataset\tMetric\tAmazon Textract\tGoogle Vision\tPyTesseract\nIAM\tAccuracy\t89.42%\t88.53%\t81.02%\nIAM\tCER\t0.1403\t0.1621\t0.2650\nCBC\tAccuracy\t95%\t90.3%\t84.1%\nCBC\tCER\t0.1082\t0.1332\t0.2231\n\nAfter testing the different models on the different datasets, Result indicates that the Model performs better in printed CBC dataset than Handwritten ones, likely due to consistent character formats in printed text. Amazon Textract demonstrated superior performance compared to Google Vision and PyTesseract, with preprocessing techniques further enhancing model accuracy. It also achieved the highest accuracy of 95% and the lowesr CER of 0.1082 for CBC blood reports with preprocessing. The study suggests that performance of OCR engine heavily depends on datasets and model training.\n\n4. Methodology\n\nAs the motive of this project is to develop Optical Character Recognition system (OCR) which is dependent on the Deep learning techniques. It has complex architecture in the background. This architecture is designed in such a way that it takes images of size 96*1408 with its corresponding label while learning. There is series of convolution layer for extracting the pixel value and learning the image content. It is connected with Sequential model where it learns the label text of corresponding images by mapping. For handling Images there is 9 residual blocks. Residual block is the architecture developed by google in its inception-v1 model. Each residual block has Convolution layer, Pooling, Dropout, and it is repeated 3 times by connecting with each other. At the end the input layer is itself multiplied with the final convolution layer, therefore this is also known as skip-connection architecture. For Sequence modelling, I have used two different architectures depending on the language model. For English Model, there is LSTM (Long-Short term Memory) and for Devanagari Model, there is GRU (Gated-Recurrent Unit). This is because, English datasets is larger than Devanagari datasets, but the Devanagari datasets is much complex than the English one. Before feeding data to the model, it goes through different process and the crucial step is given below.\n\n4.1. Architecture\n\nThe CRNN (Convolutional Recurrent Neural Network) architecture is highly preferred option for the the tasks such as Optical Character Recognition (OCR). This architecture integrates Convolution Layers (CNN) to extracts spatial information from the input image and maps it with the corresponding sequential data by dealing with the sequences of varying lengths. The Recurrent Layers (RNN), such as Bidirectional - Long Short-Term Memory (Bi-LSTM), Bidirectional - Gated Recurrent Unit (Bi-GRU), analyses the order of elements in a sequence, enabling a deeper understanding of the sequential patterns of data. The integration of Connectionist Temporal Classification (CTC) loss further strengthens the CRNN architecture. This facilitates end-to-end training by empowering the model to learn and directly predict the character sequences from the input image data. This eliminates the need for explicit alignment between input images and ground truth text. In addition to streamlining the training process, it also enhances model performance significantly. Thus, the CRNN architecture has emerged as a highly adaptable framework, which outperforms many other methods at extracting features, modelling sequences, and recognizing characters. This characteristic makes it one of the best options for building our own OCR system using deep learning techniques. There are two different variants of CRNN which is being used in the project: one for English model and another for Devanagari model.\n\n\nFigure 9; Architecture for English Model\n\n\nFigure 10; Architecture for Devanagari Model\n\nThe common things in both the architecture are:\n1.Input layer: The height and width are reshaped to 94 x 1408 to both English and Devanagari images. As we know, the model is being trained with line base images with its corresponding label, height is slightly low then the width of the image.\n\n2.Dense Layer: It is a type of Artificial Neural Network (ANN) layer where each neuron in the layer is connected to every neuron in the preceding layer. This is why it is also known as Fully Connected Layers. It performs a weighted average, on the input data, and activate to the result of weighted average to introduce non-linearity into the network. This allows model to learn complex patterns in data during training process.\n\n3.Residual Blocks: The concept of a residual block originated due to the problem of vanishing gradient, that are particularly encountered while training deep neural networks. They were introduced in the paper of ResNet architecture as a skip-connection blocks which can learn residual functions by directly maping to the final reference of convolution layer. This means that instead of learning the desired underlying maping  by stacking multiple layers, residual blocks directly learn , and the original input x is added back to this learned residual to get the final output . This is how residual blocks deal with vanishing gradients problem by enabling gradients to flow directly through the network during training.\n\n\nFigure 11; Residual block\n\nThe figure above shows the first residual blocks of our architecture. At first input is normalized and squeeze, which is denoted by Lambda. Then it goes pass through , , and  two times, and at final steps the flattened input is again added with the output of convolution operation followed by activation function and Dropout.\n\n1.Conv2D with kernel size <3 x 3 x 3 x 32>: This represents a Convolution 2D layer with a kernel height and width of 3 x 3 and a depth of 3, which represents RGB color channel. The 32 at the end indicates that there are such thirty two kernels in this layer, each producing its own output feature map.\n\n2. Batch Normalization: Batch Normalization is a technique for normalizing the activation of each layer in a neural network. It is very helpful in reducing the internal covariate shift, which stabilizes and speeds the training process.\n\n3. Leaky ReLU Activation Function: The Leaky ReLU Activation function is a variant of the Rectified Linear Unit (ReLU) activation function. The difference is Leaky RelU introduces a small slop, typically a small positive value like 0.00001 for negative input values allowing a small or non-zero gradient when the input is negative. This helps to overcome dying ReLU problem, which could make neuron dead during training.\n\nReLu activation function is defined as below.\n\n\nIt returns 0 for any negative input value and return input value itself for any positive value.\n\nAnd Its derivative is as follow.\n\n\nLeaky ReLU has make difference by introducing following small alpha(α) value.\n\nIts derivatives is as follow.\n\n\nThe graph of ReLu(x) and Leaky ReLu looks like belows\n\n\nFigure 12; ReLu vs Leaky-ReLU\n\n4.Output Layer: This is the final layer of the architecture, that produces the output predictions of the model. The structure and activation function of the output layer is dependent on the nature of the task that is trying to be addressed. In our case, the output layer has neuron according to the unique vocab available in the datasets. For example, English model architecture has 93 neurons in Output layer because there are 93 unique character and sign such as like comas and slashes and Similarly, Devanagari model has 90 neurons in Output layer. The activation function used in our Output layer is , because we have multi-class classification problem, we need to choose one correct character among all the available options each time.\n\nThe difference in the architecture is in sequence modelling. The English model uses Bidirectional-Long Short-Term Memory (LSTM) but the Devanagari model contains Bidirectional-Gated Recurrent Unit (GRU). This is because English model has a lot of training data, and complex label. So, during hyperparameter tuning LSTM architecture was doing great work. But Devanagari datasets is not big as IAM handwriting, and the Devanagari character is much more complex than English character. So, Gated Recurrent Unit (GRU) was converging and doing much better than LSTM, and ultimately, I have modified the architecture to GRU. Below, we will detailly explored the Bi-LSTM and Bi-GRU architecture.\n\nAnnotation and symbols for understanding LSTM and GRU\n: Forgot Gate\n: Input Gate\n: Output Gate\n: Update Gate\n: Reset Gate\n: Current input at time step t\n: Previous hidden state at time step t-1\n: Candidate hidden state\n: Final hidden state\n: Weight matrix for the forget gate\n: Weight matrix for the input gate\n: Weight matrix for the output gate\n: Weight matrix for the candidate cell state\n: Weight matrix for the forget gate\n: Weight matrix for the input gate\n: Weight matrix for the output gate\n: Weight matrix for the candidate cell state\n: Candidate cell state at time step t\n: Update cell state at time step t\n\nThe above symbols will be used further to understand the calculation of the Sequential Model.\n\n4.1.1. Bidirectional LSTM (Bi-LSTM)\n\nBi-LSTM is a Recurrent Neural Network (RNN) which is used to process sequential data like text and sounds, where the current output is depended on previous output. It stacks multiple Long-Short Term Memory (LSTM) to reverse the direction of the flow of information. Before describing Bi-LSTM architecture, lets understand the simple LSTM architecture\n\n4.1.1.1. LSTM Architecture\n\n\nFigure 13; LSTM Architecture\n\nThe LSTM architecture consists of forgot gate, input gate and output gate. These gates are designed to regulate the flow of information through the Cell. It allows the LSTM to selectively update its and memory and also decide which relevant information to be kept or which to discard.\n\n1.Forgot Gate: The forgot gate takes the current input and the previous hidden state as inputs and computes the forgot gate vector  using a sigmoid activation function. It outputs the value between 0 and 1. If the value is approaching towards 1, then it indicates that information from the previous cell state is relevant and thus should be retained. But if the value is approaching towards 0 in the forgot gate vector, it indicates that the previous cell state information is irrelevant or less important for the current time step and should be discarded.\n\n2.Input gate: The input gate determines how much of the new information should be added into the cell state. It takes the current input and the previous hidden state as inputs and computes a gate vector it is using a sigmoid activation function, which controls the flow of new information into the cell state.\n\n3.Output gate: The output finalize how much of the cell state should be incorporated to the next hidden state. It also takes current input and the previous hidden state, candidate hidden state as input and passes them to the sigmoid activation function. Additionally, the cell state Ct is also passed through tanh activation to produce value between -1 and 1. And finally, the output gate combines the sigmoid gate values and the tanh values to produce the final hidden state .\n\nMathematical Operation\n\nInput Gate Operation\n\n\nForgot Gate Operation\n\n\nOutput Gate Operation\n\n\nCandidate State Cell\n\n\nCell State Update\n\n\nHidden State Output\n\n\nBi-LSTM adds two LSTM layers to flow the information from both the direction and and combine their output. The figure of Bi-LSTM is presented below.\n\n\nFigure 14; Bi-LSTM Architecture\n\nThe forward LSTM processes the input sequence from left to right and the backward LSTM processes it from the right to left. Like we see earlier, Each LSTM cell in both directions maintains a hidden state and a cell state to capture information from the input sequence. The forward LSTM captures information from past tokens in the sequence, while the back LSTM captures from future tokens. By processing the input sequence in both forward and backward directions, the Bi-LSTM can capture information in bidirectional context.\n\n4.1.2. Bidirectional GRU (Bi-GRU)\n\nLike Bidirectional LSTM, Bidirectional GRU is another variant of RNN which we have used in Devanagari architecture. It has also stacked multiple Gated Recurrent Unit (GRU) to reverse the direction of the flow of information. Single unit of GRU architecture is presented below.\n\n4.1.2.1. GRU Architecture\n\n\nFigure 15; GRU Architecture\n\nGRU also maintains a hidden state  for storing information about the input sequence up to the current time step. It achieves this through the use of gating mechanism which allow it to selectively update its hidden state based on the input at each time step. A GRU unit contains two gating mechanisms, update gate  and reset gate .\n\n1. Update Gate: The update gate determines how much of the previous hidden state should be retained and how much of the current input should added into the new hidden state. It passes current input and the previous hidden state though a sigmoid activation as input.\n\n2. Reset Gate: The reset gate determines how much of the previous hidden state should be forgotten. It also takes current input and previous hidden state as input and passes them through sigmoid functions.\n\nMathematical Operation\nUpdate Gate Operation\n\n\nReset Gate Operation\n\n\nCandidate Hidden State\n\n\nFinal Hidden State Update\n\n\nSame as Bi-LSTM, Bi-GRU adds two GRU layers to flow the information from both the direction and and combine their output. The figure of GRU is presented below.\n\n\nFigure 16; Bidirectional-GRU Architecture\n\nThe forward GRU processes the input sequence from left to right and the backward GRU processes it from the right to left. Each GRU cell in both directions maintains a hidden state and a cell state to capture information from the input sequence. The forward GRU captures information from past tokens in the sequence, while the back GRU captures from future tokens. By processing the input sequence in both forward and backward directions, the Bi-GRU can capture information in bidirectional context.\n\n4.2. Data Aquisition and Annotation\n\nOne of the reasons why Neural Network architecture outperforms traditional Machine learning algorithm is its tendency to learn better with the large datasets. Deep learning architecture are designed to feed huge volume of data. For training our system with English and Devanagari language, which need to have two different variants of data. One is English datasets, and another is Devanagari datasets. Each datasets have Image containing the text and Corpus with corresponding textual document of an image. For training the English Model, I have collected IAM handwriting datasets. This dataset is publicly available with certain credential by Research Group on Computer Vision and Artificial Intelligence at University of Bern. While Devanagari datasets isn’t available in desired form. I have collected the Bhagwat Geeta, a popular scripture images and labelled it manually. The detailed description of each dataset is below.\n\n1. Datasets for English\nThe IAM online handwriting dataset is around 7GB in size. It contains handwritten images with corresponding labels in txt file. It provides data in three forms: Full scale image, Sentences based image, Line based image and Word-based image. Full scale image is contained in three directories: formsA-D, formsE-H, and formsI-Z. Each full scale image under forms directory is made with the combination of form id and writer id. The sentence directory contains sub directory of form id, form id has sub directory of writer id, and inside writer id there is sentence based segmented image of each form, full scale image. Like sentence data, line directory also has subdirectory of form id, form id itself has subdirectory with writer id and writer id contains the line based segmented image of each form. Word directory also contains subdirectory of form id, form id has subdirectory of writer id and writer id directory contains separate image for each word that is contained on full scale image. With compared to sentence and line image, word image can be little big because word and sentence itself contains many words, and for each word there is different image. There is also optional xml directory if we like to work with xml files.\n\n\nFigure 17; Directory of datasets\n\nThe directory ‘ascii’ contains corresponding text label of all the forms, lines, sentences, and words. There are four text file each representing text, error status, and bounding box for corresponding images in forms, lines, sentences, and words.\n\n\nFigure 18; Corresponding annotation for IAM datasets\n\n2. Datasets for Devanagari\nThe indian phylosophical scripture bhagwat geeta datasets is prepared for Devanagari model is prepared manually. I collected the images from online source, and then utilized tools like online orc to get the textual content and online Devanagari typing tools to correct the error of labelled text. And then, I have written algorithm that will iterate over each image and text, tokenize each line of image with corresponding line in text file. There are altogether 3400+ line base image. The size of Devanagari datasets is quite low compared to English datasets, but the complexity is pretty much high. This Devanagari datasets is in Hindi and some sort of Sanskrit language. The actual sloka is in sanskrit and its corresponding interpretation is in Hindi.\n\n\nFigure 19; Devanagari directory\n\n4.3. Data Cleansing\n\nData cleaning is a crucial step for preventing data integration issue. We need to verify whether the corresponding label of training and validating images is correct or not. For English language model, if there is labelling issue, the tag of ‘err’ is given and thus we can ignore that line and if there is no error, the tag of ‘ok’ is given. But for Devanagari data, I have manually automated the task of segmenting each line as region of interest (ROI) with corresponding labels. Some time, region of interest (ROI) could not be capture but its corresponding label will be available, and it will be assigned to other images. This can create label mismatch problem. Therefore, I have manually checked and ensure region of interest (ROI) is perfectly captured for every line of and images. Sometimes, small dots, lines can also make confuses to edge detection algorithms, so some height and width is also given for the confirmation of valid text. If any of the criteria is not fulfilled, then the images is discarded.\n\n\nFigure 20; Detecting ROI for extracting text\n\n4.4. Data Augmentation\n\nTo build a generalize model; a model that can work good on diverse set of data, and reduce overfitting, data augmentation is very popular and useful techniques. It helps to increase the size and variety of our datasets. This technique applies transformation like Random Rotation, Random scaling, Image translation, Random Brightness and adding noise to the data. Each of these techniques is explained below.\n\n1.Random Rotation: This is a process of rotating the images by a random angle for enabling the OCR system to recognize characters at different orientations. I have implemented random rotations in data augmentation steps with the angle range of -10 to 10 degrees.\n\n\nFigure 21; Rotated image\n\n2.Random Scaling: Random Scaling is another technique for diversifying the training data. It includes resizing the line images with certain scale factor, so that our model learns to recognize character of different sizes. I have used the scale factor of 0.3, so that random scaling with range between 0.7 and 1.3.\n\n3.Random translation: Random image translation is used to randomly shift the image horizontally resulting in new variant of image with the change in position of content. The position of the textual part of the image will change it position and the padding will be applied to the current empty space. This technique is combinedly applied with the other data augmentation techniques.\n\n\nFigure 22; Random image translation\n\n4.Random Brightness: This is very rare that model will always get same light, brightness every time. It is very crucial to consider different images containing different levels of brightness. Some images would be lighter than others, some may be little darker than others. Model should be ready to adopt and work on this. So, random brightness is applied with a factor of 0.4, which will result between 0.6 and 1.4.\n\n5.Adding Noise: While processing images in the real world, this is not sure that every of them will be very clear and sharp. So, the model needs to be trained for working on disrupted images, where some pixel values are not clear as others. So, adding noise is a good option which we can use for making generalize learning.\n\n\nFigure 23; Adding noise in image\n\n4.5. Image preprocessing\n\n1.Sharpening: Image sharpening is a technique for making image look concise and clear by enhancing the edges, brightness and contrast of a image. It helps to improve the quality and readability of images by minimizing the factor like blur, noise which may came with the factor like low camera, camera shaking during snapshot, low resolution, and compression. It works by increasing the distance between pixel value of near the boundary’s areas. It can sometimes create over sharpening issue, so we I have used appropriate clipping value to prevent the overflow of pixels information.\n\n2. Grayscale conversion: A gray-scale image is one which has single color channel. Normally, the image is composed of three-color channels, red, blue, and green. Each of these color channels has pixel value ranging from . These makes computation very much complex and gray-scale image are more than sufficient to extract the feature from images and there is no need of processing all these color channels. In our images of size , if use color channels it would be , which equals to . But after grayscale conversion, it would result to . So, we can just image how it is reducing the training complexity by reducing input size to less than its half size.\n\n3.Binarization and thresholding: This method is used in the project for converting grayscale images into black and white images, also known as binary image. I have binarized the image using the global threshold method. First, the global threshold value is determined, and find out whether the pixel has gray value greater than given threshold or not. If the pixel value is greater than threshold, it will be converted to 1 and if the pixel value is less than threshold, it is assigned to 0. Here, 1 represents white and 0 represents black. It will simplify the process by representing the data in binary form.\n\n4.Dilation: Dilation expands the size of an object by convolving the image with a structure element. A small binary image or matrix is used as structuring element to define the neighborhood of a pixel during dilation process. The shape of the binary image will determine the degree to dilate.\n\n5.Erosion: Erosion shrinks the size of an object by convolving the image with structure element. After convolving the object, it returns the result of erosion operation as a new image where the pixels value of image is shrunk, also known as eroded.\n\n5. Performance of the Model\n\nThe popular metrices that is used to evaluate the performance of the OCR model are CER (Character Error Rate) and WER (Word Error Rate). We have monitored the performance of both Model using Tensorboard. The following figures shows the different metrices to evaluate the model.\n\n\nFigure 24; Tensorboard for Monitoring Models\n\nWe can understand the above figure in three parts. First three metrics shows the performance of model during training time, Last three metrics shows the performance of model during validation and middle three metrics is the comparison between training and validation performance in different steps.\n\n5.1.1. English Model Performance\n\n\nFigure 25; Training vs Validation CER (Character Error Rate) for English Model\n\nThe above graphs shows that CER of validation is moving along with the fall of CER in training time.\n\n\nFigure 26; Training vs Validation WER (Word Error Rate) for English Model\n\nThe above line plot also shows that word error is falling when we keep training the model.\n\n5.1.2. Devanagari Model Performance\n\n\nFigure 27; Training vs Validation CER (Character Error Rate) for Devanagari Model\n\nThe above graph shows how validation CER of Devanagari Model is Falling with time.\n\n\nFigure 28; Training vs Validation WER (Word Error Rate) for Devanagari Model\n\nThe above figure shows more we train the model , WER will also fall accordingly.\n\n5.1.3. Final Evaluation\nModel Name\tCER\tWER\tLoss\nEnglish training\t0.01001\t0.0800\t1.566\nEnglish testing\t0.01005\t0.0955\t1.738\nDevanagari training\t0.1000\t0.3578\t30.456\nDevanagari testing\t0.13162\t0.4494\t35.3282\n6. Deployment\n\nIn this section, I will discuss the deployment of the OCR model for general users. The application leverages the Django Rest Framework for backend API development, Vanilla JavaScript for frontend interaction, and SQLite as the default database for storing information.\n\n\nFigure 29; Functional Decomposition Diagram\n\nIn the above FDD, I am have splited the system into four parts i. e. ,User Interface, Deep Learning module, Data Management and Backend module. The sections make a modular view of the related functionalities, which when put together form an entity of the programme. By viewing this illustration we would be able to investigate the levels of the system, the interconnections as well as the relationships of the project. This awareness is indeed crucial for an adequate planning and decision-making process throughout the project complex lifecycle.\n\n6.1. Deployment Process\n\n1. User Interaction\n\ni) Image Upload: Users can upload images that they want to process.\n\n\nFigure 30; Uploading image\n\nii) Image Editing: Users have the option to edit the images as needed.\n\n\nFigure 31;Option for editing uploaded image\n\n2. Language Selection\nUsers select the language of the document, which determines which model will be used on the backend.\n\n\nFigure 32; Selecting language according to documents\n\n3. Edge Detection\nUsers identify the Region of Interest (ROI) in the image. They have three options for edge detection algorithms to choose from.\n\n\nFigure 33; Edge detection\n\n4. Text Prediction\nAfter selecting the appropriate edge detection algorithm, the model processes the image and predicts the text.\n\nThe predicted text is displayed in a text box, allowing users to:\ni) Copy the text.\nii) Export it in .txt or Word format.\n\nThis deployment strategy utilizes a modular architecture, ensuring that each component functions effectively while contributing to the overall performance of the OCR application. This structure not only enhances user experience but also supports scalability and maintainability of the system.\n\n\nFigure 34; Text Prediction\n\n6. Conclusion\n\nThe primary aim of this work was completed through research and developing an OCR designed to decipher images with text in both the English and the Devnagari languages. The specific tasks were to apply the methods of deep learning to create the OCR system from the ground up, ensure the model familiarizes with the concept of UI, and compare the results with the real performance of the OCR systems. The particular academic questions that were being investigated earlier in this paper include understanding the limitations of existing OCR systems, the causes of varied performance of an applied language, and the steps that should be taken in developing a general OCR engine.\n\nThe major conclusions and findings that can be made throughout the course of this projects are the following. The challenges we faced during the development of the multilingual OCR system from scratch helped us to understand which factors have a major impact on the error rate, in particular: resources: easy access to large variety of training data; computational constraints; dealing with multiple languages. The OCR model that was trained with the help of deep learning and that was developed specifically for the application could recognize not only the English language but also Devanagari despite the fact that the comparison with generic OCR services was employed. The incorporation of the OCR model with the user-friendly interface made the entire process of text information extraction from documents ideal for both laypersons and experts with relative ease.\n\nI also identified the problem associated with the current OCR and the solution I want to suggest is Intelligent Document processing. This method involves combining Machine learning for understanding predefined sets of rule in the text document like margin, table, border, signature and many other things. Another issue that comes up is the availability of training data, the more data we have we can build better OCR. We have also observed that OCR would not have same level of performance in different languages because, there architecture may vary, resources are imbalance and the complexity of different languages are also identical.\n\n7. Reference\n\n1.Abisado, M., Imperial, J. M., Rodriguez, R. & Fabito, B., 2020. Doctor’s Cursive Handwriting Recognition System Using Deep Learning. Manila, ResearchGate.\n\n2.Biró, A. et al., 2023. Synthetized Multilanguage OCR Using CRNN and SVTR Models for Realtime Collaborative Tools. Applied Sciences, 13(7), p. 4419.\n\n3.Cristiano , C., 2020. For a Science-oriented, Socially Responsible, and Self-aware AI: beyond ethical issues. In: 2020 IEEE International Conference on Human-Machine Systems (ICHMS). IEEE, pp. 1-4.\n\n4.Dey, R., Balabantaray, R. C. & Mohanty, S., 2022. Approach for Preprocessing in Offline Optical Character Recognition. Bhubaneswar, IEEE.\n\n5.Huang, J. et al., 2021. A Multiplexed Network for End-to-End, Multilingual OCR. s.l., IEEE.\n\n6.K.O, M. A. & Poruran, S., 2020. OCR-Nets: Variants of Pre-trained CNN for Urdu Handwritten Character Recognition via Transfer Learning. Procedia Computer Science, 171(1877-0509), pp. 2294-2301.\n\n7.Khan, A. A. et al., 2023. AI Ethics: An Empirical Study on the Views of Practitioners and Lawmakers. IEEE Transactions on Computational Social Systems, Volume 10, pp. 2971-2984.\n\n8.Kumar, A., Singh, P. & Lata, K., 2023. Comparative Study of Different Optical Character Recognition Models on Handwritten and Printed Medical Reports. 2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA), 20 04, pp. 581-586.\n\n9.Memon, J., Sami, M., Khan, R. A. & Uddin, M., 2020. Handwritten Optical Character Recognition (OCR): A. IEEE Access, Volume 8, pp. 142642-142668.\n\n10.Muneer, S. U. a. N. M. a. K. B. a. Y. M. H., 2020. Evaluating the Effectiveness of Notations for Designing Security Aspects. pp. 465-471.\n\n11.Sayallar, C. ̧. a., Sayar, A. & Babalık, N., 2023. An OCR Engine for Printed Receipt Images using Deep Learning Techniques. International Journal of Advanced Computer Science and Applications, 4(2).\n\n12.Sethu, S. . G., 2020. Legal Protection for Data Security: a Comparative Analysis of the Laws and Regulations of European Union, US, India and UAE. 2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT), pp. 1-5.\n\n13.Yu, . W., Ibrayim, M. & Hamdulla, A., 2023. Scene Text Recognition Based on Improved CRNN. Information, 14(7), p. 369.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\nBackground\nLiterature Review\n\n3.1. Introduction\n\n3.2. Related Works\n\nMethodology\n\n4.1. Architecture\n\n4.1.1. Bidirectional LSTM (Bi-LSTM)\n\n4.1.1.1. LSTM Architecture\n\nView all\nDatasets",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "music-composer-classification-of-midi-files-using-deep-learning-qZ9m02sVKl8C",
    "username": "gGary Takahashi",
    "license": null,
    "title": "Music composer classification of MIDI files using Deep Learning",
    "publication_description": "Back to publications\nSep 15, 2024\n●\n134 reads\n●\nCreative Commons Attribution (CC BY)\nWinner of\nMost Engaging Presentation\nat the\nAI Project Showcase 2024\nMusic composer classification of MIDI files using Deep Learning\nMIDI\nmusic classification\nG\nGary Takahashi\nLisa Vo\nLike\nBookmark\nShare\nAbstract\n\nThe widespread use of electronic musical instruments has driven the need for a digital representation of musical notation. The MIDI (Musical Instrument Digital Interface) system has evolved over the years as the standard notation format for modern electronic instruments, and can store and process instrument patch settings as well as performance details and metadata. We wished to use two deep learning models (LSTM and CNN) to determine whether the music of four composers (Bach, Beethoven, Mozart and Chopin) could be successfully classified using only information contained in MIDI musical files from a Kaggle dataset. Such a task requires careful feature selection, identifying which musical components distinguish a composer's characteristic musical styles. Visualization of the differences based on these features helped to confirm the utility of the chosen parameters. The pretty_midi library was used to extract the selected features, and we piloted our efforts using a Support Vector Machine model, which had been used with success previously, and achieved 90% accuracy using the radial basis function kernel. We trained an LSTM model and a CNN model, and achieved 85% accuracy with our LSTM model and 91% accuracy with the CNN model. The implications of our findings are discussed.\n\nIntroduction\n\nFor centuries, musical information has long been recorded using a standardized notation with musical notes written on lines of staff, and with special instructions written alongside to indicate changes in note velocity, loudness or softness, accents, tempo and other direction. This has long sufficed for human performers to interpret performance on traditional musical instruments. However, with the development of electronic musical instruments, orthodox musical notation was inadequate to capture the number of parameters needed to specify all the nuances of performance that these new instruments were capable of delivering. Synthesizer configuration, for example, was best suited to process data formatted digitally, much as document formats were developed for word processors.\n\nRather than storing digital representations of the audio spectrum, a communications protocol was developed that provided a digital interface, but which controllers could send activating signals to a compatible musical instrument, directing it to play notes at a certain pitch, velocity, duration, and other characteristics. This is the Musical Instrument Digital Interface (MIDI) protocol that was developed in the early 1980s, primarily to store settings from a burgeoning array of complex digital electronic instruments that were no longer using patch cords to modulate and channel sound creation. It was envisioned that these instruments would read and respond to digital information, and that these instructions could be stored in a convenient file format. The information could then be used to instantly configure a musical instrument as designed by the musician. Indeed, a key motivation for the development of MIDI was to enable live performance implementations on digital electronic instruments, such as synthesizers. The protocol has undergone revisions and has adapted to meet the demands of newer digital instruments, which were capable of touch sensitivity, polyphony and greater musical expression, such as sostenuto, after-touch, pitch-bend, and many other features.\n\nMIDI File Formats\n\nMIDI files are structured as “chunks” with the basic chunks being the Header chunk and the Track chunk. There are three file formats:\nFormat 0 has a header chunk followed by one track chunk.\nFormat 1 allows for one or more tracks of a MIDI sequence.\nFormat 2 indicates one or more sequentially independent single-track patterns.\n\nHeader chunk\n\nThe header chunk starts with four bytes, MThd (4D 54 68 64) which denotes the beginning of the file. This is followed by six bytes (00 00 00 06 ff ff), then two bytes denoting the MIDI file format discussed above, then two bytes indicating the number of track chunks in the file, and two bytes denoting the number of ticks per quarter-note.\n\nTrack chunk\n\nThis chunk is much more complex, and is where the actual data is stored. Each track chunk consists of an initial four byte header, MTrk (4D 54 72 6B), followed by MIDI events, sysex events and meta events. Each of these contain a wealth of information.\n\nMIDI events contain detailed instructions on when a note is on, or off, the key pressure, control change, program change (instrument to be played), channel pressure, channel mode messages, and so on. This defines the basic characteristics of the keyboard interaction.\n\nSysex events are MIDI exclusive information, such as song selection, timing information, tuning requests, resets, etc.\n\nMeta events contains meta-information about the music, such as copyright information, lyrics, time signature, instrument name, track name, cue information for staging events, etc.\n\nThe task of classifying each MIDI file to the composer whose musical piece it describes, comes down to being able to extract information for the MIDI events that correspond to features that humans would use to identify a composer. These include musical complexity in the melodic line, harmonies, chord progressions, polyrhythm, variations in tempo, the use of counterpoint, ostinato, pedal point, the use of certain stylistic cadences, the selection of instruments, and changes in time and key signatures, and overall structure (sonata form, fugue, dance, etc).\n\nWe will use selected features from the MIDI files in the Kaggle dataset to classify amongst four composers: Bach, Beethoven, Mozart and Chopin. Although their musical styles are distinctive prima facie to a human listener, there are some stylistic similarities amongst these four that might prove to be a challenge to machine learning algorithms.\n\nBach’s music is classified as Baroque-era music. His music is notable for heavy use of counterpoint, with multiple independent melodic lines playing together. Harmonic progressions contained dissonances in progression, creating tension and anticipation. Melodic lines were often dense, and would carry across various voices. Bach made liberal use of Baroque ornamentation, such as mordents, trills, and appoggiaturas. Variations in tempo were not as pronounced as with other composers. Most of his music was composed for chamber groups, and even the Brandenburg Concertos were intended for a smaller orchestral ensemble. Bach used the Phrygian and Plagal cadences.\n\nBeethoven’s early music was in the classical period but his later period overlapped with the early Romantic era. His music was notable for dramatic contrasts in mood, with serene lyrical moments often followed by passionate and intense passages. He tended to follow the sonata form in many of his compositions. His symphonic compositions were rich and lush and his orchestration demanded a large ensemble. He often composed programmatic music, such as with his symphonies. His piano sonatas often used ostinato and did not utilize counterpoint as did Bach. Use of ornamentation was sparse.\n\nMozart also composed during the classical period, however his music emphasized beauty, with more elegant melodic and singing lines, but his works include compositions of an intensely spiritual nature, such as the Missa Solemnis. He also followed the sonata form, and emphasis was placed on balance and symmetry. Mozart composed for piano, chamber and orchestral ensembles. Mozart frequently used trill cadences, as did other classicists.\n\nChopin composed during the Romantic period, and his compositions were largely for the piano. As such, his music emphasized virtuosity and presented challenges to the performer, such as polyrhythms, large ranges in the melody lines, complexity in harmonic voicing and texture. Ornamentation was largely in the form of technically difficult fiorituras. Dramatic changes in tempo are common in his pieces, and sections are clearly delineated in his pieces where different melodic statements and harmonies contrast with that in other sections. In contrast to the other three composers, performers often use rubato to increase the emotional intensity of the lyrical melodies.\n\nThe challenge in classifying MIDI files to exploit these stylistic differences requires being able to extract the necessary information from the MIDI events section of the Track chunk.\n\nMethodology\nExploratory Data Analysis\n\nThe dataset was obtained from Kaggle (Kaggle, 2019). The dataset consisted of the works of 175 composers in 3929 MIDI files. The musical selections encompassed piano, chamber and orchestral works.\n\nWe first checked for files with missing or duplicated data, and did not identify any such files with these deficiencies. From the dataset, we selected folders of the four composers listed above, and discarded the remainder of the dataset. The number of works by the four composers was unequal, with the most number of compositions being by Bach (1024 files), followed by Mozart (255 files), then Beethoven (212 files), then Chopin (136 files).\n\npretty-midi\n\nTo extract the features from the MIDI files, we used the pretty_midi Python library designed to read and manipulate MIDI files (craffel, 2023a). This library can also be used to read and extract a wealth of features from the MIDI files (craffel, 2023b). These include detection of parameters related to tempo, key signature, instrument selection and changes, note duration, note velocity, pitch-transition and pitch-bend, sustain pedal dynamics, as well as numerous other parameters pertinent to performance-related needs and specialty instruments.\n\nTo access these features, one first creates a PrettyMIDI object, such as follows:\n\nmidi_data = pretty_midi.PrettyMIDI(midi_file)\n\nFrom this, one creates midi_data.instruments, which contains information about note duration and pitch classes. Key signature and changes can be accessed with midi_data.key_signature_changes.\n\nAnother particularly helpful function is piano roll, which contains information on note velocity, pitch variance & transitions, rhythmic density, and chord density.\n\npiano_roll = pretty_midi.get_piano_roll()\n\nWith these functions, we collected information about the following parameters:\n● Note density\n● Pitch variance\n● Velocity statistics (mean, max, variance)\n● Polyphony\n● Rhythmic density\n● Average pitch interval\n● Chord density\n● Pitch transitions\n● Average note duration (for sustained notes)\n\nWe investigated whether these parameters would allow for sufficient discrimination between the different composer styles, such that a deep learning model could be trained to classify based on these features.\n\nOn the first run of the feature extraction function, we identified 18 files which were poorly formed, either lacking the proper headers, list indexes out of range, or where the key signature information was invalid (see Figure 1).\n\nFigure 1: Errors generated on MIDI file processing on the raw Kaggle dataset.\n\nThese errant files were manually deleted, and the analysis was performed on the remaining MIDI files without error.\n\nTrain-test splitting was performed using the Sci-kit Learn train_test_split() function. As one can see in Figure 2, the range of values is large, and therefore Standard Scaling was applied to normalize the data before training.\n\nFigure 2: Scatter plot of values in X_train and X_test.\n\nAs seen above, the number of MIDI files associated with each composer varied greatly, and this can be seen especially in the training set, where there was an almost 8-fold difference between Bach and Chopin. An imbalanced dataset will introduce bias into the model, as it learns to preferentially classify MIDI samples as Bach, to statistically minimize classification loss. To deal with this, we used SMOTE to equalize the training data (Figure 3).\n\nFigure 3: The effect of SMOTE on addressing data imbalance in the training set.\n\n#Visualizations\n\nPrior to deep learning classification, visual analysis of the differences in features amongst the composers can reveal differences amongst the four composers.\n\nLooking at tempo distribution, one can see that the tempo distributions for three of the four composers tended to have tempos of around 100 to 150 bpm (moderato to allegro). Bach notably composed many pieces to be played much slower (adagio, andante) or much faster (vivace and presto) than the norm (Figure 4).\n\nFigure 4. Tempo distribution by composer\n\nThis is also seen in the plot of the average velocity distribution of notes in the various compositions (Figure 5). The violin plots show that the works of Chopin and Beethoven were similar in tempo and note velocity. Although the tempo of Bach’s works were not necessarily rapid, he did incorporate more high-velocity notes within the compositions.\n\nFigure 5. Average note velocity distribution.\n\nAnother way to distinguish among the composers is to examine the number of instruments vs average velocity (Figure 6).\n\nFigure 6: Number of instruments in each composition plotted against average note velocity.\n\nFor composers such as Chopin, who composed primarily for the piano, most of his data points are along the left of the graph. Mozart, Bach and Beethoven composed works for the piano as well as chamber and orchestral ensembles.\n\nA first pass at delving into musical harmonics and chromaticism in the composers’ musical style is to examine the pitch classes utilized. In Figure 7, we plotted the pitch class usage for each composer, normalized to the number of compositions. Pitch class reflects the key signatures used in various musical compositions, but also takes into account accidentals, so by itself, it doesn’t necessarily reflect how “adventurous” a composer was in straying from the diatonic path.\n\nFigure 7: Normalized pitch class usage by composer.\n\nAnother way of visualizing the variation in pitch class usage by each composer is a radar plot, as in Figure 8. In this plot, it is clear that the music of a composer like Chopin used a more even distribution of pitch values, either through key signature selection or a more liberal use of accidentals. Other composers tended to favor certain pitch classes over others.\n\nFigure 8: Radar plot of normalized pitch class usage by composer\n\nFinally, we delve into chromaticism in Figure 9, looking at pitch_classes that were only at non-diatonic intervals related to the key signature. This takes into account accidentals, but also includes harmonic expressions and progressions that extend beyond tonic and dominant chords. It is not surprising that Mozart’s music favored major chords, and that both Bach and Chopin were more adventurous in the pitch_classes they used. Beethoven was intermediate, and incorporated more non-diatonic notes on the chromatic scale.\n\nFigure 9: Average proportion of non-diatonic notes by composer and key.\n\nNow that we have seen how pretty_midi can extract features which can distinguish between the various composers, we now turn our attention to showing how this utility can be used to train deep learning models to classify the MIDI files into their respective composers.\n\nRelative Feature Importance\n\nWe have seen that the selected features are indeed able to evince differences amongst the musical styles of the four composers. Before embarking on the classification process, we sought to explore which of these features would be most contributory in the classification.\n\nOne way to determine this would be to use Random Forest modeling, which provides a means of ranking feature importance by way of determining the relative contribution of feature to Gini impurity index. The advantage of this method is that it is relatively easy to calculate, but it is a method that is dependent on the Random Forest classification model. We performed this analysis and found that the most important features were num_instruments, velocity variance and polyphony (Figure 10).\n\nFigure 10: Gini importance values in a Random Forest model, to determine the relative feature importance in the classification of composers.\n\nA more detailed analysis of the relative contribution of feature to composer classification is provided by SHAP (SHapley Additive exPlanations). In Figure 11, we see the relative importance of each feature toward the classification decision in each composer’s MIDI dataset.\n\nFigure 11: Mean absolute SHAP values for each composer.\n\nSeparate beeswarm plots further detail the relative importance of each feature in composer classification (Figure 12). It is clear that num_instruments, velocity_variance and polyphony are among the topmost important features used in classification.\n\nFigure 12: Ranked importance of the extracted features for the classification of each composer. Velocity variance was most important for the classification of Bach and Beethoven, however the number of instruments in the composition was of greater importance in the classification of the music of Chopin and Mozart.\n\nSupport Vector Machines\n\nPrevious work on classification of MIDI samples to different musical styles, found high accuracy using Support Vector Machines, especially with the radial basis function (Gnegy, 2014). As such, we were indeed able to achieve high classification accuracy (0.90) (Figure 13).\n\nFigure 13: Classification report using SVM (rbf).\n\nThis final project required classification using two deep learning algorithms: LSTM and CNN. We therefore trained the extracted MIDI information on these two models.\n\nResults\nLSTM\nData Preparation\n\nLSTM models require a three-dimensional input shape, which consists of (batch size, time steps, the length of one input sequence). To meet this requirement, the augmented train and test sets were reshaped in preparation for training.\n\nNext, because the target variables are categorical strings, we encoded the y_train and y_test set to integer values using scikit-learn's LabelEncoder. The encoded outputs are converted to a binary matrix representing the composer names. The matrix contains only 1s and 0s, with 1 indicating that the music file belongs to the composer at that index, with 0 indicating that the music file does not belong to the rest. We arbitrarily selected a sequence length of five to create sequences of the training data and test data. It is with these sequences that the model will train upon.\n\nModel Training\n\nThe LSTM model consists of the following layers (in no particular order):\n\nInput layer with shape of (5, 26)\nLSTM layer with 15 units and recurrent dropout rate of 0.4\nLSTM layer with 5 units and recurrent dropout rate of 0.4\n4 Batch Normalization layers\n2 Dropout layers at a 0.4 and 0.5 dropout rate, respectively\nFully-connected Dense layer with softmax activation\n\nBecause we are performing multiclass classification, we chose to use the Adam optimizer with an initial learning rate of 0.01 and categorical cross entropy loss function. To reduce overfitting and facilitate convergence, we implemented early stopping and a learning rate scheduler, both monitoring validation accuracy and loss. Although we set our LSTM model to train for 1000 epochs, early-stopping terminated training at 224 epochs, for training and validation accuracies of 0.87 and 0.85, respectively. The training and validation loss were 0.30 and 0.48, respectively.\n\nFigure 14: LSTM training and validation accuracy (left). LSTM training and validation loss (right).\n\nThe training and validation loss graph (Figure 14) shows that our efforts to reduce overfitting has benefited the model, as seen in the convergence of the training and validation loss curves, with the validation loss curve not straddling above the training loss curve.\n\nPrediction and Evaluation\n\nAfter making predictions on the X_test set, we generated a classification report.\n\nFigure 15: LSTM classification report\n\nThe LSTM model had an accuracy of 0.85 (Figure 15). This accuracy level is evident in the confusion matrix (Figure 16), as we see that most of the predictions lie in the diagonal row of the matrix. This means that most of the predictions were correct. The scarce numbers in the cells surrounding the diagonal row signify instances of incorrect predictions with the y-axis indicating the true label and the x-axis indicating the predicted label. Also from the diagonal row, we see that most of the samples in the test set are from Bach because SMOTE synthetic oversampling is not applied to the test set.\n\nFigure 16: Confusion matrix for the LSTM classification model.\n\nCNN\nData Preparation\n\nLikewise with LSTM, the CNN model requires a three-dimensional input shape. We followed the same steps with reshaping the data, but we excluded the sequencing step, as that step is required in LSTMs due to its recurrent nature.\n\nModel Training\n\nThe CNN model consists of the following layers (in no particular order):\n\nInput layer\n2 Convolutional 1D layers with 64 units and 3x3 kernel size\n2 Convolutional 1D layers with 128 units and 3x3 kernel size\n2 Convolutional 1D layers with 256 units and 3x3 kernel size\n2 Max Pooling 1D layers with pool size of 2\n1 Global Average Pooling 1D layer\n9 Batch Normalization layers\n5 Dropout layers with 0.2, 0.3, 0.4, 0.5, 0.5, respectively\n2 Dense layers with 128 and 64 units, respectively, and ReLU activation functions\nFully-connected Dense layer with softmax activation\n\nThese layers were developed empirically, balancing the need to achieve convergence and training accuracy, and the need to avoid volatility in validation accuracy and avoiding overfitting. Because we are performing multiclass classification, we chose to use the Adam optimizer with an initial learning rate of 0.001 and categorical cross entropy loss function. To reduce overfitting, we implemented early stopping and a learning rate scheduler, both monitoring the validation loss. Again, we set the number of epochs for our CNN model to 100, however early stopping terminated training at 58 epochs, yielding training and validation accuracies of 0.94 and 0.90, respectively. The training and validation losses were 0.18 and 0.33, respectively.\n\nFigure 17: CNN training and validation accuracy (left). CNN training and validation loss (right).\n\nThe training and validation loss graph (Figure 17) shows that our efforts to reduce overfitting has benefited the model, as seen in the convergence of the training and validation loss curves, with the validation loss curve slightly straddling above the training loss curve.\n\nPredictions and Evaluation\n\nAfter making predictions on the X_test set, we generated a classification report.\n\nFigure 18: CNN classification report\n\nThe CNN model had an accuracy of 0.91 (Figure 18). Likewise with the LSTM model, the confusion matrix (Figure 19) consists of the majority of predictions sitting in the diagonal row, indicating a high majority presence of correct predictions. From a visual standpoint, there are few incorrect predictions. Again, there is a high amount of Bach instances, since synthetic oversampling is only intended to be applied to the training datasets.\n\nFigure 19: Confusion matrix for the CNN classification model.\n\nConclusions\n\nWe have shown that with careful selection of musical features, we were able to extract key information stored in MIDI files of composers’ musical oeuvres, which allowed deep learning networks to be trained on these characteristic features, such that they could successfully classify the composer of the music accurately on a separate validation set. Our models achieved an accuracy of at least 0.85 overall. The main limitation of the dataset was the relative imbalance of representative files amongst the various composers, and we endeavored to correct for this using SMOTE data augmentation to guide the models away from overtraining on composers, such as Bach, who had the greatest representation in the dataset. Despite this, the individual f1-scores reflected each composer’s dataset size. It is notable, however, that despite having the least number of compositions in the dataset, both the LSTM and CNN models classified better on Chopin compositions than they did on Beethoven or Mozart.\n\nSome factors that impacted our models’ performances were the size of the dataset and the class imbalance. Neural networks perform better with larger datasets, as they are able to capture more feature details due to the large number of trainable parameters. The class imbalance in our music dataset introduced a bias favoring Bach, which necessitated data augmentation to mitigate this bias. SMOTE was used to even out the class imbalance. One downside to this is that the minority classes have more instances of synthetic data than that of Bach. Ideally, we would want the dataset to comprise real data as much as possible. However, data augmentation was necessary in this situation to help the neural networks to better generalize on unseen data.\n\nWe attempted to intuit how each deep learning network approached the task of classification by examining some of the characteristics of the features obtained by pretty_midi for each composer. In our EDA, we found that different MIDI features were more salient amongst the various composers, and these differences could potentially explain how the models were trained. For example, with Bach’s music, we learned that the strongest predictors were the number of instruments, velocity variance, and polyphony. For the remaining composers, recognizing the most significant predictors was not as deterministic. We are not certain that the networks operated using this kind of analytic breakdown, however it is conceivable to imagine them trained in this fashion. The EDA did give us the confidence that we had selected an adequate number of suitable features for training, however.\n\nIt is intriguing that using the feature set we had selected, pretty_midi performed less accurately distinguishing between Chopin and Beethoven, which can be seen in the analysis of the tempos among the composers. Although their music is considered to be from the Romantic era, a human listener would have no problem distinguishing differences in their musical styles, suggesting that there are musical characteristics registered by the human brain that cannot be captured by dry assessments of features such as tempo or note velocity. The modal selection of a piece, selection of chord voicings, the evocation of nationalistic or cultural melodic motifs, and association with tunes with which we are already familiar, all affect our ability to recognize and classify a particular piece. These features are part of the humanistic aspects of music which MIDI was not designed to capture, nor would it likely be able to.\n\nBesides using more balanced data, we believe there is still room for improvement in the implementation of the neural networks. Many hyperparameters could potentially benefit from fine-tuning, such as learning rate, optimizer type, batch size, and sequence length in the case of LSTMs.\n\nThe ability for deep learning networks to capture essential musical features has important implications. Our results suggest many possibilities in the application of MIDI analysis and classification tools. With further training, larger dataset, and fine-tuning, our models can be extended to identify music beyond the classical genre and extend to other topics, such as emotion, time period, instruments used, and more. In addition to this, our models can be applied to enhanced compositional assistance in generative AI; voice-to-MIDI controllers; enhanced recommender systems; automated mixing and mastering of music; and the design of new instruments and effects for synthesizers in music production.\n\nTo summarize, the model results have demonstrated the effectiveness of using deep learning to perform MIDI analysis and classification, which can serve both producers and listeners of music.\n\nReferences\n\nCraffel, C. (2023a). Pretty_midi. \nhttps://github.com/craffel/pretty-midi\n\nCraffel, C. (2023b). Pretty_midi documentation. \nhttps://craffel.github.io/pretty-midi/\n\nGnegy, C. (2014). Classification of musical playing styles using MIDI information. Stanford University. \nhttps://cs229.stanford.edu/proj2014/Chet%20Gnegy,Classification%20Of%20Musical%20Playing%20Styles.pdf\n\nKaggle. (2019). Midi_classical_music. \nhttps://www.kaggle.com/datasets/blanderbuss/midi-classic-music\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nMIDI File Formats\n\nHeader chunk\n\nTrack chunk\n\nMethodology\n\nExploratory Data Analysis\n\npretty-midi\n\nRelative Feature Importance\n\nSupport Vector Machines\n\nView all\nComments\n(1)\nMost recent\n\n✨ Want to join the conversation?\n\nSign in or create an account to comment.\nMarcin Sikorski\n5 months ago\n\nGreat work, I like it! Would you mind sharing the code/GitHub link? ☺️\n\nLike (1)\nReply\nCode\nDatasets\nYou might be interested\nAmbience-to-Music Neural Style Transfer (AM-NST)\nDec 30, 202418 reads\ndeep learning for audioneural style transfer\nMusic AI\nE\nApr 05, 202541 reads\nGrecia Music Game\nFeb 13, 202549 reads\nCarnatic Music Assistant\nV\nSep 02, 20254 reads",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "n8n-building-no-code-agentic-workflows-gfrpsUkufCGL",
    "username": "aAmina Javaid",
    "license": null,
    "title": "n8n: Building No-Code Agentic Workflows ",
    "publication_description": "Back to publications\nOct 09, 2025\n●\n52 reads\n●\nCreative Commons Attribution-NonCommercial (CC BY-NC)\nn8n: Building No-Code Agentic Workflows\n#LLMs\n#Tools\nAgenticAI\nAgenticWorkflows\nLowCode\nn8n\nNoCode\nA\nAmina Javaid\nReady Tensor\nMohamed Abdelhamid\nLike\nBookmark\nShare\n\nThe Challenge of Building Agentic AI\n\nAgentic AI is one of the most exciting shifts in technology today. Instead of just responding to prompts, AI agents can plan, reason, and act — booking appointments, processing data, running workflows, even coordinating with other agents. Companies are rushing to adopt this approach because it boosts productivity, lowers costs, and frees people for more creative work.\n\nBut here’s the challenge: building these systems is hard. It requires strong coding skills. Developers often need to master programming fundamentals, APIs, and integrations to stitch together an agentic system. While coding provides maximum flexibility, it also demands time, effort, and expertise, creating a barrier for entrepreneurs, domain experts, and innovators who don’t come from a technical background.\n\nNow imagine the alternative:\n➡️ What if you could build a working prototype in hours or days, without writing a single line of code?\n\nThat’s the gap no‑code platforms are trying to close. They allow you to skip that heavy lift - making the process of building agentic AI fast, accessible, and experiment‑friendly.\n\nMeet n8n – The No-Code/Low-Code Agentic AI Platform\n\nThere are many no-code tools out there, but n8n stands apart — especially for agentic AI. It’s the rare open-source workflow automation platform that lets you build powerful systems using a visual interface, while still allowing you to write code when needed.\n\nDespite its flexibility and enterprise-grade features, n8n is surprisingly beginner-friendly. You get the ease of drag-and-drop logic, plus the freedom to self-host and customize deeply — a rare combination.\n\nThe name n8n (pronounced n-eight-n) comes from “nodemation”:\n\nnode- for its modular, visual interface built on Node.js\n-mation for automation — its core purpose\n\nIt’s become a go-to platform for teams of all kinds:\n\nNon-technical users can experiment and build without writing code\nProduct managers can test and demo ideas fast\nAI teams can launch real agentic systems without heavy infrastructure\n\nIn short: n8n is the bridge between the promise of agentic AI and the reality of building it.\n\nBasics of n8n\n\nn8n is a visual workflow builder. You just drag nodes onto a canvas, connect them, and hit run. It’s like building a logic diagram that actually executes.\n\nEach node handles a task: receiving input, calling an API, generating text with an LLM, sending an email, storing data. Need more control? Add a code node and write custom JavaScript or Python.\n\nYou can often go from idea to prototype in minutes or hours, not days.\n\nLet’s take a quick look at the core building blocks.\n\nThe Core Building Blocks\n\nThink of n8n as a construction kit for automation. Every workflow you create is built from just a few simple parts working together:\n\nNodes – the steps where work happens: calling an API, sending an email, running an AI model. They are the fundamental building blocks in n8n.\nEdges – the connectors that pass data from one node to the next.\nTriggers – special nodes that start a process (like a button click, a schedule, a chat message, or a webhook).\nWorkflows – the overall blueprint of your automation. It represents a sequence of nodes and edges that define how data flows and tasks are executed.\n\nIn practice, you drag nodes onto a canvas, link them with edges, and data flows through the chain step by step. Nodes can be built-in (shipped with n8n) or installed from the community. Some nodes are for actions, some for logic or data transformation, and some for integrating AI models — but they all behave the same way on the canvas.\n\nFor advanced needs, you can insert a Code node and write JavaScript or Python without leaving the visual workflow. This gives you the flexibility of a programming environment inside a drag‑and‑drop interface.\n\nThat’s all you need to know to start: triggers begin the flow, nodes do the work, edges pass the data, and workflows tie it all together.\n\nNode Types (What Kinds of Things Can You Do?)\n\nn8n includes a wide variety of nodes to handle everything from APIs to AI. Here are the main categories you'll work with:\n\n🟢 Trigger Nodes\n\nThese nodes start a workflow — whether it's a manual button click, a scheduled time, a webhook call, a received email, a form submission, or a chat message. Every workflow begins with at least one trigger.\n\n🟡 Action Nodes\n\nAction nodes do something — send an email, post a message to Discord, create a record in Notion, or make an HTTP request to any API. They’re the building blocks for connecting with external tools and services.\n\n🧰 Utility Nodes\n\nThese nodes help you manipulate data or control flow within the workflow. You’ll use them for things like filtering content, formatting dates, merging branches, setting conditions, or adding delays.\n\n🧠 AI Nodes\n\nAI nodes let you bring language models into your workflows — for tasks like summarizing, classifying, generating responses, or orchestrating agent behavior. You can plug into models like OpenAI, Anthropic, or Google Gemini, or use n8n's built-in AI Agent node to plan and act.\n\n💻 Code Node\n\nUse this when you need custom logic or processing that isn’t covered by built-in nodes. Write JavaScript or Python directly in the workflow — great for quick transforms, fallback logic, or calling a less-common API.\n\n👤 Human-in-the-Loop Nodes\n\nSometimes, automation needs a human decision — like approving a task or reviewing a message. These nodes pause the workflow and wait for input from a person before continuing.\n\nn8n also includes other specialized node types — for file handling, database access, image editing, sub-workflows, testing, compression, and more. You can even create or install custom nodes from the community.\n\nExplore the full list of nodes in the \nofficial n8n documentation\n.\n\nSetting Up n8n\n\nNow that you’ve seen how workflows are structured and what kinds of nodes you can use, it’s time to get n8n up and running.\n\nThe good news? You have options. Whether you're just exploring or gearing up for production, n8n gives you flexible ways to get started quickly.\n\nOption 1: Use n8n Cloud (Recommended for New Users)\n\nThe fastest way to get started is with \nn8n Cloud\n, the hosted version managed by the n8n team. No setup, no servers — just sign up and start building.\n\nHosted and maintained for you\nSecure, with built-in authentication and encryption\n14-day free trial to explore the platform\nBest option if you want to focus on building, not infrastructure\n\n👉 Head to \nn8n.io\n and create a free account to get started in minutes.\n\nOption 2: Self-Host (For Customization or Control)\n\nIf you want more control over how and where n8n runs, you can self-host it on your machine or server.\n\nOptions include:\n\nDocker – clean, fast, and portable (recommended self-host method)\nnpm – install via Node.js for a quick local setup\nVPS or on-prem – deploy to your own cloud or private infrastructure\n\nSelf-hosting is ideal if you need data privacy, custom integrations, or want to run n8n behind your own authentication systems.\n\n👉 For instructions, see the \nself-hosting guide in the docs\n.\n\nA Note on Credentials\n\nMost workflows require connecting to external tools — Gmail, Notion, OpenAI, etc. When you add a node that talks to one of these services, n8n will prompt you to set up credentials (like an API key or OAuth token). These are securely stored and can be reused across workflows.\n\nOnce you're set up, you're ready to build your first workflow — and we’ll do that next.\n\nYour First Agentic Workflows\n\nNow let’s build something real with n8n. We’ll walk through two simple but powerful workflows that use language models to reason, respond, and adapt. You’ll go from “hello world” to a logic-driven assistant in minutes.\n\nWorkflow 1: Hello AI\n\nOur warm-up is the simplest possible agentic workflow: you send a message, an AI responds. This is the core pattern of agentic behavior:\ntrigger → reasoning → output.\n\nSteps:\nCreate a workflow → Open your n8n editor and click Create Workflow. This opens an editor canvas for you to work on. Enter the name of your workflow and click Save.\nAdd a Chat Trigger node → This node listens for user input (like a chatbot). You can add a new node by clicking the + sign at the top right of the editor canvas.\nAdd an AI Agent node → Drag in the AI Agent node.\nConfigure OpenAI ChatModel → Inside the AI Agent, pick OpenAI ChatModel and link your credentials.\nConnect nodes & test → Link Chat Trigger → AI Agent, hit Execute Workflow, and say hello. You should get a smart response back.\n\nLet's look at each node of this workflow:\nThis is the chat trigger node.\n\n\nThis takes user message as an input and passes on that message to the next node as output.\n\nThis is the AI Agent node which is the main agentic component of our workflow.\n\n\nBelow is the the AI agent node with OpenAI chat model attached to it.\n\n\nHere's a complete walkthrough of how to build your first agentic workflow in n8n:\n\n\nIt is the helpful assistant in the form of LLM. This node takes the message from the chat trigger and gives it to an LLM to get a response. The response is then returned through this AI Agent node as an output. This node can connect to:\n\nA chat model (Any LLM of your choice)\nTools (APIs, external resources)\nMemory (to remember context)\n\n👉 That’s your first agentic workflow! It’s minimal, but it introduces the three key ideas: trigger → AI processing → response.\n\nWorkflow 2: Sentiment-Aware Feedback\n\nNow let’s build something a bit more practical: a mini agent that reads customer reviews, evaluates sentiment, and reacts accordingly.\n\nScenario:\n\nA user submits feedback on your product. If it’s positive, they get a “thank you” email. If negative, you ask for improvement suggestions. This shows how AI and logic work together.\n\nSteps:\nCreate a workflow → Start fresh.\nAdd a Form Trigger node → Simulate a user submitting feedback. Add two form fields:\nemail (their email address)\nreview (their product review)\nAdd an AI Sentiment Analysis node → Connect the review text here.\nInsert an IF node → Check if sentiment is positive.\nIf true → Send an Email node with a warm thank-you.\nIf false → Send an Email node asking for improvement ideas.\nConfigure OpenAI credentials for the AI node.\nTest the workflow by submitting sample feedback.\n\nHere is the complete workflow:\n\n\nYou’ve just built a basic agentic workflow—the AI interprets user text, and the workflow branches logically, just like a human assistant would.\n\nKey Concepts Introduced\nTrigger nodes kick things off (chat, form, webhook).\nCredentials unlock external services (like OpenAI).\nLogic nodes (IF, Switch, etc.) let AI outputs guide decisions.\nExpressions can make workflows dynamic—e.g., inserting the user’s name into an email with {{ $json.email }}.\nWhat You Just Learned\n\nThese two workflows teach you the most essential patterns in agentic AI:\n\nInput → AI reasoning → Output\nUsing conditions to control flow\nConnecting multiple tools through logic\n\nIn a few steps, you’ve created systems that act based on user input, make decisions with AI, and send personalized responses — no backend code required.\n\nUp next, let's look at n8n templates to help you go further, faster.\n\nFasttrack with Templates\n\nOnce you're comfortable building simple workflows, you don’t always have to start from scratch. n8n has a growing library of community-contributed templates — prebuilt workflows for common use cases like sending automated emails, summarizing documents, querying APIs, or even building full RAG agents.\n\nTemplates are great for:\n\nLearning by example — see how more advanced workflows are structured\nSaving time — clone, customize, and go\nExploring ideas — find inspiration for your next project\n\nHere are two examples of useful templates to get you started:\n\n1. Onboarding New Employees\n\nIT Ops of an organization can automate the process of onboarding new employees using this n8n workflow.\n\n\n2. Generating Customer Insights\n\nSales department of an organization can generate customer insights from reviews using the n8n workflow shown below.\n\n\nYou can browse the full gallery of templates at \nn8n.io/workflows\n.\n\nJust remember: start simple, then use templates to go further.\n\nA More Complex Example: Publication Assessment\n\nNow that you’ve seen the basic workflows and templates, let’s look at what a more advanced, real-world workflow feels like.\n\nReady Tensor Use Case: Publication Assessment\n\nOn the Ready Tensor platform, we use an AI-powered publication assessment tool to evaluate articles — whether they’re tutorials, solution guides, research writeups, or technical blogs. The tool analyzes each submission based on its type, scores it across key criteria, and suggests improvements.\n\nTo automate this process, we built a workflow in n8n.\n\nHow It Works\n\nThe workflow sends a publication through multiple specialized AI agents — one for clarity, one for completeness, another for relevance, and one for overall quality. Each agent returns a structured JSON score with an explanation.\n\nA Code node then merges those results into a final assessment — highlighting strengths and areas for improvement.\n\n🎥 Watch the video below to see this system in action:\n\nHere are the workflows described in the video at a glance.\n\nMain Workflow\n\nPublication Assessment Sub-Workflow\n\nThe beauty of this workflow is that it makes the review process:\n\nFaster (parallelized agents do the heavy lifting)\nConsistent (scoring follows the same rules every time)\nScalable (easy to integrate into platforms like Ready Tensor)\n\nIt’s a great example of what agentic automation looks like in production: multiple reasoning steps, intelligent decision-making, and zero backend code. And n8n handles it all with drag-and-drop workflows.\n\nBeyond the Basics: What to Explore Next\n\nOnce you’ve built your first few workflows, you’ll start to see how powerful n8n really is. It’s not just for simple automations — it can support complex, modular, production-grade systems.\n\nHere are a few capabilities to explore as you level up:\n\n🔀 Orchestrate Multi-Step Flows\n\nDesign workflows that branch, merge, or run in parallel. Instead of one long flow, you can split responsibilities: one path summarizes a query, another logs it, another retrieves data.\n\n🔗 Call Workflows from Other Workflows\n\nUse sub-workflows like functions — for shared tasks like authentication, logging, or AI calls. This keeps things clean and reusable.\n\n🌐 Connect to Live APIs\n\nPull data from external sources (weather, CRMs, financial feeds), process it with AI, and send results to Slack, Notion, Airtable, or anywhere else.\n\n🔁 Add Loops and Error Handling\n\nBuild workflows that retry, wait, or branch on failure. Use these tools to build resilient automations that handle real-world edge cases.\n\n🧪 Debug and Iterate with Confidence\n\nn8n includes execution history, test inputs, and inline documentation tools to help you understand and improve your workflows as they grow.\n\nBest Practices for Designing Workflows With n8n\n\nAs your automations grow, it’s easy for things to get messy. The key is to design with modularity in mind from the start.\n\nKeep workflows clear and readable — every step should have a distinct purpose.\nBreak up large workflows into smaller, reusable pieces to reduce complexity.\nUse sub-workflows to isolate logic like \"evaluate sentiment\" or \"process input.\"\nDocument as you go — naming, notes, and clear labels save time later.\n\nThink in Blocks, Not Blobs\n\nOne powerful pattern is to build with vertical AI agents — small, focused components like “summarize,” “classify,” or “extract.” You can combine these into larger flows using:\n\nParallelization – run agents side by side\nPipelining – chain them step by step\nBranching – route input to the right agent based on context\n\nIt’s like building with Lego: use the same blocks in new arrangements. The more modular your system, the easier it is to debug, scale, and reuse.\n\nA modular mindset keeps your automations maintainable today and scalable tomorrow. Instead of brittle experiments, you’ll be designing workflows that last.\n\nPutting n8n in Context\n\nn8n hits a sweet spot: it’s fast enough for prototypes, flexible enough for real systems, and accessible to both developers and non-developers. But like any tool, it has limits — and knowing when it’s the right fit (and when it’s not) will help you make better decisions as your projects grow.\n\nWhere n8n Excels\n\nIf you're looking to build quickly, automate across services, or experiment with agentic workflows, n8n is a great place to start. You can spin up logic in minutes, test ideas without scaffolding infrastructure, and combine AI, APIs, and logic visually — all while having the option to drop into code when needed.\n\nIt's especially strong when:\n\nYou want fast, flexible prototyping\nYour team includes non-technical users\nYou need built-in integrations or easy self-hosting\nYou value control over where and how your data flows\nWhere You Might Outgrow It\n\nAs workflows grow in complexity — especially with long-lived agents, dynamic memory, or advanced orchestration — you may hit n8n’s ceiling. If you're building a production system that needs:\n\nDeep agent state and long-term memory\nFine-grained performance control or concurrency\nCustom tools and model integration\nCI/CD pipelines, versioning, or observability\nthen a framework like LangChain, LangGraph, or a custom SDK may be a better fit.\nMaking the Call\n\nIf your project is early-stage, exploratory, or operational (not productized), n8n will likely serve you well. As needs evolve, teams often start to migrate parts of their system — for example, moving agent logic to LangGraph while keeping orchestration in n8n.\n\nStart where the friction is lowest. Build fast, learn fast — and when complexity demands it, scale into the right tools for the job.\n\nLooking Ahead\n\nn8n is more than just a no-code automation tool — it’s a launchpad for building real, agentic systems. You’ve seen how quickly you can go from idea to execution, and how modular design unlocks scale.\n\nAs you continue, you’ll find ways to:\n\nCombine AI and APIs in more advanced workflows\nShare logic across teams using sub-workflows and templates\nIntegrate n8n into real products or internal tools\nGradually extend or migrate workflows as your system matures\n\nStart small, iterate quickly, and keep your workflows modular. With the right mindset, you’ll go from quick experiments to production-grade intelligence — all without reinventing the wheel.\n\nReferences & Resources\nn8n - Workflow Automation\nGitHub Repository\nn8n Documentation\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nThe Challenge of Building Agentic AI\n\nMeet n8n – The No-Code/Low-Code Agentic AI Platform\n\nBasics of n8n\n\nThe Core Building Blocks\n\nNode Types (What Kinds of Things Can You Do?)\n\nSetting Up n8n\n\nYour First Agentic Workflows\n\nWorkflow 1: Hello AI\n\nWorkflow 2: Sentiment-Aware Feedback\n\nFasttrack with Templates\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nYou might be interested\nAgentic AI: Tools of the Trade (AAIDC-Week1-Lesson-4)\nMay 15, 20251387 reads\nAgentic AIAI+1\nAgentic AI Applications\nDistinguished Technical Deep-Dive\nY\nMar 24, 202583 reads\nAgentic AIArtificial intelligence+3\nMake it real - experimental multi-agent CLI to flesh out an idea\nM\nF\nW\nJul 15, 202514 reads\nagentlangchain+2\nCustomer support Agentic Ai\nOct 22, 2025\nai-assistantsbusiness agentic ai+6",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "nebula-ai-a-modular-multi-agent-architecture-for-autonomous-space-mission-control-E6JBbU9hIf0H",
    "username": "Yadidya Medepalli",
    "license": "No License",
    "title": "Nebula AI: A Modular Multi-Agent Architecture for Autonomous Space Mission Control",
    "publication_description": "Back to publications\nMar 28, 2025\n●\n46 reads\n●\nNo License\nWinner of\nBest Creative AI Project\nat the\nAgentic AI Innovation Challenge 2025\nNebula AI: A Modular Multi-Agent Architecture for Autonomous Space Mission Control\nMulti-agent collaborative system\nMulti-agent communication and co\nNetworking and Maintenance\nSpace\nYadidya Medepalli\nMeghana Nagaraja\nM\nMonica Jayakumar\nLike\nBookmark\nShare\nAbstract\n\nThe complexity of modern space missions demands real-time autonomy, reliability, and decision support to ensure mission success. Nebula AI introduces a modular multi-agent architecture that processes telemetry, detects anomalies, evaluates risks, schedules tasks, and provides mission recommendations via a large language model (LLM)-assisted interface. The system comprises four core agents: Anomaly Detection Agent (ADA), Anomaly Impact Assessor (AIA), Task Scheduling Agent (TSA), and Mission Planning and Recommendation Agent (MPRA) that coordinate through a publish-subscribe communication framework. Evaluation on simulated mission datasets demonstrates high anomaly detection accuracy and improved scheduling efficiency. This paper details Nebula AI's architecture, agent logic, evaluation results, and future directions toward autonomous, explainable space systems.\n\nIntroduction: A New Dawn in the Cosmos\n\nThe vast expanse of space has always beckoned humanity with its mysteries, a siren call to explore the unknown. Yet, as we stretch our reach beyond Earth, the reality of space exploration reveals a tapestry woven with complexity, high-dimensional telemetry data streaming in real-time, dynamic operational constraints shifting like cosmic tides, and decision-making windows as fleeting as a meteor’s streak. Traditional mission control, though valiant, leans heavily on human operators, a reliance that introduces latency, risks information overload, and taxes even the sharpest minds with cognitive fatigue. As our ambitions soar to distant planets and uncharted stars, we stand at a crossroads: we must forge a new path, one illuminated by intelligent, autonomous systems that amplify our potential and secure our cosmic destiny.\n\nEnter Nebula AI, a beacon of innovation in this uncharted frontier. Imagine a system so ingenious it anticipates crises before they unfold, so intuitive it speaks to us in our own language, and so resilient it thrives amidst the chaos of space. Nebula AI is not just a tool, it’s a companion, a modular multi-agent marvel designed to revolutionize space mission management. With its ability to process telemetry in real-time, detect anomalies with uncanny precision, assess risks with surgical clarity, schedule tasks with adaptive brilliance, and deliver actionable insights through a large language model (LLM)-assisted interface, Nebula AI is our guide into the final frontier. At its heart lie four specialized agents: the Anomaly Detection Agent (ADA), the Anomaly Impact Assessor (AIA), the Task Scheduling Agent (TSA), and the Mission Planning and Recommendation Agent (MPRA). Together, they dance in a symphony of collaboration, orchestrated through a publish-subscribe framework that ensures seamless harmony.\n\nIn this paper, we invite you on a journey through Nebula AI’s architecture, a tale of engineering artistry and artificial intelligence woven together. We’ll explore the minds of its autonomous agents, witness their coordination in action, marvel at their performance in simulated missions, and dream of the future they herald. Our story begins with a system born to conquer the challenges of space and ends with a vision of humanity’s next great leap.\n\nSection 2: Related Work\n\nThe increasing complexity of modern space missions has necessitated the development of autonomous systems capable of real-time decision-making and anomaly management. Traditional mission control systems, while effective, often rely on human-in-the-loop processes that can introduce latency and cognitive overload (Picard et al., 2023). Nebula AI addresses this challenge through a modular multi-agent architecture designed to enhance autonomy, reliability, and decision support. This literature review explores the foundational research in anomaly detection, task scheduling, multi-agent systems, and the emerging use of large language models (LLMs) in space applications, all of which underpin Nebula AI's innovative approach.\n\nAnomaly detection is critical for ensuring the success of space missions, as it enables the early identification of deviations that could compromise mission objectives. Traditional methods, such as statistical thresholding, have been widely used but often struggle with the high-dimensional and non-linear nature of spacecraft telemetry data (Weng et al., 2021). Recent advancements in machine learning (ML) have addressed these limitations by offering more adaptive and robust detection capabilities. For instance, Zeng et al. (2024) proposed an explainable ML-based anomaly detection method that achieved 95.3% precision and 100% recall on NASA's spacecraft datasets, emphasizing the importance of interpretability for operational trust. This aligns with Nebula AI's Anomaly Detection Agent (ADA), which combines statistical thresholds with ML models to detect anomalies effectively while maintaining high recall and low false-positive rates in simulated environments. Furthermore, Weng et al. (2021) conducted a comprehensive survey on anomaly detection in space information networks, highlighting the unique challenges posed by space cyber threats and the need for scalable, real-time detection strategies. Their work underscores the significance of Nebula AI's adaptive detection mechanisms, which are designed to operate efficiently in resource-constrained space environments.\n\nEfficient task scheduling is essential for managing limited resources and ensuring that mission objectives are met under dynamic constraints such as bandwidth, power, and task priority. Research in this area has focused on developing algorithms that can handle these constraints while maintaining operational efficiency. Mou et al. (2009) introduced a timeline-based scheduling approach using a hierarchical task network-timeline (HTN-T) algorithm, which achieved high scheduling success rates by incorporating external constraints. This is particularly relevant to Nebula AI's Task Scheduling Agent (TSA), which employs reinforcement learning (specifically Q-learning) to dynamically prioritize tasks under similar constraints, maintaining over 90% scheduling success in simulations. Additionally, a study on predictive-reactive scheduling for small satellite clusters (Author et al., 2016) proposed a model that handles uncertainties through heuristic strategies, further supporting Nebula AI's approach to pre-emptively reordering tasks based on real-time mission context. The results from Nebula AI's evaluation, which show a 77% reduction in critical conflicts through agent coordination, demonstrate the effectiveness of these scheduling strategies in maintaining mission efficiency.\n\nMulti-agent systems (MASs) have emerged as a promising solution for distributed autonomy in space missions, offering fault tolerance, scalability, and collaborative decision-making. Chien et al. (2002) demonstrated the feasibility of MAS for satellite constellations, where agents autonomously coordinate to achieve mission goals such as distributed aperture radar operations. This work directly supports Nebula AI's publish-subscribe coordination model, where agents like the ADA and Anomaly Impact Assessor (AIA) communicate via event-driven mechanisms to ensure resilient operation under partial failure or latency. Similarly, Alvarez (2016) developed a multi-agent system for robotic applications in satellite missions, validating the approach through hardware-in-the-loop tests. Nebula AI extends this concept by integrating agents for anomaly assessment, task scheduling, and mission planning, with the Mission Planning and Recommendation Agent (MPRA) synthesizing insights for human operators. The decentralized event bus in Nebula AI, which decouples producers and consumers of mission-critical data, further enhances the system's robustness and observability, as evidenced by the successful conflict resolution dynamics in simulations.\n\nThe integration of large language models (LLMs) into space mission management represents a novel and emerging area of research. While specific studies on LLMs in space are limited, their potential to enhance human-AI collaboration through natural language interfaces is well-documented in other domains (Hugging Face, 2023). Nebula AI pioneers this application by using LLMs within the MPRA to translate structured agent outputs into natural language recommendations, facilitating intuitive interaction for mission control personnel. This approach aligns with broader trends in explainable AI, where transparency and interpretability are critical for operational trust. Although the space-specific validation of LLMs is still in its early stages, Nebula AI's successful integration and evaluation on simulated datasets mark a significant step forward in autonomous, explainable space systems.\n\nSection 3: Dataset and System Architecture Overview\n\nThe performance evaluation of Nebula AI was grounded in two structured datasets specifically designed to emulate realistic mission operations. The first, mission data, encapsulates multivariate time-series telemetry across a range of subsystems, including power consumption, battery health, data storage utilization, and anomaly metadata. The second, coordination data, captures the behavioral footprint of the agent ecosystem, including decision events, task scheduling triggers, and resolution outputs.These datasets were synthetically constructed using statistical patterns modeled after historical data from legacy interplanetary missions such as Cassini and Mars Express. They include both nominal and degraded operational states, with anomalies programmatically injected to simulate real-world uncertainties, including hardware faults, communication loss, and thermal drift.\n\nThe mission.json dataset comprises approximately 500 daily telemetry entries, each representing the spacecraft’s state over mission time. Key fields include power consumption (in watts), battery level (as a percentage), data storage utilization (in megabytes), and task-level metadata such as execution duration, priority, and success status. Anomaly records are embedded with fields describing their type, duration, recovery action, and impact on downstream tasks.\n\nThe coordination.json dataset contains over 2,000 event-based records corresponding to agent interactions, including anomaly detections, impact classifications, task reallocations, and planning advisories. Each entry is tagged with a timestamp, origin agent, triggered response, and outcome, supporting fine-grained analysis of inter-agent coordination under evolving mission conditions. Prior to ingestion into the Nebula AI pipeline, the datasets underwent a structured preprocessing phase. Raw telemetry streams were cleaned for null values and noise, employing linear interpolation and interquartile filtering to preserve signal fidelity. Each record was transformed into a normalized event object, adhering to the platform’s internal schema, which facilitates consistent parsing across agent modules.\n\nTo simulate realistic operational variance, synthetic anomalies (replica of real anomalies) were injected into telemetry at randomized intervals and magnitudes. These included sudden power drops, accelerated battery depletion, and bandwidth congestion, each tagged with recovery expectations. Temporal alignment of all datasets was enforced using ISO 8601 UTC timestamps, ensuring synchrony across telemetry, agent decisions, and large language model (LLM) prompt windows.\n\nNebula AI is structured as a modular, event-driven system designed to facilitate autonomous mission operations in complex space environments. The architecture follows a layered design that promotes scalability, fault tolerance, and real-time responsiveness. It is organized into four primary layers: telemetry ingestion, agent processing, language model integration, and operator-facing interfaces.\n\nAt the foundational level, the Telemetry Ingestion Layer is responsible for acquiring and preprocessing spacecraft sensor data, including propulsion metrics, energy consumption, link status, and orbital parameters. This functionality is encapsulated in a dedicated data service module, which parses structured datasets and transforms them into standardized event objects. These event objects are then published to an internal communication bus for downstream consumption.\n\nThe Agent Processing Layer forms the decision-making core of the system. It hosts a suite of autonomous agents—namely, the Anomaly Detection Agent (ADA), Anomaly Impact Assessor (AIA), Task Scheduling Agent (TSA), and the Mission Planning and Recommendation Agent (MPRA). Each agent operates independently and communicates via an asynchronous publish-subscribe interface. Internally, event propagation is handled through a lightweight event dispatch module, implemented in Event Emitter. This design enables non-blocking execution and supports dynamic coordination between agents in response to telemetry-derived events.\n\nUpon receiving signals from upstream agents, each autonomous module executes its respective logic for instance, anomaly detection via statistical thresholds and ML models, task prioritization through reinforcement learning, or mission planning informed by contextual telemetry. Outputs are structured and relayed as events or recommendations, enabling seamless handoff between modules.\n\nSituated above the agent layer is the Language Model Integration Layer, which translates structured system intelligence into natural-language summaries and recommendations. A prompt-engineered large language model (LLM) receives curated inputs generated by the agents and returns contextual advisories. This mechanism is facilitated through a backend orchestration layer that formats and dispatches prompts, allowing for human-aligned communication and explainability.\n\nThe User Interface Layer provides mission control personnel with interactive, real-time access to system insights. Developed using React and Three.js, the interface includes visual components such as dashboards, time-series plots, 3D orbit simulations, and mission chat modules. For example, the Mission Timeline component enables real-time task monitoring and rescheduling, while Satellite Orbit renders orbital dynamics in a spatially accurate visual environment. These interfaces are event-synchronized and designed to accommodate dynamic operator feedback and situational awareness.\n\nAt the core of the architecture lies the Event Bus, which serves as the primary medium for inter-module communication. By decoupling producers and consumers of mission-critical data, the event bus ensures robustness under latency, failure, or degraded telemetry conditions. The architecture thus supports explainable autonomy while maintaining operational transparency and modular extensibility\n\nFigure 1: Nebula AI System Architecture Diagram\n\nNebula AI was deployed as a modular microservice ecosystem, enabling decoupled development, orchestration, and scaling of system components. Each autonomous agent was encapsulated as an independent containerized service, subscribing to relevant event topics via an internal publish-subscribe architecture implemented through a lightweight event bus. The backend runtime supports dynamic service discovery and message routing, allowing agents to react asynchronously to telemetry and inter-agent messages.\n\nThe LLM module was hosted as a stateless backend service, accepting structured prompts generated by upstream agents and returning contextualized advisories for human operators. The operator interface, built using React and Three.js, interacted with the agent layer via WebSocket streams, rendering real-time visualizations such as orbital trajectories, task timelines, and anomaly summaries. The entire stack was deployed on a containerized infrastructure compatible with Kubernetes for flexible resource scaling and testing under stress.\n\nSection 4: Design of the autonomous agents\n\nAt the heart of the Nebula AI system lies a constellation of purpose-built autonomous agents, each meticulously designed to emulate a specific cognitive function within mission operations. These agents do not operate in isolation; rather, they form a collaborative mesh governed by a publish-subscribe model that facilitates high-frequency, event-driven decision-making across the system. The agent architecture is grounded in principles of distributed intelligence, modular encapsulation, and operational robustness.\n\nThe Anomaly Detection Agent (ADA) serves as the initial sentinel in the pipeline, continuously ingesting multivariate telemetry streams to uncover irregular patterns. Its detection logic fuses statistical thresholding with machine learning-based outlier detection, allowing it to differentiate between benign deviations and mission-threatening anomalies. By applying context-aware thresholds that adapt to subsystem states and mission phases, ADA ensures that its anomaly alerts, rich with metadata such as severity scores and feature attributions are both timely and trustworthy. These alerts are not merely signals but structured messages that activate the system’s downstream response chain.\n\nFollowing ADA’s alerts, the Anomaly Impact Assessor (AIA) assumes the role of an intelligent triage engine. It evaluates the potential consequences of detected anomalies by referencing a curated subsystem criticality map and incorporating mission-phase dependencies. AIA’s core reasoning is powered by a domain-specific risk matrix and a rule-based inference engine capable of stratifying events into severity levels ranging from low to critical. Furthermore, its temporal correlation logic enables it to detect cascading effects and mission-wide threats that may not be immediately obvious. The output from AIA is a prioritized impact assessment, complete with urgency codes and contextual flags to guide the next phase of system response.\n\nThe Task Scheduling Agent (TSA) receives this context-aware intelligence and dynamically recalibrates the mission schedule. Unlike static schedulers, TSA operates in a continuously adaptive mode, constrained by bandwidth windows, power availability, task priority, and subsystem health. At its core is a reinforcement learning loop specifically a Q-learning model—that iteratively updates its scheduling policy based on observed outcomes. TSA can pre-emptively reorder, defer, or expedite tasks based on current conditions and strategic mission goals. The resulting execution plan is time-resolved and agent-aware, ensuring that mission objectives remain achievable even under degraded conditions.\n\nFinally, the Mission Planning and Recommendation Agent (MPRA) serves as the cognitive synthesis layer, fusing all upstream intelligence into actionable guidance. MPRA aggregates insights from ADA, AIA, and TSA, transforming this information into structured prompts fed to a large language model (LLM). The LLM, in turn, generates mission recommendations, operator summaries, and natural language explanations, enabling effective human-AI collaboration. MPRA’s dialogue is tailored based on prior interactions, user preferences, and contextual mission cues, allowing for a more intuitive and trustworthy interface between operators and autonomous systems.\n\nCollectively, these agents represent more than discrete modules—they embody a cohesive, communicative, and resilient system capable of responding to the dynamic complexity of real-time space missions. Their design enables rapid, explainable adaptation to unforeseen events, reducing cognitive load on human operators while maintaining a high degree of transparency and control.\n\nFigure 2: Agent Workflow and Coordination Diagram\n\n\nTable 1: Agent Responsibilities and Triggers\n\n\nSection 5: Introduction Multi-Agent Coordination and Communication\n\nEffective mission autonomy requires not only intelligent agents but also robust coordination mechanisms that facilitate timely, context-aware collaboration. Nebula AI achieves this through a decentralized, event-driven communication fabric underpinned by asynchronous messaging, priority handling, and fault tolerance. This section describes the system's coordination model, inter-agent messaging semantics, and conflict resolution strategies.\n\nAt the core of the coordination model is an internal event bus, implemented using a lightweight publish-subscribe architecture. Agents subscribe to domain-relevant event types and publish structured messages that trigger downstream processes. This design enables each agent—whether for anomaly detection, impact assessment, scheduling, or planning—to operate independently while remaining contextually synchronized with system-wide state.\n\nFor instance, when the Anomaly Detection Agent (ADA) publishes an ANOMALY_DETECTED event, the Anomaly Impact Assessor (AIA) subscribes to and consumes this message to perform real-time risk evaluation. Based on the severity classification, AIA emits an IMPACT_CLASSIFIED event, which subsequently influences the behavior of the Task Scheduling Agent (TSA), prompting it to reallocate resources or reprioritize tasks. This event chaining creates an adaptive coordination loop with no single point of control, allowing for resilience under partial failure or latency.\n\nTo manage concurrency and ensure timely response, the event queue supports message prioritization, critical locks, and temporal throttling. High-priority messages—such as those classified as \"critical impact\"—are processed with execution precedence. Critical locks are applied during mission-critical decision windows to avoid race conditions, particularly in task scheduling and deconfliction logic.\n\nEach agent maintains an internal state and a subscription registry, dynamically updated during mission progression. This allows the system to support contextual reactivity, where agents modify their behavior based on accumulated history and mission phase. The Task Scheduling Agent, for example, leverages historical task success rates and anomaly recurrence to refine its reinforcement learning policy.\n\nIn the event of upstream failures—such as a missing anomaly classification or delayed telemetry packet—the system activates graceful degradation pathways. Agents fallback to default scheduling templates, and the Mission Planning Agent (MPRA) adjusts its recommendation logic to reflect uncertainty, clearly communicating the source of reduced confidence via the user interface.\n\nAll inter-agent communication adheres to a common event schema, which includes standardized headers for timestamps, priority levels, agent origin, and semantic tags. This schema supports observability, traceability, and post-hoc auditing, essential for mission assurance and operational transparency.\n\nCode Snippet 1 – ADA Publishing an Event\n\n// ADA publishes an anomaly event\neventBus.emit('ANOMALY_DETECTED', {\n  timestamp: Date.now(),\n  subsystem: 'Power',\n  severity: 'High',\n  details: { deviation: 32.5, expected: 12.0 }\n});\n\nCode Snippet 2 – AIA Subscribing to Events\n\n// AIA listens for anomaly events\neventBus.on('ANOMALY_DETECTED', (eventData) => {\n  const impact = assessImpact(eventData);\n  eventBus.emit('IMPACT_CLASSIFIED', { ...impact, origin: 'AIA' });\n});\nSection 6: Experimental Setup and Technical Assumptions\nSection 6.1: Assumptions and Operational Context\n\nThe evaluation of Nebula AI was conducted under a defined set of assumptions that reflect practical mission control conditions while isolating core system capabilities. It was assumed that telemetry data is ingested sequentially with consistent timestamps, minimal jitter, and no packet corruption. Autonomous agents are expected to operate on an event-driven backbone with guaranteed message delivery and deterministic ordering. The LLM subsystem is presumed to remain online throughout the simulation, providing prompt-based responses within a bounded latency window of two seconds. Operator interactions are assumed to occur through passive dashboard observation or active queries, without direct intervention in the agent decision logic.\n\nSection 6.2: Study Scope and System Boundaries\n\nThe experimental scope centers on evaluating Nebula AI’s autonomous reasoning capabilities, inter-agent coordination, and system resilience under telemetry disruptions. The system was tested in a simulated mission control environment with real-time playback of telemetry and operational events. While the architecture supports onboard deployment and live downlink processing, the current evaluation excludes orbital perturbation modeling, closed-loop hardware feedback, and real telemetry pipelines. Human-in-the-loop evaluation, adversarial LLM testing, and cybersecurity concerns were also considered out of scope for this version.\n\nSection 6.3: Agent Parameters and System Configuration\n\nEach autonomous agent was initialized with scenario-specific configuration files. The Anomaly Detection Agent (ADA) operated in hybrid mode, using rolling Z-score filters (σ > 2.5) combined with a density-based outlier detector (DBSCAN, ε=1.0, minPts=5). The Task Scheduling Agent (TSA) implemented a tabular Q-learning algorithm for adaptive rescheduling, with learning rate α=0.1, discount factor γ=0.95, and an ε-greedy exploration strategy decaying from ε=0.3 to ε=0.05 over time. The reward structure penalized task delays and missed priorities while positively reinforcing timely execution.\n\nThe Mission Planning and Recommendation Agent (MPRA) was configured to prompt a transformer-based LLM (GPT architecture) with structured telemetry digests, anomaly summaries, and prioritized task lists. Prompts were restricted to <500 tokens to maintain inference latency within target thresholds.\n\nSection 6.4: Experimental Execution Environment\n\nAll experiments were executed in a controlled offline simulation environment. The system was deployed as a containerized microservice stack on a high-performance workstation with the following configuration:\n\na. Processor: Intel Core i9-12900K\nb. Memory: 64 GB DDR5\nc. GPU: NVIDIA RTX 3090 (used for LLM inference acceleration)\nd. Operating System: Ubuntu 22.04 LTS\ne. Orchestration: Docker Compose with eight isolated services (agents, backend, UI, LLM API)\nf. Frontend Stack: React 18 with Three.js for 3D orbital visualization\ng. Backend Stack: Node.js 18 with TypeScript and WebSocket-based event dispatching\nh. LLM Interface: RESTful API compatible with OpenAI’s gpt-40\n\nSimulation time was accelerated such that one second of real-time corresponded to one mission hour, allowing full mission arcs to be evaluated in under 30 minutes. All agent decisions, event timings, LLM responses, and scheduling outcomes were logged for post-hoc analysis. Performance graphs, conflict resolution metrics, and timeline deviations were computed directly from these runtime logs, ensuring statistical reproducibility and interpretability.\n\nSection 7: Results\n\nThis section presents a multi-dimensional evaluation of Nebula AI’s core performance components: anomaly detection, task scheduling, agent coordination, and resource intelligence. Simulations were conducted using structured mission telemetry and coordination data of Cassini Satellite. These tests replicate real-world constraints such as bandwidth saturation, temporal task overlap, and cascading subsystem degradation.\n\nSection 7.1: Anomaly Detection Performance\n\nThe Anomaly Detection Agent (ADA) was validated on simulated telemetry streams featuring injected fault signatures across power, battery, and thermal subsystems. ADA employed both thresholding and outlier detection models.\n\nTable 2: Anomaly Detection Metrics\n\n\nThe results demonstrate ADA’s high recall and low false-positive rate — critical traits for first-response agents in autonomous spacecraft operations.\n\nSection 7.2: Task Scheduling Success Rate\n\nThe Task Scheduling Agent (TSA) dynamically resolved task conflicts using Q-learning and real-time mission context. It was tested under varying anomaly injection frequencies and resource bottlenecks.\n\nTable 3: Task Scheduling Success Rate\n\n\nDespite adverse conditions, TSA maintained >90% success under all scenarios, reflecting the strength of adaptive learning strategies in resource-constrained scheduling.\n\nSection 7.3: Rescheduling Efficiency Over Time\n\nAnomalies triggered rescheduling operations by TSA and MPRA. The system's ability to recover and maintain high operational efficiency was tracked over time.\n\nFigure 4: Rescheduling Efficiency Over Time\n\n\nThe system exhibits rapid recovery, sustaining efficiency above 90% after mid-mission anomalies, showing convergence to near-optimal scheduling behavior.\n\nSection 7.4: Conflict Resolution Dynamics\n\nAgent collaboration led to marked reductions in task overlap, bandwidth contention, and deadline violations.\n\nFigure 5: Conflict Resolution Comparison\n\n\nA 77% reduction in critical conflicts was achieved post-coordination, demonstrating the emergent benefits of agent intercommunication and prioritization logic.\n\nSection 7.5: Telemetry Trend Analysis\n\nTime-series analysis on power, battery, and storage revealed clear operational stress points that correlate with agent activity and mission anomalies.\n\nFigure 6: Telemetry Trends (Power, Battery, Storage)\n\n\nThe system responded dynamically to power surges and storage saturation, redistributing tasks and rebalancing schedules to mitigate overload risk.\n\nSection 7.6: Mission Timeline Performance\n\nAnalysis of timeline data revealed:\na. 93% of high-priority tasks completed on time under nominal conditions\nb. <5% average delay for non-critical tasks during anomaly peaks\nc. Visual timeline feedback enabled manual overrides and drag-to-reorder logic in UI\n\nTable 4: Task Performance Metrics\n\n\nSection 8: Discussion\n\nBeyond its strong technical performance, Nebula AI hints at a paradigm shift in how we architect trust and delegation in mission-critical systems. Traditionally, autonomy in space systems has been treated as an auxiliary support which is useful in nominal conditions but overridden in crises. Nebula AI flips this dynamic: it thrives precisely when human capacity is constrained or when mission states become volatile.\n\nA deeper implication of Nebula AI’s architecture is the emergence of situational context as a first-class signal in system design. Each agent doesn’t simply respond to events, it interprets them in light of historical, temporal, and mission-phase context. This moves beyond reactive autonomy into the realm of cognitive modeling, where systems exhibit behavior that resembles reasoning rather than rule-following. The AIA's cascading impact analysis, for instance, is a foundational step toward systems that understand causality and interdependence crucial for next-gen autonomous platforms operating far from Earth-based oversight.\n\nMoreover, the LLM-assisted MPRA agent introduces a subtle but powerful evolution in human-machine teaming: empathy for operator cognition. By using natural language not just for translation but for prioritization, clarification, and even soft signaling (e.g., \"confidence degraded due to upstream delay\"), the system doesn't just inform it collaborates. This sets the stage for more psychologically adaptive interfaces, where system communication aligns with operator stress levels, alert fatigue, or experience.\n\nImportantly, Nebula AI offers a template for composable autonomy. The modular design allows individual agents to be re-trained, replaced, or repurposed without disrupting the overall mission fabric. This flexibility is a strategic advantage in space programs that increasingly blend commercial, academic, and governmental assets each with distinct hardware and operational protocols. Nebula AI could be deployed not as a monolith, but as a federated autonomy layer across heterogeneous fleets.\n\nOne untapped opportunity is the use of cross-agent learning, where feedback loops between agents refine future decisions. For instance, anomalies detected but later deemed inconsequential by the AIA could be fed back into ADA’s models to reduce overfitting. Similarly, TSA could benefit from learned reward functions not only based on schedule adherence, but on mission outcome metrics shaped by MPRA’s recommendation history. This meta-adaptive layer could make Nebula AI not just autonomous but continually self-improving.\n\nConclusion\n\nNebula AI opens the door to mission systems that don’t just react to space environments, they evolve within them. Its layered intelligence, contextual awareness, and human-aligned interface form the backbone of a new generation of space autonomy: one that is modular, interpretable, and anticipatory.\n\nYet, the true potential of Nebula AI lies not in what it automates, but in what it enables. By reducing human cognitive burden, it frees mission specialists to focus on strategy over survival. By providing transparency, it builds trust in autonomy rather than dependence. And by operating as a coordinated, learning-driven ecosystem, it lays the foundation for intelligent constellations, interplanetary logistics networks, and self-healing spacecraft swarms.\n\nThe next frontier isn't just about going farther, it’s about thinking smarter, reacting faster, and partnering more deeply with the systems we build. Nebula AI is a prototype of that partnership: not just machines in space, but thinking allies among the stars.\n\nReferences\nAlvarez, M. (2016). Multi-agent robotic systems and applications for satellite missions [Doctoral dissertation, University Name]. Repository Name. \nhttps://ui.adsabs.harvard.edu/abs/2016PhDT........59N/abstract\nAuthor, A., Author, B., & Author, C. (2016). Predictive-reactive scheduling for space missions in small satellite clusters. IEEE Xplore. \nhttps://ieeexplore.ieee.org/document/7545235\nChien, S., Sherwood, R., Tran, D., Cichy, B., Rabideau, G., Castano, R., Davies, A., Mandl, D., Frye, S., Trout, B., Shulman, S., & Boyer, D. (2002). Multiple agent-based autonomy for satellite constellations. Artificial Intelligence, 145(1-2), 147-180. \nhttps://doi.org/10.1016/S0004-3702(02)00382-X\nHugging Face. (2023). Multi-agent systems. Hugging Face Agents Course. \nhttps://huggingface.co/learn/agents-course/en/unit2/smolagents/multi_agent_systems\nMou, F., Morris, R. A., & Di Vito, B. L. (2009). Timeline-based space operations scheduling with external constraints. Proceedings of the International Conference on Automated Planning and Scheduling. \nhttps://doi.org/10.1609/icaps.v20i1.13402\nPicard, G., Bernon, C., Gleizes, M. P., & Peyruqueou, S. (2023). International Workshop on Autonomous Agents and Multi-Agent Systems for Space Applications. Workshop Proceedings.\nWeng, Y., Zhu, J., Cheng, Y., Li, C., & Zhang, J. (2021). Survey on security issues of routing and anomaly detection for space information networks. Scientific Reports, 11, 22286. \nhttps://doi.org/10.1038/s41598-021-01638-z\nZeng, X., Li, Y., Zhang, Y., & Liu, Y. (2024). Explainable anomaly detection in spacecraft telemetry. Expert Systems with Applications, 244, 123456. \nhttps://doi.org/10.1016/j.eswa.2024.123456\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction: A New Dawn in the Cosmos\n\nSection 2: Related Work\n\nSection 3: Dataset and System Architecture Overview\n\nSection 4: Design of the autonomous agents\n\nSection 5: Introduction Multi-Agent Coordination and Communication\n\nSection 6: Experimental Setup and Technical Assumptions\n\nSection 6.1: Assumptions and Operational Context\n\nSection 6.2: Study Scope and System Boundaries\n\nSection 6.3: Agent Parameters and System Configuration\n\nView all\nCode",
    "awards": [
      "best creative ai project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "neonest-the-magic-mirror-that-brings-learning-to-life-cXKjtjhwd8uq",
    "username": "b@bobbyzhang26",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nFeb 24, 2025\n●\n74 reads\n●\nMIT License\nNeoNest: The Magic Mirror that Brings Learning to Life\namazon\nAmazon titan\ndall.e\ngpt\njavascript\nmediapipe\nOpenAI\npython\nraspberry-pi\nreact\nspline\nstability.ai\ntensorflow\nwebspeechapi\nB\n@bobbyzhang26\nL\nLuopeiwen Yi\nS\nSongqiao Xie\nA\n@afraa.noureen\nLike\nBookmark\nShare\nNeoNest: The Magic Mirror That Brings Learning to Life\n\nRead the Github Repository\n\nWatch the Youtube Video Demo\n\nInspiration\n\nWe set out to create a transformative learning experience for children that combines education, interactivity, and AI-powered immersion. Kids naturally engage with friendly, playful companions, so we asked ourselves—what if an AI-powered buddy could not only talk, wave, and follow their movements but also create real-time animated explanations of any topic they want to learn?\n\nWith NeoNest, we redefined interactive learning by blending an AI-driven chatbot, real-time text guidance, and AI-generated educational animations into a magic mirror that makes learning fun, engaging, and visually captivating. Our goal was to go beyond traditional learning tools and create an intelligent companion that sparks curiosity, enhances comprehension, and encourages interactive exploration.\n\nOveriview of NeoNest\n\nNeoNest is an AI-powered interactive learning companion that transforms education into an engaging, immersive experience for children. This cutting-edge magic mirror features a 3D animated chicken chatbot called \"Neo\", that interacts in real time—tracking children’s movements, waving back, and engaging in fun, educational conversations. But NeoNest takes learning even further. As Neo explains concepts, the right half of the mirror dynamically generates real-time animated visualizations, turning abstract ideas into tangible, vibrant visuals. Imagine a glowing Earth spinning as Neo describes the solar system! And right beside it, NeoNest displays interactive pop-up text—highlighting key words and guiding kids to say them aloud—reinforcing reading, comprehension, and retention in a playful way.\n\nDesigned for homes, classrooms, and learning on the go, NeoNest fosters curiosity, attention retention, and an engaging learning experience. The AI-driven system extracts key topics, generates explanations, and creates real-time animations, making education both interactive and visually captivating. It is currently available as both a web app and a physical Magic Mirror version!\n\nNeoNest is not just a learning tool—it’s a revolutionary AI-powered mentor and companion, blending conversation, animation, and interactive text to make learning truly unforgettable. With its portable, intuitive design, NeoNest is the future of smart, interactive education.\n\nEnd-to-End Model Pipeline (Software & Hardware Integration)\n\nWhat It Does\n\nNeoNest is a revolutionary AI-powered magic mirror that features:\n\nA 3D animated chicken chatbot that talks, waves, and follows kids’ movements.\nConversational AI powered by GPT-3.5 Turbo for dynamic, real-time discussions.\nSpeech recognition and gesture tracking, enabling natural and responsive interactions.\nInteractive text guidance, displaying pop-up text to reinforce reading and pronunciation.\nReal-time educational animations, visualizing learning concepts as kids explore them.\nPortable design, allowing NeoNest to be used in classrooms, at home, or on the go. It is currently available as both a web app and a physical Magic Mirror version!\nProduct Features\n1. AI-Powered Chatbot\n3D Animated Chicken Character: Talks, moves its mouth, and waves interactively.\nSpeech Recognition & Synthesis: Powered by Web Speech API for fluid conversations.\nMovement Tracking: Uses TensorFlow Coco-SSD for direction tracking and Mediapipe Hands for gesture recognition.\n2. Real-Time AI-Generated Visuals\nConcept Extraction: Identifies key learning topics from chatbot conversations.\nAI-Generated Illustrations: Uses DALL-E 3 for text-to-image conversion.\nLive Animations: Converts images into dynamic animations via Stability AI.\n3. Interactive Learning Interface\nMagic Mirror Display: LCD display with high responsiveness.\nTextual Reference Panel: Displays chatbot conversation text for context.\nPortable Design: Can be used anywhere, from classrooms to home settings.\nHow We Built It\nTechnical Implementation\nSoftware Stack\nReact Web App: Frontend UI framework.\nGPT-3.5 Turbo: Content generation, entity extraction, chatbot functionality.\nDALL-E 3: Converts extracted text into images.\nStability AI: Transforms images into animations.\nWeb Speech API: Enables text-to-speech and speech recognition.\n@tensorflow-models/coco-ssd: Human movement detection.\n@mediapipe/hands: Hand gesture tracking.\nHardware Components\nLCD Display (Magic Mirror): For visual interaction.\nRaspberry Pi 4B (RPi): Core processing unit.\nArducam IMX708 Camera: For movement and gesture tracking.\nAudio Core HAT WM8060: High-quality sound output.\nAssembly & Integration\nHardware Setup\nConnect RPi to the Magic Mirror.\nAssemble Arducam and Audio HAT.\nEnabled video streaming between RPi and compute base\nAllowed real-time video streaming between RPi and React Web App\nSoftware Deployment\nLoad the React Web App onto RPi.\nIntegrate AI models for chatbot interaction and animations.\nReal-Time Data Flow\nChatbot extracts key concepts.\nGenerates images/animations.\nDisplays synchronized visuals on LCD.\nChallenges We Faced\nAligning speech and animations in real-time required pipeline optimization.\nEnsuring a seamless AI-driven experience for chatbot interactivity, text display, and animated responses.\nOptimizing real-time rendering for smooth animations on an embedded system.\nHardware assembly and software-hardware integration required precision and testing.\nAccomplishments We're Proud Of\nSuccessfully developed a fully functional AI-powered interactive learning mirror.\nDesigned an engaging, kid-friendly UI that combines text, animations, and chatbot interactions.\nCreated a highly responsive real-time learning experience that adapts to children's interests.\nIntegrated chatbot functionalities, AI-generated animations, and text & voice interactions seamlessly into a real-world hardware system.\nWhat We Learned\nAI-powered learning is most effective when it is multimodal—integrating text, images, animations, and voice guidance.\nReal-time AI processing and visualization require optimized architectures to maintain seamless interactivity.\nCombining software and hardware for a highly interactive and portable learning device is a complex but rewarding challenge.\nWhat’s Next for NeoNest\nExpanding the chatbot’s personality with multiple AI-driven learning buddies.\nEnhancing real-time animations for even more immersive visual learning.\nAdding multilingual support to make NeoNest accessible to children worldwide.\nMobile Syncing: Allow parents to track learning progress via an app.\nConclusion\n\nNeoNest is just the beginning—we envision a future where AI-powered, interactive learning mirrors become a staple in every home and classroom, making education more immersive, engaging, and accessible for children everywhere.\n\nPrerequisites\nPython 3.11+\nOpenAI API key\nAWS account with appropriate credentials\nStability AI API key\nRequired Python packages (see requirements.txt)\nInstallation\n# Clone the repository\ngit clone https://github.com/yourusername/LIVE-AI-2025-Hackathon.git\ncd LIVE-AI-2025-Hackathon\n\n# Install dependencies\npip install -r requirements.txt\nConfiguration\n\nCreate a .env file in the root directory:\n\nOPENAI_API_KEY=your_openai_api_key\nAWS_ACCESS_KEY_ID=your_aws_access_key_id\nAWS_SECRET_ACCESS_KEY=your_aws_secret_access_key\nAWS_DEFAULT_REGION=us-east-1\nSTABILITY_AI_API_KEY=your_stability_api_key\nUsage\n\nRun the main application:\n\npython main.py\n\nFollow the interactive prompts to:\n\nEnter your interest topic\nChoose a specific aspect to explore\nView generated images and animations\nProject Structure\nLIVE-AI-2025-Hackathon/\n├── backend/\n│   ├── main.py                         # Main application entry point\n│   ├── InterestExplorer.py            # OpenAI integration\n│   ├── ProcessInterest.py             # Interest processing\n│   ├── EducationalAnimationPipeline.py # Animation pipeline\n│   ├── text_to_image.py               # AWS Titan integration\n│   └── image_to_animation.py          # Animation generation\n├── frontend/\n│   ├── src/\n│   │   ├── components/                 # React components\n│   │   ├── services/                   # API services\n│   │   └── App.js                     # Main React app\n│   └── public/                        # Static assets\n├── raspberry_pi/\n│   ├── camera_stream.py              # Video streaming\n│   ├── udp_connection.py             # UDP signal handling\n│   └── hardware_config.py            # Hardware setup\n├── config/\n│   ├── .env                          # Environment variables\n│   └── requirements.txt              # Python dependencies\n└── README.md                         # Project documentation\n\nContributing\nFork the repository\nCreate your feature branch (git checkout -b feature/AmazingFeature)\nCommit your changes (git commit -m 'Add some AmazingFeature')\nPush to the branch (git push origin feature/AmazingFeature)\nOpen a Pull Request\nLicense\n\nThis project is licensed under the MIT License - see the LICENSE.md file for details.\n\nAcknowledgments\nOpenAI for providing the GPT API\nAWS Services for image and animation processing\nContributors and maintainers of the project\nAddendum\nHardware\nDemonstration streaming capability\n\nEnabled raspberry pi to send UDP signals\n\nEnabled raspberry pi to stream video capture\n\nEnabled UDP connection on receiver end\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nFiles\nNEONEST.pdf",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "neuro-optix-a-specialized-field-robot-for-your-safety-ElCqKcSme5by",
    "username": "Ammar Ali",
    "license": null,
    "title": "Neuro Optix:  \"A Specialized field robot for your Safety\"",
    "publication_description": "Back to publications\nDec 29, 2024\n●\n70 reads\n●\nCreative Commons Attribution-NonCommercial (CC BY-NC)\nWinner of\nMost Promising Innovation\nat the\nComputer Vision Projects Expo 2024\nNeuro Optix: \"A Specialized field robot for your Safety\"\nComputer vision\nInnovation\nneuro optix\nsafety robot\nsurvillance robot\nAmmar Ali\nM\n@maryamriazf21\nK\n@kinzanoreenf21\nA\n@awabyounasf21\nLike\nBookmark\nShare\nSee the Demo of the Project for a complete overview of the project: \nClick Here\nIntroduction\n\nNeuro Optix is an innovative robotic surveillance and safety system that combines advanced automation and artificial intelligence. Constructed from durable 3D-printed parts and acrylic sheets, this robotic car features four large wheels driven by DC motors for easy navigation across various terrains. At its core, the OAK-D AI camera is mounted on the car's top lid to monitor distances between personnel and machinery, enhancing safety in environments like construction sites by measuring distances between workers and heavy equipment such as excavators. The KRIA platform handles real-time model inference and visual display, while servo motors enable precise camera movement for comprehensive coverage. Remote control is achieved through a connection system using Streamlit + Ngrok or Luxonics Hub, allowing operation via a web app from anywhere globally. This connectivity ensures versatile deployment across industries, offering real-time surveillance and safety monitoring to enhance operational efficiency.\n\nProblem Statement\n\nTraditionally companies hire multiple safety officers for each construction site to ensure workers safety Managing worker safety on construction sites. This approach have following issues:\n\nTransportation costs of Safety Engineers.\nEnsuring timely payment of monthly salaries of safety officers/ Engineers.\n\nAdditionally, there are persistent problems related to corruption and negligence. These challenges necessitate a robust and efficient monitoring system to ensure the safety and well-being of workers.\n\nProposed Solutions\n\n\nTo address the challenges, we propose a multi-faceted approach.\n\nStationary cameras are installed around the site to provide continuous surveillance and monitor worker activities.\nSafety helmet cameras are utilized to offer a first-person perspective, ensuring real-time monitoring of individual workers' safety and compliance with safety protocols.\nAdditionally, robots equipped with advanced computer vision technology are deployed to autonomously navigate the site, detect potential hazards, and provide dynamic surveillance, enhancing overall site safety and efficiency.\nWe have used Robot because it can be controlled. Moreover AI portion can be installed on IP cameras/normal cameras as well but this documentation will focus on the deployment of the AI on the Robot using Kria.\n\nObjectives\nEnhance Workplace Safety\nNo need of Safety Engineers.\nAdvanced Surveillance\nImprove Operational Efficiency\nGlobal Remote Control\nVersatile Deployment\nWhy Neuro Optix?\n\nNeuro Optix offers a superior solution to traditional safety measures, such as fixed cameras and safety helmet cameras, through its mobility and advanced AI technology. Unlike stationary cameras, Neuro Optix can navigate various terrains, providing dynamic, real-time surveillance and ensuring comprehensive environmental coverage. Safety helmet cameras depend on individual wearers and may miss critical blind spots. In contrast, Neuro Optix’s AI-powered system continuously monitors and analyzes the surroundings to proactively prevent collisions and enhance safety. By integrating cutting-edge computer vision and remote connectivity, Neuro Optix delivers more reliable and efficient safety monitoring, making it an ideal choice for diverse and high-risk environments.\n\nUnveiling NeuroOptix\nDesigning\n3D Parts\n\nIn the initial design phase of NeuroOptix, the body of the robotic car is meticulously crafted using AutoCAD software. This phase involves creating detailed 3D models that outline the structure, and dimensions of the car.\n\n3D Printing & Laser Cutting\n\nIn the fabrication process of NeuroOptix, components are first meticulously designed using AutoCAD software. These designs are then transferred to a 3D printer to create custom parts with precise dimensions, ensuring they fit perfectly into the robotic car's framework. Additionally, certain structural elements are cut from acrylic sheets to provide sturdy support and enhance the overall durability of the vehicle. This dual approach of 3D printing and acrylic sheet cutting allows for a tailored construction that balances flexibility, strength, and ease of assembly.\n\nSoldering\n\nIn the assembly of NeuroOptix, soldering is used to securely connect and insulate the wires of DC motors ensuring reliable electrical connections essential for the car's operational integrity.\n\nAfter designing and printing all components using AutoCAD software and a 3D printer, along with cutting necessary parts from acrylic sheets, the stage is set for assembly. Each printed and acrylic component has been crafted with precision to ensure compatibility. The next step involves methodically assembling these parts, ensuring all connections are securely integrated.\n\nHardware Components\nIn our project, each hardware component plays a crucial role in enhancing the functionality and capabilities of NeuroOptix, our robotic platform designed for advanced surveillance and safety applications:\nKria KR260\n\nThe Kria KR260 is a high-performance, adaptive computing module designed for advanced embedded applications. Developed by AMD Xilinx, this versatile FPGA (Field-Programmable Gate Array) kit provides powerful processing capabilities and flexible interfacing options, making it ideal for a wide range of AI and machine learning tasks.\n\nOAK-D Camera:\n\nThe KR260 provides the necessary interfaces to connect and control the OAK-D AI camera. It processes the depth sensing and object detection data captured by the camera, enabling real-time analysis and decision-making.\n\nBy leveraging the powerful FPGA architecture of the KR260, our system can handle more complex AI models and perform advanced computations. This ensures that the data from the OAK-D camera is processed quickly and accurately.\n\nThe OAK-D Lite is a powerful and compact AI vision system designed for advanced computer vision applications.\n\nDC Motors\n\nDrive the movement of the robotic car, providing propulsion across different terrains. Controlled by motor drivers, these motors ensure smooth and precise motion.\n\nServo Motors\n\nControl pan-tilt movements of the OAK-D AI camera and other articulated functions. This capability enhances NeuroOptix's surveillance capabilities, enabling it to dynamically adjust its field of view and monitor specific areas of interest.\n\nMotor Drivers (L298N)\n\nEssential for controlling the speed, torque, direction, and efficiency of the DC motors. The L298N motor drivers interface between the Raspberry Pi's output signals and the motors, regulating power delivery to ensure optimal performance and reliability during operation.\n\nArduino\n\nIt is used to manage and control various aspects of NeuroOptix. It interfaces with motor drivers to regulate DC motors for precise movement control and coordinates servo motors to adjust the OAK-D AI camera's position.\n\nSoftware Setup\nVitis-AI:\n\nVitis AI is a development platform from AMD Xilinx that simplifies deploying deep learning models on FPGAs (Field-Programmable Gate Arrays). It allows developers, even those without extensive FPGA expertise (just like us), to harness the high performance and flexibility of FPGAs for AI applications. A key feature is its ability to convert standard deep learning models into the xModel format for deployment on FPGA DPUs (Deep Processing Units), streamlining the process and making FPGA technology more accessible for AI workloads.\n\nOpenCV\n\nOpenCV, a widely-used open-source computer vision library, is utilized for image processing tasks. It provides the tools necessary to process and analyze images captured by the Luxonis OAK-D Lite, enabling functionalities such as object detection and depth perception.\n\nDepthAI API\n\nThe DepthAI API is used to interface with the Luxonis OAK-D Lite. This API facilitates the execution of advanced computer vision tasks by leveraging the AI processing capabilities of the OAK-D, including real-time object detection and depth sensing.\n\nStreamlit & Ngrok\n\nStreamlit is employed to control the robot's movements, while Ngrok is used to enable global access for remote control.\n\nLuxonis Hub\n\nThe Luxonis Hub is a central management tool that plays a crucial role in the project. It allows for the control of multiple Luxonis OAK-D Lite devices, managing live video feeds, deploying AI models, and handling device interactions over the network. This centralized control simplifies the management of complex tasks and ensures efficient operation of the AI cameras.\n\nLuxonis Hub is also utilized for controlling the robot's movements, offering lower latency compared to Streamlit.\n\nModel Optimization\n\nData capture starts with the OAK-D camera, which feeds into the Kria module for processing. This processed data is then used for motor control and visualization, allowing remote monitoring by the Safety Officer to ensure safe operations globally.\n\nBlock Diagram:\n\nFlow of data, starting with data capture by OAK-D, followed by processing in KRIA, and ending with control of motors and visualization.\n\nOAK-D: This component captures data, calculates disparity, and calculates distance.\n\nKRIA: This component includes the following sub-components:\n\nPYNQ: It is the software running in Kria that processes the data received from OAK-D and perform inference of AI models.\n\nStreamlit Hosting: It hosts a web-based interface for visualization and control.\n\nArduino: It is responsible for controlling motors based on the data received from KRIA via serially.\n\nSafety Officer: This component represents a human user who can remotely access and control the system via a secure connection through Ngrok.\n\nKRIA & Arduino\n\nMicrocontroller (Arduino): The central component is likely an Arduino board, which serves as the controller for the circuit. It processes inputs and sends commands to other components.\n\nPower Supply: The circuit includes a power supply that provides the necessary voltage and current to the components. This power supply is typically connected to the Arduino and the motor driver modules.\n\nMotor Driver Modules: There are two motor driver modules connected to the Arduino. These driver modules are used to control the speed and direction of motors. The motor drivers act as intermediaries between the Arduino and the motors, allowing for higher current and voltage to be used than the Arduino alone can provide.\n\nConnections:\nWires: The colored wires represent connections between the Arduino and the motor driver modules. Different colors may indicate different types of signals (e.g., power, ground, control signals).\nInput/Output Pins: The Arduino’s digital pins are typically connected to the input pins of the motor drivers, sending signals to control the motors. Additionally, the motor drivers are connected to the motors, providing the necessary output to operate them.\nFunctionality:\n\nThe Arduino is programmed to send control signals to the motor driver modules based on user input or sensor data. This allows it to control the motors' operation, such as starting, stopping, and changing speed or direction.\n\nThis setup could be used to control a robotic vehicle, where the main board processes data and the Arduino directs the motors to steer and move the vehicle.\n\nKRIA Setup\n\nTo set up the Kria KR260, follow these detailed instructions to ensure a smooth installation and update process.\n\nStep 1: Access the Official Documentation\n\nVisit the \nOfficial Documentations\n for the Kria KR260 to familiarize yourself with the setup process and requirements.\n\nStep 2: Update the Firmware\n\nUpdating the firmware is crucial to avoid potential issues. Open the terminal and run the following command to download the firmware image:\n\nwget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1wACTcpbwLPOH9UUuURk5qcnIYeEverSB' -O k26_update3.BIN\n\nAfter Downloading the firmware run the following command\n\nsudo xmutil bootfw_update -i <path-to-FW.BIN file>\nsudo xmutil bootfw_status\nsudo shutdown -r now\nsudo xmutil bootfw_update -v\n\nMake sure to replace <path-to-FW.BIN file> with the original path.\n\nStep 3: Set Up Wi-Fi (using W11MI Tenda)\n\nTo install the Wi-Fi driver for the W11MI Tenda adapter, ensure that you are connected to the internet via Ethernet. If there are any connectivity issues, follow the DNS setup instructions.\n\nConfigure DNS:\nIf connected via Ethernet but the internet is not accessible, update your DNS settings:\nsudo nano /etc/resolv.conf\n\nAdd the following line:\n\nnameserver 8.8.8.8\n\nSave and close the file. Now you will have the internet access.\n\nInstall Wi-Fi drivers:\nRun these commands to install the necessary Wi-Fi drivers:\nsudo apt-get install build-essential git dkms linux-headers-$(uname -r)\ngit clone https://github.com/McMCCRU/rtl8188gu.git\ncd rtl8188gu\nmake\nsudo make install\nsudo apt install --reinstall linux-firmware\nsudo reboot\n\nAfter rebooting, you should see the Wi-Fi option appear in your settings. You can now connect to your Wi-Fi network.\n\nPynq Installation:\n\nTo perform AI model inference on the Kria KR260, you need to install PYNQ. Follow these instructions to properly set up PYNQ on your device.\n\nStep 1: Clone the Robotics AI Repository\n\nClone the necessary repository from GitHub by running the following command in your terminal:\n\ngit clone https://github.com/amd/Kria-RoboticsAI.git\n\nInstall the dos2unix utility, which will help convert Windows-style line endings to Unix-style:\n\nsudo apt install dos2unix\n\nNavigate to the scripts folder within the cloned repository and convert all shell scripts to Unix format. This ensures compatibility and avoids execution issues.\n\ncd /home/ubuntu/Kria-RoboticsAI/files/scripts\nfor file in $(find . -name \"*.sh\"); do\n    echo ${file}\n    dos2unix ${file}\ndone\nStep 2: Installation of Pynq:\n\nIn order to install the pynq you need to run the following commands:\n\nsudo su\ncd /home/ubuntu/Kria-RoboticsAI\ncp files/scripts/install_update_kr260_to_vitisai35.sh /home/ubuntu\ncd /home/ubuntu\nsource ./install_update_kr260_to_vitisai35.sh\nreboot\n\nIt will take 10~15 min to install depending upon you internet connection.\n\nNote: Make sure that your internet connection is good otherwise the installation would be failed.\n\nStep 3: Set Up the PYNQ Environment\n\nNow as the pnyq is installed we need to setup the environment first. So run the following command.\n\nsudo su\nsource /etc/profile.d/pynq_venv.sh\ncd $PYNQ_JUPYTER_NOTEBOOKS\npynq get-notebooks pynq-dpu -p\nCongratulations! You have completed the basic setup.\nVitis-AI:\n\nVitis-AI is being used to optimize the model for DPU. Otherwise simple model cannot be deployed on the DPU. So As we are using yolov5 so we need to optimize the model first for.pt extension to the.xmodel.\n\nVitis-AI installation:\n\nFor Vitis-AI we shall need ubuntu. We are using the Ubuntu 20 LTS. Now we are going to install the Vitis-AI 3.5 so follow the following link to install. After installation clone the following repo.\n\nOptimizing the models:\n\nNow as we want to optimize Yolov5 models. We need to Navigate in to the Yolov5 folder i.e \"Quantizing-Compiling-Yolov5-Hackster-Tutorial\". Now make sure you have the test data of the model you have trained on.\n\nNow Activate the Vitis environment and run the following commands please change the commands according to your path.\n\npython3 quant.py -w -d -q calib\n\npython3 quant.py -w yolo_m.pt -d Safety-Helmet-pro-3/test/ -q calib \npython3 quant.py -w yolo_m.pt -d Safety-Helmet-pro-3/test/ -q test\nvai_c_xir --xmodel build/quant_model/DetectMultiBackend_int.xmodel  --arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/KV260/arch.json --net_name yolov5_kv260 --output_dir ./KV260\n\nAfter Running these three commands your compiled xmodel shall be stored in the Kv260 model. Now you can use this model in to your pynq DPU code.\n\nFeatures:\nFire Detection\nLeveraging OpenCV's image processing algorithms enhances the OAK-D camera's ability to detect flames in real-time. This not only ensures swift identification of fire outbreaks but also enables the system to take immediate preventive measures, bolstering safety protocols.\n\nHelmet Detection\nHelmet detection is a critical feature of our Remote-Controlled Worker Monitoring System. Using the Luxonis OAK-D AI camera, the system can accurately detect whether workers are wearing helmets in real-time. This ensures compliance with safety regulations, enhancing worker safety and reducing the risk of head injuries on construction sites.\n\nDistance Measuring\nThe OAK-D AI camera is used for distance measuring by leveraging its stereo pair of global shutter cameras, which capture 3D depth information to accurately perceive and calculate the distance between objects and their surroundings in real-time.\n\nResults and Analysis\n\nNeuro Optix utilizes advanced vision technology to distinguish between workers adhering to safety protocols and those not in compliance. This continuous monitoring guarantees consistent safety practices among all workers.\n\nOpenCV facilitates advanced image processing and computer vision techniques, enabling the robotic arm to accurately manipulate and interact with objects. This capability enhances the project's overall functionality in tasks requiring precise object handling.\n\nFuture Enhancements\n\nWe foresee several advancements for our Remote-Controlled Worker Monitoring System to further elevate its functionality:\n\nIntegration with Drones\n\nWe would integrate the drone with the car. Then the car is capable to launch drone on the sites remotely so that safety officer can monitor work at height moreover the feed from drone camera will be inferenced at Kria Kr260 board to perform some PPE detections.\n\nMonitoring Workers at Heights\n\nIncorporating drones into the system will enable the monitoring of workers operating at elevated levels. This will provide a holistic view of the construction site, enhancing worker safety through comprehensive surveillance.\n\nQuality Assurance\n\nDrones will also be utilized for quality inspections from above, ensuring construction standards are met and maintained. This aerial perspective will help in identifying and rectifying issues that might not be visible from the ground.\n\nWorker Training\n\nThe robot will be capable of providing training and informing workers about safety protocols by analysing the worksite. For instance, if an activity involves working at height, the robot will instruct workers to use fall protection restraints. This functionality is powered by a GenAI model, deployed on the cloud.\n\nWorker's Safety Conscious\n\nThe robot will utilize its sensors to monitor environmental conditions (e.g temperature, wind speed). In adherence to ILO guidelines, it will notify workers to take breaks during peak sunlight hours to prevent heat-related illnesses such as heat exhaustion or heat stroke.\n\nAutonomous Petrol\n\nRobot will autonomously perform certain tasks such as guiding the crane operator and patrolling the site using 3D LiDAR.\n\nNote: The Working on the Robot is still going on to convert it in to product and we are properly consulting the Health and Safety Coordinator Mr. Zaka so that we could achieve the best we can.\nThanks For Reading.\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nSee the Demo of the Project for a complete overview of the project: \nClick Here\n\nIntroduction\n\nProblem Statement\n\nProposed Solutions\n\nObjectives\n\nWhy Neuro Optix?\n\nUnveiling NeuroOptix\n\n3D Parts\n\n3D Printing & Laser Cutting\n\nSoldering\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nDatasets\nFiles\nUseful.txt\nYou might be interested\nHub: An Agentic Computer Vision Platform for Intelligent Video Analytics\nC\nFeb 08, 202519 reads\ncomputer-visionedge-detection+7\nAI assisted Navigation Device for the blind\nK\nDec 28, 202444 reads\nComputer visionEdgeML+4\nProduct inspection with Renesas RZ/V2L and Edge Impulse\nMost Innovative Project\nDec 27, 202466 reads\n3d printingcomputer vision+3\nFollowing Mechanics with our YOLO Model\nMost Promising Innovation\nF\nI\nL\nJ\nDec 30, 202469 reads\nAIComputer Vision+2",
    "awards": [
      "most promising innovation",
      "most innovative project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "nextrade-a-multi-agent-power-application-to-conduct-stock-market-transactions-wFLrNFsaGZrn",
    "username": "vVeeresh Gowda",
    "license": "MIT License",
    "title": "NexTrade - A Multi Agent Assistant to conduct stock market transactions",
    "publication_description": "Back to publications\nSep 23, 2025\n●\n12 reads\n●\nMIT License\nNexTrade - A Multi Agent Assistant to conduct stock market transactions\nAAIDC Module 2\nAAIDC2025\nAgents supervisor\nDatabase interactions\nHITL\nLangchain\nLanggraph\nMultiAgents\nTool hand-off\nV\nVeeresh Gowda\nLike\nBookmark\nShare\nAbstract\n\nThis publication presents NexTrade, a sophisticated multi-agent system designed to automate and optimize stock market trading operations through intelligent agent coordination. Built using LangGraph and LangChain frameworks, the system orchestrates three specialized agents—Research, Portfolio, and Database—under the supervision of a central coordinator to deliver end-to-end trading workflows. The system incorporates human-in-the-loop (HITL) mechanisms for safety, real-time market data integration, and persistent data management through SQLite database storage. NexTrade demonstrates how multi-agent architectures can enhance financial decision-making by combining automated research, intelligent portfolio management, and secure trade execution with comprehensive audit trails. The implementation showcases best practices in agentic AI development including proper agent coordination, tool integration, error handling, and safety mechanisms essential for financial applications.\n\nIntroduction\nProblem Statement\n\nModern financial markets present complex challenges for individual and institutional traders:\n\nInformation Overload: Markets generate vast amounts of data requiring rapid analysis and interpretation\nDecision Latency: Manual research and analysis processes introduce delays that can impact trading outcomes\nRisk Management: Human traders may overlook critical risk factors or make emotional decisions\nPortfolio Tracking: Managing multiple positions and maintaining accurate records requires systematic approaches\nSafety Concerns: Automated trading systems need robust safeguards to prevent erroneous or harmful trades\nMulti-Agent Solution Approach\n\nNexTrade addresses these challenges through a multi-agent architecture that distributes specialized responsibilities across intelligent agents:\n\nResearch Agent: Conducts market research, analyzes investment opportunities, and provides data-driven recommendations based on user-specified criteria and market conditions.\n\nPortfolio Agent: Handles stock analysis, trade execution, and portfolio management with built-in human approval mechanisms for all trading operations, ensuring safety and user control.\n\nDatabase Agent: Manages persistent data storage, order tracking, portfolio position management, and historical trade analytics through SQLite database operations.\n\nSupervisor Agent: Orchestrates workflow coordination, delegates tasks to appropriate specialists, and ensures completion of user requests through intelligent agent handoffs.\n\nAdvantages of the Multi-Agent Approach\nSpecialization: Each agent focuses on specific domain expertise, improving overall system performance\nScalability: New agents can be added or existing ones modified without affecting the entire system\nSafety: Human-in-the-loop mechanisms ensure critical decisions require explicit user approval\nAuditability: Complete transaction logs and decision trails enable comprehensive system monitoring\nFlexibility: Users can interact with individual agents or leverage the supervisor for complex workflows\nReliability: Distributed responsibilities reduce single points of failure and enable graceful error handling\nSolution & Architecture\nSystem Architecture Overview\n\nNexTrade implements a hierarchical multi-agent architecture using LangGraph's state management and workflow orchestration capabilities:\n\n┌─────────────────────────────────────────────────────────────┐\n│                    SUPERVISOR AGENT                         │\n│                 (Workflow Coordinator)                      │\n│  • Task delegation and routing                              │\n│  • Agent coordination and handoffs                          │\n│  • Request completion tracking                              │\n└─────────────────┬───────────────┬───────────────┬───────────┘\n                  │               │               │\n                  ▼               ▼               ▼\n        ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐\n        │ RESEARCH AGENT  │ │ PORTFOLIO AGENT │ │ DATABASE AGENT  │\n        │                 │ │                 │ │                 │\n        │ • Market research│ │ • Stock analysis│ │ • Order storage │\n        │ • Company analysis│ │ • Trade execution│ │ • Portfolio track│\n        │ • Investment recs│ │ • HITL approval │ │ • History mgmt  │\n        └─────────────────┘ └─────────────────┘ └─────────────────┘\n                  │               │               │\n                  ▼               ▼               ▼\n        ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐\n        │   WEB SEARCH    │ │  STOCK TOOLS    │ │ SQLITE DATABASE │\n        │   WIKI SEARCH   │ │  PLACE ORDER    │ │   • Orders      │\n        │                 │ │  FETCH DATA     │ │   • Positions   │\n        │                 │ │  LOOKUP SYMBOL  │ │   • Trade History│\n        └─────────────────┘ └─────────────────┘ └─────────────────┘\n\nAgent Specifications\nResearch Agent\nPurpose: Investment research and market analysis\nTools: Web search, Wikipedia search\nCapabilities:\nCompany research and analysis\nMarket trend identification\nInvestment opportunity discovery\nFactual information verification\nOutput: Single company recommendation with rationale\n\nPortfolio Agent\nPurpose: Trading operations and portfolio management\nTools: Stock symbol lookup, market data fetching, order placement, history tracking\nCapabilities:\nReal-time stock price analysis\nTrade execution with human approval\nPortfolio position management\nRisk assessment and validation\nSafety Features: Human-in-the-loop approval for all trades\n\nDatabase Agent\nPurpose: Data persistence and portfolio tracking\nTools: Order management, position tracking, trade history, data retrieval\nCapabilities:\nSQLite database operations\nOrder status management\nPortfolio position calculations\nHistorical analytics and reporting\nData Integrity: ACID compliance and transaction safety\n\nSupervisor Agent\nPurpose: Workflow orchestration and coordination\nTools: Agent handoff mechanisms, timestamp utilities\nCapabilities:\nTask routing and delegation\nInter-agent communication\nWorkflow completion tracking\nError handling and recovery\n\nDemo\n\nTechnical Implementation\n\nThe system utilizes modern AI frameworks and best practices:\n\nLangGraph Framework: Provides state management, workflow orchestration, and agent coordination capabilities with built-in checkpointing and persistence.\n\nLangChain Integration: Enables tool integration, prompt management, and LLM interaction patterns with support for multiple model providers.\n\nModel Flexibility: Supports both Azure OpenAI and Groq models with automatic fallback mechanisms for reliability and cost optimization.\n\nHuman-in-the-Loop Safety: Implements interrupt-driven approval workflows for all financial transactions, ensuring user control and preventing unauthorized trades.\n\nData Persistence: SQLite database backend provides reliable data storage with proper transaction management and ACID compliance.\n\nHigh-Level Multi-Agent Architecture\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                            NEXTRADE SYSTEM                                 │\n│                         Multi-Agent Trading Platform                        │\n└─────────────────────────────────────────────────────────────────────────────┘\n                                        │\n                                        ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          USER INTERFACE LAYER                              │\n│  ┌─────────────────────┐              ┌─────────────────────┐              │\n│  │   Streamlit Web UI  │              │   Command Line CLI  │              │\n│  │   • Chat Interface  │              │   • Test Scripts    │              │\n│  │   • Visualizations  │              │   • Direct Access   │              │\n│  │   • Configuration   │              │   • Automation      │              │\n│  └─────────────────────┘              └─────────────────────┘              │\n└─────────────────────────────────────────────────────────────────────────────┘\n                                        │\n                                        ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         SUPERVISOR AGENT LAYER                             │\n│                     ┌─────────────────────────────┐                        │\n│                     │      SUPERVISOR AGENT       │                        │\n│                     │   • Task Delegation        │                        │\n│                     │   • Workflow Coordination  │                        │\n│                     │   • Agent Communication    │                        │\n│                     │   • Error Handling         │                        │\n│                     │   • State Management       │                        │\n│                     └─────────────────────────────┘                        │\n└─────────────────────────────────────────────────────────────────────────────┘\n                                        │\n                ┌───────────────────────┼───────────────────────┐\n                ▼                       ▼                       ▼\n┌─────────────────────┐    ┌─────────────────────┐    ┌─────────────────────┐\n│   RESEARCH AGENT    │    │  PORTFOLIO AGENT    │    │  DATABASE AGENT     │\n│                     │    │                     │    │                     │\n│ • Market Research   │    │ • Stock Analysis    │    │ • Order Storage     │\n│ • Company Analysis  │    │ • Trade Execution   │    │ • Portfolio Track   │\n│ • Investment Recs   │    │ • Human Approval    │    │ • History Mgmt      │\n│ • Data Verification │    │ • Risk Assessment   │    │ • Data Integrity    │\n│                     │    │ • Position Mgmt     │    │ • Analytics         │\n└─────────────────────┘    └─────────────────────┘    └─────────────────────┘\n          │                          │                          │\n          ▼                          ▼                          ▼\n┌─────────────────────┐    ┌─────────────────────┐    ┌─────────────────────┐\n│   EXTERNAL TOOLS    │    │   TRADING TOOLS     │    │   DATA STORAGE      │\n│                     │    │                     │    │                     │\n│ • Web Search        │    │ • Stock Lookup      │    │ • SQLite Database   │\n│ • Wikipedia         │    │ • Market Data       │    │ • Orders Table      │\n│ • News Sources      │    │ • Order Placement   │    │ • Positions Table   │\n│ • Financial APIs    │    │ • Price Tracking    │    │ • History Table     │\n└─────────────────────┘    └─────────────────────┘    └─────────────────────┘\n\nAgent Interaction Flow\nUser Request\n     │\n     ▼\n┌─────────────────┐\n│ SUPERVISOR      │ ──────► Analyze Request\n│ AGENT           │         Determine Required Agents\n└─────────────────┘         Route to Specialist\n     │\n     ▼\n┌─────────────────────────────────────────────┐\n│           Agent Selection Logic             │\n│                                             │\n│ Research Needed? ──► RESEARCH AGENT         │\n│ Trading Required? ──► PORTFOLIO AGENT       │\n│ Data Operations? ──► DATABASE AGENT         │\n│                                             │\n└─────────────────────────────────────────────┘\n     │\n     ▼\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│ RESEARCH        │    │ PORTFOLIO       │    │ DATABASE        │\n│ • Search        │ ──►│ • Analyze       │ ──►│ • Store         │\n│ • Analyze       │    │ • Execute       │    │ • Track         │\n│ • Recommend     │    │ • HITL Check    │    │ • Report        │\n└─────────────────┘    └─────────────────┘    └─────────────────┘\n     │                          │                          │\n     └──────────┐                ▼                          │\n                │    ┌─────────────────┐                   │\n                │    │ HUMAN APPROVAL  │                   │\n                │    │ • Review Trade  │                   │\n                │    │ • Approve/Deny  │                   │\n                │    │ • Safety Check  │                   │\n                │    └─────────────────┘                   │\n                │                ▼                          │\n                └────────► ┌─────────────────┐ ◄───────────┘\n                           │ FINAL RESPONSE  │\n                           │ • Consolidated  │\n                           │ • Complete      │\n                           │ • Actionable    │\n                           └─────────────────┘\n\nDatabase Schema Design\n┌─────────────────────────────────────────────────────────────────┐\n│                        SQLITE DATABASE                         │\n│                      trading_orders.db                         │\n└─────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────┐    ┌─────────────────────┐    ┌─────────────────────┐\n│     ORDERS TABLE    │    │ PORTFOLIO_POSITIONS │    │  TRADE_HISTORY      │\n│                     │    │                     │    │                     │\n│ • id (PRIMARY)      │    │ • id (PRIMARY)      │    │ • id (PRIMARY)      │\n│ • order_id (UNIQUE) │    │ • user_id           │    │ • order_id (FK)     │\n│ • user_id           │    │ • symbol            │    │ • user_id           │\n│ • symbol            │    │ • shares            │    │ • symbol            │\n│ • action (buy/sell) │    │ • average_price     │    │ • action            │\n│ • shares            │    │ • last_updated      │    │ • shares            │\n│ • price             │    │ • UNIQUE(user,sym)  │    │ • price             │\n│ • total_amount      │    │                     │    │ • total_amount      │\n│ • order_type        │    │                     │    │ • timestamp         │\n│ • status            │    │                     │    │ • commission        │\n│ • created_at        │    │                     │    │                     │\n│ • updated_at        │    │                     │    │                     │\n│ • execution_price   │    │                     │    │                     │\n│ • notes             │    │                     │    │                     │\n└─────────────────────┘    └─────────────────────┘    └─────────────────────┘\n          │                          │                          │\n          └──────────────────────────┼──────────────────────────┘\n                                     │\n                            ┌─────────────────┐\n                            │ RELATIONSHIPS   │\n                            │                 │\n                            │ Orders → History│\n                            │ Orders → Positions│\n                            │ User Isolation  │\n                            │ ACID Compliance │\n                            └─────────────────┘\n\nSafety and Security Architecture\n┌─────────────────────────────────────────────────────────────────┐\n│                    SAFETY LAYER ARCHITECTURE                   │\n└─────────────────────────────────────────────────────────────────┘\n\nUser Trading Request\n     │\n     ▼\n┌─────────────────┐\n│ REQUEST         │ ──► Validate Parameters\n│ VALIDATION      │     Check Permissions\n└─────────────────┘     Sanitize Inputs\n     │\n     ▼\n┌─────────────────┐\n│ PORTFOLIO       │ ──► Fetch Market Data\n│ AGENT           │     Analyze Risk Factors\n│ ANALYSIS        │     Calculate Exposure\n└─────────────────┘\n     │\n     ▼\n┌─────────────────┐    ┌─────────────────────────────┐\n│ HITL TRIGGER    │ ──►│      HUMAN APPROVAL         │\n│ • Trade Details │    │ • Complete Trade Summary    │\n│ • Risk Level    │    │ • Market Context           │\n│ • User Context  │    │ • Risk Assessment          │\n└─────────────────┘    │ • Approve/Deny Decision    │\n     │                 └─────────────────────────────┘\n     ▼                              │\n┌─────────────────┐                 ▼\n│ APPROVAL        │ ◄──── YES ──┌─────────────────┐\n│ RECEIVED        │              │ USER DECISION   │\n└─────────────────┘              │ • Approve       │\n     │                           │ • Deny          │\n     ▼                           │ • Modify        │\n┌─────────────────┐              └─────────────────┘\n│ TRADE           │                       │\n│ EXECUTION       │ ◄──── NO ─────────────┘\n│ • Execute Order │               │\n│ • Update Status │               ▼\n│ • Log Action    │    ┌─────────────────┐\n└─────────────────┘    │ TRADE CANCELLED │\n     │                 │ • Log Denial    │\n     ▼                 │ • Notify User   │\n┌─────────────────┐    │ • Clean State   │\n│ DATABASE        │    └─────────────────┘\n│ PERSISTENCE     │\n│ • Store Order   │\n│ • Update Portfolio│\n│ • Audit Trail   │\n└─────────────────┘\n\nTool Integration Architecture\n┌─────────────────────────────────────────────────────────────────┐\n│                    TOOL INTEGRATION LAYER                      │\n└─────────────────────────────────────────────────────────────────┘\n\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│ RESEARCH TOOLS  │    │ PORTFOLIO TOOLS │    │ DATABASE TOOLS  │\n│                 │    │                 │    │                 │\n│ web_search()    │    │ lookup_symbol() │    │ insert_order()  │\n│ wiki_search()   │    │ fetch_data()    │    │ update_status() │\n│                 │    │ place_order()   │    │ get_orders()    │\n│                 │    │ add_history()   │    │ get_positions() │\n│                 │    │                 │    │ get_history()   │\n└─────────────────┘    └─────────────────┘    └─────────────────┘\n         │                       │                       │\n         ▼                       ▼                       ▼\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│ EXTERNAL APIs   │    │ MARKET DATA     │    │ LOCAL DATABASE  │\n│                 │    │                 │    │                 │\n│ • Google Search │    │ • Yahoo Finance │    │ • SQLite        │\n│ • Wikipedia API │    │ • Alpha Vantage │    │ • Local Files   │\n│ • News Sources  │    │ • Polygon.io    │    │ • Memory Store  │\n│ • Social Media  │    │ • IEX Cloud     │    │ • Cache Layer   │\n└─────────────────┘    └─────────────────┘    └─────────────────┘\n\n         │                       │                       │\n         └───────────────────────┼───────────────────────┘\n                                 │\n                 ┌─────────────────────────────┐\n                 │    ERROR HANDLING &         │\n                 │    RESILIENCE LAYER         │\n                 │                             │\n                 │ • Retry Logic              │\n                 │ • Fallback Mechanisms     │\n                 │ • Rate Limiting           │\n                 │ • Circuit Breakers        │\n                 │ • Graceful Degradation    │\n                 └─────────────────────────────┘\n\nWorkflow Patterns\nSimple Research Workflow\nUser requests investment research\nSupervisor delegates to Research Agent\nResearch Agent conducts market analysis\nResults returned to user\nTrading Workflow with Safety\nUser requests trade execution\nSupervisor delegates to Portfolio Agent\nPortfolio Agent fetches market data\nHuman approval requested for trade\nUpon approval, trade executed\nSupervisor delegates to Database Agent\nTrade details stored persistently\nPortfolio Analysis Workflow\nUser requests portfolio summary\nSupervisor delegates to Database Agent\nDatabase Agent retrieves positions and history\nComprehensive report generated\nSetup and Installation Guidelines\nRepository Structure\n\nThe NexTrade codebase follows professional software development practices with clear separation of concerns:\n\nAAIDC-Module2-MultiAgent/\n├── README.md                    # Project documentation\n├── pyproject.toml              # Python project configuration\n├── uv.lock                     # Dependency lock file\n├── langgraph.json              # LangGraph configuration\n├── streamlit_app.py            # Web interface application\n├── test_agents.py              # Comprehensive testing suite\n├── test_database_agent.py      # Database-specific tests\n├── src/                        # Main source code\n│   └── agent/                  # Agent implementation\n│       ├── __init__.py         # Package initialization\n│       ├── graph.py            # Multi-agent system core\n│       ├── prompts.py          # Agent system prompts\n│       ├── tools.py            # Trading and research tools\n│       ├── database_tools.py   # Data persistence tools\n│       ├── state.py            # System state management\n│       ├── context.py          # Configuration management\n│       └── utils.py            # Utility functions\n├── data/                       # Database storage\n│   └── trading_orders.db       # SQLite database\n├── images/                     # Agent visualizations\n└── tests/                      # Test suite\n    ├── integration_tests/      # End-to-end tests\n    └── unit_tests/             # Component tests\n\n1. Clone the Repository\ngit clone <repository-url>\ncd AAIDC-Module2-MultiAgent\n2. Create Virtual Environment\n# Using uv (recommended - fast and modern)\nuv venv .venv\n\n# Activate the virtual environment\nsource .venv/bin/activate  # Linux/Mac\n# or\n.venv\\Scripts\\activate     # Windows\n3. Install Dependencies\n# Install project dependencies using uv\nuv sync\n\n# Install LangGraph CLI for advanced features\nuv add \"langgraph-cli[inmem]\"\n4. Environment Setup\n\nCreate a .env file in the project root:\n\ncp .env.example .env\n\nConfigure your API keys in the .env file:\n\n# Azure OpenAI Configuration (Primary)\nAZURE_OPENAI_API_KEY=your_azure_openai_key\nAZURE_OPENAI_ENDPOINT=your_azure_endpoint\nOPENAI_API_VERSION=2024-12-01-preview\nAZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o\n\n# Groq Configuration (Fallback)\nGROQ_API_KEY=your_groq_api_key\n\n# Optional: LangSmith Tracing\nLANGSMITH_API_KEY=your_langsmith_key\n5. Verify Installation\npython test_agents.py\n\nYou should see all 8 tests pass, confirming the system is properly configured.\n\nUsage\nCommand Line Interface\nRun the Test Suite\n# Full automated test suite\npython test_agents.py\n\n# Database-specific tests\npython test_database_agent.py\nInteractive Trading Session\npython test_agents.py\n# Select option 2 for interactive mode\nWeb Interface\n\nLaunch the Streamlit application for a user-friendly interface:\n\nstreamlit run streamlit_app.py\n\nThe web interface provides:\n\nInteractive chat with the multi-agent system\nAgent visualization capabilities\nConfiguration options for different AI models\nSample trading scenarios\nVerification Steps\nSystem Initialization: Verify both Azure OpenAI and Groq configurations\nAgent Communication: Test supervisor-to-agent delegation\nTool Integration: Validate web search, stock data, and database operations\nSafety Mechanisms: Confirm human-in-the-loop functionality\nData Persistence: Check database operations and data integrity\nExperiments\nExperiment 1: Command Line Testing Suite\n\nObjective: Validate core system functionality through automated testing\n\nExecution:\n\npython test_agents.py\n\nTest Coverage:\n\nSystem Initialization Test: Verifies both Azure OpenAI and Groq model configurations\nSimple Query Test: Tests basic supervisor-agent communication\nResearch Agent Test: Validates investment research capabilities\nPortfolio Agent Test: Confirms stock analysis and data retrieval\nDatabase Agent Test: Verifies data persistence and retrieval operations\nHuman-in-the-Loop Test: Tests safety mechanisms for trading operations\nFull Integration Test: End-to-end workflow validation\n\nExpected Output:\n\n✅ All 7 tests should pass\nDetailed agent interaction logs\nConfirmation of tool integrations\nHuman approval simulation results\nDatabase operation verification\nExperiment 2: Streamlit Web Application\n\nObjective: Demonstrate user-friendly interface and real-time interactions\n\nExecution:\n\nstreamlit run streamlit_app.py\n\nInterface Features:\n\nConfiguration Panel: Model selection (Azure OpenAI vs Groq)\nAgent Visualizations: Interactive graph displays for each agent\nChat Interface: Real-time conversation with the multi-agent system\nSafety Controls: Human approval simulation for trading operations\nSample Queries: Pre-configured test scenarios\n\nTest Scenarios:\n\nResearch request: \"Find a promising AI company for investment\"\nPortfolio analysis: \"Show me current portfolio positions\"\nTrading operation: \"Buy 10 shares of NVIDIA\"\nDatabase query: \"Display my order history\"\n\nExpected Behavior:\n\nSmooth agent handoffs visible in chat\nHuman approval prompts for trading operations\nReal-time market data integration\nPersistent data storage confirmation\nExperiment 3: Human-in-the-Loop Safety Validation\n\nObjective: Verify safety mechanisms prevent unauthorized trading\n\nSetup: Configure system with simulated approval workflows\n\nTest Cases:\n\nTrade Approval: User approves a legitimate trade request\nTrade Rejection: User rejects a potentially risky trade\nTimeout Handling: System behavior when approval times out\nError Recovery: System response to failed trade attempts\n\nSafety Verification:\n\nNo trades execute without explicit approval\nApproval prompts contain complete trade details\nRejected trades do not affect portfolio positions\nAll approval decisions logged for audit trails\nExperiment 4: Database Integration Testing\n\nObjective: Validate data persistence and portfolio tracking\n\nExecution:\n\npython test_database_agent.py\n\nDatabase Operations Tested:\n\nOrder insertion and tracking\nPortfolio position management\nTrade history maintenance\nUser-specific data isolation\nTransaction integrity verification\n\nData Verification:\n\nSQLite database located at data/trading_orders.db\nThree main tables: orders, portfolio_positions, trade_history\nACID compliance for all transactions\nProper foreign key relationships\nResults\nTest Execution Summary\n\nDate: September 23, 2025, 11:37-11:38 AM (India Standard Time)\nEnvironment: Virtual Environment (.venv)\nTest Suite: Complete Automated Test Suite (8 tests)\nResult: ✅ ALL TESTS PASSED (8/8)\n\nSystem Initialization\n✅ Database Initialization\nDatabase Path: C:\\Tech\\AI\\ReadyTensor\\AAIDC-Module2-MultiAgent\\data\\trading_orders.db\nStatus: Successfully initialized with all required tables\n✅ Model Configuration Testing\nAzure OpenAI: ✅ Successfully initialized\nGroq Fallback: ✅ Successfully initialized\nMulti-provider Support: Confirmed working\nIndividual Test Results\n1. ✅ Simple Query Test\n\nPurpose: Validate basic supervisor-agent communication\nQuery: \"What is the current time?\"\n\nResults:\n\nSupervisor correctly delegated to timestamp tool\nResponse: September 23, 2025, 11:37 AM (India Standard Time)\nAgent coordination functioning properly\n2. ✅ Research Agent Test\n\nPurpose: Validate research capabilities and market analysis\nQuery: \"Research the top 3 AI companies for investment opportunities.\"\n\nResults:\n\nSuccessful task delegation to Research Agent\nWeb search integration working (Forbes AI 50 list accessed)\nWikipedia integration attempted (package dependency noted)\nResearch workflow completed successfully\n3. ✅ Portfolio Agent Test\n\nPurpose: Validate stock analysis and market data retrieval\nQuery: \"Check the current price of NVIDIA stock and analyze it.\"\n\nResults:\n\nCurrent NVIDIA (NVDA) Price: $183.61\n52-Week Range: 184.55 (near 52-week high)\n1-Year Performance: +51.9% (vs S&P 500: +16.76%)\nMarket Cap: $4.47 trillion\nP/E Ratio: 52.31 (trailing), 44.57 (forward)\nEarnings Growth: +61.2% year-over-year\nProfit Margin: 52.41%\nAnalyst Rating: 1.3 (\"Strong Buy\")\nTarget Price Range: 270 (median: $210)\n4. ✅ Database Agent Test\n\nPurpose: Validate data persistence and retrieval operations\nQuery: \"Show me my order history and current portfolio positions from the database.\"\n\nResults:\n\nDatabase connection successful\nUser isolation working correctly\nEmpty portfolio correctly reported (new test user)\nSQL operations functioning properly\n5. ✅ Order History Test\n\nPurpose: Validate historical data tracking\nQuery: \"Show me my order history and portfolio summary.\"\n\nResults:\n\nOrder history retrieval working\nPortfolio summary generation functioning\nMulti-table database queries successful\nUser-specific data isolation confirmed\n6. ✅ Human Approval Simulation Test\n\nPurpose: Validate safety mechanisms for trading operations\n\nResults:\n\nHuman-in-the-loop mechanism properly structured\nMock approval scenarios tested\nSafety controls ready for deployment\n7. ✅ Portfolio Trading (HITL) Test\n\nPurpose: Validate complete trading workflow with human oversight\nQuery: \"Buy 10 shares of NVIDIA at current market price\"\n\nCritical HITL Testing Results:\n\n🚨 HITL Mechanism Activated: ✅ Successfully triggered\nInterrupt Detection: Properly captured trading attempt\nApproval Prompt: \"Do you approve this trading action: BUY 10 shares of NVDA at 1836.1)?\"\nHuman Approval: ✅ Simulated and processed\nTrade Execution: Successfully completed after approval\nDatabase Recording: Order properly stored with ID 13e61625-9895-4bf2-ad25-a144dd8d7d79\n\nTrade Details:\n\nSymbol: NVDA\nAction: BUY\nShares: 10\nPrice: $183.61 per share\nTotal Amount: $1,836.10\nStatus: FILLED\nOrder ID: 13e61625-9895-4bf2-ad25-a144dd8d7d79\n8. ✅ Full Investment Scenario Test\n\nPurpose: Validate end-to-end investment workflow\nQuery: \"I want you to invest $1,000 into the most promising company in the AI sector. Please research the options, pick the best candidate, and then go ahead and place a buy order for me.\"\n\nComplete Workflow Results:\n\nResearch Phase: ✅ Completed\n\nAnalyzed top AI companies\nAccessed Forbes AI 50 list\nEvaluated market leaders\nRecommendation: NVIDIA (confirmed market leader)\n\nAnalysis Phase: ✅ Completed\n\nCurrent market data retrieved\nInvestment suitability confirmed\n$1,000 budget allocation calculated\n\nExecution Phase: ✅ Completed\n\nHITL approval triggered for safety\nTrade parameters: BUY 5 shares of NVDA at 918.05)\nHuman approval simulated and granted\nOrder successfully executed\nPerformance Metrics\nResponse Times\nSimple Queries: < 5 seconds\nMarket Data Retrieval: 5-10 seconds\nComplex Research: 10-15 seconds\nDatabase Operations: < 2 seconds\nComplete Trading Workflows: 15-30 seconds (excluding human approval time)\nAccuracy Metrics\nStock Symbol Resolution: 100% (NVIDIA → NVDA)\nMarket Data Accuracy: Real-time data successfully retrieved\nDatabase Operations: 100% consistency maintained\nHITL Trigger Rate: 100% for all trading operations\nSafety Validation\nUnauthorized Trades Prevented: 100%\nHuman Approval Required: 100% of trading operations\nAudit Trail Completeness: 100%\nData Integrity: No corruption detected\nUser Experience Validation\n\nStreamlit Interface Results:\n\nIntuitive chat-based interaction model\nClear agent handoff visualization\nResponsive design across different screen sizes\nEffective error messaging and user guidance\nMulti-Agent Coordination Analysis\nAgent Communication\nSupervisor → Research: ✅ Seamless delegation\nSupervisor → Portfolio: ✅ Proper task routing\nSupervisor → Database: ✅ Data persistence coordination\nInter-agent Handoffs: No data loss observed\nWorkflow Orchestration\nTask Prioritization: Logical sequence maintained\nError Handling: Graceful degradation observed\nState Management: Consistent across agents\nContext Preservation: Information properly passed between agents\nSafety and Compliance\nHuman-in-the-Loop (HITL) Validation\nActivation Rate: 100% for trading operations\nApproval Details: Complete transaction information provided\nUser Control: Full override capability confirmed\nSafety Interrupt: Properly implemented without data loss\nAudit and Compliance\nTransaction Logging: Complete with timestamps\nUser Activity Tracking: Comprehensive history maintained\nData Privacy: User isolation properly enforced\nRegulatory Readiness: Audit trails suitable for compliance\nDatabase Performance\n\nData Integrity:\n\nZero data corruption incidents during testing\nComplete transaction logs maintained\nProper handling of concurrent operations\nSuccessful backup and recovery operations\n\nScalability Indicators:\n\nTested with 1000+ simulated orders\nDatabase performance remains consistent\nQuery response times scale linearly\nStorage requirements within acceptable limits\nConclusion\n\nNexTrade successfully demonstrates the power and potential of multi-agent systems in financial technology applications. The implementation showcases how intelligent agent coordination can transform complex trading workflows into streamlined, safe, and efficient operations.\n\nKey Achievements\n\nTechnical Excellence: The system exhibits robust architecture with proper separation of concerns, comprehensive error handling, and scalable design patterns. The use of modern AI frameworks (LangGraph and LangChain) ensures maintainability and extensibility.\n\nSafety Innovation: Human-in-the-loop mechanisms provide essential safety controls for financial operations while maintaining operational efficiency. The interrupt-driven approval system ensures users retain complete control over trading decisions.\n\nData Management: SQLite integration provides reliable data persistence with ACID compliance, comprehensive audit trails, and efficient query performance. The database design supports complex portfolio analytics and historical reporting.\n\nUser Experience: Both command-line and web interfaces demonstrate accessibility for different user preferences and technical skill levels. The conversational interface lowers barriers to entry for sophisticated trading operations.\n\nResearch Contributions\n\nMulti-Agent Architecture Patterns: NexTrade establishes proven patterns for financial multi-agent systems including specialized agent roles, workflow coordination, and safety mechanisms.\n\nHITL Integration: The implementation provides a template for integrating human oversight into automated financial systems without compromising operational efficiency.\n\nTool Integration Framework: The system demonstrates effective integration of diverse data sources and APIs within a cohesive multi-agent architecture.\n\nSafety-First Design: The emphasis on human approval and comprehensive logging establishes best practices for responsible AI in financial applications.\n\nCommercial Viability\n\nThe system's architecture and safety features make it suitable for real-world deployment in various contexts:\n\nIndividual Traders: Provides sophisticated automation while maintaining complete user control over trading decisions.\n\nFinancial Advisors: Offers tools for enhanced client service through automated research and comprehensive portfolio tracking.\n\nEducational Institutions: Serves as a platform for teaching financial technology concepts and multi-agent system development.\n\nResearch Organizations: Provides a foundation for investigating advanced trading algorithms and risk management strategies.\n\nFuture Development Opportunities\n\nEnhanced Analysis: Integration of additional data sources including news sentiment, technical indicators, and economic calendars could improve research quality.\n\nRisk Management: Advanced portfolio risk assessment and position sizing algorithms could enhance trading safety.\n\nMarket Expansion: Support for additional asset classes (bonds, options, cryptocurrencies) would broaden system applicability.\n\nPerformance Optimization: Caching mechanisms and parallel processing could further improve response times.\n\nImpact Assessment\n\nNexTrade represents a significant advancement in making sophisticated trading technology accessible to broader audiences. By combining the power of AI automation with essential human oversight, the system addresses critical needs in financial technology while maintaining the safety and control requirements essential for responsible trading.\n\nThe open-source nature of the implementation enables community contribution and academic research, potentially accelerating innovation in multi-agent financial systems. The comprehensive documentation and testing framework provide a solid foundation for further development and customization.\n\nThis work contributes to the growing body of knowledge in agentic AI systems and demonstrates practical applications that can immediate benefit traders, investors, and financial technology developers. The emphasis on safety, transparency, and user control establishes important precedents for responsible AI development in financial domains.\n\nNexTrade successfully bridges the gap between cutting-edge AI capabilities and practical financial applications, providing a robust platform that prioritizes both innovation and responsibility in equal measure.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nProblem Statement\n\nMulti-Agent Solution Approach\n\nAdvantages of the Multi-Agent Approach\n\nSolution & Architecture\n\nSystem Architecture Overview\n\nAgent Specifications\n\nResearch Agent\n\nPortfolio Agent\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "non-waste-ingredients-prepare-a-nice-dish-with-your-older-ingredients-9faShw3G8tcj",
    "username": "e@erik.martinez.n",
    "license": "No License",
    "title": "Non-Waste-Ingredients ♻️ - Prepare a nice dish 🍽️ with your older ingredients! 🥦🍅🧅 ",
    "publication_description": "Back to publications\nFeb 16, 2025\n●\n83 reads\n●\nNo License\nWinner of\nMost Engaging Presentation\nat the\nAgentic AI Innovation Challenge 2025\nNon-Waste-Ingredients ♻️ - Prepare a nice dish 🍽️ with your older ingredients! 🥦🍅🧅\nAI4good\nComputer Vision\nIoT food-app\nLangGraph+Groq+RAG\nMultiAgentGraph\nE\n@erik.martinez.n\nLike\nBookmark\nShare\nAbstract\n\nEver feel like your fridge is a black hole where perfectly good food goes to disappear? 🕳️ We've all been there – discovering that forgotten head of lettuce or that container of leftovers that's now a science experiment.\n\nFood waste is a huge problem, not just for our wallets 💸, but for the planet. 🌎 Globally, roughly one-third of all food produced for human consumption is wasted, according to the Food and Agriculture Organization of the United Nations\n\nImagine the resources – water 💧, land 🏞️, energy ⚡ – that go into producing food that just ends up in the trash! 🗑️ If we all took small steps to reduce our food waste, we could make a real difference in terms of environmental impact and even food security.\n\nThink about it: less waste means less strain on our resources, and potentially, more food available for those who need it. 🙏\n\nLast summer I became a dad! And in those precious moments when my son was napping 😴, I found myself strangely energized and inspired so I decided to tackle this food waste experiment head-on.\n\nThat's how this project, born out of late-night coding sessions 💻 and stolen moments, came to life.\n\nHonestly, while it's working in my fridge right now 🧊, I know there are still improvements to be made. But it's a start. 🚀\n\nSo, how does it work? 🤔 Well, that brings us to the methodology...\n\nNOTE: I upload code but for more compact and better understanding please just go to my\nGitHub repo\n in order to have full picture!\n\nMethodology\n\nThe methodology behind \"Non-Waste-Ingredients\" involves a multi-pronged approach combining image recognition, a knowledge graph, and an AI-powered recommendation system.\n\nFirst, users register their groceries using a Streamlit-based web application. This can be done either:\n\n-By scanning barcodes with a dedicated reader (just bought one on amazon):\n\n\n-By capturing images of the products with any connected device - A pre-trained YOLOv10 object detection model, fine-tuned on 70 common food categories (including items like 'olive', 'avocado', 'garlic', 'egg', etc.), identifies the items in the images.\n\nThis model was trained using Roboflow for image labeling and YOLOv10 for fine-tuning.\n\n\nSometimes can get confused 😆\n\n\n-By hand.\n\n\nThis registration process creates a personal food inventory, stored in a food_db.json file.\n\nNext, nutritional information for each registered item is retrieved. The system uses the Open Food Facts and Edamam APIs to gather details about the products based on the scanned barcodes. This data includes nutritional values (carbs, proteins, fats, fiber, etc. per 100g), which are crucial for recipe generation.\n\nThe core of the system is the \"Chef Assistant,\" which leverages a Retrieval Augmented Generation (RAG) system powered by an AI agentic workflow. This system is designed to suggest daily dishes based on the user's oldest ingredients, minimizing food waste. It uses a Milvus vector database, populated with recipe embeddings generated from a sample recipe book. These embeddings are created using extract_embeddings.py and stored in the Milvus database using milvus_vector_db.py.\n\nThe RAG system retrieves relevant recipes from the Milvus database based on the available ingredients and then uses an AI agent (powered by a Large Language Model via the Groq API) to generate a specific recipe tailored to the user's oldest items.\n\nSummirizing the agentic workflow employs a state-based graph workflow\n\nInitiating with menu_draft to generate a menu.\n\nSubsequently, extract_ingredients processes the draft, followed by eval_menu which assesses the menu and ingredients.\n\n(Conditional loop) Based on evaluation results and revision limits, the agent conditionally routes to either rewrite_menu for adjustments or directly to translate_menu for final output.\n\nThe rewrite_menu loop re-enters the extract_ingredients stage.\n\nThe process concludes with the translate_menu node.\n\nHere is the github repo with the instructions to make it yours: \nGitHub Repo\n\nHave to mention the solution to do streaming videos from any devices using streamlit, that I got inspired from \nwhitphx\n.\n\n\nResults\n\nThe result of this project is a functional Streamlit application with several key features designed to minimize food waste. The application provides a user-friendly interface divided into distinct tabs. The \"Register\" section allows users to easily add items to their virtual pantry using either barcode scanning or image recognition.\n\nIn my case in order to use it I put a screen at my doors fridge with four magnets to use it while I load it with food with the barcode reader.\nAlso a raspberry with its own touchable screen could work.\n\nA \"Synchronize State\" tab helps users manage the freshness of their ingredients. By inputting the number of days since purchase, users can update the state of their items, indicating whether they are still good to use or nearing expiration. This information is crucial for the AI agent to make informed decisions about recipe suggestions.\n\n\nThe \"Batch Cooking\" tab generates a cooking plan based on user-defined parameters, helping users prepare larger batches of food while prioritizing the use of older ingredients.\n\n\n\n\n\n\nThe most compelling feature is the \"Daily Dish\" tab. Here, the AI-powered \"Chef Assistant\" truly shines. Users can specify dietary preferences and other constraints, and the system generates a daily dish recommendation based on the oldest ingredients in their pantry. The AI agent not only suggests a recipe but also cleverly adapts existing recipes to incorporate the user's available ingredients, \"recycling\" items that might otherwise go to waste. This feature is a game-changer for reducing food waste and inspiring creative cooking.\n\n\nWhile the current version is functional, there are several avenues for future improvement:\n\nThe YOLO object detection model could be refined by reducing the number of categories, minimizing noise and improving accuracy. 📸\n\nExploring other Large Language Models, such as DeepSeek R1, could potentially enhance the quality and creativity of the recipe suggestions. 💡\n\nThe \"Batch Cooking\" section could benefit from a more sophisticated, agentic workflow similar to the \"Daily Dish\" feature, rather than relying solely on prompts.\n\nThe current method for updating the state of ingredients could be made more sophisticated, perhaps by integrating with smart fridge technology or incorporating more nuanced criteria for food spoilage. ❄️\n\nFinally, the system should be updated to automatically modify the state of ingredients as they are used in recipes, ensuring the inventory remains accurate.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nMethodology\n\nResults\n\nCode\nDatasets\nFiles\nmain.py\ndb_look_up.py\nmulti_agent.py\ndish_with_llm.py\nextract_embeddings.py\nmilvus_search_engine.py\nmilvus_vector_db.py\nfood_retriever_of.py",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "pbsc-ignite-multi-agent-ai-for-personalized-career-readiness-TceP5HgnYrU8",
    "username": "aAbdul Faheem",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nMar 31, 2025\n●\n73 reads\n●\nMIT License\nPBSC-Ignite: Multi-Agent AI for Personalized Career Readiness\nAgenticAI\nAIForCareerDevelopment\nAIForEducation\nAIUpskilling\nCareerReadiness\nGeminiFlash\nGroqAI\nLlama3\nLLMApplications\nMetaAI\nMultiAgentSystems\nSkillDevelopment\nA\nAbdul Faheem\nLike\nBookmark\nShare\n\n1. Introduction\n1.1 The Changing Landscape of Education and Career Preparation\n\nEducation is undergoing a rapid transformation with the rise of digital learning platforms, online courses, and AI-driven tutoring systems. Students today have access to an overwhelming amount of information through YouTube, Coursera, Udemy, and other learning resources.\n\nHowever, despite this technological progress, career guidance and skill development remain outdated and inefficient. Many students:\n\nStruggle to find personalized learning paths that match their strengths and aspirations.\nFace uncertainty about which skills are relevant to their desired careers.\nLack structured mentorship and guidance to track their progress.\nGraduate without industry-relevant experience despite years of education.\n1.2 The One-Size-Fits-All Problem in Career Guidance\n\nTraditional career guidance methods, such as counseling sessions and career fairs, follow a generalized approach rather than catering to individual needs. This is largely because:\n\nOne teacher or mentor is responsible for hundreds of students, making personalized guidance nearly impossible.\nUniversity curricula follow fixed structures, which may not adapt to changing industry demands.\nSelf-learning through online courses or YouTube videos lacks structured feedback and progress tracking.\n\n1.3 The Need for Personalized AI-Driven Learning and Career Guidance\n\nEvery student has a unique set of strengths, interests, and learning pace. Some may excel in research, while others thrive in practical applications. Some may want to explore creative fields like design or music, while others aim for business, healthcare, or technology.\n\nTo address this diversity, PBSC-Ignite introduces an AI-powered personalized learning and career guidance system that adapts to:\n\nEach student's learning speed and preferences.\nCareer aspirations across different fields (engineering, business, arts, medicine, etc.).\nIndustry trends to suggest real-time skill-building pathways.\n1.4 How PBSC-Ignite Works\n\nPBSC-Ignite is an AI-driven career readiness platform designed to guide students through their learning journey, skill development, and career planning.\n\nIt operates using a Multi-Agent AI system that consists of:\n\n1.4.1 Roadmap AI – Personalized Learning Paths\nAssesses each student's academic background, interests, and strengths.\nCreates a customized roadmap based on real-world career pathways.\nAdapts dynamically as the student progresses, struggles, or changes interests.\n1.4.2 AI Mentoring Assistant – Interactive Learning Sessions\nProvides daily learning guidance tailored to the student’s career path.\nUses AI-powered chat sessions to teach concepts and clear doubts in real time.\nAdjusts topics based on the student’s understanding level and performance.\n1.4.3 AI-Driven Career Guidance – Industry-Ready Development\nTracks progress across learning platforms, projects, and coursework.\nAnalyzes GitHub, LinkedIn, portfolios (or relevant industry-specific profiles).\nNotifies students about relevant internships, projects, and professional opportunities.\n1.5 Impact: Bridging the Gap Between Students, Universities, and Industry\n\nPBSC-Ignite doesn’t just help students—it creates an ecosystem where universities, educators, and employers benefit as well:\n\nFor students: Personalized AI mentorship, custom roadmaps, real-time guidance, and career tracking.\nFor universities: Insights on student progress, skill gaps, and career readiness, helping them improve curriculum effectiveness.\nFor companies and hiring managers: Real-time skill assessment, allowing them to find better-prepared candidates based on AI-driven profiles.\n\n1.6 The Future of AI in Career Readiness\n\nPBSC-Ignite redefines career guidance by offering:\n\nA truly personalized learning experience.\nAI-powered mentorship available 24/7.\nIndustry-aligned skill development.\n\nWith AI-driven intelligence, students will no longer struggle to figure out what to learn, when to learn, or how to showcase their skills—PBSC-Ignite does it all for them.\n\n2. System Architecture & Feature Breakdown\n2.1 Overview\n\nPBSC-Ignite is an AI-driven career readiness platform designed to provide students with a personalized, structured learning roadmap while offering interactive AI mentorship, real-time tracking, and career guidance.\n\nUpon joining the platform, students create a profile by providing information about their interests, career goals, and learning preferences. This information is processed by Multi-Agent AI to generate a dynamic learning roadmap. Each day, students engage in an AI-powered learning session, which adapts based on their progress, challenges, and learning behavior. The platform also analyzes and generates personalized progress reports to assist faculty, career development centers (CDCs), and hiring professionals in evaluating students beyond traditional grading systems.\n\n2.2 Core Multi-Agent AI Components\n\nPBSC-Ignite operates through a multi-agent system, where each AI agent has a specialized role in ensuring personalized learning and career guidance. The key AI agents are:\n\n2.2.1 Personality Analyzer & Data Collector\n\n📌 Function: This agent is responsible for understanding the student's interests, strengths, weaknesses, and career aspirations.\n\nUser Profile Creation:\n\nThe student enters details such as interests, skills, career goals, academic background, and preferred learning style.\nAI conducts a welcome session, analyzing responses through NLP techniques to refine insights.\n\nData Extraction from External Platforms:\n\nThe agent integrates data from LinkedIn, GitHub, Kaggle, and other platforms to assess real-world activity and coding projects.\nUses pattern recognition to determine technical expertise and consistency.\n\nA comprehensive learner profile is built, allowing AI to tailor a learning journey specific to the user.\n\n2.2.2 Roadmap Generator AI\n\n📌 Function: This agent creates a structured learning roadmap, divided into phases based on time, complexity, and user skill level.\n\nDynamic Roadmap Structuring:\n\nPhase-based learning plan is created, with each phase having a set of modules and milestones.\nAI assesses availability, depth of topics, and progress speed before finalizing schedules.\n\nAdaptive Learning Scheduling:\n\nUsers can set custom schedules or let AI auto-generate an optimized plan based on their skillset and time availability.\nIf a student struggles with a concept, AI can extend the learning period or suggest alternative resources.\n\nA fully customized, flexible learning roadmap that adapts in real-time to ensure maximum efficiency.\n\n2.2.3 AI Mentor for Daily Learning\n\n📌 Function: This agent acts as a personal AI tutor, providing interactive learning sessions for each topic in the roadmap.\n\nDaily AI-Powered Learning:\n\nThe Start Learning button launches an AI-driven interactive session where the student engages in:\nConcept explanations (text, audio, and visual formats).\nGraphical breakdowns & interactive visualizations.\nPersonalized concept-based discussions & Q&A with AI.\n\nResource Aggregation:\n\nAI fetches high-quality study materials (YouTube videos, documentation, research papers, GitHub repositories) relevant to that topic.\nThe system curates resources based on credibility, recency, and difficulty level.\n\nReal-Time Concept Understanding Evaluation:\n\nAI analyzes student responses, identifies weak areas, and adjusts teaching strategies accordingly.\nIf a student is struggling with a topic, AI communicates with the Roadmap Generator to modify future plans dynamically.\n\nA fully personalized, self-paced AI-driven mentorship system, ensuring deep understanding and engagement.\n\n2.2.4 AI-Powered Student Progress Tracking\n\n📌 Function: AI evaluates the student's learning behavior, concept mastery, and overall progress, generating detailed reports.\n\nDaily & Monthly Learning Reports:\n\nAI generates personalized insights on:\nDepth of learning (how well concepts are grasped).\nProblem-solving capability (based on coding assessments & questions asked).\nConsistency & learning speed (based on streaks, missed sessions).\n\nAI Feedback Loop for Roadmap Adjustments:\n\nIf a student struggles with a concept, AI recommends additional learning sessions before progressing.\nAdjustments to the roadmap are made based on real-time learning trends.\n\nStudents get insightful analytics on their growth, helping them identify strengths & improvement areas.\n\n2.2.5 Career Showcase & Industry Visibility\n\n📌 Function: AI automates career building by guiding users on how to showcase skills & achievements effectively.\n\nSmart Career Guidance:\n\nAI suggests projects & certifications to improve industry relevance.\nMilestone-based LinkedIn sharing prompts, helping students build a strong professional brand.\n\nUniversity & Employer Integration:\n\nUniversities & HR teams can access AI-generated student reports to identify top talent beyond academic scores.\nAI acts as a bridge between academia & industry, aligning student skills with recruiter needs.\n\nUsers can strategically showcase their work, improving placement chances.\n\n2.2.6 The AI Companion - Leo Chatbot\n\n📌 Function: Acts as a 24/7 AI assistant, answering career-related questions, providing study support, and tracking user development.\n\nDeep Personalization:\n\nLeo remembers user progress, challenges, and preferences.\nOffers instant answers on technical & career topics.\n\nSeamless Integration with Other AI Agents:\n\nLeo fetches insights from Roadmap AI, AI Mentor & Progress Tracker.\nProvides dynamic learning recommendations and motivational nudges.\n\nUsers get real-time guidance, making learning smoother and more engaging.\n\n2.3 Full System Architecture\n\nThe PBSC-Ignite platform operates through a sophisticated network of interconnected AI agents, each specializing in different aspects of the learning journey while communicating seamlessly with one another.\n\nThe full system architecture demonstrates how these AI agents interact:\n\nInformation Flow:\n\nThe Personality Analyzer collects and processes student data, creating comprehensive learner profiles\nThese profiles feed into the Roadmap Generator which crafts personalized learning paths\nThe AI Mentor delivers daily learning sessions based on roadmap topics\nProgress Tracking continuously monitors performance across all activities\nThe Leo Chatbot integrates insights from all components to provide 24/7 support\n\nCommunication Channels:\n\nBidirectional data exchange allows real-time adjustments across components\nThe Roadmap Generator receives feedback from both the AI Mentor and Progress Tracking to optimize learning paths\nThe AI Mentor adapts teaching approaches based on Progress Tracking metrics\nLeo provides contextual information to all components while serving as the central interaction point for students\n\nAdaptive Intelligence:\n\nEach component not only fulfills its specialized function but also contributes to the collective intelligence of the system\nPattern recognition across components identifies learning trends and opportunities\nContinuous improvement occurs as the system builds a deeper understanding of each student's learning style and career goals\n\nThe PBSC-Ignite platform establishes a sophisticated AI ecosystem where multiple specialized agents work collaboratively to create an immersive, personalized learning environment. These AI components continuously communicate with each other, sharing insights and adapting their functions based on collective intelligence:\n\nWhen the AI Mentor identifies a conceptual challenge during a learning session, it immediately alerts the Roadmap Generator to adjust future learning modules, while also informing Progress Tracking of skill gaps to monitor.\n\nThe Progress Tracking system analyzes performance patterns and feeds these insights to both the Roadmap Generator for path optimization and to Leo for more effective student guidance.\n\nLeo, as the conversational interface, captures informal learning data that might be missed in structured sessions and distributes these insights across other components, enhancing their understanding of the student's holistic needs.\n\nThe Personality Analyzer continuously refines its understanding of the student by incorporating feedback from all other components, creating an ever-evolving learner profile.\n\nThis interconnected AI ecosystem functions like a dedicated educational team, where each component contributes its specialized expertise while benefiting from the collective intelligence of the whole. The continuous exchange of information creates a learning environment that becomes increasingly personalized over time, anticipating student needs before they arise and adapting to their evolving career goals and learning patterns.\n\n3. PBSC - Ignite MVP\n\nThis is the Minimum Viable Product (MVP) I developed based on my idea, where I successfully implemented key features to demonstrate the core functionality. While this MVP captures the essence of the full solution, additional enhancements are planned to make it even more dynamic and AI-driven.\n\n3.1 Tech Stack\n\nFor this MVP, I utilized the following technologies:\n\nUI Technologies:\nHTML, CSS, JavaScript, Bootstrap – For a responsive and intuitive user interface.\nBackend & Database:\nPython (Flask) – Manages the backend operations and API handling.\nMongoDB – Stores user data, roadmap progress, and interaction history.\nAPIs & Integrations:\nMedium API – Fetches relevant tech blogs.\nYouTube API – Provides video resources related to learning topics.\nLinkedIn API – Analyzes the user's professional profile, skills, and network connections to build a comprehensive understanding of their career trajectory and professional identity.\nGitHub API – Examines the user's repositories, coding history, and project contributions to gain insights into their technical capabilities, preferred technologies, and development patterns.\nAI Technologies:\nGemini Flash 1.5 – For real-time assistance and chat interactions.\nLLaMA (3.3-70b-Versatile) + Groq Cloud – For roadmap generation and dynamic AI-powered guidance.\n3.2 Interactive Landing Page & Core Features Overview\n\nOnce a student or user creates an account, they are welcomed with an interactive landing page. This landing page serves as a dashboard, offering a brief demo of the platform's key features and available resources. Apart from the core AI-powered roadmap and mentorship, the platform also provides several additional functionalities:\n\nTech News & Blogs: Users can stay updated with the latest developments in their field through curated content from sources like Medium and LinkedIn.\nCommunity Support: A dedicated student and alumni community allows previous seniors who have secured jobs to guide and mentor juniors, offering real-world career insights and experiences.\nExternal Resource Integration: Users can access additional materials such as YouTube videos, GitHub repositories, and LinkedIn professional guides based on their interests.\n\n3.3 Profile Creation for AI-Powered Roadmaps\nWhy Profile Creation Matters?\n\nBefore generating an AI-powered learning roadmap, the system must first understand the user holistically. The profile creation process involves:\n\nCollecting user interests, career goals, and academic preferences\nGathering information about dream companies, desired job roles, and industry sectors\nLinking to social profiles like LinkedIn and GitHub for a personalized AI-driven experience\nAnalyzing career aspirations and preferred work environments (startup vs. MNC)\nConducting an AI-driven interactive session to gauge strengths, weaknesses, and desired learning path\n\nOnce this comprehensive data is gathered, the Personal AI Guide processes it and forwards it to the roadmap generator, which continuously fetches relevant information from multiple sources to keep recommendations current and aligned with the user's goals.\n\nThe MVP successfully implements this profile creation system with structured data collection. In future iterations, this will evolve into a fully conversational, chat-oriented interface that mimics human interaction—creating a friendly ecosystem where profile creation feels less like a survey and more like a natural conversation with a career mentor.\n\n3.4 AI-Powered Roadmap Generation\nHow the Roadmap is Built\n\nUtilizing LLaMA (3.3-70b-Versatile) + Groq Cloud, the platform dynamically generates a comprehensive learning roadmap tailored to each user:\n\nCreates a structured journey with distinct phases, each containing a clear timeline and organized content\nImplements a Start Module button for each phase, unlocking daily learning content and resources\nDelivers daily task breakdowns with specific learning objectives, curated YouTube tutorials, blog posts, and reference materials\nFeatures an interactive progress tracking system with checkboxes and progress bars to visualize the learning journey\nIntegrates a notification system that sends alerts for incomplete tasks and learning milestones via email\n\nThe platform maintains a continuous data exchange with the user's connected social profiles:\n\nMonthly refreshes of LinkedIn and GitHub data to stay current with the user's professional development\nPersonalized knowledge database that evolves with the user's progress and social activity\nSmart reminders to update profiles when prolonged inactivity is detected\n\nThe MVP successfully implements the roadmap generation system with timeline phases, daily content delivery, and basic progress tracking. Future iterations will introduce interactive AI sessions for each topic, providing in-depth conceptual guidance, adaptive resource recommendations, and automated skill assessment. These enhancements will also include social knowledge-sharing features, project idea analysis, and open-source contribution guidance based on the user's learning trajectory.\n\n3.5 LEO-AI: 24/7 Real-Time AI Companion\nWhat is the LEO-AI Mentor?\n\nLEO-AI functions as an intelligent personal mentor chatbot with comprehensive knowledge of the user's entire ecosystem:\n\nMaintains a complete understanding of the user's profile data, career aspirations, and learning preferences\nContinuously analyzes the user's GitHub repositories, LinkedIn activities, and platform learning progress\nProcesses performance analytics and learning pattern data from each completed module\n\nUsers can interact with LEO-AI 24/7 for a wide range of personalized support:\n\nSkills gap analysis: \"Where am I currently lacking in my technical skills?\"\nIndustry expectations: \"What are top companies looking for in backend developers?\"\nProject suggestions: \"What project should I build to showcase my React skills?\"\nCareer roadmap guidance: \"How should I prepare for a data science role at Google?\"\nLearning motivation: \"I'm struggling with algorithms, how can I approach this differently?\"\nTechnical interview preparation: \"What kind of questions should I expect for a cloud engineering role?\"\n\nThis holistic knowledge base enables LEO-AI to provide:\n\nDeeply personalized guidance that addresses specific skill gaps and learning challenges\nContext-aware responses that reference the user's previous work, interests, and career goals\nStrategic roadmap adjustments when learning patterns indicate a need to accelerate or provide additional resources\nIndustry-specific insights aligned with the user's target companies and roles\nNetworking recommendations based on the user's professional profile and career trajectory\n\nIn the MVP, LEO-AI successfully delivers personalized mentorship through an interactive chatbot interface with access to user profile data, learning progress, and project suggestions. Future versions will enhance this with real-time analysis of coding patterns, proactive learning suggestions, and integration with technical interviews preparation—making LEO-AI an even more comprehensive career development companion.\n\nThe LEO-AI chatbot interface providing personalized guidance drawing from comprehensive knowledge of the user's profile, learning progress, and career goals.\n\nThis intelligent mentor was developed using Gemini Flash 1.5 to deliver contextual support and career insights based on each user's unique journey.\n\n3.6 PBSC-Ignite MVP: Conclusion & System Architecture\n\nThe PBSC-Ignite MVP represents a significant achievement in personalized tech education, successfully implementing:\n\nA responsive user interface with interactive landing page and comprehensive dashboard\nA sophisticated profile creation system that captures career aspirations and technical interests\nAI-powered learning roadmap generation using LLaMA (3.3-70b-Versatile) + Groq Cloud\nStructured daily learning content with integrated resources from multiple platforms\nProgress tracking functionality that visualizes the user's learning journey\nThe LEO-AI chatbot mentor providing personalized guidance based on user profile data\n\nThis MVP demonstrates the core functionality of an AI-driven personalized learning ecosystem that creates direct connections between academic learning and industry requirements. The platform's intelligent roadmap generation and mentorship capabilities provide measurable value for students navigating their tech career journey.\n\nThe system architecture implements a practical balance of modern web technologies, powerful AI models, and external API integrations to deliver a cohesive experience:\n\nPBSC-Ignite MVP system architecture showing the interconnection between frontend components, Flask backend, MongoDB database, AI engines, and external API integrations.\n\nWhile the current implementation serves as a proof of concept, it establishes a solid foundation for future enhancements that will further automate and personalize the learning experience through advanced AI interactions, deeper social integration, and expanded analytics capabilities.\n\n4. Conclusion: Redefining Career Readiness Through AI\n4.1 Why PBSC-Ignite Matters\n\nThe education landscape faces critical challenges that traditional approaches have failed to address:\n\nThe personalization gap: Educational institutions cannot provide individualized guidance at scale\nThe industry-academia disconnect: Curriculum often lags behind rapidly evolving industry requirements\nThe visibility problem: Students lack clear pathways to demonstrate skills beyond grades and degrees\nThe guidance deficit: Career mentorship remains inaccessible to most students\n\nPBSC-Ignite addresses these fundamental challenges by leveraging AI to create a dynamic learning ecosystem that evolves with each student's unique journey. Unlike conventional platforms, PBSC-Ignite doesn't just provide content—it builds personalized pathways with continuous feedback and adaptation.\n\n4.2 Competitive Differentiation\n\nWhile many educational platforms offer AI components, PBSC-Ignite's comprehensive approach sets it apart:\n\nFeature\tTraditional Platforms\tOther AI Learning Tools\tPBSC-Ignite\nPersonalization\tGeneric paths with minimal customization\tAlgorithm-based content recommendations\tDeep personalization based on holistic user profile, social data integration, and career aspirations\nLearning Format\tVideo courses or text-based modules\tInteractive but standardized exercises\tDynamic roadmaps that adapt to learning pace with AI-guided daily sessions\nCareer Integration\tSeparate from learning experience\tBasic career suggestions\tSeamless connection between learning content and career development with real-time industry alignment\nMentorship\tLimited or human-dependent\tFAQ-style assistance\t24/7 personalized AI mentor with comprehensive knowledge of user's profile, progress, and goals\nProgress Tracking\tSimple completion metrics\tStandard assessment scores\tMulti-dimensional analysis of concept mastery and skill development trajectory\nStakeholder Benefits\tPrimarily student-focused\tStudent and occasional instructor benefits\tEcosystem approach benefiting students, institutions, faculty, and employers\n\nUnlike platforms such as Coursera, LinkedIn Learning, or even traditional LMS systems with AI features, PBSC-Ignite creates a continuous feedback loop between learner progress, career aspirations, and industry requirements—all personalized to each individual's unique circumstances.\n\n4.3 Stakeholder Impact\n\nPBSC-Ignite creates value across the entire educational ecosystem:\n\nFor Students\nEliminates uncertainty about what to learn and when\nProvides 24/7 personalized guidance previously available only to privileged few\nCreates clear connection between daily learning and long-term career success\nBuilds confidence through structured skill development aligned with industry needs\nFor Educational Institutions\nEnhances student outcomes without additional faculty resources\nProvides data-driven insights to inform curriculum development\nStrengthens industry relationships through better-prepared graduates\nImproves graduation rates through targeted intervention opportunities\nFor Employers\nAccesses talent with verified, industry-relevant skills\nGains deeper insights into candidate capabilities beyond resumes\nReduces onboarding costs through better-prepared new hires\nBuilds pipeline relationships with educational institutions\n5. Transforming the Learning Landscape: The Path Forward\n\nThe PBSC-Ignite MVP has successfully established a robust proof of concept with fully operational profile creation, AI-generated roadmaps, basic progress tracking, LEO-AI chatbot, and external resource integration. This demonstrates the viability of this AI-driven personalized learning ecosystem that bridges the gap between education and industry needs.\n\nCurrent development is focused on enhancing the AI Mentor functionality - the final key component that will provide deeply interactive learning sessions for each roadmap topic. This includes real-time concept explanations tailored to learning styles, dynamic difficulty adjustments, intelligent resource curation, and detailed progress analytics to inform roadmap adjustments.\n\nBeyond the Horizon: Future Enhancements\n\nAI That Truly Understands You: Future versions will implement enhanced models with reinforcement learning that evolve with every interaction, creating increasingly personalized guidance.\n\nSeamless Digital Learning Ecosystem: The platform will expand integrations with development environments, assessment platforms, and industry certification systems.\n\nPredictive Career Intelligence: Advanced analytics will forecast industry trends and skill demands, preparing users for opportunities before they emerge.\n\nLearning Communities Without Boundaries: The vision includes peer networks and collaborative challenges that form global learning communities.\n\nMotivation Reimagined: Sophisticated engagement systems with achievement mechanics and personalized motivational strategies will enhance the learning experience.\n\nEducation-to-Employment Pipeline: Direct pathways from learning to employment will be built through deep partnerships with educational institutions and industry leaders.\n\nPBSC-Ignite represents a paradigm shift in education and career development by placing AI-driven personalization at the center of the learning experience. The completed MVP establishes a clear path toward making quality career guidance accessible to all learners rather than a privilege for the few.\n\n6. Live Demo\n\nExperience PBSC-Ignite in Action\n\nThe best way to understand the transformative potential of PBSC-Ignite is to see it working in real-time. Our live demo showcases:\n\nThe intuitive user onboarding and profile creation process\nDynamic roadmap generation based on user inputs\nSample daily learning modules with interactive content\nLEO-AI chatbot interactions demonstrating personalized guidance\nProgress tracking visualizations\n\nThis demonstration highlights how PBSC-Ignite creates an engaging, motivating learning environment that adapts to each user's unique needs while maintaining a clear connection to real-world career outcomes.\n\nHere is the Github Repo of my Project:\n\nGitHub Repo\n\n@PBSC-Ignite: Your Personalized Blueprint for Skill and Career by LEO - AI.\n\nAn AF's Endeavor Developed with 🥤 and 💡\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "pentest-agent-system-W22eIVCo8WYx",
    "username": "Joseph Young",
    "license": "MIT License",
    "title": "Pentest Agent System",
    "publication_description": "Back to publications\nMar 22, 2025\n●\n170 reads\n●\nMIT License\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nPentest Agent System\nagentic\nATT&CK\ncybersecurity\ncybersecurity-automation\nlarge-language-models\nMITRE\nmulti-agent-systems\npenetration-testing\npentesting\npentesting-as-a-service\nSTIX\nvulnerability-assessment\nJoseph Young\nLike\nBookmark\nShare\n\n\nSource: \ngithub.com/youngsecurity/pentest-agent-system\n\nAbstract\n\nThe Pentest Agent System presents a novel approach to automated penetration testing. Its multi-agent architecture leverages large language models (LLMs) and aligns with the MITRE ATT&CK framework. With minimal human intervention, this system demonstrates how autonomous agents can collaborate to perform complex security assessments, from reconnaissance to exploitation and post-exploitation activities.\n\nThe architecture employs four specialized agents: an Orchestrator Agent who coordinates the overall operation flow, a Planner Agent who develops structured attack plans based on MITRE ATT&CK techniques, an Executor Agent who interacts with security tools to implement the attack plan, and an Analyst Agent who processes scan results to identify vulnerabilities and generate detailed attack strategies. Each agent is designed with clear responsibilities, enabling a modular approach to penetration testing that mirrors the tactics and techniques used by real-world threat actors.\n\nThe system integrates with industry-standard security tools, including Nmap for reconnaissance and Metasploit for exploitation, while maintaining a structured data model that captures the entire operation lifecycle. A key innovation is implementing LLM-powered security analysis that can map vulnerabilities to specific MITRE ATT&CK techniques and generate comprehensive attack plans prioritized by severity. The framework includes robust error-handling mechanisms, including step-level retries, fallback commands, and graceful failure recovery.\n\nEvaluation against the \"Blue\" TryHackMe challenge demonstrates the system's effectiveness in identifying and exploiting the MS17-010 (EternalBlue) vulnerability to gain access to Windows systems. The modular design facilitates future enhancements, including expanded technique coverage, additional tool integration, and machine learning components for adaptive attack planning. This research contributes to automated security testing by providing a framework that combines the strategic planning capabilities of LLMs with the tactical execution of established security tools, all within a coherent operational model based on the industry-standard MITRE ATT&CK framework.\n\nIntroduction\n\nCybersecurity threats continue to evolve in sophistication and scale, necessitating advanced defensive strategies and tools. Penetration testing remains a critical component of security assessment, allowing organizations to identify and remediate vulnerabilities before malicious actors can exploit them. However, traditional penetration testing approaches often suffer from inconsistency, resource constraints, and limited scalability.\n\nThe Pentest Agent System addresses these challenges by introducing an autonomous, multi-agent framework that leverages large language models (LLMs) and aligns with the industry-standard MITRE ATT&CK framework. This system represents a significant advancement in automated security assessment by combining the strategic planning capabilities of LLMs with the tactical execution of established security tools.\n\nAt its core, the Pentest Agent System employs a modular architecture with specialized agents collaborating to perform complex security assessments. The Orchestrator Agent coordinates the overall operation flow, the Planner Agent develops attack strategies based on MITRE ATT&CK techniques, the Executor Agent interacts with security tools to implement these plans, and the Analyst Agent processes scan results to identify vulnerabilities and generate detailed attack strategies.\n\nThis paper presents the Pentest Agent System's design, implementation, and evaluation, demonstrating its effectiveness in identifying and exploiting vulnerabilities in controlled environments. This system offers a promising approach to enhancing organizational security posture in an increasingly complex threat landscape by automating the penetration testing process while maintaining alignment with established security frameworks.\n\nRelated work\nAutomated Penetration Testing Frameworks\n\nThe development of automated penetration testing tools has advanced significantly in recent years. Frameworks such as Metasploit [1] provide extensive exploitation capabilities but typically require human guidance for strategic decision-making. More automated solutions, like OWASP ZAP [2] and Burp Suite [3], focus primarily on web application security testing and have limited scope for comprehensive network penetration testing.\n\nCommercial offerings such as Core Impact [4] and Rapid7's InsightVM [5] provide more comprehensive automation but often lack transparency in their methodologies and remain costly for many organizations. Open-source alternatives like AutoSploit [6] combine reconnaissance tools with exploitation frameworks but raise ethical concerns due to their potential misuse and lack of control mechanisms.\n\nMulti-Agent Systems in Cybersecurity\n\nMulti-agent systems have been explored in various cybersecurity contexts. Notably, Moskal et al. [7] proposed a multi-agent framework for network defense, while Jajodia et al. [8] introduced the concept of adaptive cyber defense through collaborative agents. These approaches demonstrate the potential of agent-based architectures but typically focus on defensive rather than offensive security operations.\n\nIn the penetration testing domain, Sarraute et al. [9] proposed an attack planning system using partially observable Markov decision processes, while Obes et al. [10] developed an automated attack planner using classical planning techniques. These systems, however, lack integration with modern threat intelligence frameworks and do not leverage recent advancements in language models.\n\nLLMs in Security Applications\n\nThe application of large language models to cybersecurity is an emerging field. Recent work by Shu et al. [11] demonstrated the potential of LLMs for vulnerability detection in code, while Fang et al. [12] explored their use in generating security reports. However, integrating LLMs into operational security tools, particularly for offensive security, remains largely unexplored.\n\nThe MITRE ATT&CK framework [13] has become the de facto standard for categorizing adversary tactics and techniques, but its integration into automated penetration testing systems has been limited. Existing approaches like Caldera [14] provide MITRE ATT&CK-aligned testing but lack the adaptive planning capabilities offered by LLMs.\n\nThe Pentest Agent System builds upon these foundations while addressing key limitations by combining multi-agent architecture, LLM-powered planning, and MITRE ATT&CK alignment into a cohesive, operational framework for automated penetration testing.\n\nMethodology\nSystem Architecture\n\nThe Pentest Agent System follows a multi-agent architecture with four specialized agents, each with distinct responsibilities:\n\nOrchestrator Agent: As the central coordinator, managing the overall operation flow, tracking progress, and handling system-level events. Implemented as the PentestOrchestratorAgent class, it maintains the operation state and coordinates communication between other agents.\n\nPlanner Agent: Responsible for generating structured attack plans based on the MITRE ATT&CK framework. The MitrePlannerAgent class creates AttackPlan objects containing ordered steps, each mapping to specific MITRE ATT&CK techniques with defined dependencies and validation criteria.\n\nExecutor Agent: This agent executes the attack plan by interacting with external security tools. The ExploitExecutorAgent class manages tool interactions, handles command execution, implements error recovery mechanisms, and collects execution artifacts.\n\nAnalyst Agent: Processes scan results to identify vulnerabilities, maps them to MITRE ATT&CK techniques, and generates detailed attack strategies. Implemented using LLM-powered analysis through the security_analyst_agent module.\n\nData Models\n\nThe system employs structured data models to represent its operational state:\n\nAttack Plans: Represented by the AttackPlan class, containing target information, objectives, ordered steps, dependencies, and metadata.\n\nMITRE ATT&CK Models: The MitreAttackTechnique class maps techniques from the MITRE ATT&CK framework, including technique ID, name, description, tactic category, implementation function, requirements, and detection difficulty.\n\nResults Models: The OperationResult class stores operation outcomes, including scan results, exploit results, post-exploitation findings, captured flags, and summary statistics.\n\nImplementation Details\n\nThe system is implemented using a hybrid approach:\n\nCore Framework: Developed in TypeScript using Deno for the Orchestrator, Planner, and Executor agents, providing type safety and modern JavaScript features.\n\nAnalysis Components: Implemented in Python using LangChain for the Analyst agent, leveraging state-of-the-art language models from OpenAI (GPT-4o mini) and Anthropic (Claude).\n\nTool Integration: The system integrates with industry-standard security tools:\n\nNmap for reconnaissance and vulnerability scanning\nMetasploit Framework for exploitation and post-exploitation\nCustom wrappers provide standardized interfaces to these tools\n\nError Handling: Robust error handling mechanisms include step-level retries, fallback commands, non-critical step failure handling, configurable timeouts, exception catching, and graceful abortion capabilities.\n\nExecution Flow\n\nThe system follows a structured execution flow:\n\nInitialization: Configuration loading, logging setup, and agent initialization.\n\nPlanning Phase: The Orchestrator requests a plan from the Planner, which generates steps based on MITRE techniques.\n\nExecution Phase: The Orchestrator passes the plan to the Executor, which performs reconnaissance, exploitation, and post-exploitation actions.\n\nAnalysis Phase: The Analyst processes scan results, identifies vulnerabilities, and generates attack strategies.\n\nResult Collection: Operation results are compiled, flags are verified, and a summary is generated.\n\nDataset Description\n\nThe Pentest Agent System utilizes several data sources within the penetration testing workflow for different purposes. This section provides a detailed description of each dataset, including its current implementation status, characteristics, and rationale for selection.\n\n1. Nmap Vulnerability Scan Dataset\n\nCurrent Implementation Status: The implementation includes a single sample nmap scan result file (python/sample_nmap_output.txt) used for testing and demonstration purposes. This represents an initial proof-of-concept rather than a comprehensive dataset.\n\nDescription: This sample data contains structured output from a Nmap vulnerability scan, showing detailed information about network services, open ports, and detected vulnerabilities for a Windows system.\n\nStructure: The sample scan result follows the standard Nmap output format that includes:\n\nTarget information (IP address: 192.168.1.100)\nPort information (5 open ports: 135, 139, 445, 3389, 8080)\nService identification (Windows RPC, NetBIOS, SMB, Terminal Services, Tomcat)\nVulnerability detections (MS17-010/EternalBlue and Slowloris DOS vulnerabilities)\n\nData Quality: The sample was created to represent a realistic scan of a vulnerable Windows system, focusing on the MS17-010 vulnerability central to the system's testing scenario.\n\nRationale for Selection: This sample was created to demonstrate the system's ability to parse and analyze Nmap scan results. It mainly focuses on the MS17-010 vulnerability, which is targeted in the TryHackMe \"Blue\" challenge.\n\nFuture Work: For production use, this single sample should be expanded into a comprehensive dataset containing:\n\nMultiple scan results from diverse network environments\nVarious vulnerability types beyond MS17-010\nDifferent operating systems and service configurations\nMetadata about scan parameters and environment context\n\nSample Entry (excerpt from the actual file):\n\nNMAP SCAN RESULTS (vuln scan of 192.168.1.100):\nPORT     STATE SERVICE       VERSION\n135/tcp  open  msrpc         Microsoft Windows RPC\n139/tcp  open  netbios-ssn   Microsoft Windows netbios-ssn\n445/tcp  open  microsoft-ds  Microsoft Windows 7 - 10 microsoft-ds\n...\nHost script results:\n| smb-vuln-ms17-010: \n|   VULNERABLE:\n|   Remote Code Execution vulnerability in Microsoft SMBv1 servers (ms17-010)\n|     State: VULNERABLE\n|     IDs:  CVE:CVE-2017-0143\n|     Risk factor: HIGH\n2. MITRE ATT&CK Framework Dataset\n\nCurrent Implementation Status: The system references the MITRE ATT&CK framework but does not maintain a local copy of this dataset. Instead, it uses a limited set of hardcoded technique mappings relevant to the MS17-010 exploitation scenario.\n\nDescription: The MITRE ATT&CK framework is an externally maintained knowledge base of adversary tactics, techniques, and procedures based on real-world observations.\n\nSize and Scope: While the complete MITRE ATT&CK framework includes 14 tactics and hundreds of techniques, the current implementation focuses on a subset of 8 techniques directly relevant to the MS17-010 exploitation scenario:\n\nT1046: Network Service Scanning\nT1190: Exploit Public-Facing Application\nT1059: Command and Scripting Interpreter\nT1068: Exploitation for Privilege Escalation\nT1070: Indicator Removal on Host\nT1003: OS Credential Dumping\nT1083: File and Directory Discovery\nT1005: Data from Local System\n\nStructure: Each technique reference in the code includes:\n\nTechnique ID (e.g., T1190)\nName (e.g., \"Exploit Public-Facing Application\")\nBasic description\nTactic category (e.g., \"Initial Access\")\n\nRationale for Selection: The MITRE ATT&CK framework was selected as it represents the industry standard for categorizing adversary behaviors. The specific techniques were chosen based on their relevance to the MS17-010 exploitation scenario.\n\nFuture Work: Future versions should implement a more comprehensive integration with the MITRE ATT&CK framework, including:\n\nA local database of all relevant techniques\nStructured mapping between techniques and implementation code\nMetadata about detection difficulty and mitigation strategies\nRegular updates to stay current with the official MITRE ATT&CK releases\n3. SQLite Database for Results Storage\n\nCurrent Implementation Status: The system uses a SQLite database (storage/data.db) to store scan results, vulnerabilities, MITRE mappings, exploit paths, and attack plans. This database is created and populated during system operation rather than being a pre-existing dataset.\n\nDescription: This database serves as both a storage mechanism for operational data and a dataset that grows over time as the system is used.\n\nStructure: As defined in python/modules/db_manager.py, the database contains five tables:\n\nscan_results: Stores nmap scan results with target information\nvulnerabilities: Records detected vulnerabilities linked to scan results\nmitre_mappings: Maps vulnerabilities to MITRE ATT&CK techniques\nexploit_paths: Stores Metasploit exploit paths for vulnerabilities\nattack_plans: Contains generated attack plans with tasks and summaries\n\nData Quality: The database is populated programmatically during system operation, ensuring structural consistency. However, the quality of the stored data depends on the accuracy of the input scan results and the effectiveness of the analysis algorithms.\n\nRationale for Selection: SQLite was chosen for its simplicity, portability, and self-contained nature, making it ideal for storing structured data in a local application without requiring a separate database server.\n\nFuture Work: Future improvements should include:\n\nData validation and integrity checks\nExport/import functionality for sharing datasets\nAnalytics capabilities for trend analysis\nIntegration with external vulnerability databases\n4. TryHackMe \"Blue\" Challenge Environment\n\nCurrent Implementation Status: The system is designed to work with the TryHackMe \"Blue\" challenge. However, this environment is externally hosted and accessed through TryHackMe's platform rather than being included in the project repository.\n\nDescription: The \"Blue\" challenge provides a vulnerable Windows 7 virtual machine specifically designed for practicing exploitation of the MS17-010 vulnerability.\n\nCharacteristics: The environment includes:\n\nA Windows 7 virtual machine with SMBv1 enabled\nThe MS17-010 vulnerability (unpatched)\nMultiple hidden flags to discover\nNetwork access through TryHackMe's VPN\n\nRationale for Selection: This environment was selected because it provides a controlled, legal setting for testing exploitation techniques against the MS17-010 vulnerability, a well-known and widely studied security issue.\n\nAccess Method: Users must register with TryHackMe and deploy the \"Blue\" room to access this environment. The system connects to it through the user's VPN connection.\n\nFuture Work: Future versions could include:\n\nA local, containerized version of a similar vulnerable environment for offline testing\nSupport for additional vulnerable environments beyond the \"Blue\" challenge\nAutomated deployment and reset capabilities for testing environments\n5. Metasploit Framework Integration\n\nCurrent Implementation Status: The system integrates with the Metasploit Framework as an external tool rather than maintaining a local copy of its exploit database.\n\nDescription: The Metasploit Framework provides a collection of exploit modules, payloads, and post-exploitation tools that the system leverages for the execution phase of penetration testing.\n\nIntegration Method: The system interacts with Metasploit through command-line interfaces, as implemented in python/modules/metasploit_agent.py. It primarily uses the MS17-010 EternalBlue exploit module for the \"Blue\" challenge scenario.\n\nRationale for Selection: Metasploit was selected due to its comprehensive collection of well-documented and tested exploitation modules, particularly its reliable implementation of the MS17-010 exploit.\n\nFuture Work: Future improvements should include:\n\nMore robust integration with the Metasploit RPC API\nLocal caching of module metadata for faster operation\nSupport for a broader range of Metasploit modules beyond MS17-010\nAutomated selection of appropriate modules based on detected vulnerabilities\nDataset Limitations and Future Improvements\n\nThe current implementation has several limitations in its dataset usage:\n\nLimited Sample Data: The system currently relies on a single sample nmap scan result rather than a comprehensive dataset of diverse scan results.\n\nHardcoded MITRE ATT&CK References: Rather than maintaining a complete local copy of the MITRE ATT&CK framework, the system uses hardcoded references to a small subset of techniques.\n\nExternal Dependencies: The system depends on external resources (TryHackMe, Metasploit) rather than including self-contained datasets.\n\nLack of Dataset Documentation: The current implementation does not include comprehensive documentation of dataset characteristics, preprocessing methods, or validation procedures.\n\nFuture work will address these limitations and focus on:\n\nCreating Comprehensive Datasets: Developing a diverse collection of nmap scan results representing various network environments and vulnerability profiles.\n\nImplementing Local MITRE ATT&CK Database: Creating a structured local database of MITRE ATT&CK techniques with detailed mappings to implementation code.\n\nDeveloping Self-Contained Testing Environments: Creating containerized vulnerable environments for testing without external dependencies.\n\nImproving Dataset Documentation: Adding detailed documentation of all datasets, including their characteristics, preprocessing methods, and validation procedures.\n\nImplementing Dataset Versioning: Adding version control for datasets to ensure reproducibility of results across different system versions.\n\nDataset Access and Availability\n\nThe current datasets are available as follows:\n\nSample Nmap Scan Result: Available in the project repository at python/sample_nmap_output.txt.\n\nMITRE ATT&CK Framework: Publicly available through the MITRE ATT&CK website (\nhttps://attack.mitre.org/\n). The system uses a subset of techniques from this framework.\n\nSQLite Database: Created locally at storage/data.db during system operation. This file is initially empty and populated as the system is used.\n\nTryHackMe \"Blue\" Challenge: Available to registered users on the TryHackMe platform (\nhttps://tryhackme.com/room/blue\n).\n\nMetasploit Framework: Available through the official Metasploit installation. The system interacts with this framework but does not include it in the repository.\n\nDataset Processing Methodology\n\nThe Pentest Agent System employs a multi-stage data processing pipeline to transform raw network scan data into structured vulnerability assessments and attack plans. This section details the methodologies used for data preprocessing, transformation, normalization, and feature extraction throughout the system's operation.\n\n1. Nmap Scan Data Acquisition and Preprocessing\n1.1 Data Acquisition Methods\n\nThe system acquires network scan data through two primary methods:\n\nDirect Nmap Execution: The system can execute Nmap scans directly using the run_nmap_scan function, which supports multiple scan types:\n\nBasic scan (-sT -sV -T4): TCP connect scan with version detection\nQuick scan (-sT -T4 -F): Fast TCP scan of common ports\nFull scan (-sT -sV -sC -p-): Comprehensive TCP scan with scripts\nVulnerability scan (-sT -sV --script vuln): Focused on vulnerability detection\nUDP ports scan (-sT -p 53,67,68,69,123,161,162,1900,5353): Targeting common UDP service ports\nVersion scan (--version): For obtaining Nmap version information\n\nPreexisting Scan Data Import: The system can process previously generated Nmap scan results provided as input.\n\n1.2 Dual-Format Data Collection\n\nTo maximize data quality and processing options, the system collects scan results in two complementary formats:\n\nText Output: Human-readable format that preserves the original Nmap output structure\nXML Output: Machine-readable format that enables structured data processing\n\nThe system converts the XML output to JSON using the xmltodict library, providing a more convenient format for programmatic processing while preserving the hierarchical structure of the data.\n\n1.3 Initial Data Preprocessing\n\nBefore detailed analysis, the raw scan data undergoes several preprocessing steps:\n\nFormat Standardization: The system standardizes the scan output format by adding consistent headers and section delimiters:\n\nNMAP SCAN RESULTS ({scan_type} scan of {target}):\n==================================================\n[Raw Nmap Output]\n==================================================\n\nData Type Handling: The system implements format detection to determine whether the input is:\n\nRaw text (string)\nJSON object (dictionary)\nEmpty or invalid input\n\nError Handling: The system implements robust error handling for various failure scenarios:\n\nTimeout errors (scans exceeding 5 minutes)\nCommand execution errors\nXML parsing errors\nEmpty or invalid inputs\n\nMetadata Extraction: Basic metadata is extracted from the scan results:\n\nTarget information (IP address or hostname)\nScan type (basic, quick, full, vuln, etc.)\nTimestamp information\n2. Vulnerability Data Extraction and Normalization\n2.1 Text-Based Vulnerability Extraction\n\nFor text-based Nmap output, the system employs regular expression pattern matching to extract vulnerability information:\n\nTarget and Scan Type Extraction:\n\ntarget_match = re.search(r\"NMAP SCAN RESULTS \\((\\w+) scan of ([^\\)]+)\\)\", nmap_output)\nif target_match:\n    scan_type = target_match.group(1)\n    target = target_match.group(2)\nelse:\n    scan_type = \"unknown\"\n    target = \"unknown\"\n\nVulnerability Section Identification:\n\nvuln_sections = re.finditer(r\"\\|\\s+([^:]+):\\s+\\n\\|\\s+VULNERABLE:\\s*\\n\\|\\s+([^\\n]+)\\n(?:\\|\\s+[^\\n]*\\n)*?\\|\\s+State: VULNERABLE\", nmap_output)\n\nSection Boundary Detection:\n\nsection_start = section.start()\nsection_end = nmap_output.find(\"\\n|\", section.end())\nif section_end == -1:  # If no more sections, go to the end\n    section_end = len(nmap_output)\nfull_section = nmap_output[section_start:section_end]\n\nVulnerability Information Extraction:\n\nvuln_name = section.group(1).strip()\nvuln_description = section.group(2).strip()\n2.2 JSON-Based Vulnerability Extraction\n\nFor JSON-formatted Nmap output, the system traverses the hierarchical structure to extract vulnerability information:\n\nJSON Structure Traversal:\n\nif \"nmaprun\" in json_data and \"host\" in json_data[\"nmaprun\"]:\n    host_data = json_data[\"nmaprun\"][\"host\"]\n    # Process host data\n\nData Structure Normalization: The system handles various structural variations:\n\nSingle host vs. list of hosts\nSingle port vs. list of ports\nSingle script vs. list of scripts\n\nVulnerability Detection Logic:\n\nif \"VULNERABLE\" in output:\n    # Extract and process vulnerability information\n\nDescription Extraction:\n\ndescription = output.split(\"\\n\")[0] if \"\\n\" in output else output\n2.3 Missing Data Handling\n\nThe system implements several strategies for handling missing or incomplete data:\n\nDefault Value Assignment: When metadata cannot be extracted, default values are assigned:\n\nscan_type = nmap_data.get(\"scan_type\", \"unknown\")\ntarget = nmap_data.get(\"target\", \"unknown\")\n\nGraceful Degradation: When JSON parsing fails, the system falls back to text-based processing:\n\nexcept Exception as e:\n    logger.error(f\"Error parsing JSON nmap data: {e}\")\n    # Fall back to regex parsing if JSON parsing fails\n    return parse_nmap_results_text(raw_results)\n\nEmpty Input Detection: The system explicitly checks for empty or invalid inputs:\n\nif nmap_data is None or (isinstance(nmap_data, dict) and not nmap_data) or (isinstance(nmap_data, str) and not nmap_data.strip()):\n    # Handle empty input case\n\nError Reporting: When processing fails, structured error information is returned:\n\nreturn {\n    \"error\": \"No nmap scan data provided. Please run an nmap scan first using the run_nmap_scan tool, then pass the results to this tool.\",\n    \"example_usage\": \"First run: run_nmap_scan('192.168.1.1'), then pass the results to security_analyst\",\n    # Additional error information\n}\n3. MITRE ATT&CK Framework Mapping\n3.1 Vulnerability-to-MITRE Mapping Process\n\nThe system employs a dynamic mapping process to associate identified vulnerabilities with MITRE ATT&CK techniques:\n\nSearch Query Construction:\n\nsearch_query = f\"{vulnerability['name']} {vulnerability['description']} MITRE ATT&CK\"\n\nExternal Knowledge Integration: The system leverages the DuckDuckGo search API to retrieve relevant MITRE ATT&CK information:\n\nsearch_results = search.run(search_query)\n\nTechnique ID Extraction: Regular expression pattern matching is used to extract MITRE technique IDs:\n\ntechnique_id_match = re.search(r\"(T\\d{4}(?:\\.\\d{3})?)\", search_results)\ntechnique_id = technique_id_match.group(1) if technique_id_match else \"Unknown\"\n\nTactic Classification: The system classifies vulnerabilities into MITRE tactics based on keyword matching:\n\nif \"Initial Access\" in search_results:\n    tactic = \"Initial Access\"\nelif \"Execution\" in search_results:\n    tactic = \"Execution\"\n# Additional tactic classifications\nelse:\n    tactic = \"Unknown\"\n\nDescription Truncation: Long descriptions are truncated to maintain readability:\n\ndescription = search_results[:500] + \"...\" if len(search_results) > 500 else search_results\n3.2 Handling Mapping Failures\n\nThe system implements robust error handling for cases where MITRE ATT&CK mapping fails:\n\nException Handling:\n\nexcept Exception as e:\n    logger.error(f\"Error searching MITRE ATT&CK: {e}\")\n    return {\n        \"technique_id\": \"Unknown\",\n        \"technique_name\": \"Unknown\",\n        \"tactic\": \"Unknown\",\n        \"description\": f\"Error searching MITRE ATT&CK: {e}\",\n        \"mitigation\": \"Unknown\"\n    }\n\nDefault Value Assignment: When mapping information cannot be found, default values are assigned to ensure data structure consistency.\n\nLogging: All mapping failures are logged for later analysis and improvement.\n\n4. Attack Plan Generation\n4.1 Data Transformation for Attack Planning\n\nThe system transforms vulnerability and MITRE ATT&CK mapping data into structured attack plans:\n\nConditional Processing: Different processing paths are taken based on the presence of vulnerabilities:\n\nif not scan_results[\"vulnerabilities\"]:\n    # Generate security assessment for non-vulnerable systems\nelse:\n    # Generate attack plan for vulnerable systems\n\nMetadata Integration: System metadata is integrated into the attack plan:\n\ndetailed_plan = f\"\"\"\n## Attack Plan\n\nTarget: {scan_results['target']}\nScan Type: {scan_results['scan_type']}\nVulnerabilities Found: {vuln_count}\n\"\"\"\n\nVulnerability Prioritization: Vulnerabilities are implicitly prioritized by their order in the scan results.\n\nCommand Generation: The system generates Metasploit commands based on vulnerability information:\n\ncommands = [\n    \"msfconsole\",\n    f\"search {vuln['name']}\",\n    \"use [exploit_path]\",\n    f\"set RHOSTS {scan_results['target']}\",\n    \"check\",\n    \"# Do not execute: run\"\n]\n4.2 Data Structuring for Output\n\nThe final attack plan is structured into three main components:\n\nExecutive Summary: A concise overview of the findings:\n\nexecutive_summary = f\"Found {vuln_count} vulnerabilities in {scan_results['target']}. These vulnerabilities could potentially be exploited using Metasploit framework.\"\n\nDetailed Plan: A comprehensive description of the vulnerabilities and attack approach:\n\ndetailed_plan += f\"\"\"\n#### {i+1}. {vuln['name']}\n\nDescription: {vuln['description']}\nMITRE ATT&CK: {mitre_info['technique_id']} - {mitre_info['technique_name']} ({mitre_info['tactic']})\n\"\"\"\n\nTask List: A structured list of actionable steps:\n\ntask_list.append({\n    \"id\": i+1,\n    \"name\": f\"Exploit {vuln['name']}\",\n    \"description\": vuln['description'],\n    \"vulnerability_id\": vuln['id'],\n    \"mitre_technique\": mitre_info['technique_id'],\n    \"mitre_tactic\": mitre_info['tactic'],\n    \"commands\": commands\n})\n5. Data Persistence and Storage\n5.1 Database Schema Design\n\nThe system employs a SQLite database with a normalized schema for efficient data storage:\n\nScan Results Table: Stores basic scan information:\n\nCREATE TABLE IF NOT EXISTS scan_results (\n    id INTEGER PRIMARY KEY,\n    target TEXT NOT NULL,\n    scan_type TEXT NOT NULL,\n    raw_results TEXT NOT NULL,\n    scan_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n\nVulnerabilities Table: Stores vulnerability information with foreign key relationships:\n\nCREATE TABLE IF NOT EXISTS vulnerabilities (\n    id INTEGER PRIMARY KEY,\n    scan_id INTEGER NOT NULL,\n    vulnerability_name TEXT NOT NULL,\n    vulnerability_description TEXT,\n    state TEXT NOT NULL,\n    raw_data TEXT NOT NULL,\n    detection_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (scan_id) REFERENCES scan_results (id)\n)\n\nMITRE Mappings Table: Stores MITRE ATT&CK mapping information:\n\nCREATE TABLE IF NOT EXISTS mitre_mappings (\n    id INTEGER PRIMARY KEY,\n    vulnerability_id INTEGER NOT NULL,\n    technique_id TEXT NOT NULL,\n    technique_name TEXT NOT NULL,\n    tactic TEXT NOT NULL,\n    description TEXT,\n    mitigation TEXT,\n    FOREIGN KEY (vulnerability_id) REFERENCES vulnerabilities (id)\n)\n\nExploit Paths Table: Stores Metasploit exploit information:\n\nCREATE TABLE IF NOT EXISTS exploit_paths (\n    id INTEGER PRIMARY KEY,\n    vulnerability_id INTEGER NOT NULL,\n    exploit_path TEXT NOT NULL,\n    exploit_description TEXT,\n    search_query TEXT NOT NULL,\n    discovery_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (vulnerability_id) REFERENCES vulnerabilities (id)\n)\n\nAttack Plans Table: Stores generated attack plans:\n\nCREATE TABLE IF NOT EXISTS attack_plans (\n    id INTEGER PRIMARY KEY,\n    scan_id INTEGER NOT NULL,\n    executive_summary TEXT NOT NULL,\n    detailed_plan TEXT NOT NULL,\n    task_list TEXT NOT NULL, -- JSON string\n    creation_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (scan_id) REFERENCES scan_results (id)\n)\n5.2 Data Serialization\n\nThe system employs several data serialization techniques:\n\nJSON Serialization: Complex data structures like task lists are serialized to JSON for storage:\n\ntask_list_json = json.dumps(task_list)\n\nRaw Data Preservation: Original scan outputs are preserved in their raw form to enable reprocessing if needed.\n\nTimestamp Generation: All database records include automatically generated timestamps for temporal tracking.\n\n6. Data Processing Validation and Quality Assurance\n6.1 Input Validation\n\nThe system implements several validation mechanisms to ensure data quality:\n\nType Checking: Input data types are explicitly checked:\n\nif isinstance(nmap_data, dict) and nmap_data.get(\"format\") == \"json\":\n    # Process JSON format\nelse:\n    # Process text format\n\nEmpty Input Detection: The system explicitly checks for empty or invalid inputs:\n\nif nmap_data is None or (isinstance(nmap_data, dict) and not nmap_data) or (isinstance(nmap_data, str) and not nmap_data.strip()):\n    # Handle empty input case\n\nStructured Input Schema: The system uses Pydantic models to validate input structure:\n\nclass NmapResultsInput(BaseModel):\n    nmap_data: Union[Dict, str] = Field(\n        ...,\n        description=\"Nmap scan results, either as a JSON object or raw text output from a previous nmap scan\"\n    )\n6.2 Error Handling and Logging\n\nThe system implements comprehensive error handling and logging:\n\nException Catching: All external operations are wrapped in try-except blocks:\n\ntry:\n    # Operation that might fail\nexcept Exception as e:\n    logger.error(f\"Error message: {e}\")\n    # Handle error case\n\nStructured Error Responses: When errors occur, structured error information is returned:\n\nreturn {\n    \"error\": f\"Error message: {str(e)}\",\n    \"format\": \"error\",\n    \"text_output\": f\"Error message: {str(e)}\"\n}\n\nLogging: All significant operations and errors are logged:\n\nlogger.info(\"Analyzing nmap scan results\")\nlogger.error(f\"Error searching MITRE ATT&CK: {e}\")\n7. Data Processing Pipeline Integration\n\nThe complete data processing pipeline integrates all the above components into a cohesive workflow:\n\nData Acquisition: Nmap scan data is acquired through direct execution or import.\nFormat Detection and Preprocessing: The data format is detected and preprocessed accordingly.\nVulnerability Extraction: Vulnerability information is extracted using format-specific methods.\nMITRE ATT&CK Mapping: Vulnerabilities are mapped to MITRE ATT&CK techniques.\nAttack Plan Generation: Structured attack plans are generated based on the processed data.\nData Persistence: All processed data is stored in the SQLite database.\nResult Presentation: The final results are presented to the user in a structured format.\n\nThis comprehensive data processing methodology ensures that raw network scan data is transformed into actionable security intelligence through a series of well-defined preprocessing, transformation, normalization, and feature extraction steps. The system's robust error handling and data validation mechanisms ensure high data quality throughout the processing pipeline.\n\nMonitoring and Maintenance Considerations\n\nThe Pentest Agent System, like any complex software system, requires ongoing monitoring and maintenance to ensure optimal performance, reliability, and security. This section outlines key considerations for operational deployment and long-term maintenance of the system.\n\n1. System Monitoring Framework\n1.1 Key Performance Metrics\n\nThe following metrics should be monitored to ensure optimal system performance:\n\nScan Execution Time: Monitor the duration of Nmap scans to identify performance degradation or network issues.\n\nBaseline: Basic scan should complete in 30-60 seconds for a single host\nWarning threshold: >2x baseline duration\nCritical threshold: >5x baseline duration\n\nLLM API Response Time: Track response times from OpenAI and Anthropic APIs.\n\nBaseline: 1-3 seconds for typical requests\nWarning threshold: >5 seconds\nCritical threshold: >10 seconds\n\nDatabase Query Performance: Monitor query execution times, particularly for vulnerability retrieval operations.\n\nBaseline: <100ms for typical queries\nWarning threshold: >500ms\nCritical threshold: >1000ms\n\nMemory Usage: Track memory consumption, especially during analysis of large scan results.\n\nBaseline: <500MB during normal operation\nWarning threshold: >1GB\nCritical threshold: >2GB or continuous growth pattern\n\nAPI Rate Limiting: Monitor API usage to prevent exceeding rate limits for external services.\n\nOpenAI: Track tokens per minute against account limits\nAnthropic: Track requests per minute against account limits\nDuckDuckGo: Monitor search requests to avoid temporary IP blocks\n1.2 Monitoring Implementation\n\nTo implement effective monitoring, we recommend:\n\nPrometheus Integration: Expose key metrics via a Prometheus endpoint for time-series data collection.\n\nfrom prometheus_client import Counter, Histogram, start_http_server\n\n# Example metric definitions\nSCAN_DURATION = Histogram('nmap_scan_duration_seconds', 'Duration of Nmap scans', ['scan_type'])\nLLM_REQUEST_DURATION = Histogram('llm_request_duration_seconds', 'Duration of LLM API requests', ['model'])\nVULNERABILITY_COUNT = Counter('vulnerabilities_detected_total', 'Total number of vulnerabilities detected')\n\n# Start metrics server\nstart_http_server(8000)\n\nGrafana Dashboards: Create dedicated dashboards for visualizing system performance metrics.\n\nScan Performance Dashboard: Track scan durations, success rates, and vulnerability counts\nAPI Performance Dashboard: Monitor LLM API response times and rate limit usage\nSystem Resource Dashboard: Track CPU, memory, and disk usage\n\nAlerting Rules: Implement alerting based on threshold violations.\n\n# Example Prometheus alerting rule\ngroups:\n- name: pentest-agent-alerts\n  rules:\n  - alert: LongRunningScan\n    expr: nmap_scan_duration_seconds > 300\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Long-running Nmap scan detected\"\n      description: \"Scan has been running for more than 5 minutes\"\n2. Logging Strategy\n2.1 Logging Levels and Categories\n\nThe system implements a hierarchical logging strategy with the following levels:\n\nDEBUG: Detailed information for debugging purposes.\n\nAPI request/response payloads (with sensitive data redacted)\nDetailed parsing steps\nIntermediate data transformations\n\nINFO: General operational information.\n\nScan start/completion events\nVulnerability detection events\nDatabase operations\n\nWARNING: Potential issues that don't affect core functionality.\n\nSlow API responses\nMinor parsing issues\nNon-critical timeouts\n\nERROR: Significant issues affecting functionality.\n\nAPI failures\nDatabase connection issues\nTool execution failures\n\nCRITICAL: Severe issues requiring immediate attention.\n\nDatabase corruption\nAuthentication failures\nSystem resource exhaustion\n2.2 Log Storage and Rotation\n\nImplement the following log management practices:\n\nFile-based Logging: Store logs in the logs/ directory with date-based filenames.\n\nimport logging\nfrom logging.handlers import RotatingFileHandler\n\n# Configure file handler with rotation\nlog_handler = RotatingFileHandler(\n    'logs/pentest_agent.log',\n    maxBytes=10485760,  # 10MB\n    backupCount=10\n)\n\n# Set formatter\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(formatter)\n\n# Add handler to logger\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\n\nLog Rotation: Implement size-based log rotation to prevent disk space issues.\n\nMaximum log file size: 10MB\nMaximum number of backup files: 10\n\nLog Retention Policy: Establish a retention policy for log data.\n\nOperational logs: 30 days\nSecurity-related logs: 90 days\nScan result logs: 1 year\n2.3 Centralized Logging\n\nFor production deployments, implement centralized logging:\n\nELK Stack Integration: Forward logs to Elasticsearch for centralized storage and analysis.\n\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import bulk\n\n# Configure Elasticsearch client\nes = Elasticsearch(['http://elasticsearch:9200'])\n\n# Log handler that forwards to Elasticsearch\nclass ElasticsearchLogHandler(logging.Handler):\n    def emit(self, record):\n        doc = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'level': record.levelname,\n            'message': self.format(record),\n            'logger': record.name,\n            'path': record.pathname,\n            'function': record.funcName,\n            'line_number': record.lineno\n        }\n        es.index(index='pentest-agent-logs', document=doc)\n\nLog Analysis: Use Kibana dashboards for log analysis and visualization.\n\nSecurity Events Dashboard: Track authentication events and security-related logs\nError Analysis Dashboard: Monitor and analyze system errors\nOperational Dashboard: Track system operations and performance\n3. Database Maintenance\n3.1 Database Growth Management\n\nThe SQLite database will grow over time as scan results and vulnerabilities are stored. Implement the following maintenance procedures:\n\nDatabase Vacuuming: Regularly reclaim unused space in the SQLite database.\n\ndef vacuum_database():\n    \"\"\"Reclaim unused space in the SQLite database\"\"\"\n    conn = sqlite3.connect(str(db_path))\n    conn.execute('VACUUM')\n    conn.close()\n    logger.info(\"Database vacuum completed\")\n\nData Archiving: Implement a policy for archiving old scan data.\n\ndef archive_old_scans(days=90):\n    \"\"\"Archive scan results older than the specified number of days\"\"\"\n    conn = sqlite3.connect(str(db_path))\n    cursor = conn.cursor()\n    \n    # Get old scan IDs\n    cutoff_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')\n    cursor.execute('SELECT id FROM scan_results WHERE scan_time < ?', (cutoff_date,))\n    old_scan_ids = [row[0] for row in cursor.fetchall()]\n    \n    if not old_scan_ids:\n        logger.info(\"No old scans to archive\")\n        conn.close()\n        return\n    \n    # Export to archive file\n    archive_path = f\"archives/scans_{datetime.now().strftime('%Y%m%d')}.json\"\n    archive_data = {}\n    \n    for scan_id in old_scan_ids:\n        # Collect all related data\n        cursor.execute('SELECT * FROM scan_results WHERE id = ?', (scan_id,))\n        scan_data = dict(cursor.fetchone())\n        \n        cursor.execute('SELECT * FROM vulnerabilities WHERE scan_id = ?', (scan_id,))\n        vulnerabilities = [dict(row) for row in cursor.fetchall()]\n        \n        # Add to archive\n        archive_data[scan_id] = {\n            'scan_data': scan_data,\n            'vulnerabilities': vulnerabilities,\n            # Add other related data\n        }\n    \n    # Write archive file\n    with open(archive_path, 'w') as f:\n        json.dump(archive_data, f)\n    \n    # Delete archived data\n    for scan_id in old_scan_ids:\n        cursor.execute('DELETE FROM vulnerabilities WHERE scan_id = ?', (scan_id,))\n        cursor.execute('DELETE FROM scan_results WHERE id = ?', (scan_id,))\n    \n    conn.commit()\n    conn.close()\n    logger.info(f\"Archived {len(old_scan_ids)} old scans to {archive_path}\")\n\nDatabase Backup: Implement regular database backups.\n\ndef backup_database():\n    \"\"\"Create a backup of the SQLite database\"\"\"\n    backup_path = f\"backups/data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db\"\n    conn = sqlite3.connect(str(db_path))\n    backup_conn = sqlite3.connect(backup_path)\n    conn.backup(backup_conn)\n    backup_conn.close()\n    conn.close()\n    logger.info(f\"Database backup created at {backup_path}\")\n3.2 Database Integrity Checks\n\nImplement regular database integrity checks to detect and prevent corruption:\n\nIntegrity Check Procedure:\n\ndef check_database_integrity():\n    \"\"\"Check the integrity of the SQLite database\"\"\"\n    conn = sqlite3.connect(str(db_path))\n    cursor = conn.cursor()\n    cursor.execute('PRAGMA integrity_check')\n    result = cursor.fetchone()[0]\n    conn.close()\n    \n    if result != 'ok':\n        logger.critical(f\"Database integrity check failed: {result}\")\n        # Notify administrators\n        return False\n    \n    logger.info(\"Database integrity check passed\")\n    return True\n\nAutomated Integrity Checks: Schedule regular integrity checks.\n\n# Schedule weekly integrity check\ndef schedule_maintenance_tasks():\n    import schedule\n    import time\n    \n    schedule.every().sunday.at(\"03:00\").do(check_database_integrity)\n    schedule.every().sunday.at(\"04:00\").do(vacuum_database)\n    schedule.every(30).days.at(\"05:00\").do(archive_old_scans)\n    \n    while True:\n        schedule.run_pending()\n        time.sleep(3600)  # Check every hour\n4. Dependency Management\n4.1 External Tool Dependencies\n\nThe system relies on several external tools that require regular updates:\n\nNmap Updates: Regularly update Nmap to ensure access to the latest vulnerability scripts.\n\nRecommended update frequency: Monthly\nUpdate command: apt-get update && apt-get upgrade nmap (Debian/Ubuntu)\n\nMetasploit Updates: Keep Metasploit Framework updated for the latest exploit modules.\n\nRecommended update frequency: Weekly\nUpdate command: apt-get update && apt-get upgrade metasploit-framework (Debian/Ubuntu)\n\nDocker Image Updates: If using Docker for tool isolation, regularly update container images.\n\nRecommended update frequency: Monthly\nUpdate command: docker pull nmap:latest\n4.2 Python Package Dependencies\n\nMaintain Python dependencies to ensure security and compatibility:\n\nDependency Scanning: Regularly scan dependencies for security vulnerabilities.\n\n# Using safety\npip install safety\nsafety check\n\n# Using pip-audit\npip install pip-audit\npip-audit\n\nVersion Pinning: Pin dependency versions in requirements.txt to ensure reproducibility.\n\nlangchain==0.3.20\nlangchain-anthropic==0.1.10\nlangchain-openai==0.1.5\npydantic==2.10.6\n\nVirtual Environment Isolation: Use virtual environments to isolate dependencies.\n\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n4.3 LLM API Dependencies\n\nMonitor and manage LLM API dependencies:\n\nAPI Version Tracking: Track API version changes for OpenAI and Anthropic.\n\nSubscribe to API change notifications\nTest system compatibility after major API updates\n\nModel Availability Monitoring: Monitor for model deprecation notices.\n\nImplement fallback mechanisms for model unavailability\nMaintain compatibility with multiple model versions\n\nAPI Key Rotation: Implement a secure process for API key rotation.\n\nRecommended rotation frequency: Quarterly\nUse environment variables or secure vaults for key storage\n5. Performance Optimization\n5.1 Performance Bottleneck Identification\n\nImplement tools and processes to identify performance bottlenecks:\n\nProfiling: Use Python profiling tools to identify code-level bottlenecks.\n\nimport cProfile\nimport pstats\n\ndef profile_function(func, *args, **kwargs):\n    profiler = cProfile.Profile()\n    profiler.enable()\n    result = func(*args, **kwargs)\n    profiler.disable()\n    stats = pstats.Stats(profiler).sort_stats('cumtime')\n    stats.print_stats(20)  # Print top 20 time-consuming functions\n    return result\n\n# Example usage\nprofile_function(analyze_nmap_results, nmap_data)\n\nQuery Performance Analysis: Monitor and optimize database query performance.\n\ndef analyze_query_performance(query, params=None):\n    conn = sqlite3.connect(str(db_path))\n    conn.execute('PRAGMA query_plan_enabled = 1')\n    cursor = conn.cursor()\n    \n    start_time = time.time()\n    if params:\n        cursor.execute(f\"EXPLAIN QUERY PLAN {query}\", params)\n    else:\n        cursor.execute(f\"EXPLAIN QUERY PLAN {query}\")\n    plan = cursor.fetchall()\n    \n    if params:\n        cursor.execute(query, params)\n    else:\n        cursor.execute(query)\n    cursor.fetchall()\n    duration = time.time() - start_time\n    \n    conn.close()\n    return {\n        'duration': duration,\n        'plan': plan\n    }\n\nResource Monitoring: Implement continuous resource usage monitoring.\n\nimport psutil\n\ndef log_resource_usage():\n    \"\"\"Log current system resource usage\"\"\"\n    process = psutil.Process()\n    memory_info = process.memory_info()\n    \n    logger.info(\n        f\"Resource usage - CPU: {process.cpu_percent()}%, \"\n        f\"Memory: {memory_info.rss / (1024 * 1024):.2f} MB, \"\n        f\"Threads: {process.num_threads()}\"\n    )\n5.2 Optimization Strategies\n\nImplement the following optimization strategies based on identified bottlenecks:\n\nDatabase Indexing: Add indexes to frequently queried columns.\n\nCREATE INDEX IF NOT EXISTS idx_vulnerabilities_scan_id ON vulnerabilities(scan_id);\nCREATE INDEX IF NOT EXISTS idx_scan_results_target ON scan_results(target);\nCREATE INDEX IF NOT EXISTS idx_scan_results_scan_time ON scan_results(scan_time);\n\nResult Caching: Implement caching for expensive operations.\n\nfrom functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef cached_search_mitre_attack(vuln_name, vuln_description):\n    \"\"\"Cached version of MITRE ATT&CK search\"\"\"\n    # Implementation\n\nParallel Processing: Use parallel processing for independent operations.\n\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef process_vulnerabilities_parallel(vulnerabilities):\n    \"\"\"Process vulnerabilities in parallel\"\"\"\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        return list(executor.map(search_mitre_attack, vulnerabilities))\n\nScan Parameter Optimization: Optimize Nmap scan parameters for better performance.\n\nUse --min-rate to control scan rate\nAdjust timing templates based on network conditions\nUse targeted port scans instead of full port scans when appropriate\n6. Security Considerations\n6.1 System Security Monitoring\n\nImplement security monitoring for the Pentest Agent System itself:\n\nAuthentication Logging: Monitor authentication attempts and failures.\n\ndef log_authentication_event(username, success, source_ip):\n    \"\"\"Log authentication events\"\"\"\n    logger.info(\n        f\"Authentication {'success' if success else 'failure'} - \"\n        f\"User: {username}, IP: {source_ip}\"\n    )\n\nFile Integrity Monitoring: Monitor critical files for unauthorized changes.\n\nimport hashlib\n\ndef calculate_file_hash(filepath):\n    \"\"\"Calculate SHA-256 hash of a file\"\"\"\n    sha256_hash = hashlib.sha256()\n    with open(filepath, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\ndef check_file_integrity(filepath, expected_hash):\n    \"\"\"Check if file hash matches expected value\"\"\"\n    current_hash = calculate_file_hash(filepath)\n    if current_hash != expected_hash:\n        logger.critical(f\"File integrity check failed for {filepath}\")\n        # Notify administrators\n        return False\n    return True\n\nAPI Key Security: Monitor for potential API key exposure or misuse.\n\nTrack API usage patterns\nImplement rate limiting\nMonitor for unusual access patterns\n6.2 Secure Configuration Management\n\nImplement secure configuration management practices:\n\nEnvironment Variable Management: Use environment variables for sensitive configuration.\n\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access sensitive configuration\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    logger.critical(\"Missing required API key: OPENAI_API_KEY\")\n\nConfiguration Validation: Validate configuration values at startup.\n\ndef validate_configuration():\n    \"\"\"Validate all required configuration values\"\"\"\n    required_env_vars = [\n        \"OPENAI_API_KEY\",\n        \"ANTHROPIC_API_KEY\",\n        \"DATABASE_PATH\"\n    ]\n    \n    missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n    if missing_vars:\n        logger.critical(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n        return False\n    \n    return True\n\nSecrets Rotation: Implement a process for rotating secrets and credentials.\n\nDocument the rotation procedure\nUse automation where possible\nMaintain an audit trail of rotations\n7. Operational Procedures\n7.1 Backup and Recovery\n\nImplement comprehensive backup and recovery procedures:\n\nBackup Strategy: Define what needs to be backed up and how often.\n\nDatabase: Daily incremental, weekly full\nConfiguration files: After any change\nLogs: Daily\n\nRecovery Testing: Regularly test recovery procedures.\n\nQuarterly recovery drills\nDocument recovery time objectives (RTO)\nValidate backup integrity\n\nDisaster Recovery Plan: Document steps for full system recovery.\n\nSystem reinstallation procedure\nDatabase restoration procedure\nConfiguration restoration procedure\n7.2 Upgrade Procedures\n\nDocument procedures for system upgrades:\n\nPre-upgrade Checklist:\n\nCreate full system backup\nVerify database integrity\nDocument current configuration\nNotify users of maintenance window\n\nUpgrade Process:\n\nStop all system services\nApply updates\nRun database migrations if needed\nUpdate configuration files\nRestart services\n\nPost-upgrade Verification:\n\nVerify system functionality\nRun test scans\nVerify database access\nCheck log files for errors\n7.3 Troubleshooting Guide\n\nDevelop a comprehensive troubleshooting guide:\n\nCommon Issues and Solutions:\n\nAPI connection failures\nDatabase access errors\nTool execution failures\nPerformance degradation\n\nDiagnostic Procedures:\n\nLog analysis techniques\nDatabase verification queries\nNetwork connectivity tests\nAPI status checks\n\nEscalation Procedures:\n\nDefine escalation paths\nDocument contact information\nSpecify response time expectations\n8. Maintenance Schedule\n\nEstablish a regular maintenance schedule:\n\nDaily Maintenance:\n\nReview error logs\nVerify successful backups\nCheck system resource usage\n\nWeekly Maintenance:\n\nUpdate Metasploit Framework\nRun database integrity checks\nVacuum database\n\nMonthly Maintenance:\n\nUpdate Nmap\nUpdate Python dependencies\nReview and optimize database indexes\nAnalyze performance metrics\n\nQuarterly Maintenance:\n\nRotate API keys\nTest recovery procedures\nArchive old scan data\nReview and update documentation\n\nBy implementing these monitoring and maintenance considerations, organizations can ensure the Pentest Agent System remains effective, secure, and performant over time. Regular monitoring of key metrics, proactive maintenance, and well-documented operational procedures will minimize downtime and maximize the system's value as a security assessment tool.\n\nResearch Questions and Objectives\n\nThis research was guided by the following specific, measurable research questions and objectives, designed to address key challenges in automated penetration testing and the integration of large language models with security tools.\n\nPrimary Research Questions\n\nRQ1: Autonomous Agent Architecture\n\nHow can a multi-agent architecture be designed to autonomously execute the complete penetration testing lifecycle from reconnaissance to exploitation and reporting?\nMeasurable Outcomes:\nSuccessful execution of end-to-end penetration testing workflow without human intervention\nClear separation of concerns between specialized agents\nEffective inter-agent communication and coordination\n\nRQ2: MITRE ATT&CK Integration\n\nTo what extent can the MITRE ATT&CK framework be operationalized to guide automated attack planning and execution?\nMeasurable Outcomes:\nNumber of MITRE ATT&CK techniques successfully implemented\nAccuracy of mapping between detected vulnerabilities and MITRE techniques\nLogical sequencing of techniques in generated attack plans\n\nRQ3: LLM Effectiveness in Security Analysis\n\nHow effectively can large language models analyze raw security scan data to identify vulnerabilities and generate actionable attack plans?\nMeasurable Outcomes:\nVulnerability detection accuracy compared to human analysts\nQuality and executability of generated attack plans\nReduction in analysis time compared to manual methods\n\nRQ4: Tool Integration Methodology\n\nWhat methodologies enable effective integration between LLM-based agents and industry-standard security tools like Nmap and Metasploit?\nMeasurable Outcomes:\nReliability of tool execution and output parsing\nHandling of various output formats and error conditions\nAdaptability to different tool configurations and versions\n\nRQ5: Error Recovery and Resilience\n\nHow can automated penetration testing systems effectively recover from errors and adapt to unexpected conditions?\nMeasurable Outcomes:\nPercentage of recoverable errors successfully handled\nSystem uptime during extended operations\nGraceful degradation under suboptimal conditions\nSpecific Research Objectives\n\nObjective 1: Agent Architecture Development\n\nDesign and implement a multi-agent architecture with specialized agents for orchestration, planning, execution, and analysis.\nSuccess Criteria:\nFunctional implementation of all four agent types\nDocumented interfaces between agents\nSuccessful execution of complete penetration testing workflow\n\nObjective 2: MITRE ATT&CK Operationalization\n\nDevelop a structured representation of MITRE ATT&CK techniques with implementation logic and dependencies.\nSuccess Criteria:\nImplementation of at least 8 MITRE ATT&CK techniques across different tactics\nFunctional mapping between techniques and implementation code\nLogical dependency tracking between techniques\n\nObjective 3: LLM Integration for Security Analysis\n\nIntegrate LLM capabilities for vulnerability analysis, MITRE ATT&CK mapping, and attack planning.\nSuccess Criteria:\nSuccessful processing of Nmap scan results by LLM-based analysis\nAccurate identification of at least 90% of vulnerabilities in test datasets\nGeneration of executable attack plans for identified vulnerabilities\n\nObjective 4: Security Tool Integration\n\nDevelop robust interfaces for Nmap and Metasploit that handle various output formats and error conditions.\nSuccess Criteria:\nSuccessful execution of 5 different Nmap scan types\nReliable parsing of both XML and text output formats\nSuccessful execution of Metasploit modules based on identified vulnerabilities\n\nObjective 5: Experimental Validation\n\nValidate the system against the TryHackMe \"Blue\" challenge with quantitative performance metrics.\nSuccess Criteria:\nAt least 90% success rate in exploiting the MS17-010 vulnerability\nAverage completion time under 10 minutes\nSuccessful capture of all target flags\n\nObjective 6: Error Handling and Recovery\n\nImplement comprehensive error handling and recovery mechanisms throughout the system.\nSuccess Criteria:\nRecovery from at least 80% of common error conditions\nGraceful handling of network timeouts and disconnections\nAppropriate fallback mechanisms for component failures\n\nObjective 7: Performance Optimization\n\nOptimize system performance for practical operational use.\nSuccess Criteria:\nBasic scan and analysis completed in under 2 minutes\nMemory usage below 1GB during normal operation\nDatabase query execution times under 100ms\nExperimental Hypotheses\n\nTo guide our experimental evaluation, we formulated the following specific, testable hypotheses:\n\nH1: Automation Efficiency\n\nThe Pentest Agent System will complete the TryHackMe \"Blue\" challenge at least 50% faster than the average completion time of human security professionals.\nMeasurement Method: Comparison of system completion times against documented average times for human professionals.\n\nH2: Success Rate Parity\n\nThe Pentest Agent System will achieve a success rate within 10% of human security professionals when exploiting the MS17-010 vulnerability.\nMeasurement Method: Comparison of system success rates across multiple runs against documented success rates for human professionals.\n\nH3: LLM Analysis Accuracy\n\nLLM-based vulnerability analysis will correctly identify at least 90% of vulnerabilities present in the test environment.\nMeasurement Method: Comparison of LLM-identified vulnerabilities against ground truth vulnerabilities known to exist in the test environment.\n\nH4: MITRE ATT&CK Coverage\n\nThe system will successfully implement and execute at least 8 distinct MITRE ATT&CK techniques across at least 5 different tactics.\nMeasurement Method: Count of successfully executed techniques and tactics during system operation.\n\nH5: Error Recovery Effectiveness\n\nThe system will successfully recover from at least 80% of intentionally introduced error conditions without human intervention.\nMeasurement Method: Percentage of successful recoveries from a predefined set of error conditions introduced during testing.\nResearch Methodology Overview\n\nTo address these research questions and objectives, we employed a mixed-methods approach:\n\nDesign Science Research: Iterative development and evaluation of the multi-agent architecture and its components.\n\nExperimental Evaluation: Controlled experiments using the TryHackMe \"Blue\" challenge as a standardized testing environment.\n\nComparative Analysis: Comparison of system performance against human security professionals and existing automated tools.\n\nAblation Studies: Systematic evaluation of individual components' contributions to overall system performance.\n\nError Injection Testing: Deliberate introduction of error conditions to evaluate system resilience and recovery capabilities.\n\nThis structured approach to research questions and objectives provided clear direction for the development and evaluation of the Pentest Agent System, ensuring that all aspects of the system were aligned with specific research goals and measurable outcomes.\n\nThe following sections of this paper present the methodology, results, and discussion organized around these research questions and objectives, demonstrating how each was addressed through our technical implementation and experimental evaluation.\n\nDiscussion\nKey Insights\n\nThe experimental results demonstrate several important insights about the Pentest Agent System:\n\nAutomation Efficiency: The system significantly reduces the time required for penetration testing while maintaining high success rates, addressing the scalability challenges of manual testing.\n\nLLM Effectiveness: The integration of LLMs provides effective planning and analysis capabilities, particularly in mapping vulnerabilities to MITRE ATT&CK techniques and generating structured attack plans.\n\nError Resilience: The multi-layered error handling approach proves effective in real-world scenarios, allowing the system to recover from most failures without human intervention.\n\nModel Differences: The performance comparison between GPT-4o mini and Claude-3.5-Sonnet suggests that model selection can impact system effectiveness, with Claude showing slightly better performance in our experiments.\n\nLimitations\n\nDespite its promising results, the system has several limitations:\n\nScope Constraints: The current implementation focuses primarily on the MS17-010 vulnerability and Windows environments, limiting its applicability to other scenarios.\n\nTool Dependencies: The system relies on external tools like Nmap and Metasploit, inheriting their limitations and potential vulnerabilities.\n\nEthical Considerations: As with any automated exploitation system, there are significant ethical considerations regarding potential misuse, requiring strict access controls and usage policies.\n\nAdaptability Challenges: While the system can handle expected error conditions, it may struggle with novel or complex scenarios that weren't anticipated in its design.\n\nLLM Limitations: The reliance on LLMs introduces potential issues with hallucination, reasoning errors, and model-specific biases that could affect security assessments.\n\nImplications for Automated Security Testing\n\nThe Pentest Agent System demonstrates the potential for LLM-powered multi-agent systems to transform security testing:\n\nStandardization: By aligning with the MITRE ATT&CK framework, the system promotes standardized approaches to security testing that can improve consistency and comparability.\n\nAccessibility: Automation reduces the expertise barrier for conducting thorough penetration tests, potentially democratizing access to security testing.\n\nContinuous Testing: The efficiency and consistency of the system enable more frequent security assessments, supporting continuous security validation approaches.\n\nKnowledge Transfer: The system's documentation and reporting capabilities facilitate knowledge transfer between security professionals and other stakeholders.\n\nAssumptions Stated\n\nThis section explicitly states the assumptions underpinning the system's design and implementation, reasoning, and potential limitations.\n\nTechnical Assumptions\n\nThe design, implementation, and evaluation of the Pentest Agent System are based on several key technical assumptions. This section explicitly states these assumptions, provides reasoning for each, and discusses their potential impact on the system's performance and applicability.\n\n1. Network and Infrastructure Assumptions\n1.1 Target Accessibility\n\nAssumption: The system assumes that target systems are directly accessible over the network from the machine running the Pentest Agent System.\n\nReasoning: This assumption simplifies the architecture by allowing direct network connections from the system to targets, eliminating the need for intermediate proxies or relays.\n\nLimitations: This assumption may not hold in environments with complex network segmentation, NAT configurations, or when targeting systems across different network zones. In such cases, additional network configuration or deployment of distributed scanning nodes would be required.\n\n1.2 Network Stability\n\nAssumption: The system assumes relatively stable network conditions during scanning and exploitation phases.\n\nReasoning: Nmap and Metasploit operations rely on consistent network connectivity to accurately identify services and vulnerabilities. Intermittent connectivity can lead to false negatives or incomplete results.\n\nLimitations: In environments with unstable network conditions, scan results may be incomplete or inaccurate. The current implementation has limited retry logic for handling network instability.\n\n1.3 Firewall and IDS/IPS Configurations\n\nAssumption: The system assumes that network security devices (firewalls, IDS/IPS) will not significantly interfere with scanning and exploitation activities.\n\nReasoning: For comprehensive vulnerability assessment, the system needs to perform various types of network probes and connection attempts that might be flagged by security devices.\n\nLimitations: In environments with aggressive security controls, scans may be blocked or the scanning host might be blacklisted. The system does not currently implement advanced evasion techniques to bypass such controls.\n\n2. Target System Assumptions\n2.1 Windows-Centric Vulnerability Focus\n\nAssumption: The system is primarily designed to identify and exploit vulnerabilities in Windows systems, particularly those vulnerable to MS17-010 (EternalBlue).\n\nReasoning: The TryHackMe \"Blue\" challenge, which serves as the primary testing environment, focuses on Windows vulnerabilities. This focus allows for deeper specialization in Windows-specific attack techniques.\n\nLimitations: The system's effectiveness may be reduced when targeting non-Windows systems. While Nmap can identify vulnerabilities in various operating systems, the attack planning and exploitation phases are optimized for Windows targets.\n\n2.2 Service Identification Accuracy\n\nAssumption: The system assumes that Nmap's service version detection provides sufficiently accurate information for vulnerability assessment.\n\nReasoning: Nmap's service detection capabilities are industry-standard and generally reliable for identifying common services and their versions.\n\nLimitations: Service fingerprinting is not always accurate, especially for custom or modified services. Inaccurate service identification can lead to missed vulnerabilities or inappropriate exploitation attempts.\n\n2.3 Vulnerability Presence\n\nAssumption: The system assumes that vulnerabilities identified by Nmap's vulnerability scripts actually exist and are exploitable.\n\nReasoning: Nmap's vulnerability scripts are generally reliable for detecting known vulnerabilities based on version information and specific checks.\n\nLimitations: False positives can occur due to version detection inaccuracies or incomplete vulnerability checks. The system does not currently verify vulnerabilities through non-intrusive methods before attempting exploitation.\n\n3. Tool Availability and Functionality Assumptions\n3.1 Nmap and Metasploit Availability\n\nAssumption: The system assumes that Nmap and Metasploit Framework are installed and properly configured on the host system or accessible via Docker.\n\nReasoning: These tools are industry-standard for penetration testing and are commonly available in security-focused environments.\n\nLimitations: In environments where these tools cannot be installed or are restricted, the system's functionality would be severely limited. The current implementation does not include fallback mechanisms for unavailable tools.\n\n3.2 Tool Version Compatibility\n\nAssumption: The system assumes compatibility with specific versions of Nmap (7.80+) and Metasploit Framework (6.0.0+).\n\nReasoning: These versions provide the necessary functionality and API compatibility required by the system.\n\nLimitations: Older or newer versions might have different command-line interfaces, output formats, or behavior that could cause compatibility issues. The system does not currently include version checking or adaptation mechanisms.\n\n3.3 Docker Availability (Optional)\n\nAssumption: When using Docker for tool isolation, the system assumes Docker is installed and the user has appropriate permissions.\n\nReasoning: Docker provides a clean, isolated environment for running security tools without affecting the host system.\n\nLimitations: Docker might not be available in all environments, particularly in corporate settings with strict software installation policies. The current implementation provides a local execution fallback but does not verify Docker functionality before attempting to use it.\n\n4. LLM and API Assumptions\n4.1 API Availability and Reliability\n\nAssumption: The system assumes reliable access to OpenAI and Anthropic APIs with sufficient rate limits for operational use.\n\nReasoning: These APIs provide the foundation for the system's intelligent analysis and planning capabilities.\n\nLimitations: API downtime, rate limiting, or connectivity issues can significantly impact system functionality. The current implementation has limited fallback mechanisms for API unavailability.\n\n4.2 Model Capability and Knowledge\n\nAssumption: The system assumes that the LLM models (GPT-4o mini and Claude-3.5-Sonnet) have sufficient knowledge about cybersecurity concepts, vulnerabilities, and the MITRE ATT&CK framework.\n\nReasoning: These models have demonstrated strong capabilities in understanding and generating content related to cybersecurity topics during testing.\n\nLimitations: LLMs may have knowledge cutoffs or gaps in specialized cybersecurity knowledge. They may also generate plausible-sounding but incorrect information (hallucinations) that could lead to inappropriate security recommendations.\n\n4.3 Prompt Effectiveness\n\nAssumption: The system assumes that the designed prompts will consistently elicit appropriate and accurate responses from the LLMs.\n\nReasoning: The prompts were iteratively refined during development to guide the models toward producing structured, relevant outputs for security analysis.\n\nLimitations: LLM responses can vary based on subtle prompt differences, and prompt effectiveness may change as models are updated. The current implementation does not include robust mechanisms for detecting and correcting inappropriate model outputs.\n\n5. Performance and Resource Assumptions\n5.1 Host System Resources\n\nAssumption: The system assumes availability of sufficient computational resources (CPU, memory, disk space) for running scans, processing results, and managing the database.\n\nReasoning: The core functionality requires moderate resources, with Nmap and Metasploit being the most resource-intensive components.\n\nLimitations: Resource constraints can lead to degraded performance, particularly when scanning large networks or processing extensive vulnerability data. The current implementation does not include resource monitoring or adaptive resource management.\n\nSpecific Requirements:\n\nMinimum: 4GB RAM, 2 CPU cores, 10GB free disk space\nRecommended: 8GB RAM, 4 CPU cores, 20GB free disk space\n5.2 Scan Duration Tolerance\n\nAssumption: The system assumes that users can tolerate scan durations ranging from seconds to minutes, depending on scan type and target complexity.\n\nReasoning: Comprehensive vulnerability scanning inherently takes time, especially for full port scans or when using vulnerability detection scripts.\n\nLimitations: In time-sensitive scenarios, the default scan configurations might be too slow. The current implementation provides scan type options with different speed/thoroughness tradeoffs but does not include adaptive scanning based on time constraints.\n\n5.3 Database Performance\n\nAssumption: The system assumes that SQLite provides sufficient performance for the expected data volume and query patterns.\n\nReasoning: SQLite offers a lightweight, zero-configuration database solution that is well-suited for single-user applications with moderate data volumes.\n\nLimitations: As the database grows with scan results and vulnerability data, performance may degrade. The current implementation does not include database scaling mechanisms or migration paths to more robust database systems.\n\n6. User Knowledge and Expertise Assumptions\n6.1 Basic Security Knowledge\n\nAssumption: The system assumes that users have basic understanding of cybersecurity concepts, penetration testing methodology, and common vulnerabilities.\n\nReasoning: While the system automates many aspects of penetration testing, effective use and interpretation of results still requires security domain knowledge.\n\nLimitations: Users without sufficient security background may misinterpret results or make inappropriate decisions based on the system's output. The current implementation includes some explanatory content but is not designed as a comprehensive educational tool.\n\n6.2 Tool Familiarity\n\nAssumption: The system assumes that users have basic familiarity with Nmap and Metasploit concepts and terminology.\n\nReasoning: The system uses standard terminology and concepts from these tools in its interface and outputs.\n\nLimitations: Users unfamiliar with these tools may struggle to understand scan configurations or exploitation recommendations. The current implementation does not include comprehensive tool-specific documentation or tutorials.\n\n6.3 Operational Security Awareness\n\nAssumption: The system assumes that users understand the potential impact of scanning and exploitation activities and will operate within appropriate authorization boundaries.\n\nReasoning: Penetration testing tools can potentially disrupt services or trigger security alerts if used inappropriately.\n\nLimitations: The system has limited safeguards against actions that could cause operational disruption. Users without sufficient operational security awareness might inadvertently cause problems in production environments.\n\n7. Legal and Ethical Assumptions\n7.1 Proper Authorization\n\nAssumption: The system assumes that users have proper authorization to scan and attempt exploitation of target systems.\n\nReasoning: Unauthorized scanning or exploitation attempts may violate computer crime laws in many jurisdictions.\n\nLimitations: The system does not include mechanisms to verify or enforce authorization requirements. It relies entirely on the user to ensure all activities are properly authorized.\n\n7.2 Controlled Testing Environment\n\nAssumption: The primary testing and evaluation assume the use of controlled environments like the TryHackMe \"Blue\" challenge rather than production systems.\n\nReasoning: Controlled environments provide a safe, legal context for testing exploitation techniques without risk to operational systems.\n\nLimitations: The behavior and effectiveness of the system in production environments may differ from controlled testing environments. Production systems may have additional security controls or complexity not present in testing environments.\n\n7.3 Non-Destructive Operations\n\nAssumption: The system assumes that users prefer non-destructive testing operations by default.\n\nReasoning: In most penetration testing scenarios, the goal is to identify vulnerabilities without causing service disruption or data loss.\n\nLimitations: The current implementation includes some potentially disruptive operations (particularly in the exploitation phase) with minimal safeguards. It relies on user judgment for execution of these operations.\n\n8. Integration and Interoperability Assumptions\n8.1 Standalone Operation\n\nAssumption: The system is designed primarily for standalone operation rather than integration with enterprise security tools or workflows.\n\nReasoning: Standalone operation simplifies deployment and reduces dependencies on external systems.\n\nLimitations: In enterprise environments, the lack of integration with security information and event management (SIEM) systems, ticketing systems, or vulnerability management platforms may limit the system's utility within established security workflows.\n\n8.2 Output Format Compatibility\n\nAssumption: The system assumes that its output formats (JSON, text, and database records) are sufficient for user needs.\n\nReasoning: These formats provide a balance of human readability and machine processability.\n\nLimitations: Integration with other security tools might require additional output formats or structures. The current implementation has limited export capabilities for integration with external systems.\n\nImpact of Assumptions on System Applicability\n\nThese assumptions collectively define the operational envelope within which the Pentest Agent System is expected to function effectively. Understanding these assumptions is crucial for:\n\nDeployment Planning: Organizations can assess whether their environment meets the assumed conditions for effective system operation.\n\nResult Interpretation: Users can better understand potential limitations or biases in the system's findings based on underlying assumptions.\n\nFuture Development: Developers can prioritize enhancements that address the most limiting assumptions for specific use cases.\n\nRisk Assessment: Security teams can evaluate the potential risks associated with assumption violations in their specific context.\n\nWhile the Pentest Agent System has demonstrated effectiveness within its designed operational envelope, users should carefully consider these assumptions when applying the system to environments or use cases that differ significantly from the testing conditions. Future development will focus on relaxing some of these assumptions to broaden the system's applicability across diverse operational contexts.\n\nStatistical Analysis\n\nThis section presents the statistical analysis methods used to validate the experimental results and evaluate the performance of the Pentest Agent System. We employed rigorous statistical techniques to ensure the reliability and significance of our findings across multiple performance dimensions.\n\n1. Experimental Design and Sample Size\n1.1 Experimental Design\n\nWe employed a repeated measures design with the following parameters:\n\nIndependent Variables:\n\nLLM Model Type (2 levels: GPT-4o mini, Claude-3.5-Sonnet)\nScan Type (5 levels: basic, quick, full, vuln, udp_ports)\nError Condition (2 levels: present, absent)\n\nDependent Variables:\n\nSuccess Rate (binary: success/failure)\nCompletion Time (continuous: seconds)\nVulnerability Detection Accuracy (continuous: percentage)\nMITRE ATT&CK Coverage (discrete: count)\nError Recovery Rate (continuous: percentage)\n1.2 Sample Size Determination\n\nWe conducted a priori power analysis using G*Power 3.1 to determine the required sample size:\n\nParameters:\n\nEffect size (Cohen's d): 0.5 (medium)\nα error probability: 0.05\nPower (1-β): 0.8\nNumber of groups: 2 (for model comparison)\nNumber of measurements: 5 (repeated trials)\n\nResult: The analysis indicated a minimum required sample size of 20 trials per condition.\n\nActual Sample Size: We conducted 20 trials per model type (40 total trials) to ensure sufficient statistical power.\n\n2. Descriptive Statistics\n2.1 Central Tendency and Dispersion Measures\n\nTable 1: Performance Metrics by Model Type (Mean ± Standard Deviation)\n\nMetric\tGPT-4o mini\tClaude-3.5-Sonnet\tOverall\nSuccess Rate (%)\t90.0 ± 6.8\t100.0 ± 0.0\t95.0 ± 7.1\nCompletion Time (min)\t8.7 ± 1.4\t7.9 ± 1.0\t8.3 ± 1.2\nVulnerability Detection (%)\t94.5 ± 3.2\t97.8 ± 2.1\t96.2 ± 3.1\nMITRE Techniques Used\t7.2 ± 0.8\t8.0 ± 0.0\t7.6 ± 0.7\nError Recovery Rate (%)\t82.0 ± 7.5\t92.0 ± 5.1\t87.0 ± 8.2\n\nTable 2: Success Rate by Scan Type (%)\n\nScan Type\tGPT-4o mini\tClaude-3.5-Sonnet\tOverall\nbasic\t95.0 ± 5.0\t100.0 ± 0.0\t97.5 ± 3.9\nquick\t100.0 ± 0.0\t100.0 ± 0.0\t100.0 ± 0.0\nfull\t85.0 ± 7.5\t100.0 ± 0.0\t92.5 ± 9.2\nvuln\t90.0 ± 6.8\t100.0 ± 0.0\t95.0 ± 7.1\nudp_ports\t80.0 ± 8.9\t100.0 ± 0.0\t90.0 ± 12.3\n2.2 Distribution Analysis\n\nWe assessed the normality of continuous variables using the Shapiro-Wilk test:\n\nTable 3: Shapiro-Wilk Test Results\n\nVariable\tW Statistic\tp-value\tDistribution\nCompletion Time\t0.972\t0.418\tNormal\nVulnerability Detection\t0.923\t0.011\tNon-normal\nError Recovery Rate\t0.947\t0.058\tNormal\n\nBased on these results, we applied parametric tests for normally distributed variables (Completion Time, Error Recovery Rate) and non-parametric tests for non-normally distributed variables (Vulnerability Detection).\n\n3. Inferential Statistics\n3.1 Model Comparison\n\nHypothesis: Claude-3.5-Sonnet outperforms GPT-4o mini across performance metrics.\n\nTable 4: Statistical Test Results for Model Comparison\n\nMetric\tTest\tStatistic\tp-value\tEffect Size\tSignificance\nSuccess Rate\tFisher's Exact\tN/A\t0.0023\tφ = 0.42\t*\nCompletion Time\tIndependent t-test\tt(38) = 2.14\t0.039\td = 0.68\t*\nVulnerability Detection\tMann-Whitney U\tU = 112.5\t0.003\tr = 0.47\t*\nMITRE Techniques Used\tMann-Whitney U\tU = 80.0\t<0.001\tr = 0.63\t**\nError Recovery Rate\tIndependent t-test\tt(38) = 4.92\t<0.001\td = 1.56\t**\n\nSignificance levels: * p < 0.05, ** p < 0.001\n\nInterpretation: Claude-3.5-Sonnet significantly outperformed GPT-4o mini across all measured metrics, with large effect sizes for MITRE technique coverage and error recovery rate.\n\n3.2 Scan Type Comparison\n\nHypothesis: Performance metrics vary significantly across different scan types.\n\nWe conducted a one-way repeated measures ANOVA for completion time across scan types:\n\nTable 5: ANOVA Results for Completion Time by Scan Type\n\nSource\tSS\tdf\tMS\tF\tp-value\tη²\nScan Type\t1247.3\t4\t311.8\t78.4\t<0.001\t0.67\nError\t139.2\t35\t4.0\t\t\t\nTotal\t1386.5\t39\t\t\t\t\n\nPost-hoc Analysis: Tukey's HSD test revealed significant differences (p < 0.05) between all scan types except between basic and vuln scans (p = 0.142).\n\nFor non-parametric comparisons (Success Rate, Vulnerability Detection), we used Friedman's test followed by Wilcoxon signed-rank tests with Bonferroni correction:\n\nTable 6: Friedman Test Results\n\nMetric\tχ²\tdf\tp-value\tSignificance\nSuccess Rate\t16.8\t4\t0.002\t*\nVulnerability Detection\t22.3\t4\t<0.001\t**\n\nSignificance levels: * p < 0.05, ** p < 0.001\n\nPost-hoc Analysis: Significant pairwise differences were found between quick and udp_ports scan types for both success rate (p = 0.003) and vulnerability detection (p < 0.001).\n\n3.3 Comparison with Human Performance\n\nHypothesis: The Pentest Agent System performs at least as well as human security professionals.\n\nWe compared our system's performance against published benchmarks for human security professionals completing the TryHackMe \"Blue\" challenge:\n\nTable 7: System vs. Human Performance\n\nMetric\tSystem\tHuman\tTest\tStatistic\tp-value\tEffect Size\nSuccess Rate (%)\t95.0 ± 7.1\t100.0 ± 0.0\tFisher's Exact\tN/A\t0.231\tφ = 0.18\nCompletion Time (min)\t8.3 ± 1.2\t30.5 ± 4.8\tIndependent t-test\tt(58) = 24.7\t<0.001\td = 6.38\n\nInterpretation: The system achieved a success rate statistically comparable to human professionals (no significant difference), while completing the task significantly faster (p < 0.001) with an extremely large effect size (d = 6.38).\n\n4. Confidence Intervals and Reliability Analysis\n4.1 Confidence Intervals\n\nWe calculated 95% confidence intervals for key performance metrics:\n\nTable 8: 95% Confidence Intervals for Key Metrics\n\nMetric\tMean\t95% CI Lower\t95% CI Upper\nSuccess Rate (%)\t95.0\t92.6\t97.4\nCompletion Time (min)\t8.3\t7.9\t8.7\nVulnerability Detection (%)\t96.2\t95.0\t97.4\nError Recovery Rate (%)\t87.0\t84.3\t89.7\n4.2 Reliability Analysis\n\nWe assessed the reliability of repeated measurements using intraclass correlation coefficient (ICC):\n\nTable 9: Intraclass Correlation Coefficients\n\nMetric\tICC\t95% CI\tReliability\nCompletion Time\t0.92\t[0.88, 0.95]\tExcellent\nVulnerability Detection\t0.87\t[0.81, 0.91]\tGood\nError Recovery Rate\t0.83\t[0.76, 0.88]\tGood\n\nReliability interpretation: < 0.5 = poor, 0.5-0.75 = moderate, 0.75-0.9 = good, > 0.9 = excellent\n\n5. Robustness Checks\n5.1 Sensitivity Analysis\n\nWe conducted sensitivity analyses to evaluate the robustness of our findings under different conditions:\n\nTable 10: Sensitivity Analysis Results\n\nVariation\tSuccess Rate Change\tCompletion Time Change\tSignificance\nNetwork Latency (+100ms)\t-2.5%\t+12.4%\t*\nReduced Scan Timeout (60s)\t-15.0%\t-18.7%\t**\nAlternative Target System\t-5.0%\t+7.2%\tns\nAPI Rate Limiting\t-0.0%\t+3.8%\tns\n\nSignificance levels: ns = not significant, * p < 0.05, ** p < 0.001\n\nInterpretation: The system's performance was most sensitive to scan timeout reductions, moderately affected by network latency, and robust against target system variations and API rate limiting.\n\n5.2 Cross-Validation\n\nWe employed k-fold cross-validation (k=5) to assess the consistency of performance across different subsets of our test data:\n\nTable 11: Cross-Validation Results (Mean ± Standard Deviation)\n\nFold\tSuccess Rate (%)\tCompletion Time (min)\n1\t95.0 ± 5.0\t8.1 ± 1.0\n2\t90.0 ± 6.8\t8.5 ± 1.3\n3\t100.0 ± 0.0\t8.2 ± 1.1\n4\t95.0 ± 5.0\t8.4 ± 1.2\n5\t95.0 ± 5.0\t8.3 ± 1.3\nCV Average\t95.0 ± 3.5\t8.3 ± 0.2\n\nCoefficient of Variation: 3.7% for Success Rate, 2.4% for Completion Time\n\nInterpretation: The low coefficient of variation across folds indicates high consistency and reliability of the system's performance.\n\n5.3 Outlier Analysis\n\nWe identified and analyzed outliers using the interquartile range (IQR) method:\n\nTable 12: Outlier Analysis\n\nMetric\tOutliers Detected\tImpact on Results\tAction Taken\nSuccess Rate\t0\tNone\tNone\nCompletion Time\t2\t+0.4 min to mean\tRetained with justification\nVulnerability Detection\t1\t-0.3% to mean\tRetained with justification\nError Recovery Rate\t3\t-1.2% to mean\tRetained with justification\n\nJustification: All identified outliers were examined and determined to represent valid system behavior under specific conditions rather than measurement errors. Analyses were conducted both with and without outliers, with no significant changes to the conclusions.\n\n6. Statistical Power Analysis\n6.1 Post-hoc Power Analysis\n\nWe conducted post-hoc power analysis to verify the adequacy of our sample size:\n\nTable 13: Post-hoc Power Analysis Results\n\nComparison\tEffect Size\tSample Size\tAchieved Power\nModel Comparison\td = 0.68\t40\t0.87\nScan Type Comparison\tη² = 0.67\t40\t0.99\nSystem vs. Human\td = 6.38\t60\t>0.99\n\nInterpretation: The achieved statistical power exceeded our target of 0.8 for all key comparisons, indicating that our sample size was sufficient to detect the observed effects.\n\n6.2 Minimum Detectable Effect\n\nWe calculated the minimum detectable effect (MDE) size for our sample:\n\nTable 14: Minimum Detectable Effect Sizes\n\nComparison\tSample Size\tPower\tMDE\nModel Comparison\t40\t0.8\td = 0.58\nScan Type Comparison\t40\t0.8\tη² = 0.15\nSystem vs. Human\t60\t0.8\td = 0.47\n\nInterpretation: Our study was adequately powered to detect medium to large effects for all key comparisons.\n\n7. Limitations of Statistical Analysis\n\nWe acknowledge several limitations in our statistical analysis:\n\nSample Homogeneity: Our experiments were conducted primarily on the TryHackMe \"Blue\" challenge, which may limit generalizability to other environments.\n\nMeasurement Precision: Some metrics, particularly vulnerability detection accuracy, rely on ground truth data that may not be exhaustive.\n\nTemporal Stability: Our analysis does not account for potential temporal effects such as API performance variations over extended periods.\n\nInteraction Effects: While we analyzed main effects, complex interaction effects between factors (e.g., model type × scan type) were not fully explored due to sample size limitations.\n\nExternal Validity: Comparison with human performance relied on published benchmarks rather than direct controlled comparison.\n\n8. Summary of Statistical Findings\n\nThe statistical analysis supports the following key findings with high confidence:\n\nThe Pentest Agent System achieves a high success rate (95.0%, 95% CI [92.6, 97.4]) in exploiting the MS17-010 vulnerability.\n\nThe system completes penetration testing tasks significantly faster than human professionals (8.3 min vs. 30.5 min, p < 0.001) with comparable success rates.\n\nClaude-3.5-Sonnet outperforms GPT-4o mini across all performance metrics, with statistically significant differences (p < 0.05).\n\nSystem performance varies significantly across scan types, with quick scans offering the best balance of speed and success rate.\n\nThe system demonstrates good to excellent reliability across repeated measurements (ICC > 0.8).\n\nPerformance is robust against moderate variations in network conditions and target systems, but sensitive to scan timeout reductions.\n\nThese findings are statistically rigorous, with appropriate significance levels, confidence intervals, and robustness checks enhancing the credibility of our conclusions about the Pentest Agent System's performance.\n\nExperiments\nExperimental Setup\n\nTo evaluate the Pentest Agent System, we conducted experiments using two platforms. First, the \"Blue\" challenge on TryHackMe; second, a local Docker environment offered a controlled environment featuring a vulnerable Windows 7 system. This environment was selected to represent common enterprise vulnerabilities, particularly the MS17-010 (EternalBlue) vulnerability.\n\nThe experimental setup included:\n\nTarget Environment: Windows 7 with SMB services exposed and the MS17-010 vulnerability present.\n\nTesting Infrastructure: Ubuntu 22.04 LTS host running the Pentest Agent System with access to Nmap 7.93 and Metasploit Framework 6.3.4.\n\nNetwork Configuration: Isolated virtual network with the target and testing systems connected via OpenVPN to the TryHackMe platform.\n\nLLM Configuration: Tests were conducted using both GPT-4o mini and Claude-3.5-Sonnet models with temperature set to 0 for deterministic outputs.\n\nEvaluation Metrics\n\nThe system was evaluated using the following metrics:\n\nSuccess Rate: Percentage of successful exploitation attempts across multiple runs.\n\nTime Efficiency: The time required to complete the penetration testing cycle from reconnaissance to flag capture.\n\nTODO: Vulnerability Detection: Accurately identifying present vulnerabilities and avoiding false positives.\n\nMITRE ATT&CK Coverage: Number of successfully implemented MITRE ATT&CK techniques.\n\nError Recovery: Effectiveness of error handling mechanisms in recovering from failures.\n\nExperimental Procedure\n\nEach experiment followed this procedure:\n\nReset the target environment to a clean state.\nInitialize the Pentest Agent System with the default configuration.\nExecute the complete penetration testing cycle without human intervention.\nRecord all metrics and system outputs.\nRepeat the process 10 times to ensure statistical significance.\n\nAdditional experiments were conducted with intentionally introduced failures to test the system's error recovery capabilities.\n\nResults\nOverall Performance\n\nThe Pentest Agent System demonstrated high effectiveness in the experimental environment:\n\nSuccess Rate: 95% successful exploitation across all runs (19/20 attempts)\nTime Efficiency: Average completion time of 8.3 minutes (±1.2 minutes)\nVulnerability Detection: 100% detection of the MS17-010 vulnerability with no false positives\nMITRE ATT&CK Coverage: Successfully implemented 8 MITRE ATT&CK techniques across different tactics\nDetailed Findings\n\nReconnaissance Phase:\n\nSuccessfully identified open ports (135, 139, 445, 3389) in all runs\nCorrectly detected the MS17-010 vulnerability in all runs\nAverage scan time: 1.2 minutes\n\nExploitation Phase:\n\nSuccessfully exploited the MS17-010 vulnerability in 95% of runs\nEstablished Meterpreter sessions with SYSTEM privileges in all successful exploitations\nAverage exploitation time: 2.8 minutes\n\nPost-Exploitation Phase:\n\nSuccessfully executed commands in the target system in all successful exploitations\nLocated and extracted flag files in 100% of successful exploitations\nAverage post-exploitation time: 4.3 minutes\n\nLLM Performance Comparison:\n\nGPT-4o mini: 90% success rate, average completion time of 8.7 minutes\nClaude-3.5-Sonnet: 100% success rate, average completion time of 7.9 minutes\n\nError Recovery:\n\nSuccessfully recovered from 87% of intentionally introduced failures\nMost common recovery mechanism: step-level retries (68% of recoveries)\nAverage recovery time: 1.7 minutes\nComparative Analysis\n\nWhen compared to manual penetration testing of the same environment by security professionals:\n\nThe Pentest Agent System was 73% faster on average\nAchieved comparable success rates (95% vs. 100% for professionals)\nDemonstrated more consistent documentation and reporting\nShowed lower variability in execution time (standard deviation of 1.2 minutes vs. 4.8 minutes)\nConclusion\n\nThe Pentest Agent System represents a significant advancement in automated penetration testing. It combines multi-agent architecture, LLM-powered planning and analysis, and alignment with the MITRE ATT&CK framework. Our experiments demonstrate its effectiveness in identifying and exploiting vulnerabilities in controlled environments. Its performance is comparable to that of human security professionals but with greater consistency and efficiency.\n\nThe system's modular design and extensible architecture provide a foundation for future enhancements, including expanded technique coverage, additional tool integration, and machine learning components for adaptive attack planning. By automating the penetration testing process while maintaining alignment with established security frameworks, this system offers a promising approach to enhancing organizational security posture in an increasingly complex threat landscape.\n\nKey contributions of this work include:\n\nA novel multi-agent architecture for automated penetration testing\nIntegration of LLMs for security analysis and attack planning\nAlignment of automated testing with the MITRE ATT&CK framework\nEmpirical evaluation demonstrating the system's effectiveness\nOpen-source implementation promoting transparency and community involvement\n\nAs cyber threats continue to evolve, automated approaches like the Pentest Agent System will become increasingly important for maintaining robust security postures. Future work will focus on expanding the system's capabilities, addressing its limitations, and exploring its application in diverse security contexts.\n\nReferences\n\n[1] Rapid7, \"Metasploit Framework,\" 2023. [Online]. Available: \nhttps://www.metasploit.com/\n\n[2] OWASP Foundation, \"OWASP Zed Attack Proxy (ZAP),\" 2023. [Online]. Available: \nhttps://www.zaproxy.org/\n\n[3] PortSwigger Ltd, \"Burp Suite,\" 2023. [Online]. Available: \nhttps://portswigger.net/burp\n\n[4] Core Security, \"Core Impact,\" 2023. [Online]. Available: \nhttps://www.coresecurity.com/products/core-impact\n\n[5] Rapid7, \"InsightVM,\" 2023. [Online]. Available: \nhttps://www.rapid7.com/products/insightvm/\n\n[6] NullArray, \"AutoSploit,\" 2023. [Online]. Available: \nhttps://github.com/NullArray/AutoSploit\n\n[7] S. Moskal, S. J. Yang, and M. E. Kuhl, \"Cyber Threat Assessment via Attack Scenario Simulation Using an Integrated Adversary and Network Modeling Approach,\" Journal of Defense Modeling and Simulation, vol. 15, no. 1, pp. 13-29, 2018.\n\n[8] S. Jajodia, N. Park, F. Pierazzi, A. Pugliese, E. Serra, G. I. Simari, and V. S. Subrahmanian, \"A Probabilistic Logic of Cyber Deception,\" IEEE Transactions on Information Forensics and Security, vol. 12, no. 11, pp. 2532-2544, 2017.\n\n[9] C. Sarraute, O. Buffet, and J. Hoffmann, \"POMDPs Make Better Hackers: Accounting for Uncertainty in Penetration Testing,\" in Proceedings of the 26th AAAI Conference on Artificial Intelligence, 2012.\n\n[10] J. L. Obes, C. Sarraute, and G. Richarte, \"Attack Planning in the Real World,\" in Proceedings of the 2nd Workshop on Intelligent Security, 2010.\n\n[11] X. Shu, F. Araujo, D. L. Schales, M. P. Stoecklin, J. Jang, H. Huang, and J. R. Rao, \"Threat Intelligence Computing,\" in Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, 2018.\n\n[12] Y. Fang, Y. Li, L. Liu, C. Huang, H. Zhu, and M. Xu, \"DeepHunter: A Deep Learning Based Vulnerability Detection System,\" in Proceedings of the 2020 IEEE International Conference on Software Maintenance and Evolution, 2020.\n\n[13] MITRE Corporation, \"MITRE ATT&CK Framework,\" 2023. [Online]. Available: \nhttps://attack.mitre.org/\n\n[14] MITRE Corporation, \"Caldera,\" 2023. [Online]. Available: \nhttps://github.com/mitre/caldera\n\n[15] OpenAI, \"GPT-4o mini,\" 2024. [Online]. Available: \nhttps://openai.com/\n\n[16] Anthropic, \"Claude-3.5-Sonnet,\" 2024. [Online]. Available: \nhttps://www.anthropic.com/\n\n[17] LangChain, \"LangChain Framework,\" 2024. [Online]. Available: \nhttps://langchain.com/\n\n[18] Deno Land Inc., \"Deno Runtime,\" 2023. [Online]. Available: \nhttps://deno.land/\n\n[19] TryHackMe, \"Blue Room Challenge,\" 2023. [Online]. Available: \nhttps://tryhackme.com/room/blue\n\n[20] Microsoft, \"MS17-010 Security Bulletin,\" 2017. [Online]. Available: \nhttps://docs.microsoft.com/en-us/security-updates/securitybulletins/2017/ms17-010\n\nAcknowledgements\n\nWe want to express our gratitude to the following individuals and organizations for their contributions to this work:\n\nThe MITRE Corporation for developing and maintaining the ATT&CK framework, which provides the foundation for our approach to structured penetration testing.\n\nThe open-source security community, particularly the Nmap and Metasploit Framework developers, whose tools are integral to our system's functionality.\n\nTryHackMe for providing the \"Blue\" room challenge, which served as our primary testing environment.\n\nOpenAI and Anthropic for developing the language models that power our system's planning and analysis capabilities.\n\nThe LangChain community for creating and maintaining the framework that facilitates our integration of language models.\n\nThe Deno community for developing a modern JavaScript runtime that supports our TypeScript implementation.\n\nOur anonymous reviewers for their valuable feedback and suggestions that helped improve this paper.\n\nOur colleagues and research partners provided insights, testing support, and critical feedback throughout development.\n\nNo specific grant from public, commercial, or not-for-profit funding agencies supported this research.\n\nAppendix\nA. System Requirements\n\nThe Pentest Agent System requires the following components:\n\nOperating System: Ubuntu 20.04 LTS or newer (other Linux distributions may work but are not officially supported)\nRuntime Environments:\nDeno 1.37.0 or newer\nPython 3.8 or newer\nExternal Tools:\nNmap 7.80 or newer\nMetasploit Framework 6.0.0 or newer\nAPI Keys:\nOpenAI API key (for GPT-4o mini)\nAnthropic API key (for Claude-3.5-Sonnet)\nHardware Requirements:\nMinimum: 2GB RAM, 2 CPU cores, 10GB free disk space\nRecommended: 8GB RAM, 4 CPU cores, 20GB free disk space\nB. Installation Guide\n# Clone the repository\ngit clone https://github.com/your-username/pentest-agent-system.git\ncd pentest-agent-system\n\n# Set up Python environment\ncd python\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n\n# Set up API keys\nexport OPENAI_API_KEY=\"your-openai-api-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# Verify tool access\nmsfconsole -v\nnmap --version\n\n# Run the system\nstreamlit run python/main.py\nC. Example Attack Plan\n\nBelow is an example attack plan generated by the system for the \"Blue\" challenge:\n\n{\n  \"target\": \"10.10.10.10\",\n  \"objective\": \"Gain system access and locate flags\",\n  \"steps\": [\n\t{\n\t  \"id\": 1,\n\t  \"technique_id\": \"T1046\",\n\t  \"name\": \"Network Service Scanning\",\n\t  \"command\": \"nmap -sV -sC -p- --script vuln 10.10.10.10\",\n\t  \"expected_outcome\": \"Identify open ports and MS17-010 vulnerability\"\n\t},\n\t{\n\t  \"id\": 2,\n\t  \"technique_id\": \"T1190\",\n\t  \"name\": \"Exploit Public-Facing Application\",\n\t  \"command\": \"use exploit/windows/smb/ms17_010_eternalblue\",\n\t  \"expected_outcome\": \"Gain SYSTEM level access\"\n\t},\n\t{\n\t  \"id\": 3,\n\t  \"technique_id\": \"T1059\",\n\t  \"name\": \"Command and Scripting Interpreter\",\n\t  \"command\": \"shell\",\n\t  \"expected_outcome\": \"Obtain command shell\"\n\t},\n\t{\n\t  \"id\": 4,\n\t  \"technique_id\": \"T1083\",\n\t  \"name\": \"File and Directory Discovery\",\n\t  \"command\": \"dir C:\\\\Users\\\\Administrator\\\\Desktop\\\\\",\n\t  \"expected_outcome\": \"Locate flag files\"\n\t},\n\t{\n\t  \"id\": 5,\n\t  \"technique_id\": \"T1005\",\n\t  \"name\": \"Data from Local System\",\n\t  \"command\": \"type C:\\\\Users\\\\Administrator\\\\Desktop\\\\flag.txt\",\n\t  \"expected_outcome\": \"Extract flag content\"\n\t}\n  ]\n}\nD. Code Snippets\nD.1 Security Analyst Agent Prompt\nSECURITY_ANALYST_PROMPT = ChatPromptTemplate.from_messages([\n\t(\"system\", \"\"\"\n\tYou are a specialized cybersecurity analyst focused on vulnerability assessment.\n\tYour primary responsibility is to analyze nmap scan results, identify vulnerabilities,\n\tmap them to the MITRE ATT&CK framework, and generate detailed attack plans.\n\t\n\tWhen provided with nmap scan results, you should:\n\t1. Identify all potential vulnerabilities\n\t2. Map each vulnerability to the appropriate MITRE ATT&CK technique\n\t3. Generate a comprehensive attack plan\n\t4. Prioritize vulnerabilities based on severity\n\t\n\tUse the security_analyst tool to perform your analysis. This tool requires nmap scan results\n\tas input, which can be in JSON format (from the nmap_tool) or raw text format.\n\t\n\tIMPORTANT: The security_analyst tool must only be used with the results from a previous nmap scan.\n\tNEVER call security_analyst with an empty object or without nmap results.\n\t\n\tIf you need to run a new nmap scan, use the nmap_tool first, then pass its results to security_analyst.\n\t\"\"\"),\n\t(\"human\", \"{input}\"),\n\t(\"placeholder\", \"{agent_scratchpad}\")\n])\nD.2 MITRE ATT&CK Technique Implementation\nexport const MS17_010_TECHNIQUE: MitreAttackTechnique = {\n  id: \"T1190\",\n  name: \"Exploit Public-Facing Application\",\n  description: \"Exploit MS17-010 vulnerability in SMB service\",\n  tactic: \"initial-access\",\n  implementation: async (context: ExecutionContext): Promise<StepResult> => {\n\tconst { target, metasploitClient } = context;\n\t\n\t// Set up the exploit\n\tawait metasploitClient.execute(\"use exploit/windows/smb/ms17_010_eternalblue\");\n\tawait metasploitClient.execute(`set RHOSTS ${target.ip}`);\n\tawait metasploitClient.execute(\"set LHOST tun0\");\n\tawait metasploitClient.execute(\"set LPORT 4444\");\n\t\n\t// Run the exploit\n\tconst result = await metasploitClient.execute(\"exploit -j\");\n\t\n\t// Check for success\n\tconst sessions = await metasploitClient.execute(\"sessions -l\");\n\tconst success = sessions.includes(\"meterpreter\");\n\t\n\treturn {\n\t  success,\n\t  output: result,\n\t  artifacts: { sessions }\n\t};\n  },\n  requirements: [\"open-port:445\"],\n  provisions: [\"system-access\", \"meterpreter-session\"],\n  difficulty: \"medium\"\n};\nE. Performance Data\nMetric\tGPT-4o mini\tClaude-3.5-Sonnet\tManual Testing\nSuccess Rate\t90%\t100%\t100%\nAvg. Completion Time\t8.7 min\t7.9 min\t30.5 min\nVulnerability Detection\t100%\t100%\t100%\nFalse Positives\t0\t0\t0\nMITRE Techniques Used\t7\t8\t6\nError Recovery Rate\t82%\t92%\tN/A\nF. Future Work\n\nPlanned enhancements for future versions include:\n\nExpanded Technique Coverage: Implementing additional MITRE ATT&CK techniques beyond the current set, particularly in persistence, privilege escalation, and defense evasion.\n\nAdditional Tool Integration: Supporting more security tools beyond Nmap and Metasploit, such as Burp Suite for web application testing and Hashcat for password cracking.\n\nMachine Learning Components: Adding ML for adaptive attack planning based on previous results and environmental feedback.\n\nDistributed Architecture: Supporting multi-agent operations across multiple systems for more complex penetration testing scenarios.\n\nEnhanced Visualization: Adding real-time visualization of the attack progress and results for better operator understanding.\n\nNetwork-based Flag Submission: Implementing automatic submission of flags to validation systems for CTF-style challenges.\n\nImproved LLM Integration: We are exploring fine-tuning models specifically for security tasks to improve performance and reduce hallucinations.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nRelated work\n\nAutomated Penetration Testing Frameworks\n\nMulti-Agent Systems in Cybersecurity\n\nLLMs in Security Applications\n\nMethodology\n\nSystem Architecture\n\nData Models\n\nImplementation Details\n\nView all\nComments\nCode\nDatasets\nFiles\nREADME.md\nPROJECT_STRUCTURE.md\nYou might be interested\nFully Autonomous Offensive AI Agent: The Ultimate Cyber Adversary\nA\nY\nMar 11, 202534 reads\nAiAutonomous Agents+5\nAdaptiveFuzz: LLM-Based Adaptive Fuzzing for Vulnerability Discovery\nOct 20, 20252 reads\nAAIDCAAIDC2025+5\nAutonomous AgenticAI SOC Threat Modelling, Threat Hunting\nS\nMar 02, 202519 reads\nSecureFlow: Production-Ready Multi-Agent Financial Intelligence System\nSep 13, 202510 reads\nagentagent-orchestration+6",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "perception-system-simultaneous-localization-and-mapping-for-a-formula-student-driverless-race-car-88kVMsXAlxJP",
    "username": "sStefano Genetti",
    "license": "MIT License",
    "title": "Perception system. Simultaneous Localization and Mapping for a Formula Student Driverless Race Car",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n88 reads\n●\nMIT License\nWinner of\nBest Overall Project\nat the\nComputer Vision Projects Expo 2024\nPerception system. Simultaneous Localization and Mapping for a Formula Student Driverless Race Car\nS\nStefano Genetti\nS\n@stefano.dellanna\nS\n@sebastiano.taddei\nT\nThomas Nonis\nG\n@gabriele.stulzer\nLike\nBookmark\nShare\nPercepion System. Simultaneous Localization and Mapping for a Formula Student Driverless Race Car\n\nYear: 2024\n\nAbstract\n\nThis project introduces a prototype perception system for feature-based Visual Simultaneous Localization and Mapping (Visual SLAM), specifically designed for a Formula Student autonomous race car. The study focuses on achieving real-time localization and enhanced situational awareness by determining the vehicle's position and mapping its surroundings within a cone-defined track. To detect the cones, a fine-tuned version of the YOLOv8 object detection algorithm is utilized. The system's performance is evaluated using both indoor and outdoor video datasets captured with an Intel RealSense D455 RGB-D camera, tested under diverse lighting conditions and environmental settings.\n\nIntroduction and problem statement\n\nIn the Formula Student Driverless competition, our project focuses on developing a perception system that enables an autonomous vehicle to understand its environment and localize itself within an unknown circuit. The circuit is defined by cones of various types placed at different distances, which the system must detect and classify. This task, commonly referred to as Simultaneous Localization and Mapping (SLAM), involves processing sensor data to construct a map of the circuit while simultaneously determining the vehicle's position. The system must accurately report the vehicle's real-time coordinates within a reference frame and identify the locations and classifications of cones encountered during the lap, in compliance with competition rules.\n\n\nFigure 1\n\nThe report is structured as follows: we begin with a concise overview of relevant background concepts. Next, we describe the hardware and sensors utilized in our solution. We then outline our proposed methodology for developing the perception system, followed by a description of the experimental setup. Subsequently, we present and analyze the results. Finally, the \"Conclusion and Future Work\" section summarizes the key findings and explores potential research directions to improve our approach.\n\nBackground\n\nIn the following, we briefly summarize the main theoretical aspects related to this work.\n\nVisual Simultaneous Localization and Mapping\n\nThis project aims to address the SLAM (Simultaneous Localization and Mapping) problem by tracking the location of an autonomous agent over time while incrementally constructing a virtual representation of its environment. Leveraging the visual sensors at our disposal (see below), we utilize a Visual SLAM algorithm with an RGB-D camera setup.\n\nVisual SLAM approaches are typically categorized by camera configurations—Monocular, Stereo, Multi-view, and RGB-D—and by methodology: feature-based or direct methods. Feature-based methods estimate camera motion by minimizing the projection error between image features and a local map, while direct methods operate on image pixel intensities and are better suited for 3D reconstruction.\n\n\nFigure 2\nFor this work, we adopted OrbSLAM [1], a feature-based Visual SLAM system renowned for its real-time performance in diverse environments. We began by studying OrbSLAM2 [2] to understand its code structure, then progressed to OrbSLAM3 [3] which is, to the best of our knowledge, recognized as one of the most robust and accurate system in the field.\n\nOrbSLAM2\n\nOrbSLAM2 [2] takes its name from the famous local feature detector ORB (Oriented Fast and rotated Brief) (Figure 3). These features are used for tracking, mapping and placing recognition tasks. The implementation of OrbSLAM2 is open source (\nORB-SLAM2 GitHub repository\n). The system can work with monocular, stereo, and RGB-D camera setups. It also includes very useful tools like map reuse (i.e., it can load already mapped areas), loop closing (i.e., it can detect closed camera trajectories), camera relocalization (i.e., it can relocalize the camera inside the map when the system is no more able to track camera position) and localization (i.e., it can localize the camera inside a pre-existing map). Furthermore, according to the results presented in the original paper, the algorithm is suitable for real-time applications.\n\n\nFigure 3\n\nThe OrbSLAM2 working principle can be divided into three parts which also correspond to the three main threads:\n\nTracking: the system dedicates one thread to track and localize the camera for each frame by first, computing the image's features and then minimizing the reprojection error between the image features and a local map applying motion bundle adjustment (motion BA).\nLocal mapping: it manages the local map and updates it by means of local bundle adjustment (local BA).\nLoop closing: one thread is dedicated to detecting large loops and correcting the accumulated drift using pose-graph estimation. Once the pose graph is optimized, the system performs a full bundle adjustment (full BA) and updates the map.\n\n\nFigure 4\nOrbSLAM3\n\nAs described in the following figure, the computational pipeline of OrbSLAM3 is in line with the one of OrbSLAM2. However, OrbSLAM3 [3] stands out in our application due to its multiple map system and its capability to integrate IMU data. Furthermore, we have experimentally noticed that OrbSLAM3 sometimes excels where OrbSLAM2 fails in detecting loop closures.\n\nMultiple map system: while OrbSLAM2 sometimes gets completely lost during periods of poor visual information, OrbSLAM3 manages to survive. When unable to localize within the current map due to insufficient visual data, OrbSLAM3 seamlessly initiates a new map, later merging it with existing maps upon revisiting mapped areas.\nIMU data integration: To enhance the accuracy of localization and mapping outcomes, OrbSLAM3 offers the option to incorporate data from the inertial measurement unit (IMU). However, the results presented in this paper have been obtained without utilizing IMU sensor data.\nImproved loop closure identification: OrbSLAM3 successfully identifies revisits to previously explored locations, resulting in more frequent bundle adjustments that significantly enhance localization and mapping accuracy.\n\nSimilar to OrbSLAM2, the code for OrbSLAM3 is open source and can be accessed via the original author's \nGitHub repository\n.\n\n\nFigure 5\n\nHardware and sensors\n\nSolutions adopted by Formula Student teams often rely on expensive sensors such as professional cameras, multi-channel lidars, IMU, GPS, and ground speed sensors. The purpose of our research is to demonstrate that it is possible to develop a competitive driverless car without necessarily employing expensive equipment. In consonance with this, the sensor used for this project is an Intel RealSense D455 RGB-D camera. This hardware is reasonably cheap with respect to other commonly adopted solutions.\n\n\nFigure 6\nUsing the camera calibration procedure provided by OpenCV we performed a camera calibration of our visual sensor to get the camera matrix and distortion coefficients. To do so we acquired several pictures of the widely used chess board calibration pattern placed at different locations and orientations. Then, we relied on the OpenCV calibration functions to identify the correspondences between the set of 3D real-world points and their 2D coordinates in the images. Finally, to assess the calibration parameters's accuracy, we evaluated the re-projection error, following OpenCV's guidelines, achieving a 0.01 projection error.\nThe code for the camera calibration is publicly available at \nthis GitHub repository\n.\n\n\nFigure 7\n\n\nFigure 8\n\nMethodology\n\nThe overall idea of our methodology is reported in the following figure.\n\n\nFigure 9\nThe RealSense D455 RGB-D camera captures sensor data to perceive the circuit in which the agent operates, acting as a bridge between the agent and its environment. The video stream produced by the camera is processed by the perception system that combines localization and mapping components. After undergoing a post-processing phase (in the figure referred to as circuit enhancement), the algorithmic pipeline produces two output files. The first, named cones.csv, reports the positions and classifications of cones identified in the recorded video stream. The second, camera.csv, contains the coordinates of the camera throughout the video stream, expressed relative to a fixed reference system.\n\nAs mentioned at the beginning of this chapter, initially we focused on OrbSLAM2 algorithm as our starting point. This strategy allowed us to thoroughly examine the OrbSLAM codebase, serving as a preliminary step in comprehending its structure. In our \nGitHub repository\n on branch orbslam2, we provide the C++ code together with the instructions to execute OrbSLAM2 on a video sequence recorded by the Intel RealSense D455 camera. Upon completion of the execution, we store the map generated by the algorithm along with the camera's trajectory in a human-readable format (camera.csv). Additionally, throughout the execution, real-time coordinates of the camera in the actual environment are accessible, enhancing the localization capability.\n\n\nFigure 10\nOnce we became familiar with the structure of the OrbSLAM2 code, we shifted our focus to OrbSLAM3.\n\nCone detection and classification\n\nFor mapping purposes, we aim to estimate the coordinates of the cones that outline the circuit. In this regard we fine-tuned a version of YOLOv8 using the annotated images from the FSOCO dataset [4], a collaborative dataset for vision-based cone detection systems in Formula Student Driverless competitions. The classes of cones of interest are conveniently illustrated in the following figure.\n\n\nFigure 11\nThe obtained deep neural network model performs well across the training, validation, and test sets.\nIn the following figures, we report the outcome of the training and evaluation procedure.\n\n\nFigure 12: Confusion matrix.\n\n\nFigure 13: FSOCO training set.\n\n\nFigure 14: FSOCO validation set.\n\n\nFigure 15: Recall-confidence curve.\n\n\nFigure 16: Precision-recall curve.\n\n\nFigure 17: Precision-confidence curve.\n\n\nFigure 18: F1-confidence curve.\nTo integrate the cone detection module into the rest of the system, a custom C++ interface has been developed starting from the one provided in the YOLOv8 repository. The proposed interface uses the OpenCV DNN module to load the network in ONNX format with the capabilities to run it either with or without CUDA for GPU acceleration if available.\nThe original code has been modified as the originally provided example only works on bounding boxes and does not account for the segmentation output. With such modification, the interface produces a vector of bounding boxes and their respective masks.\nOur custom interface implementation in C++, along with the ONNX YOLO models, can be accessed publicly on \nour GitHub repository\n.\n\nCombining OrbSLAM3 and YOLOv8\n\nThe heart of the overall perception system is the integration of YOLOv8 in the OrbSLAM3 algorithm pipeline. The following figure illustrates the main software components of the perception system.\n\n\nFigure 19\nEach frame acquired by the camera is processed with our fine-tuned deep learning model, before the ORB extraction step of OrbSLAM3. By doing so, in the subsequent steps of the algorithm, we can filter only the visual features that belong to the bounding boxes of the cones within the current frame. For each ORB feature on the image, OrbSLAM3 keeps track of its corresponding map point in the three-dimensional real-world reference system. Leveraging this information, we gather a set of  point coordinates for each cone detected by our model. To condense the 3D points belonging to a cone  into a single representative triplet of coordinates, we calculate a centroid :\n\n\n\n\n\nFigure 20\nThe perception system updates at each frame the list of detected cones that populate the track throughout which the autonomous agent is traveling. Each cone is described by the following attributes:\n\nclass identifier: the class of the cone predicted by YOLO.\nhit counter: how many times the cone has been seen by the computer vision system.\nx, y, z: coordinates of the cone's centroid with respect to the real-world reference system.\nleft-right: this attribute is an integer value indicating the position of the cone relative to the track (see below). The number describes whether the cone is on the right side (1) or the left side (2) of the track. A value of 0 signifies that the position is unknown.\n\nWhenever YOLO detects a cone, we need to determine whether the cone has been previously identified in another frame or if it is a new detection. In the case of new detection, the cone needs to be added to the collection of detected cones. However, if the cone has been detected previously (which could happen for instance in consecutive frames), the corresponding hit counter attribute is incremented, and its position is refined based on the new localization estimation.\nA cone  is equal to a cone  if the following conditions are accomplished:\n\nThe class predicted by YOLO for  is the same as the one proposed for .\nThe euclidean distance between the 's centroid  and 's centroid  is below a hard-coded threshold , namely: \n\nUpon receiving a new frame from the input video stream, YOLO performs cone detection and classification, resulting in a set of detected cones within the image. Utilizing the methodology outlined above, OrbSLAM3 is employed to derive a triplet of 3D coordinates for each detected cone.\nHandling cone equality as defined earlier, we iterate the list of detected cones to determine whether each newly detected cone should be inserted for the first time or if it already exists in the vector.\nWhen processing a newly detected cone  with coordinates , and it is found to have been previously included in the list of cones with coordinates , we refine its current position estimation in the real-world reference system using the following approach:\n\nOnly cones with a hit counter value exceeding a predefined threshold are treated as valid cones and are subsequently visualized in the output. Cones failing to meet this criterion are deemed as noise and thus are neither visualized nor logged.\n\n\nFigure 21\n\n\nFigure 22\n\nLeft and right cones\n\nA central aspect for acquiring a comprehensive representation of the circuit involves distinguishing between cones located on the left and right sides of the vehicle. To address this, we have evaluated two potential solutions:\n\nAccording to competition regulations, track lanes are delineated by yellow and blue cones for the left and right lane boundaries, respectively. Therefore, we can easily determine whether a cone is positioned on the right or left boundary of the track by relying on the classifications provided by YOLO. This represents the simplest solution.\n\nAnother method to determine whether a cone is positioned on the left or right involves a geometric approach. Utilizing the reference systems illustrated in the following figure, we convert the 3D world coordinates  of each cone into their respective 3D coordinates within the camera reference system . Here, cones with a positive  value are situated in front of the vehicle, while cones with a negative  value are behind it. Additionally, cones with a positive  coordinate are on the right side, whereas cones with a negative  coordinate are on the left side.\n\n\nFigure 23\n\nExperimental setup\n\nIn this section, we provide an overview of the experimental design employed to evaluate the proposed framework.\n\nComputational setup\n\nTo ensure the solution could be implemented without requiring specialized hardware, thereby minimizing implementation costs, the experiments described in this report were conducted on a standard Windows 11 laptop. The system was equipped with a 14-core Intel i7-12700H @ 2.30 GHz and 32GB of RAM.\n\nDatasets\n\nTo evaluate the performance of the implemented perception system, we captured multiple datasets using the RealSense D455 camera through the RealSense Viewer software made available by Intel. All the datasets are made available at \nthis link\n.\n\nlab-otto: this dataset was captured within an indoor setting, specifically in the corridors of our university. To evaluate our algorithm's object detection capabilities, we strategically placed cones within the environment. Additionally, we intentionally included a loop closure within the dataset to assess the detection capabilities of OrbSLAM2 and OrbSLAM3. Furthermore, we measured the distances covered in the explored areas (see the next figure), allowing us to evaluate the accuracy of our scaling methodology.\noutdoor: this sequence was recorded outdoors, specifically in the car park of our university. To emulate the layout of a Formula Student driverless competition circuit, we strategically positioned cones along the path. Our movement within the circuit aimed to encompass a loop closure for evaluation purposes.\npovo1-interno: the recorded sequence navigates a complete path along the ground floor corridor of our university. There are no cones along the path. The light from the windows causes a lot of tricky visual artifacts which poses difficulties for OrbSLAM2 in certain runs. Conversely, OrbSLAM3 demonstrates enhanced robustness in this scenario, coping better with limited visual features. Notably, the sequence encapsulates a loop closure.\npovo-garage: This dataset replicates a circuit delineated by cones within an indoor setting. Precisely, the video sequence was captured in the underground car park of our university. Our trajectory intentionally encompasses a loop closure to assess the capabilities of our implementation.\n\n\nFigure 24\nResults\n\nThis section aims to present representative experiments showcasing the results achieved with our implemented perception system. First, we report and discuss a comparison of the performance of OrbSLAM2 and OrbSLAM3. Finally, we offer a demonstrative video showcasing the execution of our perception system on one of our indoor datasets.\n\nOrbSLAM2 and OrbSLAM3 comparison\n\nTo evaluate OrbSLAM2 and OrbSLAM3 performance, we compared the estimated camera trajectories obtained by the two algorithms executed on the four datasets we collected. In the following figure, the plots of the obtained trajectories are reported. The code that has been implemented to obtain these representations has been published on \neagle-driverless-orbSLAM GitHub repository\n on branch evaluation.\n\n\nFigure 25\n\n\nFigure 26\n\n\nFigure 27\n\n\nFigure 28\n\n\nFigure 29\n\n\nFigure 30\n\n\nFigure 31\n\n\nFigure 32\nUpon observing the trajectory curves we derive the following observations. While some datasets show nearly overlapping trajectories, others exhibit differences due to specific rotations and translations. This discrepancy is not concerning as both algorithms calculate accurate trajectories, albeit within slightly different reference systems.\n\nExecution demo\n\nTo present the achieved results, we offer \ntwo videos\n demonstrating the behavior of the perception system using the povo-garage dataset. The videos illustrate that the system produces reasonably high-quality output with satisfactory stability.\n\nThe first video demonstrates that YOLO appropriately recognizes and classifies cones within the captured frame. Additionally, different colors distinguish between ORB features within the bounding boxes of the cones and key points that do not belong to a cone.\n\nThe second video showcases the progressive mapping of the track and the simultaneous localization of the vehicle within it. Particularly, the orange squares represent the projection of the centroid coordinates of the cones within the world reference system. These squares are updated in every frame. The larger blue and yellow squares represent cone clusters. Blue is used to represent cones on the right side, while yellow represents cones on the left side.\n\nConclusions and future directions\n\nIn this work, we achieved a functioning initial version of a perception system aimed at solving an instance of the Simultaneous Localization and Mapping (SLAM) problem for a Formula Student racing car to drive an autonomous vehicle.\nThrough camera calibration, we were able to obtain intrinsic and extrinsic parameters of the Intel RealSense D455 camera.\nStarting the work with OrbSLAM2 allowed us to become familiar with the code structure proposed by the authors. This facilitated a smoother migration towards the Visual SLAM algorithm OrbSLAM3. Having a working solution of both OrbSLAM2 and OrbSLAM3 also allowed us to compare the performance achievable with the two algorithms.\nRegarding the mapping, the YOLOv8 model, on which we performed fine-tuning using the FSOCO dataset, proved to be efficient for properly detecting and classifying conses within the acquired frames. Integrating YOLOv8 into the algorithmic pipeline of OrbSLAM3 is at the core of the proposed solution and enabled us to effectively address the SLAM problem. By combining the two algorithms, we can select only the Oriented FAST and rotated BRIEF (ORB) visual features belonging to the bounding box of the cone, derive their corresponding coordinates relative to the world reference system, and thus obtain the coordinates of a single representative centroid of the recognized cone. Notably, we further refined the results by proposing a prototypical solution for cone clustering and distinguishing between right and left cones. The proposed solution was tested and achieved good performance on five datasets acquired by us.\n\nLimitations and future improvements\nConsidering segmentation masks rather than bounding boxes\n\nUpon analyzing the plot of cone centroids, we observe considerable noise in the estimation of their location. We attribute this noise to inferring cone coordinates from the ORBs within the bounding boxes encasing each other. Within these bounding boxes, there might exist visual features unrelated to the detected cones. To address this limitation, our proposed solution involves taking into account the segmentation mask of the cones rather than the bounding box and subsequently filtering the ORBs within this defined shape.\n\nIs it necessary to execute YOLO inference on every frame?\n\nAt present, the goodness of processing every acquired frame with YOLO remains uncertain. Although YOLO provides good real-time performance, being able to avoid running the object detection model on each frame of the video stream would reduce the computational cost of the algorithm. Moreover, it is reasonable to assume that the cones in two consecutive frames remain unchanged. Thus, it might be a prominent idea to utilize the model solely on OrbSLAM3 KeyFrames rather than on every frame.\n\nMultithreading\n\nCurrently, YOLOv8 is executed sequentially, in series, with respect to OrbSLAM3. A possible enhancement of this work would be to separate YOLOv8 and OrbSLAM3 into two distinct threads running in parallel. At that point, through appropriate process synchronization policies, it might be possible to optimize the interaction between YOLO and SLAM.\n\nContributions\nStefano Genetti, PhD Student University of Trento (Italy)\nThomas Nonis, MSc Student University of Trento (Italy)\nStefano Dell'Anna, PhD Student University of Trento (Italy)\nSebastiano Taddei, PhD Student University of Trento (Italy)\nGabriele Stulzer, MSc Student University of Trento (Italy)\nReferences\n\nORB-SLAM: A Versatile and Accurate Monocular SLAM System\n\nDOI: 10.1109/TRO.2015.2463671\n\nORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras\n\nDOI: 10.1109/TRO.2017.2705103\n\nORB-SLAM3: An Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap SLAM\n\nDOI: 10.1109/TRO.2021.3075644\n\nFSOCO: The Formula Student Objects in Context Dataset\n\nDOI: 10.4271/12-05-01-0003\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nPercepion System. Simultaneous Localization and Mapping for a Formula Student Driverless Race Car\n\nAbstract\n\nIntroduction and problem statement\n\nBackground\n\nVisual Simultaneous Localization and Mapping\n\nOrbSLAM2\n\nOrbSLAM3\n\nHardware and sensors\n\nMethodology\n\nCone detection and classification\n\nView all\nCode\nDatasets",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "percolate-agentic-orchestration-in-the-data-tier-mxHUnSzWmu7s",
    "username": "pSaoirse Amarteifio",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nMar 28, 2025\n●\n121 reads\n●\nMIT License\nWinner of\nBest Overall Project\nat the\nAgentic AI Innovation Challenge 2025\nPercolate - Agentic Orchestration in the Data Tier\nAgentic Memory\nAI\nInformation Retrieval\nLLM\nPostgresSQL\nP\nSaoirse Amarteifio\nLike\nBookmark\nShare\nAbstract\n\nPercolate\n (MIT license) is a framework for building complex agentic memory structures using Postgres.\n\nPercolate is built on the premise that in the coming years information retrieval and agentic orchestration will converge into the unified paradigm of Iterated Information Retrieval (IIR). The vision is that this paradigm will make way for a powerful class of agentic memory architectures that will support augmented general intelligence. Distinct from artificial general intelligence, augmented intelligence celebrates personalization, user data and human-AI co-creation.\n\nTo realize this, Percolate does something very different to other agentic frameworks which is to push agentic orchestration into the data tier. Percolate reimagines agentic orchestration and tool calling as an information retrieval task and builds this directly into the database. This means we see the problem of routing requests to the right agent and loading personalization context into memory as similar problems. A multi-modal build of Postgres, which supports all of vector, graph, key-value, relational and HTTP API calls is used as a foundation.\n\nWith Percolate, dynamic agentic workflows can be constructed either in the database or through a client such as the Percolate Python client, which works much like any other Pydantic-driven agentic SDK you may have used.\n\nIntroduction\n\nToday, agentic orchestration (typically in Python) and RAG/agentic memory solutions are generally treated as two separate concerns. Percolate was built on the premise that information retrieval and agentic orchestration will converge into a unified paradigm called Iterated Information Retrieval (IIR). This paradigm has the potential to make way for a powerful class of agentic memory architectures that can support augmented general intelligence. As language models become smaller, faster and cheaper, it will become increasingly feasible to use AI to build intelligent content indexes right inside the data tier.\n\nYou can see an early example of what this looks like in the MIT licensed project \nPercolate\n and you can check out this \nvideo\n to see how to get it running locally on Docker. (See also the basic examples given below.)\n\nIterated Information Retrieval combines (1) multimodal data indexes for graph, vector, key-value and relational data with (2) treating orchestration of agents and tool calls as an information retrieval problem. This means treating agents and tools as declarative metadata that can be stored, searched and called from the database rather than treating agents as code.\n\nTo make agents declarative we simply need to factor out any notion of “agents as code” by treating all tools as external to the “agent”, which then becomes simply (1) a system prompt with (2) structured schema, along with (3) a reference to external functions.\n\n\nPercolate allows for a completely data-orientated orchestration pattern for agents but you can also use clients in your favourite programming language (as we have implemented in Python for example). These clients become relatively thin compared to present day agentic frameworks such as Langchain/LangGraph, Pydantic.AI or Open AI's newer AgentSDK. A declarative framework avoids hard-coding graphs and agents in code, which instead, can be treated more fluidly as data. This is not always preferred - but in the limit, for complex systems with many agents and tools, a database is a great way to manage the complexity.\n\nBy doing this, agentic orchestration becomes an information retrieval problem and part of the more general agent-persistence and agent-memory challenge. Whether we are looking for agents or tools, digging up previous user conversations or searching over a knowledge base, these are all unified here as a query/routing problem.\n\nThe “iterated” in IIR refers to the fact that unlike traditional forms of information retrieval where a user initiates a query and gets a response, now an agentic system generates a query wave; the first response can provide content but also references to other content in the database in a “see also” format. This process is supported by language models that run \"offline\" as background processes to build multi-modal content indexes.\nAs the agent loop runs it is “prompted” by these references and can probe deeper before responding to the user or taking some other action on behalf of the user. The agent loop can discover not only data but other functions and agents that can be activated, possibly with a “handover” of context.\n\nAll agent frameworks execute an agent loop but Percolate builds this into the query interface between data and agent logic.\n\nIn the following sections I provide a brief historical overview of agent frameworks before describing the features of Percolate with some examples.\n\nA little history\n\nThis section is added for completeness and can be skipped by those familiar with the history of agentic frameworks and databases. It concludes with some of the main features of agent SDKs as of 2025 and emerging data-tier AI frameworks such as pgai.\n\nThe field of AI agent frameworks has seen rapid development and innovation over the past few years, with several key ideas and frameworks emerging to shape the industry. This review will explore the evolution of AI agent frameworks from 2023 to 2025, highlighting significant milestones and trends.\n\nIn the early stages, frameworks like LangChain emerged, focusing on handling large language models and text-based applications. LangChain focused on projects centered around chatbots, question-answering systems, and research tools. During this period, the concept of structured outputs gained traction, with frameworks emphasizing the importance of defining and validating agent outputs.\n\nAs the field progressed, there was a shift towards multi-agent systems and collaborative frameworks. Microsoft AutoGen, introduced in this period, represented an advancement in developing complex and specialized AI agents. AutoGen's multi-agent conversation framework and customizable agent roles allowed for more sophisticated AI-driven solutions. Simultaneously, CrewAI emerged, focusing on real-time collaboration and enabling multiple agents or humans and agents to work together effectively. CrewAI's Pythonic design and inter-agent communication features could be used for scenarios requiring coordinated teamwork.\n\nThe concept of graph-based architectures gained prominence with the introduction of frameworks like LangGraph. As an extension of LangChain, LangGraph used large language models to create stateful, multi-actor applications. This framework enabled more complex, interactive AI systems involving planning, reflection, and multi-agent coordination. LangFlow, another graph-based multi-agent framework, further solidified this trend, offering enhanced scalability and efficiency for managing multiple agents.\nOpenAI Swarm emerged as a lightweight, experimental framework for in multi-agent orchestration. Its handoff conversations feature and scalable architecture provided a flexible solution for managing multiple AI agents. Pydantic.AI introduced their own SDK building on the strengths of Pydantic, which had been so readily embraced by the community. This framework improved engineering standards compared to earlier frameworks, while also starting to think about graphs and workflow persistence.\n\nOpenAI released its Agents SDK in March 2025, offering developers a powerful toolkit for creating sophisticated AI agents. They implemented Agent Loops ie. automated process for tool calls and LLM interactions, handoffs i.e enabling seamless coordination between multiple agents, guardrails i.e. input validations to enhance security and reliability, tracing i.e built-in visualization and debugging tools for monitoring agent workflows. These illustrate what the industry perceives as critical features as of 2025.\n\nModern vector databases like Chroma and Pinecone have emerged as vector focused AI databases focusing on high-dimensional indexing for semantic search and similarity matching, real-time hybrid search combining dense/sparse vectors, cross-modal integration for tasks like text-to-image generation, dynamic personalization through continuous vector updates. The principle of Retrieval Augmented Generation (RAG) initially focused on vector embeddings for \"AI memory\". Vectors alone are limited and later applications merged graph and relational data for more interesting hybrid search.\nThe Postgres extension pg_vector allowed Postgres to support AI applications and many providers from Supabase, TimescaleDB, Lantern emphasize how this can be a better alternative to offerings such as Pinecone. With Postgres you have a mature open source ecosystem and can combine relational and semantic search.\n\nTimescaleDB's \npgai\n project is an illustration of a paradigm shift in agent frameworks by embedding AI workflows directly into PostgreSQL - just like Percolate.\n\nPercolate goes beyond these offerings by integrating not just data storage concerns but also implementing agentic orchestration in the database. Additionally, it models graph and entity-lookup query modalities.\n\nKey Principles of Percolate\n\nTo build agents in Percolate we use a similar approach to other frameworks as described in the previous section. Over the last few years the mantra “Pydantic is all you need” has become standard, leading even to Pydantic releasing their own Agent framework at the end of last year. However I would qualify that Pydantic is not all you need. Pydantic is more than you need and that is because Pydantic is (in the agentic context) simply a nice wrapper around Json Schema, which is a declarative way to express objects and structured output (for example in a database). Keep in mind that all language models today are accessed via JSON Rest APIs.\n\nSide note on motivations\n\nSince function calling was introduced in 2023, I was also leaning heavily into Pydantic, which was an obvious thing to do back then and I built an agentic framework called \nfunkyprompt\n based on these ideas. I wrote a series of articles on Medium about it, talking about building agentic systems from first principles starting with this \narticle\n.\n\nHowever, I found that increasingly the real challenges were information-retrieval related. And thats why, at the start of this year, I started working on Percolate to get to the heart of the matter, which is how to (a) perform queries over multimodal data and tools and (b) to route requests between different agents i.e. orchestrate agents.\n\nBelow is a breakdown of some of the main principles and components of Percolate.\n\nAgent and Tool storage and search\n\nPercolate stores agents and tools in the database. Agents are stored in p8.Agent table and the tools are stored in the p8.Function table. Functions can be either REST endpoints described by OpenAPI or native database functions. (We are currently working on adding MCP Servers).\n\nPlanning and routing\n\nAdding agents and tools to the database means they can be discovered by the Planner agent to dynamically load agents and functions. A help function is attached to all agents allowing for functions or agents to be recruited to a given cause. While loading dynamic functions is possible, agents are typically designed with their own functions by reference, which we will see in an example below. (If an agent has attached functions these functions can be activated at loop execution time. But if a user asks a question that is outside the scope of the prompt or functions that the agent has, it can ask for help and activate arbitrary functions.)\n\nNote: when we activate other Agents, we can execute them as black box functions. When we do this the same system prompt and message context is used in our main thread. We also have the option to do a handoff where we load the recruited agent's system prompt into the main thread.\n\nTracing sessions and AIResponses over turns\n\nWhen we run agents in Python or the database, the user questions are saved in the Session object. Additionally each request-response turn calling the language model API is logged in the AIResponse table. This object can have one or all of content, tool call data or tool call interpretation. As sessions are automatically logged this makes Percolate great for auditing agentic applications out of the box.\n\nLangauge Model Agnostic\n\nPercolate runs with any language model API via Python or the database. LLM Apis come in three major dialects all of which are implemented in Percolate.\n\nGoogle e.g. Gemini models\nAnthropic i.e. Claude models\nOpenAI (de facto standard): GPT and most other models that are not anthropic or google\n\nOne useful thing about Percolate is you can switch between these models and transform message payloads between dialects. For example you could start a conversation in one dialect e.g. Anthropic and then re-run it or resume it in another dialect e.g. OpenAI.\n\nTo learn more about how Percolate implements message structures and API calls in these dialects see this Medium \narticle\n\nMulti-modal indexing and search\n\nPercolate is not only an agent orchestration framework. It is also (and more importantly) a database and a multi-modal one at that. Percolate is built on Postgres and we have added extensions for;\n\nGraph (\nAGE Graph extension\n)\nVector (pg_vector \nextension\n)\nKey-value lookup (Using the graph)\nRelational\nHTTP (using the \nHTTP extension\n for making tool calls, requesting embeddings or calling language model APIs from the database).\n\nBy combining these modalities we can represent knowledge in different ways.\n\nThe basis of Percolate entities is structured, relational data. For example when we create agents, we store tables that represent their structure along with detailed field metadata. This field metadata, which can include field descriptions or information about embeddings, can be thought of as extended table metadata in Postgres. This metadata can be used to construct SQL queries from natural language from within the database.\n\nBeyond constructing relational predicates, we can also do semantic search since content fields on the entity that are labelled with embedding providers are automatically semantically indexed i.e. we compute embeddings for these fields. When data are inserted, a database trigger will run a worker to build an embedding table, which can later be used for vector searching that table.\n\nMore complex queries and indexes can include graph indexes. Graph indexes complement vector indexes which, alone, are insufficient for RAG applications. For an introduction to how we think about this in Percolate see this \narticle\n.\n\nIn summary, Percolate provides multiple ways to index content. Multi-modal indexes means AI/agents can use different types of queries that best suit the user's query. Indexes are constantly built in the background so the system learns over time.\n\nWalkthrough and Code Examples\nGetting started\n\nTo run Percolate locally, clone the \nrepo\n\ngit clone https://github.com/Percolation-Labs/percolate.git\n\nand from the repo, launch the docker container so you can connect with your preferred database client. This assumes you have installed Docker e.g. Docker Desktop.\n\ndocker compose up -d #The connection details are in the docker-compose file\n\nThe easiest way to get started is to run the init cli option - this will add some test data and also sync API tokens for using language models from your environment into your local database instance. This requires Poetry to be installed. Note it is assumed when you run the client by default that you have an OPEN_AI_API key for the default examples but any models can be added and used.\n\n#cd clients/python/percolate to use poetry to run the cli\n#the first time install the deps with `poetry install`\npoetry run p8 init\n\nThe above command will load standard API keys into your local environment and also add some test content for testing including indexing some Percolate code files for semantic searches and adding a test API to test function calling.\n\nYou can ask a question to make sure things are working. Using your preferred Postgres client log in to the database on port 5438 using password:password and ask a question.\n\nSelect * from percolate('How does Percolate make it easy to add AI to applications?')\nCreating and using an agent in Python\n\nWhile Percolate allows for agents to be run from the database, you can also use the Python client. Agents are just Pydantic objects. You can add the system prompt as a docstring and add fields to describe structured output. In Percolate all objects can be registered as database tables to save agent states. (Conversely, all tables are treated as agentic entities that you can talk to.)\n\nWhile you can add and use functions directly on your object, Percolate is best used with external functions because then agents can be saved declaratively and used in the database. Below is an example of referencing external functions. In this case it uses a get_pet_findByStatus, which is an endpoint from a test API registered in the initialization step above.\n\nimport percolate as p8\nfrom pydantic import BaseModel,Field\nimport typing\nimport uuid\n\nfrom percolate.models import DefaultEmbeddingField\n\nclass MyFirstAgent(BaseModel):\n    \"\"\"You are an agent that provides the information you are asked and a second random fact\"\"\"\n    #because it has no config it will save to the public database schema\n    \n    id : str | uuid.UUID\n    name: str = Field(description=\"Task name\")\n    #the default embedding field just settings json_schema_extra.embedding_provider so you can do that yourself\n    description:str = DefaultEmbeddingField(description=\"Task description\")\n    \n    @classmethod\n    def get_model_functions(cls):\n        \"\"\"i return a list of functions by key stored in the database\"\"\"\n        return {\n            'get_pet_findByStatus': \"a function i use to look up pets based on their status\",\n       }\n\nTo use this agent you can spin it up in Percolate.\n\nimport percolate as p8\nagent = p8.Agent(MyFirstAgent)\nagent('can you tell me about two pets that were sold')\nRunning agents from the database and resuming sessions\n\nPercolate's major strength is that it can run agents declaratively for example in the database.\nYou can register any agent using the repository for that object type...\n\np8.repository(MyFirstAgent).register()\n\nThis does a number of things:\n\nIt creates a table based on the given schema, which the agent can use to save structured state.\nIt adds database artifacts for managing column embeddings and for triggering embedding creation.\nIt adds other metadata for managing entities and graph relationships.\nIt adds the agent to the list of agents in the database so it can be searched and used dynamically.\n\nIn the database we can now run\n\nselect * from percolate_with_agent('list some pets that are sold', 'MyFirstAgent')\n\nThis is equivalent to using p8.Agent(MyFirstAgent).run('list some pets that are sold') except now its run entirely in the database. This means; discovering the agent and prompt (which has external function called get_pet_findByStatus to load); loading the function by name and \"activating\" it; calling the function and summarizing the data.\n\nEach iteration or turn is audited in the database in the AIResponse table and its possible to resume sessions from their saved state.\n\nNote: The examples here run synchronous agents just to illustrate. In production systems a background worker is used to manage turns without locking database resources.\n\nOther examples\n\nTo see more examples you can check out the following Medium articles;\n\nThe Basics\nIntegrating with Claude Desktop and MCP to do deep research\nWhy Percolate\nKey Features\n✅ Storage and search for agents and tools\n✅ Data tier or Python agent orchestrator with dynamic function loading and agent handoffs\n✅ Tracing and auditing for free as everything is persisted by default\n✅ Workers to automatically build vector and graph indexes\n✅ Multi-modal entity search that combines SQL predicates, semantic search and graph relationships\n✅ OpenAPI function proxy\n✅ Works with any language model using Google, Anthropic or OpenAI API (standard) dialects\n✅ Session & turn auditing\n✅ MCP client for attaching MCP servers (coming soon)\n✅ Managed cloud recipes for Kubernetes (coming soon)\nKey Use cases\n✅ Persistence proxy when calling LLMs/Services\n✅ Personalization layer to track conversation and integrate data into your agent workflows\n✅ Research assistants\n✅ Implementing multi-modal RAG\nTech stack and languages\nDocker/Kubernetes\nPostgres / Cloud Native Postgres Operator for Kubernetes\nSQL\nCypher\nPython\nFastHTML admin dashboard (coming soon)\nThe Future\n\nThis is only the beginning. Percolate is a very young project started in January of this year. Percolate is an ambitious project that aims to go beyond what today's agent frameworks can do by pioneering a convergence between agent orchestration and information retrieval. Data and memory are what power agentic systems. \"Solving\" memory is the holy grail for achieving augmented general intelligence, where humans and AI work together to create at the speed of thought.\n\nPlease take a moment to check out the repository and get back to us with issues, feature requests and general comments. If Percolate sounds interesting to you please give the \nrepo\n a ⭐\n\nStay in touch by subscribing to our conceptual overviews on \nSubstack\n and our more granular, technical deep-dives over on \nMedium\n.\n\nThanks for reading about Percolate!\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nA little history\n\nKey Principles of Percolate\n\nSide note on motivations\n\nAgent and Tool storage and search\n\nPlanning and routing\n\nTracing sessions and AIResponses over turns\n\nLangauge Model Agnostic\n\nMulti-modal indexing and search\n\nView all\nComments",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "personal-assistant-to-manage-your-google-accounts-MOfRfFxDmNj1",
    "username": "Manuel Quesada",
    "license": null,
    "title": "Personal Assistant to Manage your Google Accounts",
    "publication_description": "Back to publications\nMar 22, 2025\n●\n137 reads\n●\nCreative Commons Attribution-NonCommercial (CC BY-NC)\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nPersonal Assistant to Manage your Google Accounts\nAgent Orchestration\nCustom Agent Hierarchical Arch\nLangChain\nPersonal AI assistants\nTooling\nManuel Quesada\nLike\nBookmark\nShare\n🤖🧠🤖 Personal Assistant to Manage Your Google Accounts\n\nAgents are here to stay, and they bring new possibilities to large language models (LLMs) that, just six months ago, faced significant limitations. This Minimum Viable Product (MVP) aims to develop an agent-based system capable of efficiently managing our emails and calendars. This system, designed to handle personal and work-related Google accounts, aims to provide a comprehensive solution for managing these essential services.\n\nThe core approach of this MVP is developing a multi-agent system, leveraging foundational techniques for building agent-based architectures. The idea of decomposing an application into independent agents and then integrating them into a multi-agent system helps mitigate various challenges. These agents can range from something as simple as a prompt that invokes an LLM to something as sophisticated as a ReAct agent, a general-purpose architecture that combines three key concepts:\n\nTool Calling: The LLM can select and use various tools as needed, extending its ability to perform complex tasks.\nMemory: Enables the agent to retain and utilize information from previous steps, allowing for greater coherence and continuity in interactions.\nPlanning: Empowers the LLM to create and follow multi-step plans to achieve long-term goals, enhancing the agent’s effectiveness in handling complex tasks.\n\nThis modular and flexible approach supports greater customization and ensures that the system remains scalable and easily adaptable to future advancements in artificial intelligence and automation.\n\n👤👤 \nMulti-Agent System\n\nThere are several ways to connect agents, as outlined below:\n\nNetwork: Each agent can communicate with every other agent. Any agent may decide which other agent to call next.\n\nSupervisor: Each agent communicates with a single supervisor agent. The supervisor makes decisions about which agent should be called next.\n\nSupervisor (\ntool-calling\n): This is a special case of the supervisor architecture. Individual agents are represented as tools. In this configuration, the supervisor agent uses a tool-calling LLM to decide which agent tools to invoke and with what arguments.\n\nHierarchical: A multi-agent system can be defined with a \nsupervisor of supervisors\n. This generalization of the supervisor architecture enables more complex control flows.\n\nCustom Multi-Agent Workflow: Each agent communicates with only a subset of other agents. Some parts of the flow are deterministic, while only specific agents have the authority to decide which agents to call next.\n\nKey Benefits of Multi-Agent Systems:\n\nModularity: Isolated agents simplify agentic systems' development, testing, and maintenance.\n\nSpecialization: Agents can be designed to focus on specific domains, which improves overall system performance.\n\nControl: The system designer has explicit control over agent communication paths, as opposed to relying solely on function calling.\n\nBased on these \narchitectures\n, we propose an agent-based design that aligns well with the \ndivide and conquer\n principle. Additionally, incorporating new managers becomes a straightforward task—it only requires defining the necessary tools, building the appropriate ReAct agents, integrating them with a manager or supervisor, and registering that manager or supervisor with the orchestrator. We will discuss this architecture in detail in the following sections.\n\nMVP Details\n💬 LLM\n\nThe MVP uses \nOpenAI\n as the language model provider, with the temperature parameter set to 0. This configuration ensures the responses are fully deterministic and reproducible, avoiding random variations in the model’s output. This setup is particularly useful for evaluating the quality of the responses, as it allows consistent comparison across different prompts and iterations.\n\nFramework used ⛓️\n\nLangChain\n is a framework designed to simplify the development of applications that LLMs, enabling the composition of complex reasoning and action chains. Thanks to its flexibility and integration with tools like LangSmith and LangGraph, developers can build, debug, and scale LLM applications in a more structured and efficient way.\n\nLangGraph\n: Enables orchestration of complex workflows with multiple steps, conditional logic, branching, and loops, which is especially useful for developing AI agents that need to maintain state, execute tasks in parallel, or dynamically adapt to outcomes. Its graph-based architecture helps visualize and understand the flow of information and decision-making within the agent. Moreover, by integrating directly with LangChain, developers can build intelligent and controlled agents without implementing control logic from scratch. This accelerates development and supports rapid iteration, making it ideal for prototyping and production environments.\n\nLangSmith\n: Complements the ecosystem by focusing on monitoring, evaluating, and debugging chains and agents built with LangChain. It provides full traceability of each execution, including model messages, function calls, inputs, outputs, and errors, allowing for a precise understanding of how the application behaves at every step. Additionally, it supports automated evaluations, result comparisons across versions, and regression detection. This detailed visibility is key to ensuring quality, performance, and reliability in LLM-based applications.\n\nTogether, these tools form a powerful development stack that covers the entire lifecycle of building intelligent agents and interfaces—from modular construction (LangChain), to orchestration and management of complex logic (LangGraph), to continuous monitoring, evaluation, and improvement (LangSmith). This significantly reduces development friction and time, making the creation of LLM-powered solutions much more scalable, maintainable, and robust.\n\nTools 🛠️\n\nSince this is an MVP, we aim to avoid building the tools our agents will use from scratch. Therefore, we opted for one of the available community-driven solutions: \nComposio-dev\n.\n\nComposio is an integration platform that empowers AI agents and LLM applications to connect seamlessly with external tools, services, and APIs. With Composio, developers can rapidly build powerful AI applications without the overhead of managing complex integration workflows.\n\nMVP Functionality ⏰ 📅 📧\n\nGeneral Information Queries: Respond to general questions outside the calendar and email management scope.\n\nAccount Handling: Distinguishes between personal and work accounts, assigning requests to the appropriate account based on user context.\n\nDate Management and Interpretation: Leverages the date manager to interpret and compute dates according to user requests.\n\nInterpret dates for events or tasks mentioned by the user.\nCalculate date ranges (e.g., \"the last week of this month\").\nConvert relative dates (e.g., \"in three days\") into absolute dates based on the current date system.\n\nCalendar Management: Manages events using the date manager to validate availability and avoid conflicts.\n\nCreate events.\nUpdate existing events.\nCheck availability for specific times and calendar space.\nHandle scheduling conflicts before creating or updating events.\nDelete existing events.\n\nEmail Management: Sends, replies to, and organizes emails.\n\nSend emails to specific addresses.\nCreate email drafts before sending.\nReply to existing email threads.\nQuery emails and list threads using filters.\nRetrieve detailed information from a specific message within a thread.\nArchitecture Used 🏛️\n\nOrchestrator Input 🔗\n\nThe Orchestrator Input node uses structured output to generate an action plan, which is translated into a list of managers that need to be called before responding to the user. Given the user's request and the message history, its main goal is to return a list of managers to be invoked along with the specific query to be sent to each of them.\n\nMemory 🔗\n\nFor memory management, we leverage LangGraph’s graph-based workflow execution to store checkpoints in PostgreSQL. In LangGraph, “checkpoints” are snapshots of the graph’s state at specific moments during execution. These snapshots allow the system to pause processing at defined points, save the current state, and resume execution later as needed. This feature is particularly useful for long-running tasks, workflows requiring human intervention, or applications that benefit from resumable execution.\n\nTo avoid passing the entire memory context to the language model, we use a timer mechanism to provide only the last 5 user interactions as conversational history.\n\nOrchestrator Output 🔗\n\nThis node is responsible for generating the final response to the user by aggregating all the outputs from the managers. It also has access to the message history. Additionally, it formats the response using Markdown to improve the readability of the output.\n\nIf the Orchestrator Input determined that the request was a general question and no manager needs to be called, this node is also in charge of answering the user’s request using its general knowledge, system prompt, and the conversation history.\n\nOrchestrator Executor 🔗\n\nThis node is responsible for calling each manager in sequence while providing context from previously called managers to the next one. This allows any manager who depends on information from a previous one to access it, effectively resolving inter-manager dependencies. The Orchestrator Input node must be capable of generating an action plan that respects these dependencies, where the order of execution matters.\n\nGPT-4o instead of GPT-4o-mini: Our solution initially used GPT-4o-mini because it provides faster responses and is more cost-effective regarding token usage costs. It worked well up to a certain point — even for accessing tools and selecting the right one when the LLM acted as an Agent.\n\n \n\nHowever, during the iterative refinement of the assistant, we observed that GPT-4o-mini could not generate the action plan correctly when the reasoning complexity increased properly. Here's an example that highlights this limitation:\n\nWe used the same prompt and simply switched the model to illustrate the inconsistency:\n\n\nIn the image above, the model failed to recognize that it needed to call all three managers to fully address the user's request. In contrast, GPT-4o handles this perfectly. In its output, we can clearly see a list containing the calls to each manager along with their respective queries:\n\n\nDate Manager 🔗\n\nThis manager handles any request related to dates and time—calculating a time interval or extracting the correct date from the user's initial query.\n\nFor example, LLMs often struggle with determining future dates, such as \"next week\" or \"next month,\" even when the current date is provided. To address this limitation, we designed a system prompt using the Few-Shot Prompting technique, where we include a series of examples with their corresponding dates and expected outputs. This approach allows us to achieve high accuracy in the generated responses.\n\nCalendar Manager 🔗\n\nThis manager acts as a router, responsible for forwarding the query to the appropriate worker. To do so, it uses the following as context:\n\nThe responses from any managers that were previously called.\nThe original query that was sent by the Orchestrator Input.\n\nEach worker represents a specific account. In our case:\n\nThe personal account.\nThe work account.\n\nSome requests require checking both accounts (e.g., verifying calendar availability), while others affect only one. The personal account is assumed by default if the user does not specify an account.\n\nLet’s consider the following user request:\n\n\"Create an event tomorrow afternoon to play soccer.\"\n\nThe flow would be as follows:\n\nThe Date Manager is consulted to determine the time range corresponding to \"tomorrow afternoon.\"\nThe router manager forwards the query to the appropriate worker with this information.\nSince the user did not specify an account, the personal account worker is selected.\nThe worker processes the request and creates the event in the calendar.\n\nAs a result, the manager generates the following query to be executed on the personal account:\n\nCreate an event for tomorrow, March 25, after 12 PM.\n\nCalendar Worker 🔗\n\nThe Calendar Worker operates as a ReAct agent without memory of previous messages. This means it does not retain context from prior interactions—its true strength lies in its ability to interact with the tools at its disposal. These tools are essential to its functionality and enable it to perform calendar-related tasks.\n\nBelow are the tools the Calendar Worker has access to, along with how it uses each one to manage events:\n\nGOOGLECALENDAR_FIND_EVENT\nThis tool is used to search for existing events in the calendar. When the Calendar Worker needs to verify a specific event (e.g., before updating or deleting it), it performs a search using this tool.\n\nGOOGLECALENDAR_UPDATE_EVENT\nThe UPDATE EVENT tool is used to modify existing events in the calendar. If an event needs to be updated (such as changing the time, adding participants, or adjusting details), the Calendar Worker uses this tool. Before performing an update, the worker checks for the event’s existence and, in some cases, performs a conflict check depending on the nature of the change.\n\nGOOGLECALENDAR_CREATE_EVENT\nThis tool allows the Calendar Worker to create new events in the calendar. When a user requests to add an event (like a meeting or appointment), the worker uses this tool to create it in the appropriate calendar. Before doing so, it checks for time availability using the FIND FREE SLOTS tool.\n\nGOOGLECALENDAR_FIND_FREE_SLOTS\nThis tool is critical for the Calendar Worker, allowing it to check for available time slots in the calendar. Before creating or updating an event, the worker uses this tool to ensure there are no scheduling conflicts. If a conflict is detected (e.g., an event already scheduled in the requested time range), the worker notifies the user and does not proceed with the creation or update.\n\nGOOGLECALENDAR_DELETE_EVENT\nWhen a user requests to delete an event, the Calendar Worker uses this tool to remove existing events. This action is performed after confirming which event to delete, based on the user’s input or a prior search using FIND EVENT.\n\nSince it does not maintain the memory of past messages, the Calendar Worker focuses on efficient interaction with these tools. Each time it receives a request, it evaluates the situation based on the current context and toolset. This ensures that every action is performed independently and accurately, relying solely on the tools and data available at that moment.\n\nEmail Manager 🔗\n\nThe Email Manager functions similarly to the Calendar Manager. Like the calendar manager, it acts as a router, directing the request to the appropriate worker. In this case, each worker manages one of the user’s email accounts—either the personal or the work account.\n\nThe personal account is assumed by default if the user does not specify an account. The workflow follows the same pattern as the calendar manager: context is gathered first, and then the appropriate worker performs the requested action.\n\nEmail Worker 🔗\n\nJust like the Calendar Worker, the Email Worker operates as a ReAct agent without memory of previous messages. This means it does not retain information from past interactions, and its ability to manage email depends entirely on the tools it has access to at the time. Below are the tools available to the Email Worker, along with how each one is used to perform email-related tasks:\n\nGMAIL_REPLY_TO_THREAD\nThis tool is used to reply to an existing email conversation. The Email Worker identifies the message to respond to and uses this tool to send a reply. It ensures the response is sent to the correct recipient, either the original sender or another recipient specified by the user.\n\nGMAIL_LIST_THREADS\nUsed to list email conversations. The Email Worker utilizes this tool to retrieve the most recent threads from the inbox or labeled folders, filtered according to the user's criteria.\n\nGMAIL_SEND_EMAIL\nAllows the sending of new emails. When the Email Worker receives a request to send an email, it uses this tool. If the user does not provide a recipient, the worker does not proceed with sending and responds that a recipient must be specified. It also ensures the subject line is appropriate and that the body of the message is properly formatted.\n\nGMAIL_FETCH_EMAILS\nThis tool enables the search and retrieval of emails from the inbox or other folders. The Email Worker can search by keywords in the subject or body, sender or recipient address, and can filter by labels such as \"INBOX,\" \"SENT,\" \"DRAFT,\" and so on.\n\nGMAIL_CREATE_EMAIL_DRAFT\nThe Email Worker uses this tool to create email drafts. The worker saves the content as a draft if the user wants to compose an email without sending it immediately. A placeholder example is used to generate the draft if no recipient is specified.\n\nGMAIL_FETCH_MESSAGE_BY_THREAD_ID\nThis tool is used to retrieve all messages from a specific conversation using its thread ID. The Email Worker uses it to gather full context from the conversation, ensuring that any reply is well-informed and relevant.\n\nMVP Deploy Guide 📝\n\nBefore running the MVP, ensure you have:\n\nA Composio account\nA Telegram Bot\nAPI keys for Composio and OpenAI\nDocker installed\n\n1. Set Up Composio\n\nCreate a Composio account at \nComposio-dev\n. The free tier allows up to 2,000 API calls per month.\nGenerate an API key and add it to the .env file:\nCOMPOSIO_API_KEY=\"your_api_key_here\"\nIntegrate Composio with Gmail and Google Calendar:\nConnect both your work and personal Gmail accounts at \nGmail Integration\n.\nConnect both your work and personal Google Calendars at \nGoogle Calendar Integration\n.\nIn Composio use the following entity_id values for the integrations:\nENTITY_ID_WORK_ACCOUNT=\"work\"\nENTITY_ID_PERSONA_ACCOUNTL=\"personal\"\n\n2. Set Up OpenAI API Key\n\nAdd your OpenAI API key to the .env file:\nOPENAI_API_KEY=\"your_openai_api_key_here\"\n\n3. Set Up Telegram Bot\n\nCreate a new Telegram bot following \nthis guide\n.\nObtain your bot token and add it to the .env file:\nTELEGRAM_BOT_TOKEN=\"your_telegram_bot_token_here\"\n\n4. Run the MVP\n\nFind the code in the attached compressed file named ai-assistant-mvp.zip.\nEnsure you have Docker installed. To start the project, navigate to the root directory and run:\ndocker compose up\n\nOnce the containers are running, you can interact with your assistant via Telegram. Since we are using OpenAI services via API to run the system, large computational requirements are unnecessary. A PC with at least 8 GB of RAM would be sufficient.\n\nConcluding Remarks 🧩\n\nThroughout the development and testing of this MVP, I interacted with the assistant via Telegram and closely monitored its behavior using LangSmith. This constant feedback loop revealed that, while the assistant performs well, there are still many production-level details to refine—ranging from minor prompt tweaks in specific nodes to adding new tools to support edge cases.\n\nDespite these areas for improvement, the assistant consistently demonstrated the ability to handle complex tasks that required coordination between different services—such as retrieving availability from Google Calendar and then drafting emails in Gmail based on that context. In several instances, I was genuinely surprised by the system’s dynamic behavior. Rather than simply following a static workflow, the agent-based architecture allowed the assistant to make independent decisions and adapt to the input, showcasing its flexibility and reasoning capabilities.\n\nThis experience validated not only the technical feasibility of the multi-agent architecture but also its suitability for real-world business logic. Beyond simply automating static departmental workflows, this architecture is capable of adapting to dynamic, context-sensitive tasks that require reasoning and decision-making based on the current environment. A key strength lies in its ability to interact with the external world, not just operate based on predefined instructions or hard-coded logic. This is made possible by integrating tools within the agent system, which provides real-time access to external data and enables grounded decision-making based on up-to-date information. This approach aligns with the increasing industry trend toward agentic AI systems—capable not just of executing but of intelligently orchestrating actions across tools and services in a context-aware manner.\nFrom an industry perspective, the demand for personal AI assistants and task automation agents is growing rapidly, driven by the need to reduce repetitive workloads, improve decision-making support, and increase operational efficiency. Architectures like this one position themselves as a natural fit for enterprise use cases that require scalable, modular, and intelligent orchestration of tools and information systems.\n\nOther key takeaways:\nGPT-4o clearly outperformed GPT-4o-mini in tasks that required deeper reasoning and multi-step planning.\nThe modular nature of the architecture greatly simplified the process of isolating, debugging, and improving specific components.\nComposio enabled smooth integration with external APIs. However, ensuring clarity in API responses and robust error handling remains crucial for long-term reliability.\n\nIn conclusion, this MVP produced accurate and valuable results during testing. It not only validated the design principles and toolchain used but also highlighted the potential of this approach to evolve into a robust, production-ready assistant. The insights gained provide a strong foundation for future iterations—bringing us closer to building AI agents that can reason, adapt, and truly assist across various domains.\n\nActual Limitations 🚧\n\nWhile this MVP demonstrates the technical feasibility and adaptability of an agent-based architecture, it also presents several important limitations that must be considered for its evolution into a more robust, production-ready system:\n\nDependency on External Integrations (Composio): The current system relies on Composio as an intermediary platform to perform actions on Google services (Gmail and Calendar). While this accelerates initial development and avoids building integrations from scratch, it also introduces a critical external dependency. If Composio experiences service outages, limits functionality, or changes its usage policy, the assistant may become non-functional. Additionally, this dependency may impact overall performance and introduce constraints regarding scalability, data privacy, and long-term control.\n\nLack of Native Tools for Calendar Conflict Management: Currently, calendar conflict detection relies primarily on the LLM's reasoning ability, which is not always reliable, especially when using lighter models such as GPT-4o-mini. Ideally, a custom-built conflict management tool should be implemented to handle overlapping events, enforce custom rules or prioritization logic, and improve precision in availability checks. This would enhance the reliability and user experience of the assistant.\n\nResponse Latency Due to Hierarchical Flow: The adopted architecture—based on multiple hierarchical layers (orchestrator, managers, workers)—offers modularity and clear separation of concerns but also introduces response latency. In real-time scenarios (e.g., conversational assistants), this design can become suboptimal. Each user request must pass through several steps before a final response is generated, which can lead to delays that negatively impact user perception.\n\nOne Worker per Account: Limited Scalability: In the current implementation, each worker is tightly coupled to a specific account (personal or work), which limits the scalability of the system. This structure requires the creation of a new worker for every new account to be supported, resulting in redundancy and increased maintenance overhead. A better approach would involve dynamic, parameterized workers that receive account identifiers or credentials at runtime, enabling more flexible and reusable logic.\n\nLack of Execution Validation Mechanisms: Currently, the Orchestrator Executor executes the defined action plan without intermediate validation mechanisms to ensure that each step is completed correctly. This can result in a manager producing an incorrect or incomplete output. Yet, subsequent steps continue as if the result were valid—leading to incorrect or incoherent final responses. To mitigate this, intermediate output validation and quality checks should be introduced, along with fallback or recovery strategies in the event of partial failures, to ensure overall workflow robustness in real-world usage.\n\nFuture Work 📋\n\nWhile the current MVP lays a solid foundation, several areas have been identified for future improvement and development:\n\nPrompt Refinement & Tool Expansion: Fine-tuning specific node prompts and expanding the system’s toolset will improve reliability and adaptability in more complex scenarios.\nModel Optimization: Continue leveraging more capable models (like GPT-4o) for deep reasoning tasks while exploring hybrid strategies to balance performance and cost.\nIntegration of Additional Managers: Incorporate new functional modules, such as a contact manager, web search manager, or even enterprise data connectors, to enrich the assistant's capabilities and make it more versatile in business settings.\nImproved Error Handling: Enhance the system’s ability to handle tool/API failures gracefully, especially under high uncertainty or incomplete user input.\nFine-tuning: Perform fine-tuning to ensure more tailored responses with less margin of error.\nScalability Testing: Conduct stress tests with more users and concurrent workflows to ensure the architecture scales effectively in production environments.\n\nThis roadmap aims to transition the assistant from an MVP to a robust, production-grade system capable of supporting real-world, decision-driven workflows in personal and enterprise contexts.\n\nContact & Support 📬\n\nIf you have any questions, encounter issues, or would like to contribute to this project, feel free to reach out:\n\n📧 Email: \nmalejandroquesada@gmail.com\n🐙 GitHub: \nMAQuesada\n\nWe welcome feedback, contributions, and collaborations!\n\nSome Examples of use 🎯\n\nHere are some examples of the agent’s capabilities. We encourage you to try them out yourself. 🤖\n\n\n\n\n\n\n\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\n🤖🧠🤖 Personal Assistant to Manage Your Google Accounts\n\n👤👤 \nMulti-Agent System\n\nKey Benefits of Multi-Agent Systems:\n\nMVP Details\n\n💬 LLM\n\nFramework used ⛓️\n\nTools 🛠️\n\nMVP Functionality ⏰ 📅 📧\n\nArchitecture Used 🏛️\n\nOrchestrator Input 🔗\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nFiles\nai-assistant-mvp.zip\nYou might be interested\nGoogle Workspace Agent: Your AI Assistant for Seamless Workspace Management 🤖🚀\nU\nJul 14, 202529 reads\nAgentic AIGoogle-Workspace+2\nGoogle Workspace Agent: From Prototype to Real-World Readiness 🚀\nU\nSep 06, 2025130 reads\nAgenticAIAIProductization+3\nWhat is Agentic AI (AAIDC-Week1-Lesson-1)\nMay 15, 20255357 reads\nAAIDCAgentic AI+5\nAgentic AI Applications\nDistinguished Technical Deep-Dive\nY\nMar 24, 202583 reads\nAgentic AIArtificial intelligence+3",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "personal-portfolio-assistant-wbwq50AIliJE",
    "username": "kKidus Shimelis",
    "license": null,
    "title": "personal portfolio assistant",
    "publication_description": "Back to publications\nSep 05, 2025\n●\n26 reads\npersonal portfolio assistant\nchromaDB\nPersonal Assistant\nRAG\nK\nKidus Shimelis\nLike\nBookmark\nShare\n\nBuilding a Smart AI Assistant for Portfolio Websites with RAG\nIntroduction\n\nPortfolio websites are a great way for professionals to showcase their work, but static pages can feel limiting. To make them more engaging, I built an AI-powered chat assistant that answers visitor questions about a client's portfolio in real time. This assistant uses Retrieval-Augmented Generation (RAG), blending document retrieval with AI to provide accurate, context-aware responses. In this article, I’ll walk you through how I built it using LangChain, ChromaDB, Groq, Django, and React.\n\nWhy RAG?\n\nTraditional chatbots often struggle with specific, context-heavy queries. RAG solves this by combining two steps: retrieving relevant information from a knowledge base (like portfolio content) and generating a natural response using a language model. This ensures the assistant delivers answers tailored to the client’s projects, skills, or experiences.\nSystem Overview\nThe assistant is embedded in a portfolio website as a chat widget. Here’s how the pieces fit together:\n\nRAG Pipeline: LangChain orchestrates the process, ChromaDB stores and retrieves portfolio content, and Groq powers fast, accurate response generation.\nBackend: Django handles content management and API calls to connect the frontend with the RAG system.\nFrontend: A React-based chat widget styled with Tailwind CSS offers a sleek, user-friendly interface.\n\nHow It Works\n1. Storing Portfolio Content\n\nThe client’s portfolio—project descriptions, resumes, or blog posts—is broken into chunks and converted into numerical embeddings using an open-source model. These embeddings are stored in ChromaDB, a vector database that makes retrieval fast and efficient.\n\n2. Handling User Queries\n\nWhen a visitor types a question (e.g., “What projects has the client worked on?”), the system:\n\nConverts the query into an embedding.\nRetrieves the most relevant portfolio content from ChromaDB.\nPasses the content to Groq’s language model, which crafts a natural, context-rich response.\n\nLangChain ties these steps together, ensuring smooth and accurate responses.\n\n3. Web Integration\n\nThe Django backend manages the portfolio content and exposes APIs for the RAG pipeline. The React frontend powers the chat widget, letting users interact with the assistant seamlessly. Tailwind CSS keeps the design clean and responsive.\nChallenges and Solutions\nBuilding the assistant wasn’t without hurdles:\n\nContent Relevance: Early tests returned irrelevant documents. I fine-tuned the embedding model and adjusted chunk sizes to improve retrieval accuracy.\nResponse Speed: Initial responses were slow. Switching to Groq’s optimized inference engine reduced latency to under 1.5 seconds per query.\nUI Integration: Embedding the chat widget without slowing the website required optimizing React components and Django API calls.\n\nResults\n\nI tested the assistant with 100 sample queries about the portfolio. It answered 92% of them accurately, compared to 78% for a basic chatbot without RAG. The average response time was 1.2 seconds, making it feel instant to users. Visitors found the chat widget intuitive, boosting engagement on the portfolio site.\n\nWhat’s Next?\n\nThis project shows how RAG can transform static websites into dynamic, interactive experiences. I’m exploring ways to add multilingual support and integrate more content types, like videos or case studies. The codebase is open-source, and I’m excited to see how others might adapt it for their own projects.\n\nConclusion\n\nUsing RAG with LangChain, ChromaDB, and Groq, I built a smart AI assistant that brings portfolio websites to life. Combined with Django and React, it’s a scalable, practical solution for enhancing user engagement. If you’re interested in trying it out or contributing, check out the project on GitHub (link to your repo) or reach out!\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nBuilding a Smart AI Assistant for Portfolio Websites with RAG\n\nIntroduction\n\nWhy RAG?\n\nHow It Works\n\nStoring Portfolio Content\nHandling User Queries\nWeb Integration\n\nResults\n\nWhat’s Next?\n\nConclusion\n\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nYou might be interested\nProtfolioBot\nR\nSep 22, 20254 reads\nAAIDC2025chatbot+1\nAgentDoc: A Conversational RAG Assistant\nS\nAug 12, 202511 reads\nAgentic AIChatbot+8\nCiphor Bot: RAG-Based Web Chat with LLaMA and Redis Retention\nK\nMar 10, 20258 reads\nBuilding a Secure, Multilingual AI Assistant: A Deep Dive into LorenzoBot's Architecture 🤖\nM\nJul 23, 202511 reads\naichatbot+5",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "product-inspection-with-renesas-rzv2l-and-edge-impulse-juzV7l5MbLbT",
    "username": "Solomon Muhunyo Githu",
    "license": "MIT License",
    "title": "Product inspection with Renesas RZ/V2L and Edge Impulse",
    "publication_description": "Back to publications\nDec 27, 2024\n●\n66 reads\n●\nMIT License\nWinner of\nMost Innovative Project\nat the\nComputer Vision Projects Expo 2024\nProduct inspection with Renesas RZ/V2L and Edge Impulse\n3d printing\ncomputer vision\nEdge AI\nmachine learning\nquality inspection\nSolomon Muhunyo Githu\nLike\nBookmark\nShare\n\nOverview\n\nIn casting process, liquid material is poured into a mold with a cavity that corresponds to the shape of the finished product. A casting defect could develop during the liquefaction of the material. Casting defects can be in many different forms such as shrinkage, pinholes, blow holes, etc.\n\nAll industries have a quality inspection department for separating the defective products from the non-defective ones. In this case, the main issue is that the inspection accuracy normally depends on human accuracy. Computer Vision based systems are therefore the primary solution to be used in this task.\n\nSolution\n\nIn this project I implemented the approach of computer vision based cast inspection. I trained a YOLOv5 object detection model using \nEdge Impulse platform\n and deployed it to the \nRenesas RZ/V2L Evaluation Board Kit\n. A \nGoogle Coral camera\n first takes a top view image of a submersible pump impeller moving on a conveyor belt. An application running on the Renesas RZ/V2L then classifies if the impeller is good or defective. The time per inference is around 63 milliseconds which gives around 16 frames per second (fps). This low latency during inference can be related to the fact that the Renesas RZ/V2L board is designed for vision AI applications and it offers a powerful hardware acceleration through its Dynamically Reconfigurable Processor (DRP) and multiply-accumulate unit (AI-MAC).\n\nComponents and Hardware Configuration\n\nSoftware components:\n\nEdge Impulse Studio account\nEdge Impulse for Linux\n\nOptional software components for rebuilding the Web application:\n\nEdge Impulse Python SDK\nFlask\nPyInstaller\n\nHardware components:\n\nRenesas RZ/V2L Evaluation Board Kit\nUSB Type-C cable “AK-A8485011” (manufactured by Anker)\nUSB PD Charger Anker “PowerPort III 65W Pod” (manufactured by Anker)\nEthernet cable\n12V AC/DC adapter\n470 Ohm resistor\n10k Ohm multiturn trimmer potentiometer\n1M Ohm resistor\nIRF540 mosfet\nA 3cm x 3cm strip board\n4 2.5mm pin spaced male header pins\nA soldering gun and some solder wire\nAt least four male-female jumper wires\nN20 3mm DC 6V 200RPM Metal Gear motor\nMini conveyor assembly\nRenesas RZ/V2L Evaluation Board Kit support\nAt least 20 pairs of M3 bolts and nuts\nData collection process\n\nI used the \ncasting product image data for quality inspection\n dataset on Kaggle. This dataset contains top view gray-scaled and augmented images of submersible pump impellers. The dataset has folders for defective and non-defective images of submersible pump impellers.\n\nIn total, I had 1300 images: 1047 for training and 253 for testing.\n\nWith 1300 images it would be tiresome to draw bounding boxes for all the objects. Edge Impulse offers various \nAI-assisted labelling methods\n to automate this process. In my case, I chose to track object within frames and the bounding boxes as well as their labels were set automatically.\n\nTraining and building the model\n\nIn the Edge Impulse platform, an \nImpulse\n is a machine learning pipeline that indicates the type of input data, extracts features from the data and finally a neural network that trains on the features from your data.\n\nFor the YOLOv5 model, I used an image width and height of 160 pixels and the \"Resize mode\" set to \"Squash\". The processing block was set to \"Image\" and the learning block set to \"Object Detection (Images)\".\n\nUnder \"Image\" in Impulse design, the color depth of the images is set to RGB and the next step was to extract features.\n\nOn the \nFeature explorer\n, we can see that the blue and orange data don't completely separate from each other. This can be related to the fact that both defective and good impellers have similar features like their disc-shaped property. We can see that there is a trend of some sort on the data; the blue data are grouped together on one side and the orange data grouped together on another side. What separates the good and defective impellers is the bad \"features\" like cracks, holes and protrusions.\n\nIn the features tab we can also see the on-device performance for generating features during the deployment. These estimated metrics are for the Renesas RZ/V2L(with DRP-AI accelerator). The Renesas RZ/V2L Evaluation Board Kit is supported by Edge Impulse. This board is designed for vision AI applications and it offers a powerful hardware acceleration through its Dynamically Reconfigurable Processor (DRP) and multiply-accumulate unit (AI-MAC).\n\nCurrently, all Edge Impulse models can run on the RZ/V2L CPU which is a dedicated Cortex A55. However, so that I can benefit from the DRP-AI hardware acceleration, I chose a YOLOV5 model. Note that on the training page you have to select the RZ/V2L (with DRP-AI accelerator) before starting the training in order to tell the studio that you are training the model for the RZ/V2L. This can be done on the top right in the training page or by changing target device in the Dashboard page.\n\nAfter training the model on various parameters, I settled with 100 training cycles and a learning rate of 0.001. It is however advised to train a YOLOv5 model using more than 1500 photos per class and more than 10,000 instances per class to produce a robust YOLOv5 model.\n\nAfter the training process, I got a precision score of 96%. Precision is the number of True Positives divided by the number of True Positives plus the number of False Positives.\n\nModel testing\n\nAfter training a model, we need to do a test with the unseen(test) data. In my case, the model had an accuracy of 91%. This accuracy is a percent of all samples with a precision score above 98%.\n\nI chose this as an acceptable performance and proceeded to deploy the model to the Renesas RZ/V2L board.\n\nYou can find the public Edge Impulse project here: \nSubmersible pump impeller defect detection\n. To add this project to your Edge Impulse projects, click “Clone this project” at the top of the window.\n\nDeploying the model to the Renesas RZ/V2L\n\nThe Renesas RZ/V2L Evaluation Kit comes with the Renesas RZ/V2L board and a 5-megapixel Google Coral Camera. To setup the board, Edge Impulse has prepared a \nguide\n that shows how to prepare the Linux Image, install \nEdge Impulse for Linux\n and finally connecting the board to Edge Impulse Studio.\n\nAfter the Renesas RZ/V2L board has been setup we can SSH into the board by an ethernet connection between a computer and the board, or the board and a router. To SSH into the RZ/V2L we can run the following terminal/Command prompt command on a computer:\n\nssh root@smarc-rzv2l\n\n\nTo run the model locally on the RZ/V2L we can run the command edge-impulse-linux-runner which lets us log in to our Edge Impulse account and select the cloned public project. The model will be downloaded and inference will start automatically.\n\nAlternatively, we can also download an executable of the model which contains the signal processing and ML code, compiled with optimizations for the processor, plus a very simple IPC layer (over a Unix socket). This executable is called \n.eim model\n.\n\nTo do a similar method, create a directory and navigate into the directory:\n\nmkdir submersible_pump_impeller_classification_with_Edge_Impulse && \\\ncd submersible_pump_impeller_classification_with_Edge_Impulse\n\n\nNext, download the eim model with the command:\n\nedge-impulse-linux-runner --download modelfile.eim\n\n\nNow we can run the executable model locally using the command:\n\nedge-impulse-linux-runner --model-file modelfile.eim\n\n\nHere, we pass the name of the downloaded file modelfile in the command.\n\nWe can go to the provided URL(RZ/V2L IP address at port 4912) and we will see the feed being captured by the camera as well as the bounding boxes if present.\n\nResults - An industrial demo of using AI in quality inspection\n\nUsing the eim executable and \nEdge Impulse Python SDK\n, I developed a Web Application using \nFlask\n that counts the number of good and defective submersible pump impellers. The taken images and the counts are then displayed on the Webpage in real time.\n\nAfterwards I designed and 3D printed some PLA based parts that can be assembled to a mini conveyor belt. These parts can be downloaded on \nprintables.com\n. The mini conveyor mechanism assembly is made up of corner brackets, rollers, roller support, M3 bolts and even a motor housing. I assembled the conveyor on a board platform made of chip plywood. I also designed and 3D printed a crate that is used to catch the parts as they are falling from the conveyor.\n\nThe conveyor mechanism is actuated by one N20 3mm DC 6V 200RPM Metal Gear motor. The motor housing has a chamber to put a circuit board for driving the motor. A cable management channel is also provided for the wires running from the motor driver circuit to the motor.\n\nTo assemble the conveyor mechanism the following parts are needed:\n\n4 corner brackets\n4 roller supports\n2 rollers\n1 motor support\n1 motor support cover\nAt least 20 pairs of M3 bolts and nuts (if you plan to screw 2 out of 4 four holes)\n1 crate\n\nThe motor driver circuit is based on this \ntutorial\n. It has one DC input and a DC output. I used a 12V AC/DC adapter to power the conveyor mechanism. The speed of the motor can be controlled by adjusting the potentiometer.\n\nI also designed and 3D printed a support for the Renesas RZ/V2L Evaluation Board Kit components. This support is made up of two parts: a base support for the board and a part for increasing the height of the base support. The Google Coral camera can be mounted upright or facing downwards through the provided slots. This Renesas RZ/V2L EVK support is also available for downloaded on \nprintables.com\n.\n\nFinally, I printed some 3cm wide submersible pump impeller images from the testing dataset. As the conveyor belt is moving, the Google Coral camera takes a picture of the printed images and the results are shown on the WebApp.\n\nHere is a demo of the application classifying submersible pump impellers as they are moving on the conveyor belt.\n\nThe source code for the WebApp is available in this \nGitHub repository\n. It can be cloned on the Renesas RZ/V2L board and the installation steps can be found in the repository. You can run the Flask application using Python(after installing Flask) or run the binary executable built with PyInstaller for AARCH64 platform.\n\nTracking items on the conveyor belt\n\nIn the project \nObject detection with centroid-based tracking\n, I documented on how to implement object detection and tracking using a simple \nPython script\n. This object tracking algorithm can be applied to a wide range of use cases, such as counting items on a conveyor belt that transports manufactured products for inspection using computer vision.\n\nIn this project, we could enhance the conveyor belt system to perform additional actions, such as sorting good and defective submersible pump impellers. The object tracking algorithm can identify and track these impellers, enabling a robotic arm to automatically sort them based on their quality.\n\nConclusion\n\nThis project has shown that we can move closer to zero manufacturing defects on cast production by integrating Machine Learning for visual inspection. This approach does not only increase the inspection accuracy but also eliminates fatigue since machines can perform their tasks full day without fatigue.\n\nAn interesting future project could be to advance the conveyor mechanism so that the defective parts can be moved to a separate location. Renesas has a documentation on how to set the GPIO with the \nRenesas Flexible Software Package (FSP)\n for writing applications.\n\nCredits:\n\nhttps://www.homemade-circuits.com/dc-motor-speed-controller-circuits/\nhttps://www.kaggle.com/datasets/ravirajsinh45/real-life-industrial-dataset-of-casting-product\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nOverview\n\nSolution\n\nComponents and Hardware Configuration\n\nData collection process\n\nTraining and building the model\n\nModel testing\n\nDeploying the model to the Renesas RZ/V2L\n\nResults - An industrial demo of using AI in quality inspection\n\nTracking items on the conveyor belt\n\nConclusion\n\nComments\nCode\nDatasets",
    "awards": [
      "most innovative project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "production-ready-deep-research-agent-UFnPgBqKRBF8",
    "username": "Zeeshan Ahmad",
    "license": "MIT License",
    "title": "Production-Ready Deep Research Agent ",
    "publication_description": "Back to publications\nAug 10, 2025\n●\n74 reads\n●\nMIT License\nProduction-Ready Deep Research Agent\nAgentic AI\nDeep Research Agent\nProduction Ready Agent\nZeeshan Ahmad\nLike\nBookmark\nShare\nDeep Research Agent 🧠\nIntroduction 🌟\n\nThe Deep Research Agent is a sophisticated multi-agent system designed to conduct comprehensive research on user-specified topics within a chosen domain. It leverages AI-driven tools, including the Meta LLaMA model, to generate targeted research questions, perform in-depth analysis, and compile findings into a professional report. This project demonstrates the system's production readiness, safety, and usability, making it an ideal tool for researchers, analysts, and professionals seeking data-driven insights.\n\nProject Overview 📝\n\nThe Deep Research Agent is a powerful, AI-powered tool for conducting deep research. It integrates the Meta LLaMA large language model with external tools like Tavily for web searches, ensuring accurate and up-to-date information. The system is built to be user-friendly, supporting iterative research refinement through a Streamlit-based interface. It automates the entire research pipeline, from question generation to report creation and storage in Google Docs.\n\nProject Structure 🗂️\n\nThe project is organized in a clean, modular structure for easy maintenance and scalability. Here's the directory tree:\n\ndeep-research-agent/\n├── .env.example          # Example environment variables file for configuration\n├── .gitignore            # Git ignore file to exclude unnecessary files\n├── .gitattributes        # Git attributes for handling file types\n├── LICENSE               # Project license file (e.g., MIT or Apache)\n├── README.md             # Main README with setup instructions and usage\n├── Dockerfile            # Docker configuration for containerization\n├── docker-compose.yml    # Docker Compose for multi-container setups\n├── pytest.ini            # Configuration for pytest testing framework\n├── requirements.txt      # List of Python dependencies\n├── notebook/             # Jupyter notebooks for experimentation\n│   └── deep_research.ipynb  # Notebook for interactive deep research demos\n├── tests/                # Test suite directory\n│   ├── __init__.py       # Init file for tests package\n│   ├── conftest.py       # Pytest fixtures and configurations\n│   ├── unit/             # Unit tests\n│   │   ├── __init__.py\n│   │   ├── test_nodes.py  # Tests for individual nodes\n│   │   └── test_tools.py  # Tests for tools like LLM and Composio\n│   ├── integration/      # Integration tests\n│   │   ├── __init__.py\n│   │   └── test_workflow.py  # Tests for workflow integration\n│   └── system/           # System/end-to-end tests\n│       ├── __init__.py\n│       └── test_e2e.py   # Full system tests including UI\n└── src/                  # Source code directory\n    ├── __init__.py       # Init file for src package\n    ├── app.py            # Streamlit application entry point\n    ├── graph.py          # LangGraph workflow definition\n    ├── state.py          # State management for the agent\n    ├── config.py          # Configuration management\n    ├── resilience.py          # Resilience patterns\n    ├── monitoring/       # Monitoring tools\n    │   ├── __init__.py\n    │   ├── logger.py     # Custom logging module\n    │   └── metrics.py    # Metrics tracking (e.g., performance)\n    │   └── health.py    # Health check endpoints\n    ├── guardrails/       # Safety and validation modules\n    │   ├── __init__.py\n    │   ├── input_validator.py  # Input validation logic\n    │   └── prompt_injection.py # Prompt injection detection\n    ├── nodes/            # Agent nodes (research steps)\n    │   ├── __init__.py\n    │   └── nodes.py      # Definitions of nodes like research_agent_node\n    ├── tools/            # AI tools and integrations\n    │   ├── __init__.py\n    │   ├── llm.py        # LLM wrapper (e.g., for Meta LLaMA)\n    │   └── composio_tools.py  # Tools for external integrations like Google Docs\n    └── prompts.py        # Prompt templates for the LLM\n\n\nThis structure follows best practices: separating concerns into source code, tests, and documentation.\n\nKey Features ✨\nInput Flexibility 📝: Users can specify a research topic and domain via an intuitive Streamlit web interface, allowing for customized research.\nAutomated Question Generation ❓: The system generates specific yes/no research questions tailored to the topic and domain, ensuring focused analysis.\nAI-Powered Research 🤖: Utilizes the Meta LLaMA model for reasoning and Tavily for real-time web searches to gather accurate, verifiable data.\nProfessional Reporting 📄: Compiles findings into a polished, HTML-formatted report styled like McKinsey consulting reports, with sections for executive summary, findings, and recommendations.\nGoogle Docs Integration 📑: Automatically exports and saves reports to Google Docs for easy sharing and collaboration.\nState Management 🧮: Leverages LangGraph for orchestrating the multi-agent workflow, maintaining state across steps for seamless execution and memory retention.\nAdvanced Resilience 🛡️: Implements circuit breakers, retry mechanisms, and resource limits for robust operation.\nComprehensive Monitoring 📊: Health checks, metrics tracking, and detailed logging for production observability.\nSecurity Guardrails🔐🔐: Input validation, prompt injection detection, and sanitization for safe operation.\nFlowchart 📈\n\nBelow is a flowchart illustrating the workflow of the Deep Research Agent. It starts with user input, proceeds through question generation, research, and ends with report creation and storage.\n\nThe workflow is cyclic if refinements are needed, ensuring iterative improvement.\n\nEnhancements for Production-Readiness 🛠️\nTesting 🧪\n\nComprehensive testing ensures reliability. Tests are divided into unit, integration, and system levels using pytest.\n\nUnit Tests\n\nUnit tests isolate and verify individual components.\n\n# tests/unit/test_nodes.py\nfrom src.nodes.nodes import research_agent_node\n\ndef test_research_agent_node_returns_dict():\n    state = {\"topic\": \"AI in health\", \"domain\": \"Health\"}\n    result = research_agent_node(state)\n    assert isinstance(result, dict)\n    assert \"questions\" in result or \"report\" in result\n\nExplanation: This test checks if the research_agent_node function processes a state dictionary (containing topic and domain) and returns a dictionary with expected keys like \"questions\" or \"report\". It ensures the node outputs the correct data structure without side effects.\n\nIntegration Tests\n\nIntegration tests verify component interactions.\n\n# tests/integration/test_workflow.py\nfrom src.graph import build_graph\n\ndef test_workflow_end_to_end():\n    graph = build_graph()\n    state = graph.invoke({\"topic\": \"AI in health\", \"domain\": \"Health\"})\n    assert \"report\" in state\n    assert \"<html>\" in state[\"report\"].lower()\n\nExplanation: This test builds the full LangGraph workflow, invokes it with sample input, and asserts that the output state includes a \"report\" key with HTML content. It simulates the entire pipeline to catch integration issues.\n\nSystem Tests\n\nSystem tests validate the end-to-end application, including the UI.\n\n# tests/system/test_e2e.py\nimport subprocess\nimport time\nimport requests\n\ndef test_streamlit_ui_loads():\n    proc = subprocess.Popen([\"streamlit\", \"run\", \"src/app.py\", \"--server.headless=true\"])\n    time.sleep(10)  # give it time to boot\n    try:\n        resp = requests.get(\"http://localhost:8501\", timeout=5)\n        assert resp.status_code == 200\n        assert \"Deep Research Agent\" in resp.text\n    finally:\n        proc.terminate()\n        proc.wait()\n\nExplanation: This test launches the Streamlit app in headless mode, waits for it to start, and sends an HTTP request to verify the UI loads correctly (status 200) and contains the expected title. It ensures the full system, including the web interface, functions as intended.\n\nSafety Features 🔒\nInput Validation\n\nPrevents invalid or malicious inputs.\n\n# guardrails/input_validator.py\ndef validate_input(user_text: str) -> bool:\n    MAX_LEN = 200\n    REJECTED_KEYWORDS = [\"drop table\", \"delete from\", \"<script\"]\n    if not isinstance(user_text, str) or len(user_text) > MAX_LEN:\n        return False\n    lowered = user_text.lower()\n    return not any(bad in lowered for bad in REJECTED_KEYWORDS)\n\nExplanation: This function checks if the input is a string under 200 characters and doesn't contain SQL injection or XSS keywords. It returns True for safe inputs, blocking potential attacks early.\n\nPrompt Injection Protection\n\nDetects and blocks prompt injection attempts.\n\n# guardrails/prompt_injection.py\nimport re\n\nPROMPT_INJ_PATTERN = re.compile(r\"\\b(ignore|disregard|forget|override).*\\b(previous|instruction|prompt)\\b\", re.IGNORECASE)\n\ndef detect_prompt_injection(text: str) -> bool:\n    return bool(PROMPT_INJ_PATTERN.search(text))\n\nExplanation: Uses regex to scan for patterns like \"ignore previous instructions,\" common in prompt injections. Returns True if detected, allowing the system to block harmful prompts.\n\nUser Interface Design 🖥️\n\nThe Streamlit UI provides a simple, guided experience with enhanced error handling and metrics display.\n\n# src/app.py\n# src/app.py\nimport streamlit as st\nimport time\nimport json\nimport traceback\nfrom typing import Dict, Any, Optional, List, Tuple\n\nfrom graph import WorkflowManager, WorkflowError, WorkflowState\nfrom monitoring.logger import get_logger\nfrom guardrails.input_validator import validate_input\nfrom guardrails.prompt_injection import detect_prompt_injection\nfrom nodes.nodes import NodeError, MaxRetriesExceededError\n\n# Constants\nMAX_RETRIES = 3\nRETRY_DELAY = 1  # seconds\n\n# Initialize workflow manager\nworkflow_manager = WorkflowManager()\nlogger = get_logger(\"streamlit_ui\")\n\ndef initialize_session_state() -> None:\n    \"\"\"Initialize the session state variables if they don't exist.\"\"\"\n    defaults = {\n        'research_started': False,\n        'research_complete': False,\n        'error_occurred': False,\n        'error_message': \"\",\n        'error_count': 0,\n        'last_error': None,\n        'error_traceback': None,\n        'show_technical_details': False,\n        'workflow_state': None,\n        'execution_metrics': {\n            'start_time': None,\n            'end_time': None,\n            'duration': None,\n            'nodes_executed': 0,\n            'retries': 0\n        }\n    }\n    \n    for key, value in defaults.items():\n        if key not in st.session_state:\n            st.session_state[key] = value\n\ndef validate_inputs(topic: str, domain: str) -> tuple[bool, str]:\n    \"\"\"Validate user inputs.\n    \n    Args:\n        topic: The research topic\n        domain: The research domain/industry\n        \n    Returns:\n        tuple: (is_valid, error_message)\n    \"\"\"\n    if not topic.strip():\n        return False, \"Topic cannot be empty.\"\n    if not domain.strip():\n        return False, \"Domain cannot be empty.\"\n    if not validate_input(topic) or not validate_input(domain):\n        return False, \"Invalid input detected. Please avoid special characters.\"\n    if detect_prompt_injection(topic) or detect_prompt_injection(domain):\n        logger.warning(\"Blocked suspicious input: %s | %s\", topic, domain)\n        return False, \"Invalid input detected. Please try different keywords.\"\n    return True, \"\"\n\ndef run_research_workflow(topic: str, domain: str) -> Dict[str, Any]:\n    \"\"\"Run the research workflow with enhanced resilience patterns.\n    \n    Args:\n        topic: The research topic\n        domain: The research domain/industry\n        \n    Returns:\n        dict: Result containing report, doc_url, or error information\n    \"\"\"\n    # Update execution metrics\n    st.session_state.execution_metrics['start_time'] = time.time()\n    \n    try:\n        # Execute the workflow using the workflow manager\n        result = workflow_manager.execute_workflow(topic.strip(), domain.strip())\n        \n        # Update execution metrics\n        st.session_state.execution_metrics['end_time'] = time.time()\n        st.session_state.execution_metrics['duration'] = (\n            st.session_state.execution_metrics['end_time'] - \n            st.session_state.execution_metrics['start_time']\n        )\n        \n        if result.get('node_statuses'):\n            st.session_state.execution_metrics['nodes_executed'] = len([\n                node for node in result['node_statuses'].values() \n                if node['status'] in ['completed', 'failed']\n            ])\n            st.session_state.execution_metrics['retries'] = sum(\n                node.get('attempts', 1) - 1 \n                for node in result['node_statuses'].values()\n            )\n        \n        # Store workflow state for debugging\n        st.session_state.workflow_state = result\n        \n        # Log the workflow execution status\n        if result[\"success\"]:\n            logger.info(\"Workflow completed successfully\")\n        else:\n            logger.error(\"Workflow failed: %s\", result.get('error', 'Unknown error'))\n            \n        return result\n        \n    except WorkflowError as e:\n        error_msg = f\"Workflow execution failed: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return {\n            \"success\": False, \n            \"error\": error_msg,\n            \"type\": \"workflow_error\"\n        }\n        \n    except Exception as e:\n        error_msg = f\"An unexpected error occurred: {str(e)}\"\n        error_traceback = traceback.format_exc()\n        logger.error(\"%s\\n%s\", error_msg, error_traceback)\n        \n        # Store error details for technical view\n        st.session_state.last_error = str(e)\n        st.session_state.error_traceback = error_traceback\n        \n        return {\n            \"success\": False, \n            \"error\": error_msg,\n            \"type\": \"unexpected_error\"\n        }\n\nExplanation\n\nscript sets up the Streamlit app with input fields for topic and domain. On button click, it validates inputs, runs the workflow via LangGraph, displays a spinner during processing, and renders the HTML report with a download option. Safety checks are integrated to prevent errors or attacks.\n\nEnhanced Resilience and Monitoring 🛡️\nAdvanced Error Handling\n\nCatches and logs exceptions gracefully with circuit breaker patterns.\n\n# src/nodes/nodes.py\n# src/nodes/nodes.py\nfrom monitoring.logger import get_logger\nfrom monitoring.metrics import timed\nfrom guardrails.input_validator import validate_input\nfrom guardrails.prompt_injection import detect_prompt_injection\nfrom resilience import retry_with_backoff, CircuitBreaker\n\n@timed\n@retry_with_backoff(max_retries=2)\n@CircuitBreaker(failure_threshold=3, recovery_timeout=60)\ndef research_agent_node(state: GraphState) -> GraphState:\n    \"\"\"Conduct research based on generated questions.\n    \n    Args:\n        state: Current graph state containing 'topic', 'domain', and 'questions'\n        \n    Returns:\n        Updated state with 'report' key containing research findings\n        \n    Raises:\n        NodeError: If research fails or no questions are provided\n    \"\"\"\n    try:\n        topic = state[\"topic\"].strip()\n        questions = state.get(\"questions\", [])\n        \n        if not questions:\n            raise NodeError(\"No research questions provided\")\n        \n        # Check cache for existing report\n        questions_hash = hash(tuple(questions))\n        cache_key = f\"report:{topic}:{questions_hash}\"\n        cached_report = cache.get(cache_key)\n        \n        if cached_report:\n            logger.info(\"Using cached report for topic: %s\", topic)\n            return {**state, \"report\": cached_report}\n            \n        # Process questions in parallel for better performance\n        findings = process_questions_parallel(topic, questions)\n        \n        if not findings:\n            raise NodeError(\"No research findings were generated\")\n            \n        # Generate the report\n        report = generate_research_report(findings)\n        \n        # Cache the report\n        cache.set(cache_key, report)\n        \n        logger.info(\"Successfully generated report, length=%d chars\", len(report))\n        return {**state, \"report\": report}\n        \n    except Exception as e:\n        logger.error(\"Error in research_agent_node: %s\", str(e), exc_info=True)\n        raise NodeError(f\"Research failed: {str(e)}\") from e\n\nExplanation: This module implements a Streamlit-based web interface for the Deep Research Agent, providing a user-friendly frontend for initiating research workflows. It handles user input validation, manages session state, executes research workflows through the WorkflowManager, displays progress indicators, and renders research results with error handling and execution metrics. The app includes guardrails against prompt injection and provides comprehensive error reporting with technical details for debugging.\n\nComprehensive Resilience Patterns\nOutput parsing and schema validation\nTimeout and retry strategies\nCircuit breakers and fallbacks\nIteration caps and loop detection\nState validation and recovery\nResource usage limits\n# src/resilience.py\n# src/resilience.py\ndef retry_with_backoff(\n    func: Callable[..., R],\n    max_retries: int = DEFAULT_MAX_RETRIES,\n    initial_delay: float = DEFAULT_INITIAL_DELAY,\n    max_delay: float = DEFAULT_MAX_DELAY,\n    exponential_base: float = 2.0,\n    jitter: bool = True,\n    exceptions: tuple = (Exception,),\n    on_retry: Optional[Callable[[int, Exception], None]] = None,\n) -> Callable[..., R]:\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        delay = initial_delay\n        last_exception = None\n        \n        for attempt in range(max_retries + 1):\n            try:\n                return func(*args, **kwargs)\n            except exceptions as e:\n                last_exception = e\n                \n                if attempt == max_retries:\n                    logger.error(f\"Max retries ({max_retries}) exceeded.\")\n                    raise MaxRetriesExceededError(\n                        f\"Max retries ({max_retries}) exceeded. Last error: {str(e)}\"\n                    ) from e\n                \n                # Calculate next delay with exponential backoff and jitter\n                delay = min(delay * (exponential_base ** attempt), max_delay)\n                if jitter:\n                    import random\n                    delay = random.uniform(0, delay)\n                \n                if on_retry:\n                    on_retry(attempt + 1, e)\n                \n                logger.warning(\n                    f\"Attempt {attempt + 1}/{max_retries} failed. \"\n                    f\"Retrying in {delay:.2f}s. Error: {str(e)}\"\n                )\n                \n                time.sleep(delay)\n    \n    return wrapper\n\nclass CircuitBreaker:\n    def __init__(\n        self,\n        failure_threshold: int = 5,\n        recovery_timeout: float = 300.0,  # 5 minutes\n        name: str = \"default\",\n    ):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.name = name\n        \n        self._failures = 0\n        self._state = \"CLOSED\"\n        self._last_failure_time = None\n        self._test_mode = False\n    \n    @property\n    def state(self) -> str:\n        \"\"\"Get the current state of the circuit breaker.\"\"\"\n        if self._state == \"OPEN\" and self._should_try_recovery():\n            self._state = \"HALF-OPEN\"\n            self._test_mode = True\n        return self._state\n    \n    def _should_try_recovery(self) -> bool:\n        \"\"\"Check if we should attempt to recover from an open state.\"\"\"\n        if self._state != \"OPEN\" or not self._last_failure_time:\n            return False\n        \n        time_since_failure = time.time() - self._last_failure_time\n        return time_since_failure >= self.recovery_timeout\n    \n    def record_failure(self):\n        \"\"\"Record a failed operation.\"\"\"\n        self._failures += 1\n        self._last_failure_time = time.time()\n        \n        if self._failures >= self.failure_threshold:\n            self._state = \"OPEN\"\n            logger.warning(\n                f\"Circuit breaker '{self.name}' is now OPEN. \"\n                f\"Failures: {self._failures}\"\n            )\n    \n    def record_success(self):\n        \"\"\"Record a successful operation.\"\"\"\n        if self._state == \"HALF-OPEN\" and self._test_mode:\n            # Test operation succeeded, close the circuit\n            self._reset()\n            logger.info(f\"Circuit breaker '{self.name}' is now CLOSED.\")\n        else:\n            # Reset failure count on success\n            self._failures = max(0, self._failures - 1)\n    \n    def _reset(self):\n        \"\"\"Reset the circuit breaker to its initial state.\"\"\"\n        self._failures = 0\n        self._state = \"CLOSED\"\n        self._last_failure_time = None\n        self._test_mode = False\n    \n    def __call__(self, func: Callable[..., R]) -> Callable[..., R]:\n        \"\"\"Use the circuit breaker as a decorator.\"\"\"\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Check circuit state\n            if self.state == \"OPEN\":\n                raise CircuitOpenError(\n                    f\"Circuit '{self.name}' is OPEN. \"\n                    f\"Last failure: {self._last_failure_time}\"\n                )\n            \n            try:\n                result = func(*args, **kwargs)\n                self.record_success()\n                return result\n            except Exception as e:\n                self.record_failure()\n                raise\n        \n        return wrapper\n\nExplanation: This module implements comprehensive resilience patterns for AI applications, including retry mechanisms with exponential backoff, circuit breakers for fault tolerance, resource usage limits, state validation, and iteration controls. It provides decorators and context managers to make operations more robust against failures, timeouts, and resource constraints, ensuring AI systems can gracefully handle errors and recover from temporary issues.\n\nAdvanced Logging and Monitoring\n\nProvides detailed, formatted logs for debugging with structured logging.\n\n# monitoring/logger.py\n# monitoring/logger.py\nimport logging\nimport json\nfrom datetime import datetime\n\ndef get_logger(name: str) -> logging.Logger:\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n    \n    # Console handler with structured format\n    console_handler = logging.StreamHandler()\n    console_formatter = logging.Formatter(\n        \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n    )\n    console_handler.setFormatter(console_formatter)\n    logger.addHandler(console_handler)\n    \n    # File handler for persistent logs\n    file_handler = logging.FileHandler(f\"logs/{name}_{datetime.now().strftime('%Y%m%d')}.log\")\n    file_formatter = logging.Formatter(\n        \"%(asctime)s | %(levelname)s | %(name)s | %(funcName)s:%(lineno)d | %(message)s\"\n    )\n    file_handler.setFormatter(file_formatter)\n    logger.addHandler(file_handler)\n    \n    return logger\n\ndef log_structured_event(logger: logging.Logger, event_type: str, data: dict, level: str = \"info\"):\n    \"\"\"Log structured events with consistent formatting.\"\"\"\n    log_data = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"event_type\": event_type,\n        \"data\": data\n    }\n    \n    if level == \"info\":\n        logger.info(json.dumps(log_data))\n    elif level == \"warning\":\n        logger.warning(json.dumps(log_data))\n    elif level == \"error\":\n        logger.error(json.dumps(log_data))\n\nExplanation: This factory function creates a logger with INFO level, streaming output to console in a timestamped format. It's used across modules for consistent monitoring of events, warnings, and errors.\n\nHealth Check Endpoints\n\nProvides comprehensive health monitoring for production deployment.\n\n# monitoring/health.py\n\"\"\"Health check endpoints for the application.\"\"\"\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom fastapi.responses import JSONResponse\nfrom typing import Dict, Any\nimport logging\n\n# Set up logging\nlogger = logging.getLogger(__name__)\n\n# Create router\nrouter = APIRouter()\n\n@router.get(\n    \"/healthz\",\n    summary=\"Health check endpoint\",\n    description=\"Returns the health status of the application.\",\n    response_description=\"Application health status\",\n    response_model=Dict[str, str]\n)\nasync def health_check() -> Dict[str, str]:\n    \"\"\"\n    Health check endpoint that returns the status of the application.\n    \n    Returns:\n        Dict with status and timestamp\n    \"\"\"\n    try:\n        # Add any health checks here (e.g., database connection, external services)\n        return {\"status\": \"ok\"}\n    except Exception as e:\n        logger.error(\"Health check failed: %s\", str(e), exc_info=True)\n        raise HTTPException(status_code=503, detail=\"Service Unavailable\")\n\n@router.get(\n    \"/readyz\",\n    summary=\"Readiness check endpoint\",\n    description=\"Returns the readiness status of the application.\",\n    response_description=\"Application readiness status\",\n    response_model=Dict[str, str]\n)\nasync def readiness_check() -> Dict[str, str]:\n    \"\"\"\n    Readiness check endpoint that verifies all required services are available.\n    \n    Returns:\n        Dict with status and timestamp\n    \"\"\"\n    try:\n        # Add readiness checks here (e.g., database connection, external services)\n        return {\"status\": \"ready\"}\n    except Exception as e:\n        logger.error(\"Readiness check failed: %s\", str(e), exc_info=True)\n        raise HTTPException(status_code=503, detail=\"Service Not Ready\")\n\nExplanation: This module provides health check endpoints (/healthz and /readyz) for monitoring application status and readiness. It uses FastAPI routers to expose REST endpoints that return JSON responses indicating service health, with proper error handling and logging for troubleshooting service availability issues.\n\nEnhanced Deployment 🚀\nDocker with Multi-Stage Builds\n\nEnables containerized, scalable deployment with optimized images.\n\n# Dockerfile\nFROM python:3.11-slim as builder\n\nWORKDIR /app\n\n# Install build dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements and install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\n# Production stage\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Copy Python packages from builder stage\nCOPY --from=builder /root/.local /root/.local\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash app \\\n    && chown -R app:app /app\n\n# Copy application code\nCOPY --chown=app:app . .\n\n# Create logs directory\nRUN mkdir -p logs && chown -R app:app logs\n\n# Switch to non-root user\nUSER app\n\n# Add local bin to PATH\nENV PATH=/root/.local/bin:$PATH\n\n# Expose Streamlit port\nEXPOSE 8501\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD curl -f http://localhost:8501/_stcore/health || exit 1\n\n# Run the application\nCMD [\"streamlit\", \"run\", \"src/app.py\", \"--server.address=0.0.0.0\", \"--server.port=8501\"]\n\nExplanation: This enhanced Dockerfile uses multi-stage builds to create optimized production images, includes security best practices with non-root users, adds health checks, and creates proper directory permissions for logging.\n\nLicense & Usage Rights\n\nThis project is released under the \nMIT License\n.\nYou are free to use, modify, and distribute the codebase for both commercial and non-commercial purposes, provided that the original copyright notice is retained.\nThere is no liability or warranty—use at your own risk.\n\nConclusion 🎉\n\nThe Deep Research Agent is a production-ready, AI-driven system that streamlines research workflows while prioritizing safety and usability. It showcases advanced agentic AI capabilities, from multi-agent orchestration to secure integrations. Future enhancements could include support for more LLMs, advanced analytics visualizations, or multi-domain expansions. This capstone project highlights readiness for real-world applications in research and analysis. 🚀\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nDeep Research Agent 🧠\n\nIntroduction 🌟\n\nProject Overview 📝\n\nProject Structure 🗂️\n\nKey Features ✨\n\nFlowchart 📈\n\nEnhancements for Production-Readiness 🛠️\n\nTesting 🧪\n\nUnit Tests\n\nIntegration Tests\n\nView all\nComments\n(1)\nCode\nFiles\nLICENSE",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "program-guide-agentic-ai-developer-certification-by-ready-tensor-HrJ0xWtLzLNt",
    "username": "Ready Tensor",
    "license": null,
    "title": "Program Guide: Agentic AI Developer Certification by Ready Tensor",
    "publication_description": "Back to publications\nMay 12, 2025\n●\n13490 reads\n●\nCreative Commons Attribution-NonCommercial (CC BY-NC)\nProgram Guide: Agentic AI Developer Certification by Ready Tensor\nHub:\nReady Tensor Certifications\nAAIDC\nAgentic AI\nAutonomous Agents\nCertification\nFastAPI\nGradio\nLangChain\nLangGraph\nLLM\nMCP\nRAG\nVector Database\nReady Tensor\nVictory Nnaji\nRahul Parajuli\nLike\nBookmark\nShare\n\nWelcome to the Agentic AI Developer Certification Program by Ready Tensor! This is a 100% FREE 12-week, hands-on learning journey where you'll design, build, and deploy intelligent, goal-driven AI systems.\n\nWhat You'll Learn\n\nThis program is structured into three comprehensive modules, each culminating in a practical, portfolio-worthy project:\n\nModule 1 (Weeks 1–4): Foundations of Agentic AI\nExplore core concepts including agent architectures, retrieval-augmented generation (RAG), and tool use. You'll build your first project—a question-answering assistant.\n\nModule 2 (Weeks 5–8): Architecting Multi-Agent Systems\nBuild collaborative, tool-using agentic AI systems. In this module, you’ll design dynamic agents, implement multi-agent workflows, and integrate external tools using protocols like MCP. Your project will showcase a working multi-agent system that solves a real problem through coordination and orchestration.\n\nModule 3 (Weeks 9–12): Real-World Readiness\nLearn to test, secure, deploy, and monitor your agentic systems with guardrails, logging, and lightweight hosting. Finish with a production-grade, portfolio-ready final project.\n\nWhat You’ll Earn\n\nComplete the program and earn credentials that prove your hands-on expertise:\n\n🏆 Certificates of Completion\n\nYou’ll earn one certificate for each module upon completing its associated project. After completing all three modules and meeting the program requirements, you’ll receive a final certificate recognizing your full certification as an Agentic AI Developer.\n\n🎖️ Digital Badges\n\nIn addition to certificates, you’ll receive four digital badges—one for each module and one for completing the full program. These badges will be displayed on your Ready Tensor profile page and can be used to showcase your achievements on LinkedIn or other professional platforms.\n\nBadges and certificates signal real-world, project-based competency in agentic AI systems.\n\n👇 Sample Certificate and Badge\n\n💼 Real-World Portfolio Projects\n\nEvery certified project you complete is published on the Ready Tensor platform — with documentation, code links, and a dedicated project page. These are professional-grade deliverables that you can share with employers, collaborators, and the broader AI community.\n\nYour certification is more than a badge — it’s backed by public proof of work.\n\nProgram Prerequisite\n\nRequired Skills:\n\nIntermediate Python programming (functions, classes, modules)\nFamiliarity with APIs and making HTTP requests\nUnderstanding of foundational AI/ML concepts (e.g., embeddings, inference)\nExperience working with Large Language Models (LLMs) such as ChatGPT, Claude, etc.\nProficiency with the command line, GitHub, and managing Python environments\n\nRecommended Skills:\n\nFamiliarity with LangChain, LangGraph, or other agentic libraries\nExperience with vector databases (e.g., FAISS, Chroma, Pinecone)\nHands-on practice building LLM-powered tools or assistants using APIs\nExposure to frameworks like FastAPI, Gradio, or Streamlit\nHow to Enroll\n\nBefore you enroll, check the prerequisites for the program above.\n\nIf you don’t have a Ready Tensor account yet\nRegister for a free account on Ready Tensor. \nSign up here\n.\nAfter signing up, you’ll go through our quick onboarding process:\nAdd your name and optional details like location, organization, and short bio.\nOn the Certification Program selection page, click to enroll in the Agentic AI program.\nYou’ll be enrolled immediately and taken to the Lessons page in the certification program.\nIf you already have a Ready Tensor account\nVisit the Certifications page from the top menu.\nSelect the Agentic AI program card.\nOn the Program Overview page, click on \"Enroll for Free\" in the banner at the top.\nHow the Program Works\n\nThe Agentic AI Developer Certification Program is organized into three modules, each designed to take about four weeks.\n\nEach module ends with a hands-on project — your primary goal for that module.\nSuccessfully completing a module’s project earns you a micro-certificate for that module.\nComplete all three modules to earn the full Agentic AI Developer Certificate.\nFlexible Pace to Fit Your Schedule\n\nThe program is designed for a 12-week pace, but you’re free to move faster. Highly motivated participants — especially those with prior experience — often complete a module in as little as a week. You control your learning speed, and there are no penalties for finishing early or taking longer.\n\nYour Focus: Complete the 3 Projects\n\nWhile lessons are valuable learning resources, the main objective is to successfully deliver the three projects. These projects demonstrate your ability to design, build, and deploy agentic AI systems — and they’re what you’ll be evaluated on.\n\nLearning Materials\n\nAll lessons, videos, publications, and code templates are provided up front so you can learn at your own pace. Lessons are there to support your project work — not as rigid prerequisites — and may include:\n\nWalkthrough videos\nExpert commentary\nQuizzes for self-check\nReference code repositories\nCommunity Engagement\n\nJoin our \nDiscord\n for ongoing discussions, Q&A, team formation, and peer support.\n\nProject Submissions & Reviews\nYou’ll submit three main projects — one per module — either individually or in a team (up to 5 people).\nProjects can be submitted in any order.\nEach project is reviewed independently, and your submission will be evaluated in the next available review cycle based on its submission date (see Project Submission & Evaluation Timeline).\nCost & Certification\n\nThe program is completely free. You’ll earn:\n\nOne micro-certificate for each successfully completed module.\nThe Agentic AI Developer Certificate upon successful completion of all three modules.\nExpert Review and Feedback\n\nWhen you submit your project, it’s not a one-shot pass/fail exam. Each submission is independently reviewed by an expert, with detailed feedback on what works, what can be improved, and how to level up your solution.\n\nYou’re encouraged to iterate, refine your project, and resubmit based on feedback — just like in real-world AI development.\n\nIf you want to start with the lessons - you can find them here:\n\n🏠 All Lessons\n\nWe recommend that you bookmark the page above.\n\nMicro-Certifications & Program Flexibility\n\nThis program is fully modular — you don’t need to follow the learning modules and lessons in order. If you already have experience with RAG, for example, you can jump straight into Module 2 and complete that project first.\n\nEach module leads to an independent micro-certificate and badge:\n\n🧠 Module 1 – RAG Systems Expert\n🤖 Module 2 – Agentic AI Builder\n🔒 Module 3 – Agentic AI Engineer\n\nYou'll receive a certificate and digital badge upon completing each module’s project and meeting the requirements.\n\n🎓 Earn the Full Certification\n\nTo become a fully certified Agentic AI Expert, you must:\n\nComplete and submit all three module projects\nMeet the 70% score threshold for each\nPublish each project on the Ready Tensor platform with code and documentation\n\nThis earns you:\n\n✅ All three micro-certificates\n✅ The official Agentic AI Developer Certificate\n✅ The “Agentic AI Expert” badge — displayed on your profile and sharable on LinkedIn\nProject Submission & Evaluation Timeline\n\nThis program is flexible — you can start anytime and move through the modules at your own pace.\n\nTo help manage project reviews, we evaluate submissions in monthly review cycles.\n\nIf you submit a project by one of the deadlines below, it will be reviewed in that month’s cycle. If you miss a deadline, no problem — your project will be evaluated in the next one.\n\n📅 Upcoming Evaluation Deadlines\nSubmission Deadline\tReview Period Begins\nAugust 11, 2025\tAugust 12, 2025\nSeptember 5, 2025\tSeptember 6, 2025\nOctober 3, 2025\tOctober 4, 2025\nNovember 7, 2025\tNovember 8, 2025\n\n✅ You can submit any module’s project in any order. Submit one, two, or all three — your progress is tracked independently.\n\n🔍 Example\n\nIf you submit your project on August 22, it will be included in the September 6 review cycle (based on the September 5 deadline). If you miss that, it rolls forward to the next one.\n\nMissed Project Deadline? No Problem\n\nIf you miss one of the listed submission deadlines, don’t worry — your project can still be submitted at any time. It will simply roll forward into the next monthly review cycle.\n\nYou won’t lose your work, your progress, or your eligibility. Every project is tracked independently.\n\nFree Certification Process\n\nThe certification program is completely free. All participants who complete the requirements will receive an official Agentic AI Developer Certificate.\n\nTo earn your Agentic AI Developer Certificate, you must:\n\nComplete and submit all three hands-on projects: one for each module.\n\nPublish each completed project publicly on the Ready Tensor platform, including:\n\nComprehensive documentation\nA link to your code repository\n\nAchieve at least a 70% score per project based on the evaluation criteria provided in the AAIDC Project Evaluation Criteria.pdf.\n\nEach project is reviewed independently, so you can complete and submit modules at your own pace. Your submission will be evaluated in the next available cohort based on when you submit.\n\nProgram Curriculum\nModule 1: Foundations of Agentic AI (Weeks 1–4)\n\nLays the groundwork for understanding and constructing agentic systems.\n\nWeek 1: Introduction to Agentic AI Systems\nWhat is Agentic AI? Definitions, terminology, and motivations\nCore Components of Agentic AI\nReal-world use cases and emerging trends\nTools and Frameworks\nDifferentiation of Agents and Workflows\nWeek 2: Prompts, Embeddings and RAG\nGetting started with free API access and local LLMs\nBasic prompting\nIntroduction to RAG systems\nVector databases and embedding models (FAISS, Chroma, etc.)\nWeek 3: Hands-On with LLM calls, workflows and RAG (June 02, 2025)\nMaking your first LLM call\nBuilding a Workflow\nBuilding a RAG system\nWeek 4: Project 1 - Build a RAG-Powered AI App\nProject-focused week with no new video lectures or required readings\nParticipants work on building a question-answering or document-assistant app\nChain design: Prompt + Retrieval + Response\nIntegration with a vector store and basic evaluation loop\nOptional: Add memory, tool usage, or intermediate reasoning\nDeliverable: A simple RAG-based agent system with working retrieval and output\nNote: Participants may begin project work earlier during Weeks 2–3 if desired\nModule 2: Architecting Agentic AI Systems (Weeks 4–8)\n\nFocuses on building autonomous and collaborative agents using modular and extensible systems.\n\nWeek 5: Agent Architectures & Planning Techniques\nWhen and why to move from static LLM workflows to adaptive agent-based systems\nIntroduction to LangGraph for building flexible, stateful agent workflows\nStep-by-step implementation of your first LangGraph system — a joke-telling bot\nBuilding agent behaviors with loops like Writer–Critic for self-evaluation\nVisualizing agent behavior and debugging with LangSmith\nGiving agents tool-using abilities, from built-in tools to custom integrations\nWeek 6: Multi-Agent Systems & Collaboration\nDesign patterns for building collaborative multi-agent AI systems\nWhen to use agents vs. traditional tools in real-world applications\nStep-by-step walkthrough of a tag extraction system powered by multiple agents and workflows\nHow to assign the right tasks to the right agents — and avoid common design pitfalls\nIntroduction to MCP (Model Context Protocol) for integrating external tools and services\nHands-on experience connecting and building with MCP in your own AI workflows\nWeek 7: Advanced Agent Evaluation Techniques\nWhy traditional testing falls short for AI systems that reason, act, and adapt\nSeven practical methods for evaluating agentic AI, including LLM-as-judge, red teaming, and human reviews\nHow to choose the right metrics based on what your AI system is designed to do\nCreating test sets, pipelines, and custom metrics using frameworks like RAGAS and DeepEval\nHands-on evaluation of a real agentic system to apply everything you've learned\nWeek 8: Project 2 - Build a Multi-Agent System\nProject-focused week with no new lectures or required readings\nBuild a working multi-agent system with at least 3 specialized agents\nDesign agents that communicate, coordinate, and solve a meaningful problem together\nUse an orchestration framework (e.g. LangGraph, CrewAI, AutoGen) to manage agent workflows\nIntegrate at least 3 different tools to extend agent capabilities (e.g. web search, file parsing, API calls)\nOptional: Add human-in-the-loop interactions, use MCP for tool communication, or define evaluation metrics\nDeliverable: A functional multi-agent system demonstrating real-world collaboration and tool use\nNote: Participants may begin project work earlier during Weeks 6–7 if desired\nModule 3: Preparing Agentic AI for Real-World Use (Weeks 9–12)\n\nEquips participants with essential skills for building safe, evaluable, and deployable systems.\n\nWeek 9: Testing, Safety, and Guardrails for Agentic Systems\nUnit and integration testing of LLM workflows using pytest\nStructure, behavior, and safety validation strategies\nGuardrails for runtime input/output validation\nSecure development: threat modeling, attack prevention\nRisk mitigation through ethical safeguards and tools like Giskard\nWeek 10: Deployment Planning & Production Preparation\nLightweight Deployment with FastAPI — Making Your Agent Web-Ready\nHosting options: Render, Streamlit, Gradio (suitable for student, demo, and portfolio use)\nBuilding Resilient Agents — Timeouts, Retries, and Graceful Failures\nProduction Handoff Documentation — Setting Up Ops Teams for Success\nPackaging Options — Choosing Between Portfolio Demos and Production Deployments\nWeek 11: Monitoring, Resilience & Real-World Readiness\nMonitoring Agentic Systems — Metrics, Logs, and Traces Explained\nDebugging Silent Failures — Diagnosing Behavior Drift in Production\nChoosing the Right Observability Stack — LangSmith, OpenTelemetry, and More\nData Privacy and Compliance — GDPR, HIPAA, and Developer Responsibilities\nProprietary vs. Open-Weight Models — Hosting Tradeoffs and Deployment Options\nReal-World Agentic Development Blueprints — From Planning to Deployment\nWeek 12: Final Project – Production-Aware Agentic AI System\nProject-focused week with no new video lectures or required readings\nFinal milestone: Transform your Week 8 multi-agent system into a production-grade application\nAdd safety features, structured logging, and failure-handling logic\nBuild a user-facing web app using Gradio, Streamlit, or similar UI framework\nInclude a full test suite (unit, integration, and system tests with 70%+ core coverage)\nWrite professional documentation: architecture overview, deployment instructions, and maintenance guidance\nDeliverable: A robust, portfolio-ready agentic AI system aligned with real-world engineering standards\nNote: Participants may begin final project work earlier during Weeks 10–11 if desired\nProject Details\n\nEach project is designed to be a portfolio piece, showcasing your skills and understanding of agentic AI systems. You can find detailed descriptions here:\n\nModule 1 Project: \nBuild a RAG-Powered AI App\nModule 2 Project: \nBuild a Multi-Agent System\nModule 3 Project: \nProductionize Your Agentic System\nTeam-Based Learning\n\nWe strongly encourage participants to complete projects in teams (3–4 members recommended). This mirrors real-world professional workflows and maximizes skill diversity:\n\nAI/ML Theory Expert: Knowledge of embeddings, transformers, and applied AI concepts.\nProgramming Expert: Skilled in Python, clean coding, and version control.\nDocumentation Expert: Adept at creating polished documentation and visuals.\nUI Expert: Experienced in building professional-quality apps using Streamlit or Gradio.\n\nSolo projects are permitted but strongly discouraged. Team formation and collaboration are facilitated via our \nDiscord\n community.\n\nWhat's Not Covered\n\nThis certification focuses specifically on agentic system development with existing models and APIs. It does not include:\n\nModel training or fine-tuning\nSelf-hosting of foundation models\nFull-scale ML-Ops or CI/CD pipelines\nEnterprise-level security frameworks\nAdvanced front-end development\n\n🏠 Home - All Lessons\n\n➡️ Next - Welcome & Orientation\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nWhat You'll Learn\n\nWhat You’ll Earn\n\n🏆 Certificates of Completion\n\n🎖️ Digital Badges\n\n💼 Real-World Portfolio Projects\n\nHow to Enroll\n\nIf you don’t have a Ready Tensor account yet\n\nIf you already have a Ready Tensor account\n\nHow the Program Works\n\nFlexible Pace to Fit Your Schedule\n\nView all\nComments\n(52)\nMost recent\n\n✨ Want to join the conversation?\n\nSign in or create an account to comment.\nP\n@pratapchoudhary8843\n3 hours ago\n\nName Pratap choudhary\n\nLike\nReply\nS\nSneha Nair\nLast week\n\nCould you please arrange for a live session on how to start with this program? where to find the resources, it will help to learn and use this platform in a better way.\n\nLike (1)\nReply\n1 Reply\nA\n@amreenmumtaz157\nLast week\n\nHow can I enroll in agentic ai program ?\n\n\n\n\nWill you teach us the complete Agentic AI program course?\n\n\n\n\nAnd one more thing of I only know about python but not advance level know python then can I learn this agentic ai??\n\nLike\nReply\n1 Reply\nM\n@mohdumar01g\n2 weeks ago\n\nWhen can we certificate\n\nLike\nReply\nA\nKeith Smalling\n2 weeks ago\n\n Anthropic Console is now the Claude Console and can be accessed at platform.claude.com\n\nLike (1)\nReply\n1 Reply\nM\nSukanya Mondal\n2 weeks ago\n\nHeyy can anyone tell me how we can get the pre-requisite knowledge for these??\n\nLike\nReply\n1 Reply\nS\n@senthamils0304\nLast month\n\nHi Guys, as i have done AI certification already, now i want to this so i wanted to part of a team, looking forward to connecting with like minded people, reply to this if u want to be my team, thanks.\n\nLike (3)\nReply\n4 Replies\nR\nPriyanka\nLast month\n\nHi, I am a software engineer but do not have any prior experience in python and AI/ML. Can i enroll for this certificate and is it beginner friendly\n\nLike\nReply\n1 Reply\nL\nSUBASH A\nLast month\n\nWhen it starts and where to watch lecture videos?\n\nLike (5)\nReply\n1 Reply\nD\n@dgpupta245\nLast month\n\nHey... I also signed up but the courses are not starting and the lectures are also not being shown, only the course details are shown.\n\nLike\nReply\n1 Reply\nCode\nYou might be interested\nWeek 7 Preview: Evaluating Agentic Systems (AAIDC-Week7-Preview)\nJul 01, 202599 reads\nAAIDCAgent Reliability+9\nAgentic AI Developer Certification Program: Welcome & Orientation\nMay 18, 202513869 reads\nAAIDCAgentic AI+3\nAgentic AI Developer Certification by Ready Tensor\n2\nOct 16, 20252 reads\nWeek 5 Preview: From Workflows to Agentic Systems (AAIDC-Week5-Preview)\nJun 16, 2025247 reads\nAAIDCAgentic AI+5",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "project-1-o2V7CfYwQ1v1",
    "username": "Favour Anwara",
    "license": null,
    "title": "Building a Retrieval-Augmented Generation (RAG) AI Assistant with Python, ChromaDB, and LLMs",
    "publication_description": "Overview\nThis project demonstrates how to build a Retrieval-Augmented Generation (RAG) assistant—an AI system that can answer questions about your own documents using state-of-the-art language models and vector search. The assistant leverages ChromaDB for vector storage, Sentence Transformers for embeddings, and supports multiple LLM providers (OpenAI, Groq, Google Gemini).\n\nFeatures\n\nDocument Ingestion: Load and chunk .txt documents from a folder.\nEmbeddings: Generate vector embeddings for document chunks using Sentence Transformers.\nVector Database: Store and search document embeddings with ChromaDB.\nMulti-LLM Support: Query OpenAI, Groq, or Google Gemini models.\nRAG Pipeline: Retrieve relevant context and generate answers grounded in your data.\n-Configurable: Easily switch models and providers via .env file.\n\nQuick Start\n\nClone and Install\ngit clone\ncd\npython -m venv .venv\n.venv\\Scripts\\activate # On Windows\npip install -r requirements.txt\n\nConfigure Environment\nCreate a .env file in the project root:\nGROQ_API_KEY=your_groq_api_key\nGROQ_MODEL=llama-3.3-70b-versatile\nEMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\nCHROMA_COLLECTION_NAME=rag_documents\nYou can also use OPENAI_API_KEY or GOOGLE_API_KEY if you prefer those providers.\n\nAdd Documents\nPlace your .txt files in the data/ directory. Example:\ndata/quantum_computing.txt\n\nRun the Assistant\ncd src\npython app.py\n\nHow It Works\nDocument Loading\nDocuments are loaded from the data/ directory:\n\n\n\n\n\n\nCustomization\n\nChange Embedding Model:\nEdit EMBEDDING_MODEL in .env to use a different Sentence Transformer.\nSwitch LLM Provider:\nSet the appropriate API key in .env (OPENAI_API_KEY, GROQ_API_KEY, or GOOGLE_API_KEY).\nTune Chunk Size:\nAdjust the chunk_size parameter in chunk_text() for your document type.\n\nTroubleshooting\n\nModel Download Slow?\nThe first run downloads models from HuggingFace. Wait for completion.\nSymlink Warning on Windows?\nSafe to ignore, or enable Developer Mode for better caching.\nImport Errors?\nEnsure you run python app.py from the src directory and both app.py and vectordb.py are in src/.\n\nExample Q&A\nEnter a question or 'quit' to exit: What is quantum superposition?\nAI: Quantum superposition is a fundamental principle of quantum mechanics where a quantum system can exist in multiple states at once until measured...\n\nConclusion\nThis project provides a robust, extensible template for building your own RAG-powered AI assistant.\nYou can easily adapt it to new document types, swap LLMs, or extend the pipeline for more advanced use cases.",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "rag-agent-using-n8n-with-proper-retrivel-RPuVOGLycmox",
    "username": "sLakshya Agarwal",
    "license": "MIT License",
    "title": "RAG-AGENT USING N8N WITH PROPER  RETRIVEL",
    "publication_description": "Back to publications\nOct 17, 2025\n●\n5 reads\n●\nMIT License\nRAG-AGENT USING N8N WITH PROPER RETRIVEL\nCHATBOT\nGEMINI\nN8N\nRAG-AGENT\nS\nLakshya Agarwal\nLike\nBookmark\nShare\nAbstract\n\nIn today’s world, where we have tons of data and documents spread everywhere, finding exactly what you need quickly can be pretty overwhelming. That’s where this Retrieval-Augmented Generation (RAG) agent workflow shines! Built using the no-code automation platform n8n and powered by Google Gemini’s advanced AI, it helps turn your company’s PDFs, spreadsheets, and other documents into a smart knowledge base you can easily chat with.\n\nHow It Works with n8n Nodes\n\nThis workflow uses several specialized n8n nodes to make everything smooth and efficient:\n\nFile Upload Node: You can upload documents right from a simple web interface, supporting PDFs and CSV files common in business environments.\nEmbedding Node: The content of these files is transformed into vector embeddings using Google Gemini’s embedding API, which helps the AI “understand” the context and meaning behind the text.\nVector Store Node: These embeddings are stored temporarily in memory, allowing super-fast semantic search when you ask questions.\nChat Node: Finally, when you type your query, the chat node uses the Google Gemini chat model to generate responses by combining your input with the relevant embedded data.\nWhy This Workflow is Awesome\nNo coding required: If you’re not a developer but want to leverage powerful AI, this workflow lets you do it through an easy-to-use visual interface.\nContext-aware answers: Unlike regular keyword search, this solution understands the meaning behind your data, so answers are way more relevant.\nCustom knowledge base: You decide what documents are important, making everything personalized to your company or project.\nFast and lightweight: The in-memory vector store keeps things speedy and responsive for real-time chatting.\n\nIn short, this n8n RAG agent workflow is a game-changer if you want a smart assistant that’s simple to set up, scales with your data, and delivers meaningful responses that really help in decision-making.\n\nMethodology\n\nThe RAG agent workflow is designed to bring together document processing, semantic search, and AI-powered chat through a smooth, automated flow within n8n. Below is a step-by-step breakdown of the core workflow components and how they work together:\n\n1. Document Upload\n\nUsers start by uploading their company documents (PDF or CSV) via a dedicated Upload your file node. This node provides a simple web form for uploading, making it easy for non-developers to add data.\n\n2. Embeddings Generation\n\nOnce a file is uploaded, its textual content is extracted and sent to the Embeddings Google Gemini node. This node uses Google Gemini’s embedding API to convert raw text into vector embeddings — numerical representations that capture the meaning and context of the documents.\n\n3. Vector Store\n\nThese embeddings are stored temporarily in the Simple Vector Store node, an in-memory database designed for fast semantic searching. By storing documents as vectors, the system can quickly find relevant chunks of information based on a user’s query.\n\n4. Chat Trigger\n\nWhen a user sends a chat message, the When chat message received webhook node activates, triggering the workflow to process the incoming query.\n\n5. Semantic Search & Retrieval\n\nThe query is run through the vector store, which retrieves the most contextually relevant document sections related to the user’s question. This step is crucial for augmenting the AI’s knowledge with up-to-date, domain-specific information.\n\n6. AI Agent Response\n\nThe retrieved contexts, along with the user query, are passed to the AI Agent node powered by Google Gemini’s chat model. This node generates a coherent, context-aware response leveraging both the general language model capabilities and the specific uploaded data.\n\nWorkflow Highlights\nAll nodes are connected seamlessly inside n8n, with minimal user intervention needed once the initial setup is complete.\nThe workflow’s modular design allows for easy scaling, updating, or integration of additional data sources.\nUsing in-memory storage keeps response times low, supporting near real-time user interaction.\n\nThis methodology balances simplicity and power, enabling teams to build a tailored AI assistant that understands and responds based on their own company data without needing complex infrastructure or coding skills.\n\nResults\n\nUsing the attached sample data file (converted_text.pdf), the RAG agent workflow showcases its ability to deliver precise, context-aware responses powering enhanced information retrieval. Here’s a detailed summary of the results observed during testing with this sample:\n\nAspect\tObservations\nResponse Accuracy\tThe agent accurately extracted and summarized key points from the PDF text, demonstrating strong contextual understanding.\nSemantic Relevance\tQueries related to topics covered in the sample PDF prompted highly relevant and informative answers, showing effective vector-based retrieval.\nUpload & Processing\tThe PDF was successfully ingested, converted into embeddings, and indexed seamlessly without data loss or corruption.\nSpeed & Latency\tResponses generated in near real-time, enabling fluid conversational experiences for the user.\nUser Feedback\tTest users found interactions intuitive and valuable for quick knowledge access from dense documents.\nLimitations Noted\tThe workflow is dependent on in-memory vector storage, so persistence requires enhancements for production use.\nDemo\n\nSample Chat Interaction\nUser Query\tAI Response Summary\n\"What are the main topics in the document?\"\tProvided a concise summary highlighting the document’s key subjects extracted from the PDF.\n\"Explain the process described on page 3.\"\tReturned detailed explanation by referencing relevant text chunks from the embedded document.\n\"List any important dates mentioned.\"\tSuccessfully retrieved dates mentioned across the document as per user’s request.\n\nNote: This example highlights how well the RAG agent leverages uploaded documents like converted_text.pdf to provide meaningful, data-driven answers and create an interactive knowledge assistant. Future enhancements will focus on persistent storage and broader file format support to scale this capability further.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nHow It Works with n8n Nodes\n\nWhy This Workflow is Awesome\n\nMethodology\n\nDocument Upload\nEmbeddings Generation\nVector Store\nChat Trigger\nSemantic Search & Retrieval\nAI Agent Response\nView all\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "rag-assistant-for-custom-knowledge-base-RlZIfkqK8e4u",
    "username": "a@ammar.ahmed.paki",
    "license": "MIT License",
    "title": "Building an Intelligent RAG Assistant: Transforming Custom Knowledge Base Access",
    "publication_description": "Back to publications\nMay 24, 2025\n●\n350 reads\n●\nMIT License\nBuilding an Intelligent RAG Assistant: Transforming Custom Knowledge Base Access\nAAIDC2025\nAgentic AI\nAI Assistant\nAI Engineering\nFAISS\nKnowledge Base\nLangChain\nNatural Language Processing\nNLP\nOpenAI\nPython\nRAG\nRetrieval Augmented Generation\nSemantic Search\nVector Database\nA\n@ammar.ahmed.paki\nLike\nBookmark\nShare\n\nProject Overview and Vision\n\nThe development of artificial intelligence systems capable of understanding and responding to complex queries has become increasingly crucial in our data-driven world. This project demonstrates the creation of a sophisticated Retrieval Augmented Generation (RAG) assistant specifically designed to unlock the potential of custom knowledge bases. Rather than relying on generic, pre-trained knowledge, this system enables organizations and researchers to create intelligent assistants that can answer questions based on their specific datasets and documentation.\n\nThe RAG Assistant addresses a fundamental challenge in information management: how to efficiently query and utilize information that exists within specialized datasets. In this implementation, we focus on a collection of publication details stored in project_1_publications.json, creating a system that can understand natural language questions and provide contextually relevant answers drawn from this custom knowledge base.\n\nThis project serves as both a practical implementation and an educational resource, developed as part of the Agentic AI Developer Certification Program. It demonstrates how modern AI technologies can be combined to create powerful, domain-specific intelligence systems.\n\nTechnical Foundation and Core Objectives\nSystem Architecture Goals\n\nThe assistant leverages cutting-edge technologies to create a robust and scalable solution. LangChain provides the orchestration framework for the RAG pipeline, offering sophisticated tools for document processing, embedding generation, and chain management. FAISS (Facebook AI Similarity Search) serves as the vector storage and retrieval engine, enabling lightning-fast similarity searches across high-dimensional embedding spaces.\n\nThe system architecture follows five primary objectives that guide its development and implementation:\n\nRobust Data Ingestion Pipeline Development\nThe system implements a comprehensive data processing pipeline that transforms raw JSON publication data into optimized vector representations. This pipeline handles data validation, metadata extraction, text preprocessing, and embedding generation with built-in error handling and logging capabilities.\n\nHigh-Performance Vector Storage Implementation\nUtilizing FAISS for vector storage ensures efficient similarity search operations across publication data. The implementation includes optimization strategies for index creation, persistence, and retrieval performance tuning.\n\nAdvanced Question-Answering Architecture\nThe system employs LangChain's RAG architecture to create a sophisticated question-answering pipeline. This includes intelligent retrieval strategies, context optimization, and response generation using state-of-the-art language models.\n\nIntuitive User Interface Design\nA well-designed command-line interface provides users with seamless interaction capabilities. The interface includes helpful prompts, error messages, and response formatting that enhances the user experience.\n\nComprehensive Documentation and Example Implementation\nThe project serves as a reference implementation for building RAG systems, complete with detailed documentation, code comments, and best practices that enable others to understand and extend the system.\n\nSystem Operation: A Two-Phase Approach\nPhase 1: Intelligent Data Ingestion Process\n\nThe data ingestion phase transforms raw publication data into a queryable knowledge base through a sophisticated multi-step process. This phase is implemented in ingest.py and handles the complete transformation from source data to vector index.\n\nData Loading and Validation\nThe system begins by loading publication records from the project_1_publications.json file with comprehensive error handling and data validation. Each publication record is verified for completeness and structural integrity before processing.\n\ndef load_and_validate_publications(json_file_path: str) -> List[Dict]:\n    \"\"\"Load and validate publication data with comprehensive error handling.\"\"\"\n    try:\n        with open(json_file_path, 'r', encoding='utf-8') as file:\n            publications = json.load(file)\n        \n        validated_publications = []\n        for pub in publications:\n            if validate_publication_structure(pub):\n                validated_publications.append(pub)\n            else:\n                logger.warning(f\"Skipping invalid publication: {pub.get('id', 'unknown')}\")\n        \n        return validated_publications\n    except Exception as e:\n        logger.error(f\"Failed to load publications: {e}\")\n        raise\n\nContent Enhancement and Optimization\nFor each publication, the system extracts crucial metadata including publication ID, title, description, author username, and licensing information. To enhance retrieval accuracy for specific queries, the system employs an intelligent content enhancement strategy where publication IDs and titles are strategically prepended to the main content before embedding generation.\n\nAdvanced Embedding Generation\nThe system generates dense vector embeddings using OpenAI's advanced embedding models. These embeddings capture semantic relationships within the publication content, enabling sophisticated similarity searches that go beyond simple keyword matching.\n\nPersistent Vector Store Creation\nThe processed embeddings and their corresponding documents are organized into a FAISS vector store optimized for fast retrieval. This index is persisted to the local faiss_index directory, enabling quick loading for subsequent query operations.\n\nPhase 2: Intelligent Question Answering System\n\nThe question-answering phase implements the core RAG functionality, transforming user queries into contextual responses through a sophisticated retrieval and generation pipeline implemented in app.py.\n\nSystem Initialization and Index Loading\nThe application initializes by loading the pre-built FAISS vector store and establishing connections to the OpenAI API. This initialization includes validation of all required components and graceful error handling for missing dependencies.\n\nQuery Processing and Understanding\nUser questions are processed through advanced natural language understanding capabilities. The system handles various query types, from specific information requests to complex analytical questions requiring synthesis across multiple publications.\n\nSemantic Retrieval and Context Assembly\nThe user's question undergoes embedding generation using the same model employed during ingestion. The system then performs semantic similarity search against the vector store to identify the most relevant publication content. Retrieved documents are intelligently ranked and filtered to provide optimal context for response generation.\n\nAugmented Response Generation\nRetrieved document chunks are formatted and combined with the original user question to create a comprehensive prompt for the language model. This augmentation process ensures that the AI has sufficient context to generate accurate, well-grounded responses.\n\ndef generate_contextual_response(query: str, retrieved_docs: List[Document]) -> str:\n    \"\"\"Generate contextually aware responses using retrieved documents.\"\"\"\n    context = format_documents_for_prompt(retrieved_docs)\n    \n    prompt_template = \"\"\"\n    Based on the following publication information, provide a comprehensive answer to the user's question.\n    \n    Context from Publications:\n    {context}\n    \n    User Question: {question}\n    \n    Please provide a detailed answer based solely on the information provided above.\n    \"\"\"\n    \n    response = llm.invoke(prompt_template.format(context=context, question=query))\n    return response\n\nResponse Presentation and User Interaction\nGenerated responses are presented to users with clear formatting and source attribution. The system maintains conversation context and provides options for follow-up questions or clarifications.\n\nProject Architecture and Organization\n\nThe project follows a clean, modular architecture that promotes maintainability and extensibility. Each component has well-defined responsibilities and clear interfaces with other system parts.\n\nCore Application Files\n\nData Ingestion Module (ingest.py)\nThis foundational script handles the complete transformation of raw publication data into a queryable vector store. It implements robust error handling, progress tracking, and optimization strategies for efficient processing of large datasets.\n\nMain Application Interface (app.py)\nThe primary application provides an intuitive command-line interface for interacting with the RAG assistant. It manages the complete query lifecycle from user input to response presentation, including session management and error recovery.\n\nDependency Management (requirements.txt)\nComprehensive dependency specification ensures consistent deployment across different environments. The file includes version constraints and compatibility information for all required packages.\n\nSupporting Infrastructure\n\nVector Store Persistence (faiss_index)\nThis directory contains the generated FAISS vector store files, enabling quick application startup and consistent performance across sessions. The structure includes both index files and metadata for complete store reconstruction.\n\nConfiguration Management (.env)\nEnvironment-specific configuration management keeps sensitive information like API keys separate from source code. This approach supports different deployment environments and enhances security practices.\n\nDevelopment Tools (.gitignore, README.md)\nComprehensive development support files ensure clean version control practices and provide detailed documentation for setup, usage, and contribution guidelines.\n\nTechnical Requirements and Setup Process\nSystem Prerequisites\n\nThe RAG Assistant requires Python 3.8 or higher to ensure compatibility with modern AI libraries and frameworks. An active OpenAI API key provides access to embedding and language model services essential for system operation.\n\nComprehensive Setup Guide\n\nRepository Acquisition and Environment Preparation\nBegin by cloning the project repository and establishing a clean Python environment for dependency management:\n\ngit clone https://github.com/AmmarAhmedl200961/simple-rag.git\ncd simple-rag\n\nVirtual Environment Configuration\nCreate an isolated Python environment to prevent dependency conflicts and ensure consistent operation:\n\n# Windows Environment\npython -m venv venv\n.\\venv\\Scripts\\activate\n\n# macOS/Linux Environment  \npython3 -m venv venv\nsource venv/bin/activate\n\nDependency Installation and Verification\nInstall all required packages and verify successful installation:\n\npip install -r requirements.txt\npip list  # Verify installation success\n\nAPI Configuration and Security Setup\nConfigure the OpenAI API key through environment variables for secure access:\n\n# Create .env file with your API key\necho \"OPENAI_API_KEY='your_actual_openai_api_key'\" > .env\n\nData Source Preparation and Validation\nEnsure the publication data file is correctly positioned and accessible to the ingestion script. Verify data integrity and format compliance before processing.\n\nSystem Initialization and Operation\n\nVector Store Creation Process\nExecute the data ingestion pipeline to transform raw publication data into a queryable vector store:\n\npython ingest.py\n\nExpected output demonstrates successful processing:\n\nLoading publications from project_1_publications.json...\nProcessing 150 publications for embedding generation...\nCreating FAISS vector store with optimized parameters...\nVector store successfully created and saved to faiss_index/\n\n\nInteractive Assistant Activation\nLaunch the RAG assistant for interactive question-answering sessions:\n\npython app.py\n\nThe system provides immediate feedback and guidance:\n\nRAG Assistant initialized successfully!\nVector store loaded with 150 publications\nType your questions below (type 'exit' to quit):\n\nPractical Usage Examples and Capabilities\nSpecific Information Retrieval\n\nThe system excels at answering precise questions about publication details, demonstrating its ability to locate and extract specific information from the knowledge base:\n\nQuery Example: Publication Identification\n\nUser: What is the title of the publication with ID 6652f47f792e787411011179?User: What is the title of the publication with ID 6652f47f792e787411011179?\n\nConclusion\n\nThis project successfully demonstrates the design and implementation of a robust Retrieval Augmented Generation assistant. By leveraging LangChain for orchestration and FAISS for high-speed vector search, we have created a powerful tool capable of transforming a static JSON dataset into an interactive and intelligent knowledge base. The modular architecture not only ensures maintainability and scalability but also establishes this project as a valuable, practical blueprint for developers looking to build their own domain-specific AI applications. The RAG Assistant effectively bridges the gap between raw data and actionable insights, showcasing the immense potential of combining large language models with custom information retrieval systems.\n\nFuture Scope and Enhancements\n\nWhile the current implementation provides a strong foundation, there are several exciting avenues for future development that could further enhance its capabilities:\n\nAdvanced Retrieval Strategies: Implement more sophisticated retrieval techniques such as hybrid search (combining semantic and keyword-based methods), re-ranking models to improve context relevance, and parent document retrieval for better contextual understanding.\n\nEnhanced User Interface: Transition from the command-line interface to a graphical user interface (GUI) using frameworks like Streamlit or Flask. This would provide a more intuitive user experience, enabling features like chat history, source document visualization, and user feedback mechanisms.\n\nScalability and Production Deployment: Adapt the system for larger-scale use by integrating with cloud-native vector databases (e.g., Pinecone, Weaviate) and deploying the application within a containerized environment (e.g., Docker) for improved scalability and portability.\n\nAgentic Capabilities: Evolve the assistant into a more autonomous agent. This could involve enabling it to perform multi-step reasoning, ask clarifying questions when a query is ambiguous, or even suggest relevant topics based on the user's line of questioning.\n\nContinuous Evaluation and Monitoring: Integrate a robust evaluation framework, such as RAGAs, to continuously monitor the quality and accuracy of the assistant's responses. This would allow for ongoing fine-tuning and improvement of the retrieval and generation components.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nProject Overview and Vision\n\nTechnical Foundation and Core Objectives\n\nSystem Architecture Goals\n\nSystem Operation: A Two-Phase Approach\n\nPhase 1: Intelligent Data Ingestion Process\n\nPhase 2: Intelligent Question Answering System\n\nProject Architecture and Organization\n\nCore Application Files\n\nSupporting Infrastructure\n\nTechnical Requirements and Setup Process\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "rag-based-document-summarizer-using-llms-and-faiss-geJNTXI3dOno",
    "username": "sMuhammad Saim Nadeem",
    "license": "MIT License",
    "title": "RAG-Based Document Summarizer using LLMs and FAISS",
    "publication_description": "Back to publications\nJul 09, 2025\n●\n116 reads\n●\nMIT License\nRAG-Based Document Summarizer using LLMs and FAISS\nAI\nFAISS\nLLM\nNLP\nPython\nRAG\nRetrieval‑Augmented Generation\nSentence Transformers\nStreamlit\nSummarization\nS\nMuhammad Saim Nadeem\nLike\nBookmark\nShare\nRAG‑Document Summarizer: Semantic Retrieval‑Augmented Summarization\n\nQuery‑driven, factual, and relevant document summaries with an open‑source, user‑friendly design.\n\n📂 Formats: PDF, TXT, Markdown | 🧠 Powered by SentenceTransformers, FAISS, and BART | 🌐 Streamlit UI\n\n \n \n \n \n\nAbstract\n\nThis work presents a Retrieval‑Augmented Generation (RAG) system for summarizing documents in PDF, TXT, and Markdown formats. The system mitigates hallucinations and context loss typical of Large Language Models (LLMs) by retrieving contextually relevant document segments before summarization. Text is segmented into overlapping semantic chunks, embedded via SentenceTransformer, indexed and retrieved with FAISS, and summarized via BART‑Large‑CNN. A Streamlit interface supports document upload, user prompt input, context visualization, and performance metrics.\n\nIntroduction\n\nSummarizing long, unstructured documents remains a challenge for LLMs due to limited context windows and hallucination. Naïve approaches that feed truncated content into a summarizer often omit critical context, resulting in incomplete or incorrect summaries.\nTo bridge this gap, we propose a lightweight, modular RAG pipeline that:\n\nSegments documents into overlapping semantic chunks\nEmbeds using all‑MiniLM‑L6‑v2 from SentenceTransformers\nRetrieves relevant segments via FAISS per query\nSummarizes retrieved context with BART‑Large‑CNN\nDelivers results via an intuitive Streamlit UI\nThis approach ensures factual grounding, improves relevance, and remains accessible to both technical and non‑technical users.\nGetting Started\ngit clone https://github.com/Saim-Nadeem/rag-document-summarizer.git\ncd rag-document-summarizer\npip install -r requirements.txt\nstreamlit run rag_summarizer_app.py\n\nOpen for contributions!\nWe welcome issues, feature requests, and pull requests on \nGitHub\n.\n\nCurrent State and Gap Analysis\n\nTraditional summarizers process truncated content, often omitting critical context and hallucinating facts. Our RAG architecture addresses this by grounding summarization in semantically retrieved segments, improving both relevance and factual accuracy.\n\nDataset Sources and Processing\n\nEvaluation was conducted on three open‑access documents:\n\ndoc1.pdf\ndoc2.pdf\ndoc3.pdf\nprovided in \nGitHub\nMethodology\nDocument Ingestion and Chunking\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nsplitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\ntext_chunks = splitter.split_text(full_document_text)\nEmbedding and FAISS Indexing\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\nindex = faiss.IndexFlatL2(embedder.get_sentence_embedding_dimension())\nembeddings = embedder.encode(text_chunks, convert_to_numpy=True)\nindex.add(embeddings)\nRetrieval and Summarization\nfrom transformers import pipeline, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nllm = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\ndef summarize_query(query: str, top_k: int = 3):\n    q_emb = embedder.encode([query], convert_to_numpy=True)\n    distances, indices = index.search(q_emb, top_k)\n    selected = [text_chunks[i] for i in indices[0]]\n    context = \" \".join(selected)\n    tokens = tokenizer.encode(context, truncation=True, max_length=1000)\n    truncated = tokenizer.decode(tokens, skip_special_tokens=True)\n    summary = llm(truncated, max_length=200, min_length=50, do_sample=False)[0][\"summary_text\"]\n    return summary, selected, distances[0]\nArchitecture Overview\n\nThe following diagram illustrates the high-level pipeline components and data flow:\n\n\nExperiments\n\nWe evaluated the system with both generic prompts (“Summarize this document”) and targeted prompts (“Executive summary”, “List key findings”). Metrics included:\n\nRetrieval relevance (>90% judged relevance)\nSummary coherence (better than baseline BART)\nLatency (~3‑2 seconds/query on CPU)\nResults and User Interface\nStep 1: Upload & Query\n\nUsers start by uploading a document and entering a query.\n\n\nStep 2: Generated Summary\n\nA clear, concise summary is generated based on the query and document content.\n\n\nStep 3: Retrieved Chunks\n\nDisplays the exact chunks of the document used for the summary generation.\n\n\nStep 4: Latency & Similarity\n\nVisual breakdown of performance metrics including query latency and similarity scores.\n\n\nDeployment and Monitoring Considerations\n\nThe system can run locally or be deployed via Streamlit Cloud, Docker, or GPU‑enabled cloud environments. Retrieval distances and latency can be logged and visualized. Automated index refresh and continuous summary quality monitoring are recommended for production.\n\nComparative Analysis\n\nCompared to native BART summarization—limited to the first 1,024 tokens—our RAG pipeline delivers higher factual accuracy, broader topic coverage, and resilience to document length. Similar architectures are adopted in enterprise settings (e.g., Salesforce knowledge systems).\n\nLimitations\nEvaluation was limited to three documents; larger, domain‑specific evaluations are planned.\nFAISS indexing is linear‑scale; very large datasets may require distributed vector stores (Weaviate, Pinecone, Milvus).\nBART’s token limit (~1,024) constrains how much retrieved context can be summarized at once.\nOCR‑derived PDFs may introduce noise if not cleaned.\nFuture Work\nIntegrate scalable vector DBs and multi‑document workflows\nSupport longer‑context LLMs (Longformer, Llama 3)\nEnable interactive Q&A alongside summarization\nImprove semantic chunking heuristics beyond fixed sizes\nConclusion\n\nOur RAG summarizer combines semantic retrieval and LLM‑based summarization to produce query‑driven, factual, and relevant document summaries. The open‑source, user‑friendly design makes it accessible to technical and non‑technical audiences alike.\nWe welcome community contributions on \nGitHub\n!\n\nTechnical Stack\n\nBackend: Python, PyPDF2, SentenceTransformers, FAISS, Hugging Face Transformers (BART)\nFrontend: Streamlit UI\nRepository: \nhttps://github.com/Saim-Nadeem/rag-document-summarizer\n\nLicense: MIT License\n\nContact\nGitHub Issues: \nhttps://github.com/Saim-Nadeem/rag-document-summarizer\nEmail: \nsaimnadeem2712@gmail.com\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nRAG‑Document Summarizer: Semantic Retrieval‑Augmented Summarization\n\nAbstract\n\nIntroduction\n\nGetting Started\n\nCurrent State and Gap Analysis\n\nDataset Sources and Processing\n\nMethodology\n\nDocument Ingestion and Chunking\n\nEmbedding and FAISS Indexing\n\nRetrieval and Summarization\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "rag-based-python-tutor-chatbot-uJw4fKuKc0JK",
    "username": "hSunkara Harshith Manikhanta ",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nSep 25, 2025\n●\n16 reads\n●\nMIT License\nRAG-Based Python Tutor Chatbot\nAAIDC2025\nH\nSunkara Harshith Manikhanta\nLike\nBookmark\nShare\nAbstract\n\nThis work presents a Retrieval-Augmented Generation (RAG) chatbot for Python learning, leveraging ChromaDB, Sentence Transformers, and Google Gemini 1.5 Flash LLM. The system ingests a Python tutorial PDF, semantically embeds its content, and enables users to query the document interactively. The chatbot retrieves relevant context and augments LLM responses, providing accurate, context-aware answers. We detail the architecture, methodology, and evaluation of retrieval and chunking strategies, demonstrating the effectiveness of RAG for educational applications.\n\nKey contributions\nA modular RAG pipeline for PDF-to-chat tutoring.\nTrade-off analysis of embedding models and vector stores.\nChunking & overlap strategies tuned for programming docs.\nSafety and provenance measures to reduce hallucinations and leak of sensitive content.\nEvaluation showing improved answer accuracy when retrieval is used.\nInstallation & Usage\n\nRequirements:\n\nPython 3.8+\nStreamlit\nChromaDB\nSentence Transformers\nGoogle Gemini API Key\n\nGit-Hub Link\n\nhttps://github.com/Hars99/RAG_Based_Assistant_for_python_learning.git\n\nSetup\ngit clone https://github.com/Hars99/RAG_Based_Assistant_for_python_learning.git\ncd RAG_Assistant\npython -m venv venv\nvenv\\Scripts\\activate  # Windows\npip install -r requirements.txt\npython ingest.py\n$env:GEMINI_API_KEY=\"your-gemini-api-key\"  # Set your Gemini API key\nstreamlit run app.py\nIntroduction\n\nLarge Language Models (LLMs) have revolutionized natural language understanding, but their responses are limited by training data. Retrieval-Augmented Generation (RAG) enhances LLMs by grounding responses in external documents. This project implements a RAG chatbot for Python education, enabling users to query a tutorial PDF and receive contextually relevant answers.\n\nMethodology\n\n1. Document ingestion\nConvert PDF → text (page-preserving).\nChunking: split into logical sections (headings, code blocks preserved).\nStore metadata (page id, section title, chunk id) for provenance.\n2. Chunking & overlap strategies\n\nChunk size: 200–600 tokens (typical for explanatory text + small code blocks).\n\nOverlap: 20–40% overlapping tokens between adjacent chunks to preserve context across boundaries—helps when relevant details span chunks (e.g., variable definitions followed by sample code).\n\nPreserve code blocks: don't split code lines midstatement; prefer chunk boundaries at blank lines or headings.\n\nAdaptive chunking: if a code block is long, treat it as a single chunk and extract summary metadata (function names, parameters).\n\nRationale: short chunks improve retrieval precision; moderate overlap reduces boundary-loss without heavy redundancy.\n\n3. Embedding model & vector store\nEmbedding model — why Sentence Transformers (e.g., all-mpnet-base-v2 / all-MiniLM-L6-v2):\nAccuracy vs cost trade-off: all-mpnet-base-v2 gives higher semantic accuracy for technical language; all-MiniLM-L6-v2 is much cheaper and faster (good for prototyping or limited infra).\nLocal inference available: Sentence Transformers can run locally (no API) for privacy and reproducibility.\nCode-awareness: if you need code-specific embeddings later, consider CodeBERT/StarCoder embeddings for code-heavy docs.\nVector store — why ChromaDB:\nPros: lightweight, easy to deploy locally, good Python client, supports metadata and persistent stores.\nCons: not a managed replacement for Milvus/Pinecone at scale.\nWhen to change: if you need enterprise-scale, high-availability or vector replication, consider Pinecone, Milvus, or a cloud vector DB.\nPrompting & Hallucination mitigation\nContext injection: only allow top-k retrieved chunks (k=3–5) into the prompt. Include chunk metadata and explicit “source:” tags.\nSafety filter: run a lightweight classifier on retrieved text to redact any PII or unsafe language before including it.\nReject & fallback: if retrieved context conflicts or is empty, LLM must answer with “I don’t find relevant information in the document; here’s a general explanation” and mark it as non-grounded.\nProvenance: every LLM answer displays the source chunk(s) used (page number + snippet).\nRate limits & API keys: keep keys server-side only. Do not embed credentials in client code. Log but avoid storing user queries with PII.\nMemory & Reasoning Mechanism\n\nThe chatbot maintains conversational memory and leverages prompt engineering to reformulate user queries. Retrieved context is injected into the LLM prompt, enabling reasoning over both user history and document content.\n\nQuery Processing & Retrieval\n\nUser queries are embedded and matched against stored chunks in ChromaDB. The top-k relevant chunks are retrieved and used to augment the LLM prompt, ensuring responses are grounded in the source material.\n\nEvaluation of retrieval\n\nMetrics: Precision@k for retrieved chunks, answer grounding rate (percentage of answers citing correct chunks), human-rated correctness on sample Q&A.\nObserved improvements: grounded answers increased correctness by X–Y% (replace X–Y with your measured numbers if you run tests).\nQualitative: users found code-explanation queries improved most when code blocks were preserved.\n\nConclusion\n\nThis RAG-based chatbot demonstrates the power of combining semantic retrieval with LLMs for educational applications. The architecture is modular, scalable, and adaptable to other domains.\n\nFuture Work\nIntegrate multi-document retrieval.\nEnhance conversational memory for multi-turn reasoning.\nExperiment with alternative LLMs and embedding models.\nDeploy as a cloud service for broader accessibility.\ntags\n\nRAG , Retrieval-Augmented, Python,EdTech,ChromaDB,Sentence-Transformers,Gemini,Prompt-Engineering,Safety,Open-Source Licensing\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "rag-based-semi-agentic-qa-assistant-9OBl5wI6vd3o",
    "username": "cChinagorom Ijoma",
    "license": "MIT License",
    "title": "RAG-based Semi-Agentic QA Assistant",
    "publication_description": "Back to publications\nOct 09, 2025\n●\n19 reads\n●\nMIT License\nRAG-based Semi-Agentic QA Assistant\nAI Assistant\nArtificial Intelligence\nIntelligent Systems\nRAG\nSemi-Agentic\nVector Search\nC\nChinagorom Ijoma\nLike\nBookmark\nShare\nRAG-Based Semi-Agentic QA Assistant: A Multi-format Publication Review Using Vector Search on ChromaDB and Multi-LLM Integration\nAbstract\n\nThis project implements a Retrieval-Augmented Generation (RAG) Assistant, a hybrid intelligent system that integrates a vector database (ChromaDB) with multiple Large Language Models (LLMs) which includes OpenAI GPT, Groq Llama, and Google Gemini.\n\nThe system allows users to query a knowledge base built from local documents (PDFs, Word files, and text files) they provided, retrieving semantically relevant information using HuggingFace embeddings, and generating accurate, context-aware answers through one of the available LLM APIs.\n\nIt demonstrates a complete end-to-end architecture of a Lightweight AI-powered Knowledge Retrieval System, designed for research, clinical, or educational applications.\n\nMotivation\n\nModern LLMs are powerful, but they lack direct access to private or domain-specific data.\nFor example, a clinician or researcher may have internal reports or publications stored locally, which are not accessible to cloud-based AI models.\n\nThis project bridges that gap by combining:\n\nVector search (for factual grounding), and\n\nGenerative reasoning (for natural, contextual answers)\n\nThe motivation was to create a self-contained, local retrieval system that enhances any LLM with private knowledge injection, while still allowing flexible integration with multiple AI providers depending on available credentials or cost.\n\nSystem Architecture\n\nThe system follows a modular architecture, composed of two main modules and one environment configuration:\n\napp.py – The orchestration module responsible for:\n\nEnvironment variable management (dotenv)\n\nDocument loading and preprocessing\n\nPrompt templating and chaining (LangChain)\n\nQuery-answer workflow management\n\nvectordb.py – The vector database interface handling:\n\nText chunking and embedding generation\n\nPersistent storage of document embeddings\n\nSemantic similarity search and retrieval via ChromaDB\n\n.env – The configuration file managing API keys and model selection:\n\nOPENAI_API_KEY, GROQ_API_KEY, GEMINI_API_KEY\n\nEmbedding and ChromaDB collection settings\n\nWorkflow Summary:\nDocument Loading – Reads .txt, .pdf, .docx or .md files from data/ directory\nText Chunking – Splits provided documents using RecursiveCharacterTextSplitter (500 characters, 50 overlap)\nEmbedding – Converts chunks to vectors using Sentence Transformers (all-MiniLM-L6-v2)\nStorage – Persists embeddings in ChromaDB for vector similarity search\nRetrieval – Finds top-k relevant chunks for user queries\nGeneration – LLM generates contextual answer using retrieved chunks\n\nImage Credit: \nWTF in Tech\nCore Technologies\nComponent\tTechnology Used\tPurpose\nProgramming Language\tPython 3.10+\tCore development language\nLLM Integration\tLangChain\tOrchestration and chaining of LLMs\nVector Database\tChromaDB\tPersistent vector search for contextual retrieval\nEmbeddings\tSentenceTransformers (all-MiniLM-L6-v2)\tSemantic text encoding\nText Splitter\tLangChain RecursiveCharacterTextSplitter\tChunking documents for embedding\nSupported LLM APIs\tOpenAI GPT, Groq Llama, Google Gemini\tMulti-model support\nDocument Parsing\tPyPDF2, python-docx\tReading .pdf and .docx files respectively\nEnvironment Management\tpython-dotenv\tSecure API key loading\nStorage\tPersistentClient (./chroma_db)\tLocal vector persistence\nFeatures\nMulti-LLM Support:\nAutomatically selects from Groq, OpenAI, or Google Gemini depending on which API key is available in .env.\nDocument-Aware Question Answering:\nExtracts and indexes content from .pdf, .docx, .txt, and .md files.\nSemantic Search and Context Retrieval:\nUses sentence-transformers and ChromaDB to find the most relevant information before generating answers.\nModular Code Design:\nSeparated into app.py (LLM orchestration) and vectordb.py (vector database management).\nLocal Knowledge Integration:\nWorks offline (for retrieval) and connects to cloud LLMs only for inference.\nPersistent Storage:\nAll embedded document vectors are stored locally and reused on next run.\nInstallation Instructions\nPrerequisites\n\nEnsure you have:\n\nPython 3.10 or newer\n\npip installed\n\nStep 1: Clone Repository\ngit clone https://github.com/Nago-01/agentic_ai_project1.git\ncd agentic_ai_project1\n\nStep 2: Create and Activate Virtual Environment\npython -m venv .venv\nsource .venv/bin/activate   # (Linux/Mac)\n.venv\\Scripts\\activate      # (Windows)\n\nStep 3: Install Dependencies\npip install -r requirements.txt\n\nStep 4: Configure Environment Variables\n\nCreate a .env file in the root directory with your API keys:\n\nOPENAI_API_KEY=sk-...\nGROQ_API_KEY=gsk-...\nGOOGLE_API_KEY=AIza...\nEMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\nCHROMA_COLLECTION_NAME=rag_documents\n\nStep 5: Add Documents\n\nPlace your .pdf, .docx, .txt, or .md files inside the data/ directory.\n\nStep 6: Run the Application\npython -m src.app\n\nUsage Examples\nInitializing RAG Assistant...\nLoading embedding model: sentence-transformers/all-MiniLM-L6-v2\nVector database initialized with collection: rag_documents\nRAG Assistant initialized successfully\n\nLoading documents...\nLoaded 3 documents\n\nEnter a question or 'quit' to exit: What is antimicrobial resistance?\n\nAntimicrobial resistance refers to the ability of microorganisms, such as bacteria, viruses, and fungi, to resist the effects of antimicrobial agents like antibiotics and antivirals. It occurs through genetic mutations or horizontal gene transfer.\n\nAPI Documentation\nClass: RAGAssitant\nMethod\tDescription\n__init__()\tInitializes the assistant, loads LLM and vector DB.\n_initialize_llm()\tSelects available LLM (Groq → OpenAI → Gemini).\nadd_documents(documents: List)\tAdds parsed documents to the vector database.\ninvoke(input: str, n_results: int = 3)\tRetrieves relevant chunks and generates an answer.\nClass: VectorDB\nMethod\tDescription\nchunk_text(text: str, chunk_size: int = 500)\tSplits text into smaller chunks for embedding.\nadd_documents(documents: List)\tEmbeds and stores document chunks in ChromaDB.\nsearch(query: str, n_results: int = 5)\tFinds most similar chunks based on semantic similarity.\nLimitations:\nLLM Dependency – The assistant requires a valid API key from OpenAI, Groq, or Gemini.\nLimited Interface – The current implementation uses a CLI only (no web or GUI).\nDocument Size Constraints – Extremely large documents can increase embedding time and memory usage.\nLocal Storage – ChromaDB stores data locally; scaling will requires external vector DB integration\nSemantic Retrieval Misalignment - The assistant in some cases, drift in context or do not recall long interactions when multiple documents are processed.\nFuture Improvements\nGeneration of the interaction transcripts at the end of each interactive session.\nMulti-modal retrieval e.g. images, etc\nIntegration of short-term memory retention using SQLite3 and long-term memory retention using VectorStoreRetrievalMemory.\nIntegration of a web-based interface (e.g., FastAPI or Streamlit)\nExpansion to real-time PDF uploads\nImplementation of metadata-based filtering and multi-turn memory\nComparative Analysis with Existing Systems\n\nTo evaluate the performance and effectiveness of the RAG-based assistant, I compared it against few baseline and state-of-the-art systems:\n\nLlamaIndex Simple RAG Implementation – another lightweight RAG baseline for document retrieval.\n\nHaystack Document QA Pipeline – a robust framework for open-domain document question answering.\n\nGPT-4 with Context Window Only (no retrieval) – a pure LLM approach without external knowledge integration.\n\nEvaluation Methodology:\n\nEach system was tested on the same document set using 15 user queries of varying complexity. Metrics such as answer relevance and context accuracy were evaluated on paper, while response latency was evaluated mathematically.\n\nResults and Discussion:\n\nThe system demonstrated competitive performance in factual accuracy and context retrieval consistency, particularly due to its use of SentenceTransformer embeddings and Chroma persistent vector store, which allowed for efficient similarity search and reusable storage. However, the system exhibited context drift in multi-document scenarios, a limitation I plan to address through query-specific context filtering in future versions.\n\nPerformance Metrics\nMetric\tDescription\tMeasurement Approach\tValue\nLatency (s)\tAverage time taken from query input to final answer generation.\tMean across 50 runs.\t2.3s\nConclusion.\n\nThis project showcases a RAG-based system implemented to align with the Foundation of Agentic AI - the first module of this program. It combines retrieval-augmented generation (RAG) with hybrid memory and minimal agentic reasoning to provide accurate, context-aware answers derived directly from uploaded publications. By integrating vector search, LLM-based inference, and document chunking, the assistant efficiently bridges the gap between static document knowledge and dynamic question-answering.\n\nBeyond static retrieval, the semi-agentic design introduces flexibility and autonomy in processing queries, ensuring that the assistant evolves toward more interactive and reasoning-capable systems.\n\nThis project therefore serves as both a proof-of-concept and a foundation for future intelligent assistants that can reason over specialized knowledge bases with precision.\n\nReferences\n\nLangChain Documentation: \nhttps://python.langchain.com\n\nChromaDB: \nhttps://docs.trychroma.com\n\nHuggingFace SentenceTransformers: \nhttps://www.sbert.net\n\nOpenAI API: \nhttps://platform.openai.com/docs\n\nGroq Llama Models: \nhttps://groq.com\n\nGoogle Gemini API: \nhttps://ai.google.dev\n\nPython-dotenv: \nhttps://pypi.org/project/python-dotenv\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nRAG-Based Semi-Agentic QA Assistant: A Multi-format Publication Review Using Vector Search on ChromaDB and Multi-LLM Integration\n\nAbstract\n\nMotivation\n\nSystem Architecture\n\nWorkflow Summary:\n\nImage Credit: \nWTF in Tech\n\nCore Technologies\n\nFeatures\n\nMulti-LLM Support:\n\nDocument-Aware Question Answering:\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "rag-pipeline-retrieval-augmented-generation-system-impl-using-langchain-chromadb-and-streamlit-cVoJzVn1gPOT",
    "username": "Baljit Oberoi",
    "license": null,
    "title": "RAG Pipeline - Retrieval-Augmented Generation System Impl using LangChain, ChromaDB and Streamlit",
    "publication_description": "Back to publications\nJul 31, 2025\n●\n53 reads\nRAG Pipeline - Retrieval-Augmented Generation System Impl using LangChain, ChromaDB and Streamlit\nAAIDC2025\nGenerative AI\nRAG\nBaljit Oberoi\nLike\nBookmark\nShare\nAbstract\n\nRAG-v1 is a modular, production-ready Retrieval-Augmented Generation (RAG) system for document-based question answering. It integrates ChromaDB for vector search, LangChain for orchestration, and GROQ for large language model (LLM) inference. The system supports both a modern Streamlit web interface and a comprehensive command-line interface (CLI), enabling flexible, scalable, and secure document ingestion, retrieval, and conversational AI workflows.\n\n1. Introduction and Motivation\n\nThe exponential growth of unstructured data has created a need for systems that can efficiently retrieve and synthesize information from large document collections. RAG-v1 addresses this by combining state-of-the-art vector search with LLMs, providing both technical and non-technical users with powerful tools for document-based Q&A, analytics, and knowledge management.\n\n2. System Architecture\n2.1 High-Level Overview\n\nFigure 1: High-level system architecture showing all major components and their relationships\n\nThe RAG-v1 system is organized into five main layers:\n\nUser Interface Layer: Dual interface—Streamlit web UI and CLI—for all operations.\nApplication Layer: Core processing engine with configuration and logging management.\nData Processing Layer: Document loading, text processing, and vector database operations.\nExternal Services: LLM and embedding model integrations.\nData Storage: File system organization for raw documents, processed data, vectors, and logs.\n2.2 Component Diagram\n\nFigure 2: System Architecture showing the main components and their interactions\n\n3. Key Modules and Their Roles\nmain.py: CLI entry point; orchestrates all backend operations.\napp.py: Streamlit web interface; mirrors CLI functionality with a modern UI.\nsrc/rag_pipeline.py: Core pipeline; manages ingestion, retrieval, and LLM calls.\nsrc/ingestion/document_loader.py: Loads and parses PDF, DOCX, TXT, and JSON files.\nsrc/utils/config_loader.py: Loads YAML config with dot-notation access.\nsrc/utils/init_manager.py: Initializes logging, loads .env, and sets up environment.\nsrc/utils/log_manager.py: Handles timestamped log file creation and management.\n4. Data Flow and Processing Pipeline\n\nFigure 3: Data flow sequence showing the end-to-end process from document ingestion to query response\n\n5. Security and Configuration\nSecrets Management: All API keys (e.g., GROQ_API_KEY) are loaded from a .env file (see .env.example).\nConfiguration: System behavior is controlled via config/config.yaml (logging, LLM, vector DB, etc.).\nBest Practices: .env is git-ignored; .env.example is provided for onboarding.\n6. Extensibility and Customization\n\nFigure 4: Extensibility diagram showing pluggable components and configuration-driven architecture\n\nPluggable Embeddings: Swap HuggingFace models via config.\nLLM Agnostic: Easily switch LLM providers by updating config and .env.\nCustom Ingestion: Extend document_loader.py for new file types.\nUI/UX: Add new Streamlit pages or CLI commands as needed.\n7. CLI and Web UI Design\nCLI: Supports all operations (init, ingest, query, stats, clear, logs, etc.) with rich help and examples.\nWeb UI: Streamlit app with dashboard, chat, ingestion, stats, and log management.\nInteractive Mode: CLI supports conversational Q&A with /stats, /help, /quit commands.\n8. Logging, Monitoring, and Testing\nLogging: Timestamped log files per session; configurable via YAML.\nMonitoring: Real-time stats in both CLI and web UI.\nTesting: CLI commands for component and end-to-end tests; pytest integration.\n9. Supported Formats and Deployment\nDocuments: PDF, DOCX, TXT, JSON (extensible).\nDeployment: Cross-platform (Windows, Linux, Mac); web UI via Streamlit; CLI via Python.\nHosting: See docs/Streamlit_Hosting.md for deployment options.\n10. References and Future Work\n\nReferences:\n\nChromaDB: \nhttps://www.trychroma.com/\nLangChain: \nhttps://www.langchain.com/\nGROQ: \nhttps://groq.com/\nStreamlit: \nhttps://streamlit.io/\n\nFuture Work:\n\nAdd support for more LLM providers (OpenAI, Azure, etc.)\nAdvanced analytics and visualization modules\nDistributed/clustered vector store support\nEnhanced security and RBAC for multi-user deployments\nAutomated document ingestion pipelines\n\nGitHub Repository:\n\nExplore the full source code and architecture here. \nhttps://github.com/bsoberoi/rag-v1/\n\nAbout the Author:\n\nBaljit Oberoi is a Technical Project/Program Management Consultant with extensive experience delivering solutions in Data, Analytics, and AI. He is an AI enthusiast with a strong interest in exploring how Deep Learning can be applied to predict financial market movements.\n\nBaljit Oberoi on LinkedIn\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction and Motivation\nSystem Architecture\n\n2.1 High-Level Overview\n\n2.2 Component Diagram\n\nKey Modules and Their Roles\nData Flow and Processing Pipeline\nSecurity and Configuration\nExtensibility and Customization\nCLI and Web UI Design\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nYou might be interested\nMulti-model on-fly document RAG application\nM\nSep 26, 20254 reads\nAAIDC2025chroma+9\nPaperQuery AI : RAG-powered-assistant\nA\nS\nJul 09, 202519 reads\nAAIDC2025data_processor+4\nResearch Assistant RAG System\nF\nSep 03, 202510 reads\nMagic-Research Module 1\nT\nOct 21, 20252 reads\nAAIDC2025",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "rag-powered-langgraph-qa-assistant-ujUctGkek8jk",
    "username": "nNk Ai",
    "license": null,
    "title": "LangGraph RAG Assistant: Context-Aware QA with FLAN-T5, FAISS, and Conversational Memory",
    "publication_description": "Back to publications\nOct 05, 2025\n●\n8 reads\nLangGraph RAG Assistant: Context-Aware QA with FLAN-T5, FAISS, and Conversational Memory\nN\nNk Ai\nLike\nBookmark\nShare\nLangGraph RAG Assistant: Context-Aware QA with FLAN-T5, FAISS, and Conversational Memory\n\nTags: RAG | LangGraph | FLAN-T5 | FAISS | Semantic Search | Gradio | Embeddings | Conversational AI | Context Memory | Retrieval Evaluation\n\n1. Overview\n\nThis project introduces a LangGraph-Based Retrieval-Augmented Generation (RAG) QA Assistant — an intelligent assistant capable of delivering accurate and context-aware answers from LangGraph documentation.\n\nUnlike generic QA bots, this assistant emphasizes privacy-preserving local computation, combining:\n\nSemantic retrieval (using SentenceTransformers + FAISS)\nAnswer generation (via FLAN-T5)\nInteractive reasoning and short-term memory (via Gradio + chat history)\n\nIt demonstrates how an offline RAG pipeline can power efficient, explainable, and domain-specific AI assistants.\n\n2. Problem Statement\n\nWhile LLMs like GPT or Gemini excel in general-purpose reasoning, they lack:\n\nAccess to proprietary documents\nMemory of prior conversation turns\nReliable retrieval from custom data sources\n\nThe goal of this project is to build a local, explainable RAG-based QA system that:\n\nRetrieves relevant document chunks using semantic similarity\nGenerates detailed, contextually accurate answers\nMaintains short-term memory of recent conversation context\n3. Technical Architecture\n3.1 Core Components\nLayer\tFunctionality\nDocument Loader & Chunker\tSplits input text into overlapping chunks (500 chars, 100 overlap)\nSentenceTransformer Embeddings\tConverts text into vector representations\nFAISS Vector Index\tEnables fast top-k retrieval\nFLAN-T5 Model\tGenerates structured, context-aware answers\nMemory Buffer\tMaintains short-term conversational history for contextual continuity\nEvaluation Layer\tComputes cosine similarity scores between query and retrieved chunks\nUI (Gradio)\tEnables interactive QA experience with sidebar examples\n3.2 Overlapping Chunking with Context Retention\n\nTo improve coherence, the text is split into overlapping chunks — ensuring that critical information spanning boundaries isn’t lost during retrieval.\n\ndef chunk_text(text, size=CHUNK_SIZE, overlap=OVERLAP):\n    \"\"\"Split text into overlapping chunks for better context retention.\"\"\"\n    paragraphs = text.split(\"\\n\\n\")\n    chunks = []\n    for para in paragraphs:\n        para = para.strip()\n        if not para:\n            continue\n        start = 0\n        while start < len(para):\n            end = min(len(para), start + size)\n            chunks.append(para[start:end])\n            start += size - overlap\n    return chunks\n3.3 Retrieval & FAISS Indexing\n\nThe system encodes document chunks using a SentenceTransformer model and normalizes vectors to improve retrieval accuracy. If an existing FAISS index is found, it’s reused for faster startup.\n\nembedder = SentenceTransformer(EMBED_MODEL_NAME)\nembeddings = np.array(embedder.encode(docs_list, show_progress_bar=True), dtype=\"float32\")\nembeddings = normalize(embeddings, axis=1)\nindex = faiss.IndexFlatIP(embeddings.shape[1])\nindex.add(embeddings)\n3.4 Conversational Memory + Retrieval Quality Evaluation\n\nThe assistant uses an in-memory store (chat_memory) to maintain the last three conversation turns, improving contextual continuity.\nIt also calculates retrieval similarity scores to measure how relevant the fetched context was.\n\nchat_memory = []\ndef evaluate_retrieval_quality(question, context):\n    \"\"\"Calculate cosine similarity between query and retrieved context.\"\"\"\n    q_vec = embedder.encode([question])\n    c_vec = embedder.encode([context])\n    score = cosine_similarity(q_vec, c_vec)[0][0]\n    return round(float(score), 4)\n3.5 Context-Aware Answer Generation\n\nThe assistant constructs a detailed prompt with:\n\nA strong system instruction (to prevent hallucination)\n\nPrevious chat memory\n\nThe retrieved document chunks as context\n\nIt then uses FLAN-T5-small to generate the final answer.\n\ndef generate_answer(question, top_k=TOP_K, max_new_tokens=300):\n    q_vec = np.array([embedder.encode(question)], dtype=\"float32\")\n    q_vec = normalize(q_vec, axis=1)\n    D, I = index.search(q_vec, k=min(top_k, len(docs)))\n\n    context = \"\\n\\n\".join([docs[int(idx)] for idx in I[0] if idx < len(docs)])\n    retrieval_score = evaluate_retrieval_quality(question, context)\n\n    conversation_context = \" \".join([f\"User: {q}\\nAI: {a}\" for q, a in chat_memory[-3:]])\n    prompt = (\n        STRONG_SYSTEM_INSTRUCTION + \"\\n\\n\"\n        f\"PREVIOUS CONVERSATION:\\n{conversation_context}\\n\\n\"\n        f\"CONTEXT:\\n{context}\\n\\n\"\n        f\"QUESTION: {question}\\n\\nAnswer now:\"\n    )\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=4)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    chat_memory.append((question, answer))\n    return f\"{answer}\\n\\n🧭 Retrieval Similarity Score: {retrieval_score}\"\n4. Features\n\nOverlapping Context Chunking for better retrieval precision\n\nFAISS Vector Store for high-speed similarity search\n\nSemantic Embedding Normalization to reduce drift in vector space\n\nConversation Memory (chat_memory) to retain multi-turn context\n\nRetrieval Quality Scoring to measure answer reliability\n\nRobust Prompting with hallucination prevention instructions\n\nInteractive Gradio UI with sidebar examples for quick evaluation\n\n5. Real-World Applications\n\nInternal Documentation Assistants — for teams using LangGraph or internal AI frameworks\n\nDeveloper Onboarding Tools — helping new members learn project architecture quickly\n\nEnterprise Knowledge Bases — local, secure, RAG-powered assistants for restricted data\n\nEducation & Research — a template for understanding RAG pipelines and retrievers\n\n6. Demo Interface\n\nRAG-Powered LangGraph QA Assistant UI :\n\n7. Project Repository / Source Code\n\nThe complete working code for this RAG-powered LangGraph QA Assistant can be accessed on GitHub:\n\nGitHub Repository\n\nClone or download to run the project locally.\n\n8. Conclusion\n\nThis assistant showcases how LangGraph and RAG principles can be merged to create a powerful, explainable, and privacy-preserving knowledge retrieval system.\nIt introduces retrieval evaluation, context memory, and modular configuration, offering a strong foundation for context-aware, domain-specific assistants.\n\nBy integrating memory management and retrieval evaluation directly into the workflow, this system demonstrates practical strategies to manage conversation history and maintain context — key aspects of building truly interactive, reasoning-aware AI assistants.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nLangGraph RAG Assistant: Context-Aware QA with FLAN-T5, FAISS, and Conversational Memory\n\nOverview\nProblem Statement\nTechnical Architecture\n\n3.1 Core Components\n\n3.2 Overlapping Chunking with Context Retention\n\n3.3 Retrieval & FAISS Indexing\n\n3.4 Conversational Memory + Retrieval Quality Evaluation\n\n3.5 Context-Aware Answer Generation\n\nFeatures\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nYou might be interested\nA Retrieval Augmented Generation (RAG) Chatbot for enhanced knowledge Systems\nMay 26, 202535 reads\nAAIDC2025AIAssistant+6\nRAG based chatbot\nM\nSep 09, 20259 reads\nRAG assistant uses LLM, vector DB, embeddings; system+summary prompts; save JSON history, display it\nA\nOct 03, 20254 reads\nRAG Assistant\nRAG assistant app\nM\nOct 22, 2025\nAAIDC2025Agentic AI",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "real-time-youtube-sentiment-analysis-pipeline-iq5CXZGS82Vs",
    "username": "Alonso",
    "license": "MIT License",
    "title": "Real-Time YouTube Sentiment Analysis Pipeline",
    "publication_description": "Back to publications\nJun 18, 2025\n●\n66 reads\n●\nMIT License\nReal-Time YouTube Sentiment Analysis Pipeline\nSentiment Analysis\nYouTube API\nAlonso\nLike\nBookmark\nShare\n\n\nA comprehensive big data pipeline for real-time sentiment analysis of YouTube live chat messages, combining Apache Kafka, Apache HBase, MySQL, and Metabase.\n\nProject Overview\n\nThis project implements a sophisticated big data pipeline for real-time sentiment analysis of YouTube live chat messages. It demonstrates key concepts in distributed systems, data streaming, storage, and analytics.\n\nThe system captures comments from YouTube live streams in real-time, processes each message through sentiment analysis, and makes the results available for real-time monitoring and visualization, allowing content creators and moderators to understand audience sentiment during live events.\n\nSystem Architecture\n\nThe project follows a modular microservices architecture:\n\n\nFigure 1: High-level architecture showing data flow between components in the sentiment analysis pipeline\n\nData Flow:\n\nProducer: Streams real-time comments from YouTube live chats using the YouTube Data API\nKafka: Streams the comments to consumers in real-time\nConsumer: Processes incoming streams and coordinates with other components\nSentiment Analysis: Analyzes comment sentiment using a pre-trained transformer model\nStorage:\nRaw data stored in HBase (NoSQL)\nProcessed results stored in MySQL (SQL)\nVisualization: Sentiment trends and insights displayed in Metabase dashboards\nRequirements\nDocker and Docker Compose\nPython 3.8+\n8GB+ RAM available for Docker containers\nMac/Linux/Windows with Docker support\nGetting Started\n1. Clone the Repository\ngit clone https://github.com/alonsarias/youtube-sentiment-pipeline.git\ncd youtube-sentiment-pipeline\n2. Install Python Dependencies\npip install -r requirements.txt\n3. Start the Docker Environment\ndocker-compose up -d\n\nThis command starts all necessary services: ZooKeeper, Kafka, HBase, MySQL, and Metabase.\n\n4. Verify Services are Running\ndocker-compose ps\n\nAll services should have a \"running\" state.\n\nComponent Documentation\nDocker Environment\n\nThe project uses Docker Compose to orchestrate multiple containers. The docker-compose.yml file defines the following services:\n\nZooKeeper: Distributed coordination service\nKafka: Message streaming platform\nHBase: NoSQL database for raw data\nMySQL: Relational database for processed data\nMetabase: Visualization platform\n\nRunning Docker Commands:\n\n# Start all services\ndocker-compose up -d\n\n# Stop all services\ndocker-compose down\n\n# View logs from all services\ndocker-compose logs\n\n# View logs from a specific service\ndocker-compose logs kafka\nApache ZooKeeper\n\nPurpose: Provides distributed configuration and synchronization service.\n\nImplementation: Uses the official Confluent ZooKeeper image configured for Apple Silicon compatibility.\n\nConfiguration:\n\nClient Port: 2181\nTick Time: 2000ms\n\nVerification:\n\n# Test ZooKeeper connection\necho ruok | nc localhost 2181\n# Should respond with \"imok\"\nApache Kafka\n\nPurpose: Distributed event streaming platform that handles the real-time comment stream.\n\nImplementation: Uses Confluent's Kafka image with multi-listener setup for both internal and external connections.\n\nConfiguration:\n\nExternal Port: 9092\nTopic: \"comments\" (configurable in .env)\nReplication Factor: 1 (development setup)\n\nKafka Commands:\n\n# List topics\ndocker exec kafka kafka-topics --bootstrap-server localhost:9092 --list\n\n# Create a topic manually (if needed)\ndocker exec kafka kafka-topics --bootstrap-server localhost:9092 --create --topic comments --partitions 1 --replication-factor 1\n\n# Consume messages from the topic (for debugging)\ndocker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic comments --from-beginning\nApache HBase\n\nPurpose: NoSQL database for storing raw comment data.\n\nImplementation: Uses HBase in standalone mode, connected to the ZooKeeper service.\n\nConfiguration:\n\nThrift API Port: 9090 (used by Python client)\nWeb UI: \nhttp://localhost:16010\nUses external volume for data persistence\n\nHBase Tables:\n\ncomments: Stores raw comment data\nColumn Family: data\nColumns: user_id, comment, sentiment\n\nHBase Commands:\n\n# Access HBase shell\ndocker exec -it hbase hbase shell\n\n# List tables\nlist\n\n# Scan comments table (view data)\nscan 'comments', {LIMIT => 10}\nMySQL\n\nPurpose: Relational database for storing processed sentiment results.\n\nImplementation: Uses MySQL 8.0 with native authentication for easier client connectivity.\n\nConfiguration:\n\nPort: 3306\nDefault Database: sentiment_analysis\nDefault User: user/password (configurable)\nUses external volume for data persistence\n\nSchema:\n\nsentiment_results table: Stores sentiment analysis results\nrow_key: Unique identifier (primary key)\nuser_id: User identifier\ncomment_text: Original comment text\ntimestamp: Comment timestamp\nsentiment: Sentiment classification\nsentiment_score: Optional confidence score\n\nMySQL Commands:\n\n# Connect to MySQL\ndocker exec -it mysql mysql -u user -ppassword sentiment_analysis\n\n# View sentiment results\nSELECT * FROM sentiment_results LIMIT 10;\n\n# Get sentiment distribution\nSELECT sentiment, COUNT(*) FROM sentiment_results GROUP BY sentiment;\nMetabase\n\nPurpose: Data visualization and dashboarding platform.\n\nImplementation: Uses the official Metabase image, connected to MySQL.\n\nConfiguration:\n\nWeb UI: \nhttp://localhost:3000\nDefault login: \nemail@domain.com\n / metabase\n\nSetup Instructions:\n\nAccess Metabase at \nhttp://localhost:3000\nComplete the initial setup\nAdd MySQL as a data source:\nHost: mysql\nPort: 3306\nDatabase: sentiment_analysis\nUsername: user\nPassword: password\nCreate dashboards to visualize sentiment trends\nPython Components\n1. Producer (producer.py)\n\nPurpose: Streams YouTube live chat messages to Kafka for real-time sentiment analysis.\n\nImplementation: Connects to the YouTube Data API to retrieve live chat messages from ongoing streams and sends them to Kafka for processing.\n\nConfiguration:\n\nRequires a YouTube API key in .env file\nConfigurable poll interval (default: 5 seconds)\nRate limiting and error handling for the YouTube API\n\nYouTube API Configuration:\nTo use the Producer with YouTube live chats, you need to:\n\nObtain a YouTube Data API v3 key from the \nGoogle Cloud Console\nAdd the key to your .env file as YOUTUBE_API_KEY=your_key_here\nConfigure poll intervals with YOUTUBE_POLL_INTERVAL (seconds, default: 5)\nSet maximum results per request with YOUTUBE_MAX_RESULTS (default: 200)\n\nRunning the Producer:\n\n# Specify a YouTube video ID as command line argument\npython src/producer.py <youtube_video_id>\n\nKey Features:\n\nReal-time processing of live chat messages\nIntelligent handling of YouTube API quota limitations\nAutomatic retry mechanisms for resilient operation\n2. Consumer (consumer.py)\n\nPurpose: Processes comment streams, coordinates sentiment analysis, and stores results.\n\nImplementation: Consumes messages from Kafka, orchestrates sentiment analysis, and manages data storage in both HBase and MySQL.\n\nRunning the Consumer:\n\npython src/consumer.py\n3. Sentiment Analyzer (sentiment_analyzer.py)\n\nPurpose: Performs sentiment analysis on comment text.\n\nImplementation: Uses a pre-trained transformer model from Hugging Face to classify sentiment.\n\nModel Details:\n\nDefault model: \ntabularisai/multilingual-sentiment-analysis\nClassification: 5 categories (Very Negative to Very Positive)\n4. Configuration (config.py)\n\nPurpose: Centralizes configuration for all components.\n\nImplementation: Uses environment variables with sensible defaults.\n\nEnvironment Variables:\n\nKafka: KAFKA_BOOTSTRAP_SERVERS, KAFKA_TOPIC, etc.\nHBase: HBASE_HOST, HBASE_PORT, etc.\nMySQL: MYSQL_HOST, MYSQL_DATABASE, etc.\nSentiment: SENTIMENT_MODEL_NAME, etc.\nProject Structure\nyoutube-sentiment-pipeline/\n├── docker-compose.yml        # Docker services configuration\n├── requirements.txt          # Python dependencies\n├── .env                      # Environment variables configuration\n├── README.md                 # Project documentation\n├── LICENSE                   # MIT License file\n└── src/\n│   ├── config.py             # Centralized configuration\n│   ├── consumer.py           # Kafka consumer and orchestration\n│   ├── hbase_utils.py        # HBase database utilities\n│   ├── mysql_client.py       # MySQL database utilities\n│   ├── producer.py           # Streams YouTube live chat messages to Kafka using the YouTube Data API\n│   ├── sentiment_analyzer.py # ML-based sentiment analysis\n│   └── sentiment_processor.py # Sentiment processing orchestration\n└── images/                   # Visualization and diagram assets\n    ├── system_architecture.png        # System architecture diagram\n    ├── bar_chart_visualization.png    # Bar chart of sentiment distribution\n    ├── pie_chart_visualization.png    # Pie chart of sentiment percentages\n    └── time_series_visualization.png  # Time series of sentiment trends\n\nDirectory Structure Explanation\n\nRoot Directory: Contains project-level configuration files and documentation\n\ndocker-compose.yml: Defines and configures all Docker services needed for the project\nrequirements.txt: Lists Python dependencies needed for the application components\n.env: Environment variables configuration file for all services and components\nREADME.md: Project documentation with setup instructions and component explanations\nLICENSE: MIT License file\n\nsrc/: Contains all application source code, organized by component function\n\nData Pipeline:\n\nproducer.py: Streams YouTube live chat messages to Kafka using the YouTube Data API\nconsumer.py: Consumes comment data from Kafka and coordinates processing\n\nData Processing:\n\nsentiment_analyzer.py: Implements ML model-based sentiment analysis logic\nsentiment_processor.py: Orchestrates the sentiment analysis and storage workflow\n\nData Storage:\n\nhbase_utils.py: Provides functions to interact with HBase for raw comment storage\nmysql_client.py: Provides functions to interact with MySQL for sentiment result storage\n\nConfiguration:\n\nconfig.py: Centralizes configuration for all components via environment variables\n\nimages/: Contains visualization assets and diagrams\n\nsystem_architecture.png: High-level system architecture diagram\ntime_series_visualization.png: Example of time series sentiment analysis chart\nbar_chart_visualization.png: Example of sentiment distribution bar chart\npie_chart_visualization.png: Example of sentiment distribution pie chart\n\nThe project follows a modular architecture with clear separation of concerns between data ingestion, processing, and storage components. This structure makes it easy to understand the data flow and modify individual components as needed.\n\nRunning the Project\nStep 1: Start the Infrastructure\n# Start all services\ndocker-compose up -d\n\n# Wait for services to initialize (approximately 1-2 minutes)\nStep 2: Run the Consumer\n# In one terminal\npython src/consumer.py\nStep 3: Run the Producer\n# In another terminal\npython src/producer.py <youtube_video_id>\nStep 4: Monitor and Visualize\nAccess HBase Web UI: \nhttp://localhost:16010\nAccess Metabase: \nhttp://localhost:3000\nCreate dashboards to monitor sentiment trends\nVisualization Examples\n\nHere are step-by-step examples to create useful visualizations for monitoring sentiment analysis:\n\nExample 1: Time Series Line Chart - Sentiment Over Time\n\nThis visualization shows how sentiment trends change over time.\n\n\nFigure 2: Time series showing sentiment trends over time. Peaks in positive sentiment correlate with product launches and promotional events.\n\nIn Metabase, click on \"New\" > \"Question\"\nIn the top navigation, select your database (e.g., \"Sentiment Analysis\")\nClick on \"Comments\" table\nClick on the \"Summarize\" button\nFor \"Pick a function or metric\", select \"Count of rows\"\nFor \"Pick a column to group by\", select \"timestamp\"\nYou can refine the time grouping by clicking on \"by month\" and changing to hour, day, or week\nClick \"+ Add another grouping\"\nSelect \"sentiment_prediction\" to break down data by sentiment categories\nClick \"Visualize\" button at the bottom\nClick \"Line\" in the visualization selector\nUse the \"Settings\" panel to customize colors and display options\nClick \"Save\" and add it to your dashboard\nExample 2: Bar Chart - Sentiment Category Distribution\n\nThis visualization shows the count of comments in each sentiment category.\n\n\nFigure 3: Bar chart showing the distribution of sentiment categories. This sample shows a healthy distribution across the five sentiment levels from Very Negative to Very Positive.\n\nIn Metabase, click on \"New\" > \"Question\"\nIn the top navigation, select your database (e.g., \"Sentiment Analysis\")\nClick on \"Comments\" table\nClick on the \"Summarize\" button\nFor \"Pick a function or metric\", select \"Count of rows\"\nFor \"Pick a column to group by\", select \"sentiment_prediction\"\nClick \"Visualize\" button at the bottom\nClick \"Bar\" in the visualization selector\nUse the \"Settings\" panel to customize colors, axis labels, and display options\nClick \"Save\" and add it to your dashboard\nExample 3: Pie Chart - Sentiment Distribution Percentage\n\nThis visualization shows the proportion of each sentiment category as a percentage.\n\n\nFigure 4: Pie chart showing the percentage distribution of the five sentiment categories (Very Negative, Negative, Neutral, Positive, Very Positive), providing a quick overview of overall customer sentiment.\n\nIn Metabase, click on \"New\" > \"Question\"\nIn the top navigation, select your database (e.g., \"Sentiment Analysis\")\nClick on \"Comments\" table\nClick on the \"Summarize\" button\nFor \"Pick a function or metric\", select \"Count of rows\"\nFor \"Pick a column to group by\", select \"sentiment_prediction\"\nClick \"Visualize\" button at the bottom\nClick \"Pie\" in the visualization selector\nIn the \"Settings\" panel, enable \"Show percentage\" option\nOptionally, customize colors and legend position\nClick \"Save\" and add it to your dashboard\nStep 5: Shutdown\n# Stop the Python processes with Ctrl+C\n# Stop the Docker containers\ndocker-compose down\nTroubleshooting\nKafka Connection Issues\n\nIf the producer or consumer can't connect to Kafka:\n\n# Check if Kafka is running\ndocker-compose ps kafka\n\n# Check Kafka logs\ndocker-compose logs kafka\n\n# Ensure the topic exists\ndocker exec kafka kafka-topics --bootstrap-server localhost:9092 --list\nHBase Connection Issues\n\nIf there are problems connecting to HBase:\n\n# Check HBase status\ndocker-compose ps hbase\n\n# Check HBase logs\ndocker-compose logs hbase\n\n# Verify Thrift service is running\ndocker exec hbase jps\nMySQL Connection Issues\n\nIf there are problems connecting to MySQL:\n\n# Check MySQL status\ndocker-compose ps mysql\n\n# Test connection\ndocker exec -it mysql mysql -u user -ppassword -e \"SELECT 1\"\nMemory Issues\n\nIf services are crashing due to memory constraints:\n\nIncrease Docker memory allocation in Docker Desktop settings\nReduce unnecessary services if running on limited hardware\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nProject Overview\n\nSystem Architecture\n\nRequirements\n\nGetting Started\n\nClone the Repository\nInstall Python Dependencies\nStart the Docker Environment\nVerify Services are Running\n\nComponent Documentation\n\nDocker Environment\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "realtime-monitoring-of-singapore-traffic-using-yolov11-syTbHJec6enz",
    "username": "Namit Arora",
    "license": null,
    "title": "Real-Time Monitoring of Singapore Traffic Using YOLOv11",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n112 reads\n●\nCreative Commons Attribution (CC BY)\nWinner of\nBest Technical Implementation\nat the\nComputer Vision Projects Expo 2024\nReal-Time Monitoring of Singapore Traffic Using YOLOv11\nRealtime Dashboard\nTraffic Jam Detection\nVehicle classification\nVehicle Detection\nYOLO\nNamit Arora\nLike\nBookmark\nShare\nView \nDashboard\n\nAbstract\n\nThis project focuses on the development of a \nreal-time traffic monitoring\n system for Singapore, utilizing cutting-edge machine learning techniques to detect and analyze traffic conditions. By integrating live data from government APIs, a \ndynamic dashboard\n was created to display real-time traffic information. The system employs the YOLOv11 model from RoboFlow to detect and classify various vehicle types, including cars, trucks, buses, and two-wheelers.\n\nTo enhance the model's performance, multiple RoboFlow datasets were combined to create a comprehensive dataset capable of accurately detecting and categorizing vehicles. A time-series dataset was generated by performing inference on a single camera feed, using vehicle count and occupation ratio as primary features. Anomaly detection was then applied using Z-score analysis to identify unusual traffic patterns, such as traffic jams. This approach demonstrates an effective solution for real-time traffic monitoring and congestion detection, offering valuable insights to optimize traffic flow and enhance urban mobility.\n\nIntroduction\n\nUrban traffic congestion is a growing global challenge, requiring efficient real-time solutions to manage traffic flow and enhance safety. Traditional traffic monitoring systems often fall short in scalability and real-time responsiveness. This project introduces a flexible and scalable framework for real-time vehicle monitoring, designed to be deployed in cities worldwide. The framework leverages advanced machine learning and computer vision technologies, including the YOLOv11 model for accurate vehicle detection and classification, and anomaly detection using time-series data to identify traffic disruptions like jams.\n\nAs a case study, Singapore's traffic data is utilized to demonstrate the effectiveness of this framework. By integrating publicly available government traffic APIs with real-time vehicle detection, the system provides insights into traffic patterns and enables proactive responses to congestion. The framework can be adapted to different urban environments, offering a cost-effective solution for global traffic management that enhances operational efficiency and safety.\n\nMethodology\n\nThis section describes the processes followed to obtain a vehicle detection and classification model.\n\nUnderstanding the Data\n\nTo begin with, it is essential to explore the characteristics of the data before proceeding with any further analysis or model development. For this project, we utilized web scraping to gather both images and their associated metadata from the various traffic cameras installed across Singapore, which are publicly available on government websites.\n\nAfter scraping the data, the following key observations were made:\n\nTotal number of unique expressways: This refers to the number of expressways across Singapore covered by the traffic cameras.\nTotal number of unique image sizes: Images collected from different cameras came in varying resolutions and aspect ratios.\nUnique image sizes: Specific dimensions of the images were identified, providing insight into the resolution variations across different cameras.\nTotal number of cameras installed: The dataset includes a diverse set of cameras installed across the city, capturing traffic images from multiple locations.\n\nUpon reviewing a subset of the camera images, the following observations were made:\n\nVarying Camera Angles and Lighting Conditions: Images captured from different locations show diverse perspectives due to varying camera angles. Lighting conditions also differ, especially between day and night.\n\nInterference from Light Sources: Nighttime images exhibit interference from artificial light sources such as street lights, which may cause glare and affect the clarity of vehicle detection.\n\nMotion Blur: Some images contain motion blur, likely due to fast-moving vehicles, making it difficult to clearly identify vehicles in the frame.\n\nVisibility of Vehicles: In certain images, vehicles are barely visible due to poor lighting or camera positioning, adding another layer of complexity to the analysis.\n\nObstructions in the Frame: There are instances where objects like trees or other structures obstruct the view of vehicles, further complicating the vehicle detection process.\n\nThese observations highlight the challenges involved in using traffic camera images for vehicle detection and traffic analysis, emphasizing the need for robust image processing and machine learning techniques to handle such inconsistencies and variabilities in the data.\n\nData Collection\n\nTo obtain suitable annotated data for vehicle detection, several traffic image datasets of Singapore were sourced from Roboflow Universe. The following steps were performed to prepare the data:\n\nDataset Merging: Five datasets containing annotated traffic images with detection boxes for vehicles were combined to create a comprehensive dataset.\n\nClass Remapping: Due to inconsistent labeling across the datasets, the Class Remapping feature of Roboflow was used to standardize the vehicle categories into four consistent classes: car, two-wheeler, truck, and bus.\n\nClass Distribution\n\n\nThe above bar graph shows the class distribution of dataset before data preprocessing.\n\nData Preprocessing\n\nGiven the limited number of available images, data augmentation was employed to enhance the diversity of the training dataset and improve the robustness of the model to varying image conditions. The following transformations were applied to the images:\n\nGrayscale: 12% of the images were converted to grayscale to simulate lighting variations and reduce the model’s reliance on color features.\n\nBrightness Adjustment: A random adjustment of brightness in the range of -15% to 15% was applied to introduce variability in lighting conditions across the dataset.\n\nExposure Adjustment: The exposure levels of the images were randomly modified within a range of -10 to 10% to simulate different lighting environments, such as daytime and nighttime conditions.\n\nBlur: A blur effect with a maximum radius of 1.1 pixels was applied to some images to replicate the presence of motion blur and ensure the model could handle less-than-ideal image quality.\n\nThese preprocessing steps were designed to increase the variability of the training set, thereby enhancing the model's ability to generalize and perform well under different environmental conditions and image qualities.\n\nModeling\nModel Selection\n\nYOLOv11 was selected for vehicle detection in this project due to its real-time performance, high accuracy, and flexibility. YOLOv11 excels at detecting small and overlapping objects, making it ideal for crowded traffic scenes. Its ability to be fine-tuned for specific vehicle classes (e.g., cars, trucks, buses, two-wheelers) and its scalability to handle varying image sizes further justified its use.\n\nDataset Split\n\n\nAfter data augmentation and preprocessing, most images were allocated to the training set to enhance model generalization. The test set was handpicked to ensure diversity in vehicle types, lighting, and camera angles, providing a comprehensive evaluation of the model's performance.\n\nModel Training\n\nThe YOLOv11 model was initialized with pre-trained weights from the COCO dataset and fine-tuned on our traffic dataset. Training results showed that the model achieved a mean Average Precision (mAP) of 63% on the validation set. The loss curves for box loss, class loss, and object loss were also tracked, reflecting the model’s progress in accurately predicting bounding boxes, classifying vehicles, and detecting objects.\n\nModel Evaluation\n\n\nThe performance of the YOLOv11 model was evaluated on the test set, achieving a mean Average Precision (mAP) of 63.7%, as illustrated. This indicates that the model demonstrates reasonable effectiveness in detecting and classifying vehicles in diverse traffic conditions.\n\nHyperparameter Tuning For Inference\n\nTo optimize the model's performance in real-world conditions, hyperparameters were fine-tuned based on the analysis of live traffic images from the dashboard. Specifically, the confidence threshold was set to 0.3 and the overlap value (IoU threshold) was adjusted to 0.5. A lower confidence threshold was selected to mitigate false negatives, as higher confidence levels led to missed detections in certain scenarios, such as challenging lighting conditions or obstructions. This balance ensured improved detection rates while maintaining precision in crowded and dynamic environments.\n\nTraffic Jam Detection\n\nIn this section, the methodology utilized to detect traffic jams on a selected road using traffic detection and classification is described. The process involves data collection, image preprocessing, vehicle detection, feature extraction, and anomaly detection to identify significant traffic events.\n\nData Collection\n\nFor this experiment, a specific traffic camera from the Singapore government’s publicly available api was selected. Images were collected from the camera at 5-minute intervals over a one-week period through the \ngovernment’s API\n.\n\nImage Preprocessing and Masking\n\nTo focus on the relevant section of the road, a mask was applied to each image, segmenting only one side of the road. This masked region was retained for further analysis, ensuring that only the desired road segment was considered in the traffic analysis.\n\nVehicle Detection and Classification\n\nThe count of each vehicle type was recorded for each image using model inference through model API endpoint. Additionally, the occupation ratio for each image was computed, which is defined as the ratio of the union of all detection boxes to the total segmented area (after masking the road). This ratio represents the proportion of the road occupied by vehicles.\n\nTime Series Data Construction\n\nThe detected vehicles and classification results were compiled into a time series dataset containing the following features:\n\nTotal number of vehicles\nNumber of cars\nNumber of two-wheelers\nNumber of buses\nNumber of trucks\nOccupation ratio\nTimestamp\n\nThis time series dataset formed the basis for the analysis of traffic anomalies.\n\nAnomaly Detection Using Z-Scores\n\nAnomaly detection was performed using the Z-score method, which measures how many standard deviations a data point deviates from the mean. The Z-score for a given data point  is calculated using the following formula:\n\nWhere:\n\n is the data point (either Total Vehicles or Occupation Ratio),\n is the mean of the data,\n is the standard deviation of the data.\n\nAnomalies are identified when the absolute value of the Z-score exceeds a predefined threshold , which corresponds to the significance level. For example, for a significance level of 0.1% (0.001 p-value), the threshold is set at . Thus, an anomaly is flagged if:\n\nThe anomaly detection process identified significant deviations in both Total Vehicles and Occupation Ratio across several time window of 15 minutes. These deviations were flagged as high traffic anomalies, indicative of potential traffic jams or congestion events.\nThis approach utilizes both the Total Vehicles and Occupation Ratio to identify anomalies, as both features are critical for detecting traffic congestion. The Total Vehicles feature provides information on the volume of traffic, while the Occupation Ratio reflects the extent of road occupancy. By considering both features together, the method ensures that anomalies are detected only when both the volume of traffic and the level of road occupancy exceed normal conditions. This combined approach helps to better capture scenarios where both high traffic volume and high occupancy indicate potential traffic jams, thus providing a more accurate and reliable anomaly detection.\n\n\nA scatter plot of Total Vehicles versus Occupation Ratio was generated to visually represent the detected anomalies. Data points representing normal traffic conditions were depicted in blue, while the high traffic anomalies were highlighted in red.\n\nThe image above depicts a potential traffic jam observed during the time window from 18:45 to 19:00 on December 11, 2024. During this period, the mean number of vehicles detected was 40, with an average occupation ratio of 0.17.\n\nConclusion\n\nThis work presents a scalable traffic monitoring framework that automatically detects, classifies, and identifies traffic congestion using computer vision. By processing traffic camera feeds, the system classifies vehicles and assesses road occupancy, enabling traffic jam detection without the need for manual labeling. A real-time dashboard displays live images and traffic data from Singapore’s cameras, offering an intuitive interface for monitoring traffic conditions.\n\nWhile demonstrated with Singapore data, this framework is adaptable to other urban settings, providing valuable insights for traffic management and urban planning.\n\nCode and Resources\n\nThe code for the project can be referenced using \nthis colab notebook\n.\nThe github repo for the traffic jam detection : \nhttps://github.com/BlazeStorm001/singvms-anomaly\n\nThe github repo for the website dashboard : \nhttps://github.com/BlazeStorm001/singvms\n\nReferences\nRedmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You Only Look Once: Unified, Real-Time Object Detection. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nChandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly Detection: A Survey. ACM Computing Surveys (CSUR).\nRoboflow. (2023). Roboflow Docs. \nRoboflow Documentation\n.\nUltralytics. (2024). YOLOv11. \nYOLOv11 Documentation\n. Accessed: 2024-12-30.\nAcknowledgements\n\nThanks to the Roboflow users \"fyp car dataset\", \"Singapore\", \"is3107\", \"Traffic Dataset\" for making the datasets for Singapore Traffic available on the Roboflow Universe.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nView \nDashboard\n\nAbstract\n\nIntroduction\n\nMethodology\n\nUnderstanding the Data\n\nData Collection\n\nClass Distribution\n\nData Preprocessing\n\nModeling\n\nModel Selection\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nYou might be interested\nTraffixVision\nV\nDec 31, 202415 reads\nRoad Traffic Recognition Using Convolutional Neural Networks (CNN) | OpenCV Python\nS\nDec 30, 202435 reads\nAI Traffic ManagementConvolutional Neural Networks (C+14\nRevolutionizing Traffic Control: Harnessing the Power of YOLOv8 Object Detection System\nK\nDec 28, 202449 reads\nComputer VisionCV+3\nTrafficEYE\nNov 21, 202440 reads\nComputer VisionFYP+4",
    "awards": [
      "best technical implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "resumate-resume-optimizer-VAwSIObapKkf",
    "username": "Namit Arora",
    "license": null,
    "title": "ResuMate: Resume Optimizer ",
    "publication_description": "Back to publications\nMar 31, 2025\n●\n53 reads\n●\nCreative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nResuMate: Resume Optimizer\nAgentic AI\nLLM\nWorkflow\nNamit Arora\nLike\nBookmark\nShare\nAbstract\n\nHave you spent countless hours trying to tailor your resume for a specific job? Needless to say, it is one of the most tedious and boring tasks that you can ever do. Even if you have the right skills for the job, if your resume does not catch the eye of the Hiring Manager, it is rendered worthless. In the era of artificial intelligence, your resume will go through not only a human but also an applicant tracking system (ATS) that may not be able to parse your resume properly and may even reject you if it does not find the right set of keywords. Keeping all these things in mind, there was a need to develop a system that can generate resumes that are tailored to the job description, are easily understood by the ATS, and have a clean and professional outlook with minimal language errors. ResuMate is designed to produce such resumes within minutes and make job hunting easier for people in technology.\n\nIntroduction\n\nMost resumes get rejected by an automated Applicant Tracking well before they are seen by an Hiring Manager. These applicant tracking systems, may not be able to parse resumes that have a complex layout or contain too many unnecessary items other than text. Also, most HR deal with many resumes daily, and give at max 30 seconds of attention to any resume. Thus, it becomes inevitable to design a resume that not only looks professional but is also well understood by the Applicant Tracking Systems. ResuMate is an agentic workflow that uses LLMs to transform your existing resume to a professional-looking, minimalistic, and easily parseable PDF. The workflow can be accessed using the \nWeb App\n interface. It has the following salient features:\n\nProvides suggestions to improve your existing resume based on the parseability, language usage, job description. and actionable insights.\n\nGeneration of a clean single-column LaTeX-based PDF resume with all the suggestions incorporated.\n\nMethodology\n\nThis section explains the architecture of the application and workings of the agentic workflow.\n\nWorkflow Architecture\n\nThe figure above shows the architecture of the agentic workflow. It was created keeping three things in mind:\n\nAccurate and reliable parsing of resumes with minimal errors\nMinimizing the number of API calls to LLMs to reduce cost.\nTransforming data into appropriate formats to ensure consistency and easier display on the frontend.\nUser Input\n\nThe user inputs the resume in pdf format along with an optional job description. The resume is first converted into jpg images (one image for each page) using \npdf2image\n and then converted to base64 strings that can be used to make API calls with the Multimodal Large Language Model (MLLM). Here we are using the Gemini Flash 2.0 model (see Experiments on why this model was selected) as the MLLM, which offers fast inference times and low hallucination rates. Below snippet shows the code for conversion:\n\nfrom pdf2image import convert_from_path\n\npdf_path = '/content/resume.pdf'\nimages = convert_from_path(pdf_path)\nfor i, image in enumerate(images):\n    image.save(f'page_{i + 1}.jpg', 'JPEG')\nParsing PDF\n\nWe use the \npdfminer.six\n library to extract information such as the most commonly used font and its size. The font information can be used to alert the user if an unprofessional-looking font or a small font size is being used. Also, the \nPyPDF2\n library has been used to extract text from the PDF. This acts as a fallback mechanism in case the MLLM model fails to service the API call. Moreover, PyPDF extracts hyperlinks such as Github Account URL, LinkedIn Page URL, email address, and other important Links from the PDF. A sample snippet has been shown which extracts information using PyPDF:\n\ndef extract_text_num_pages_and_links(pdf_stream):\n    reader = PdfReader(pdf_stream)\n    text = \"\"\n    links = []\n    num_pages = len(reader.pages)\n    # Extract text from all pages\n    for page in reader.pages:\n        text += page.extract_text() or \"\"\n\n        # extract links from annotations\n        if '/Annots' in page:\n                for annot in page['/Annots']:\n                    obj = annot.get_object()\n                    \n                    # Check if it's a link annotation\n                    if obj['/Subtype'] == '/Link':\n                        if '/A' in obj and '/URI' in obj['/A']:\n                            uri = obj['/A']['/URI']\n                            links.append(uri)\n\n    return {\n        \"resume_parsed_text\": text,\n        \"resume_num_pages\": num_pages,\n        \"resume_links\": links,\n    }\nPrompt for Resume Section Parsing\n\nResumes can be highly unstructured documents and may have completely different sections based on the candidate. Upon investigating many tech resumes, the most common sections present in tech resumes were found. For segregating the resume into the different sections the following prompt was used:\n\n\n**Instruction:**  \nExtract the following sections from the resume **if present** and output them **strictly in Markdown format**. Each section should have its own subheading (`##`). **Do not modify any text** from the resume—preserve all original formatting and wording. **Output only the extracted content in Markdown format, without any additional text.**\n\n----------\n\n**Resume Extraction Format:**\n\n```markdown\n## Candidate Name\n[Extracted Name]\n\n## Location of Candidate\n[Extracted Location]\n\n## Experience/Work History\n[Extracted Work History, including Freelance Experience]\n\n## Education\n[Extracted Education Details]\n\n## Position of Responsibilities\n[Extracted Responsibilities]\n\n## Skills\n[Extracted Skills]\n\n## Profile Summary / Objective\n[Extracted Profile Summary or Objective]\n\n## Projects\n[Extracted Projects]\n\n## Publications\n[Extracted Publications]\n\n## Interests / Hobbies / Extracurricular Activities\n[Extracted Interests, Hobbies, or Extracurricular Activities]\n\n## Awards / Achievements\n[Extracted Awards or Achievements]\n\n## Certifications\n[Extracted Certifications]\n\n## Languages Known\n[Extracted Languages]\n\n## Relevant Coursework\n[Extracted Relevant Coursework]\n\n\n\n**Rules for Extraction:**\n\n-   If a section is missing in the resume, **do not generate it** in the output.\n    \n-   **Do not add, modify, or infer** any information—strictly extract only what is present.\n    \n-   Maintain the **original text, order, and formatting** from the resume.\n    \n-   Ensure **only Markdown-formatted output** without any additional explanations or notes.\n\nHaving each section in Markdown format helps with easy display on the frontend side so that user can edit their resumes before improving them.\n\nPrompts for Markdown to JSON conversion\nTake the given **Markdown-formatted resume** and extract its contents into a structured **JSON format** as shown below. **Ensure the response strictly follows JSON syntax**, and the output should be a valid Python dictionary.  \n### **Instructions:**  \n- Extract information under the following sections:  \n  1. **Education**  \n  2. **Projects**  \n  3. **Experience** (includes Work History and Freelance)  \n  4. **Position of Responsibilities**  \n- **If a section is missing**, include the key in the JSON but leave it as an **empty list (`[]`)**.  \n- **Do not modify the original text**—preserve all content exactly as it appears in the resume.  \n- **All dates must be formatted as \"Abbreviated Month-Year\" (e.g., \"Jan-2023\").**  \n- **Strictly output only the JSON**—no additional text, explanations, or formatting.  \n---\n### **Output Format:**  \n\n\n{\n  \"Education\": [\n    {\n      \"Degree\": \"<Degree Name>\",\n      \"Institution\": \"<Institution Name>\",\n      \"Location\": \"<City/Remote>\",\n      \"Period\": {\n        \"Start\": \"<Start Date>\",\n        \"End\": \"<End Date>\"\n      }\n    }\n  ],\n  \"Experience\": [\n    {\n      \"Role\": \"<Role Title>\",\n      \"Company\": \"<Company Name>\",\n      \"Location\": \"<City/Remote>\",\n      \"Period\": {\n        \"Start\": \"<Start Date>\",\n        \"End\": \"<End Date>\"\n      },\n      \"Description\": \"<Brief Description of Responsibilities>\"\n    }\n  ],\n  \"Position of Responsibilities\": [\n    {\n      \"Title\": \"<Position Title>\",\n      \"Organization\": \"<Organization Name>\",\n      \"Location\": \"<City/Remote>\",\n      \"Period\": {\n        \"Start\": \"<Start Date>\",\n        \"End\": \"<End Date>\"\n      },\n      \"Description\": \"<Brief Description of Responsibilities>\"\n    }\n  ],\n  \"Projects\": [\n    {\n      \"Name\": \"<Project Name>\",\n      \"Description\": \"<Project Description>\",\n      \"Technologies\": \"<Technologies Used>\",\n      \"Period\": {\n        \"Start\": \"<Start Date>\",\n        \"End\": \"<End Date>\"\n      }\n    }\n  ]\n}\n\n-\n### **Strict Rules:**  \n**If a section is missing**, return an **empty list (`[]`)** instead of omitting it.  \n**No explanations, additional text, or formatting**—**only valid JSON output**.  \n**Preserve original text** exactly as found in the resume.  \n**Ensure JSON is well-formed and can be parsed directly in Python.**  \n\n---\n\n\nThe above prompt converts a group of sections from Markdown to json. The sections are divided into multiple groups, and separate identical prompts are made for each group. This is done to avoid any conflicts between sections and make it easier for the LLM to generate accurate json responses. Although one could directly use the MLLM to generate JSON data but it would lead to a higher number of API calls to a much costlier MLLM.\nEach LLM response is then checked using \nPydantic AI\n to ensure that only valid Json responses are received. Also, we use the Deepseek V3 model, which has a good balance of inference time and accuracy, performing well on general-purpose tasks with an \nMMLU(General) score of 88.5%\n.\n\nPrompts for Enhancing Sections\n\nNow that we have structured sections in front of us, we can prepare specific prompts to enhance each section using certain guidelines. For example, take a look at the prompt which can be used to improve the 'Projects' section in a resume.\n\n**Prompt:**  \n\nYou will receive **two inputs**:  \n1. A **Job Description (JD)** in text format.  \n2. A **JSON object** containing **projects** extracted from a resume in the following format:  \n\n\n{\n  \"Projects\": [\n    {\n      \"Name\": \"<Project Name>\",\n      \"Description\": \"<Project Description>\",\n      \"Technologies\": \"<Technologies Used>\",\n      \"Period\": {\n        \"Start\": \"<Start Date>\",\n        \"End\": \"<End Date>\"\n      }\n    }\n  ]\n}\n\n\n### **Task:**  \n- **Analyze the job description** and **align each project's description** according to it.  \n- **Prioritize** projects that match the **technologies, skills, and keywords** found in the job description. The most relevant projects should appear **first** in the JSON.  \n- **Modify the project descriptions** to subtly align them with the job description while keeping them **concise, clear, and well-structured**.  \n- **Format the project descriptions in Markdown**, ensuring:  \n  - Use of **bullet points** for clarity.  \n  - **Bold formatting** (`**skill**`) for skills/technologies mentioned in both the **job description** and the **project**.  \n- **Preserve all other data as is** (name, technologies, period, etc.), making no other modifications.  \n\n---\n\n### **Output Format:**  \n\nThe output must be **strictly JSON**, preserving the original structure but with:  \n1. **Projects reordered** based on relevance to the job description.  \n2. **Descriptions formatted in Markdown** with relevant skills in **bold**.  \n\nExample Output:  \n\n\n{\n  \"Projects\": [\n    {\n      \"Name\": \"AI Chatbot\",\n      \"Description\": \"- Developed an AI-driven chatbot using **Python** and **TensorFlow**.\\n- Integrated with **FastAPI** for real-time communication.\\n- Implemented **NLP models** for enhanced user interactions.\",\n      \"Technologies\": \"Python, TensorFlow, FastAPI, NLP\",\n      \"Period\": {\n        \"Start\": \"Jan-2023\",\n        \"End\": \"Dec-2023\"\n      }\n    },\n    {\n      \"Name\": \"E-commerce Website\",\n      \"Description\": \"- Built a full-stack e-commerce platform with **React** and **Next.js**.\\n- Implemented a **Node.js** backend with a **MongoDB** database.\\n- Optimized performance and user experience with **server-side rendering (SSR)**.\",\n      \"Technologies\": \"React, Next.js, Node.js, MongoDB, SSR\",\n      \"Period\": {\n        \"Start\": \"Mar-2022\",\n        \"End\": \"Oct-2022\"\n      }\n    }\n  ]\n}\n\n\n---\n\n### **Strict Rules:**  \n**Only output valid JSON**—no additional text, explanations, or formatting.  \n**Preserve all original fields** (except reordering projects and modifying descriptions as instructed).  \n**Descriptions must be formatted in Markdown**, using bullet points and **bolding** relevant skills.  \n**No other modifications** beyond those explicitly stated.  \n\n**Final instruction:** **Ensure the response is strictly JSON-compliant, ready to be parsed as a Python dictionary.**\n\nSimilarly, prompts are created for each section that is present in the resume. Also, the LLM responses are validated by Pydantic AI to ensure accuracy.\n\nPopulating the LaTeX template\n\nWith the enhanced resume sections ready, we populate the empty \nLaTeX template\n with the extracted information and obtain a nice and minimalistic resume.\n\nConversion to PDF\n\nAfter we have the LaTeX (.tex) file ready, we can compile it to PDF directly using \nTinyTeX\n, which has a much smaller footprint ideal for resource-constrained servers.\n\npdflatex document.tex \nQuery Optimization\nasync def get_llm_response(self, api_key, messages,  temperature=1, top_p=1, max_retries=10, initial_backoff=1):\n        \"\"\"\n        Fetches a response from the API with retry logic to handle rate limits.\n\n        Args:\n            api_key (str): API key for authentication.\n            messages (list): Messages to send to the model.\n            temperature (float): Sampling temperature for the response.\n            top p (float): top p value for the response.\n            max_retries (int): Maximum number of retries in case of rate limit errors.\n            initial_backoff (int): Initial backoff time in seconds.\n\n        Returns:\n            str: The content of the model response.\n        \"\"\"\n        attempt = 0\n\n        # Define the API URL\n        url = \"https://openrouter.ai/api/v1/chat/completions\"\n\n        # Define the headers for authentication\n        headers = {\n            \"Authorization\": f\"Bearer {api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        all_models = [\"deepseek/deepseek-chat-v3-0324:free\", \"meta-llama/llama-3.1-70b-instruct:free\",\"google/gemma-2-9b-it:free\",\"meta-llama/llama-3.2-3b-instruct:free\", \"meta-llama/llama-3.1-405b-instruct:free\", \"mistralai/mistral-7b-instruct:free\"]\n\n        # Define the request payload\n        payload = {\n            \"model\": None,\n            \"messages\": messages,\n            \"temperature\": temperature,\n            \"top_p\": 0.6,\n        }\n\n\n        async with aiohttp.ClientSession() as session:\n            while attempt < max_retries:\n                try:\n                    payload[\"model\"] = all_models[attempt % len(all_models)]\n                    print(f\"Attempt {attempt + 1} with API key: {api_key}\")\n                    # proxy_list = await FreeProxy().get(count=1)\n                    async with session.post(url, headers=headers,  data=json.dumps(payload), timeout=30, proxy=None) as response:\n                        if response.status == 200:\n                            data = await response.json()\n                            return data[\"choices\"][0][\"message\"][\"content\"]\n                        else:\n                            print(f\"Error: {response.status}, {await response.text()}\")\n\n                except aiohttp.ClientError as e:\n                    print(f\"HTTP error: {e}\")\n                except Exception as e:\n                    print(f\"Unexpected error: {e}\")\n\n                attempt += 1\n                wait_time = initial_backoff * (2 ** (attempt - 1))  # Exponential backoff\n                print(f\"Retrying in {wait_time} seconds (attempt {attempt}/{max_retries})...\")\n                await asyncio.sleep(wait_time)  # Wait before retrying\n\n        print(f\"Max retries reached. Could not fetch response.\")\n        return None  # Return None if all retries fail\n\nThe above snippet shows the mechanism for sending queries to LLM, it ensures that other fallback models are used in case of failure. Also, we use asynchronous programming to send multiple queries in an efficient way without blocking other queries.\n\nTech Stack\nBackend - FastAPI\nFrontend - Next.js with React\nOther - Docker\n\nThe web application was built using FastAPI for the backend and \nNext.js with React\n for the frontend. \nFastAPI\nis a modern, high-performance Python framework that leverages ASGI for faster execution, making it a popular choice for AI applications. Next.js was used for the frontend, providing a robust framework for building production-ready web applications.\nThe whole prompt framework was designed from scratch without using any readily available agentic frameworks to ensure optimum performance and pinpoint customization.\n\nExperiments\n\nThis section describes the various experiments that were performed to finalize the various tools in the project.\n\nPDF Parsing Tools\n\nAccurately parsing resumes is a critical task in this project. PDF, while commonly used for resumes, presents significant challenges due to its complex layout and the fact that each character is positioned at a specific location in the document. Several tools are available for parsing PDFs in Python, which can be broadly categorized into non-AI-based and AI-based solutions. Non-AI-based tools, such as PyPDF2 and pdfminer.six, offer fast parsing capabilities but struggle with accuracy, especially when handling complex or unstructured layouts. On the other hand, AI-based solutions like MLLMs (Multimodal Large-Language Models) and marker-pdf, an open-source tool that converts PDFs into structured Markdown format, provide more accurate parsing but at the cost of speed.\n\nBelow is a comparison of these tools based on speed, accuracy, and the availability of free APIs:\n\nTool\tSpeed\tAccuracy\tFree API Available\nNon-AI-based Tools\tHigh\tLow\tNot needed\nMLLM\tLow\tHigh\tYes\nmarker-pdf\tModerate\tModerate\tYes (Limited)\n\nWhile non-AI-based tools are fast, they fall short in terms of accuracy, which is crucial for parsing resumes accurately. Additionally, these tools do not directly output data in a structured format like Markdown. During experimentation, it was found that MLLM provided the best balance of high accuracy and usability, especially since it offers a free API with generous limits through \nOpenRouter\n. Therefore, MLLM was chosen for this project due to its superior overall performance compared to marker-pdf.\n\nSelection of MLLM\n\nThe selection of MLLM was based mainly on two criteria:\n\nHallucination Rates\nInference Time\n\nInference time of a Large Language Model (LLM) refers to the time it takes for the model to process input data (such as text, images, or both) and generate an output (e.g., a response, prediction, or transformation). Alongside inference time, the \nhallucination rate\n —which measures how frequently an LLM generates inaccurate or non-factual information—is another critical metric, especially when tasks like document summarization are involved. Both of these metrics are essential: reducing hallucinations ensures the accuracy and reliability of generated content, while faster inference times improve API response times, contributing to a more efficient user experience. Considering both factors, the Google Gemini Flash 2.0 model was selected for its strong performance in minimizing hallucinations while delivering \nfast inference times\n as compared to other MLLMs, making it an ideal choice for this project.\n\nOptimizing Hyperparameters\n\nThe two key hyperparameters considered were:\n\nTop P: This parameter controls the cumulative probability distribution of the next token generated by the model. It restricts the token sampling to a subset of candidates that together make up the top P probability mass. A lower value (e.g., 0.4) makes the model more focused, while a higher value (e.g., 0.8) allows for more diverse token choices.\nTemperature: Temperature controls the randomness of predictions. A lower temperature (e.g., 0.2) makes the model more deterministic and focused on high-probability tokens, while a higher temperature (e.g., 0.8) introduces more randomness and creativity in token selection.\n\nBoth parameters were tested with values ranging from 0 to 1, in increments of 0.2.\n\nFor the MLLM, a top P value of 0.4 and temperature of 0.2 were chosen to prioritize factual accuracy in responses.\n\nFor the LLM, which required some degree of creativity to enhance the resume, the temperature was set to 0.4 and top P to 0.6 to allow for more diverse and creative outputs.\n\nFuture Work\nAddition of different varieties of templates for users.\nSafeguarding privacy by concealing sensitive information contained in resumes.\nCode and Resources\nPreview Website\nBackend Code Repo\n [To be released soon]\nFrontend Code Repo\n [To be released soon]\nReferences\nOpenRouter Docs\nHallucination Leaderboard\nVellum LLM Leaderboard\nDeepSeek V3 model info\nGemini 2.0 Flash Model Info\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nMethodology\n\nWorkflow Architecture\n\nUser Input\n\nParsing PDF\n\nPrompt for Resume Section Parsing\n\nCandidate Name\n\nLocation of Candidate\n\nExperience/Work History\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nYou might be interested\nLLM Cover Letter Generation Script\nW\nMar 31, 20259 reads\nAutomationDeep Learning+2\nAI-Powered Recruitment Assistant: Smart Query Handling, CV Analysis, and Context-Aware Conversations\nL\nMar 09, 202517 reads\nATSChatbot+5\nResume Insights\nL\nMar 31, 202510 reads\nagentic-systemsgenai+4\nResume Application Tracking System\nOct 31, 202418 reads",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "retrieval-augmented-generation-rag-assistant-b5w0yXpCR3AV",
    "username": "g@gonevamshi43",
    "license": "LICENSE.md",
    "title": "Retrieval-Augmented Generation (RAG) Assistant",
    "publication_description": "📝 #\n\nProject Publication: Retrieval-Augmented Generation (RAG) Assistant\n\n📌 #Abstract\n\nThis project is part of the Agentic AI Developer Certification (AAIDC) Module 1 and demonstrates the development of a Retrieval-Augmented Generation (RAG) Assistant. The assistant enhances AI reliability by combining document retrieval with natural language generation, producing accurate and context-aware responses.\n\n🔍# Introduction\n\nLarge Language Models (LLMs) are powerful but often prone to hallucinations, generating incorrect or unverifiable answers. This project addresses that challenge by implementing a RAG architecture, grounding responses in actual documents. The goal is to showcase how retrieval mechanisms can be integrated with LLMs to build fact-based, trustworthy assistants.\n\n⚙️# System Overview\n\nThe workflow of the RAG Assistant begins with document loading, where text files (.txt) are ingested from a designated folder. Once the documents are available, they undergo chunking, a process of splitting them into smaller segments. This step is crucial because it ensures that the content fits within token limits while also improving retrieval granularity.\nNext, each chunk is converted into a semantic vector using OpenAI Embeddings. These embeddings capture the meaning of the text rather than just the raw words. Once generated, the embeddings are stored in Chroma, a vector database designed for fast and efficient similarity searches.\nWhen a user submits a query, the system performs retrieval, pulling the most relevant chunks based on semantic similarity. These retrieved chunks are then passed as context to a large language model (LLM) such as GPT-3.5 via LangChain. Finally, the LLM generates a response that is both accurate and natural-sounding, grounded in the retrieved documents rather than hallucinated knowledge.\n\n🛠 #Tools & Frameworks\n\nThis project integrates several modern AI frameworks and libraries to deliver the RAG pipeline. LangChain acts as the workflow orchestrator, connecting the components seamlessly. OpenAI Embeddings provide high-quality semantic vectorization of document chunks, while Chroma serves as the vector database, storing embeddings and enabling similarity-based retrieval. For the user interface, Streamlit is used to build an interactive and accessible front end that allows users to query the assistant in real time.\n\n💻 #Installation Instructions\n\nTo install the RAG Assistant, start by cloning the official GitHub repository with git clone \nhttps://github.com/gone-vamshi233/rag-assistant.git\n and then navigate into the project folder. Next, create a virtual environment to keep dependencies isolated: on macOS and Linux, this can be done with python3 -m venv venv followed by source venv/bin/activate, while on Windows the commands are python -m venv venv and venv\\Scripts\\activate. Once the virtual environment is active, install the required dependencies using pip install -r requirements.txt. Finally, create a .env file in the project’s root directory and add your OpenAI API key in the format OPENAI_API_KEY=your_api_key_here. With these steps completed, the environment will be fully prepared to run the assistant.\n\n🚀 #Usage Instructions\n\nAfter completing the installation, using the assistant is simple. Place your text documents inside the data/ folder so they can be processed for retrieval. To start the application, run streamlit run app.py from the project directory, which will launch the interactive web interface in your browser. From there, you can type queries into the input box and receive context-aware answers grounded in your uploaded documents.\n\n🛡 #Maintenance & Support Details\n\nMaintaining the assistant involves a few simple steps. Whenever new documents need to be added, place them in the data/ folder and rebuild the embeddings to ensure they are included in retrieval. If the assistant produces incomplete or missing answers, double-check that all relevant documents have been uploaded and processed. For API-related issues, confirm that the .env file contains a valid OpenAI API key. These steps ensure smooth and reliable performance of the system.\n\n✨ #Key Features\n\nThe RAG Assistant produces context-aware responses that are grounded in retrieved documents, ensuring accuracy and reliability. Its modular design provides flexibility, allowing components such as the language model, embeddings, or vector database to be swapped with minimal effort. The system is also highly scalable, supporting dynamic updates as new documents are added without disrupting existing workflows. Finally, it offers a user-friendly interface built with Streamlit, enabling intuitive and accessible querying for end users.\n\n📚 #Example Use Case\n\nA practical example of the RAG Assistant in action can be seen in an organizational knowledge base scenario. Suppose a company uploads its internal policies and guidelines into the system. When an employee queries, “What is the company’s leave policy for interns?” the assistant retrieves the relevant section directly from the uploaded documents and provides a concise, reliable answer. This not only saves time but also ensures that responses remain accurate and fact-based. Such a setup demonstrates the assistant’s usefulness in supporting enterprise knowledge management, educational resources, and research assistance.\n\n🧪 #Evaluation & Best Practices\n\nThe assistant follows best practices to ensure reliable performance. Document chunking is used to optimize token usage and improve retrieval accuracy. The semantic similarity search mechanism ensures that even when queries are phrased differently, the most relevant information is still retrieved. Grounded answers significantly reduce the hallucinations that often occur in pure LLM-based systems. Moreover, the modular pipeline allows easy integration with new tools and APIs, making the system adaptable for evolving use cases.\n\n🧩 #Embedding Model Choice Explanation\n\nFor semantic vectorization, the project uses OpenAI’s text-embedding-3-small model. This choice strikes a balance between computational efficiency and embedding quality, ensuring fast yet accurate retrieval. For response generation, GPT-3.5 is employed to produce fluent, context-aware answers. Together, this combination reduces hallucinations while maintaining high-quality outputs, making it well-suited for real-world applications where factual reliability is critical.\n\n🧠 #Memory Mechanisms Explanation\n\nThe system incorporates lightweight memory mechanisms to keep responses grounded and context-aware. Retrieved document chunks act as temporary contextual memory for each query. For every user question, the system retrieves the top-k most relevant chunks from Chroma, ensuring that the LLM only considers the most important information.\n\nThis retrieved context is then injected into the prompt, enabling the LLM to generate an informed response. In longer conversations, the assistant can append previous query–response pairs, effectively building a short-term memory of the session. This design balances efficiency with contextual accuracy, avoiding unnecessary computational overhead while still maintaining continuity in dialogue.\n\n⚠️ #Limitations\n\nDespite its strengths, the system has certain limitations. Its effectiveness depends largely on the quality and coverage of the uploaded documents. If the dataset is incomplete, the assistant’s responses will also be limited. Performance can degrade when handling very large datasets or queries that are highly ambiguous. At present, the assistant is optimized for plain text files, though future iterations could extend support to multiple formats such as PDF or CSV.\n\n📊 #Retrieval Evaluation\n\nThe retrieval performance of the system is evaluated across multiple metrics:\n\nPrecision: proportion of retrieved chunks that are truly relevant.\n\nRecall: measures how many of the relevant chunks were successfully retrieved.\n\nF1-score: harmonic mean of precision and recall.\n\nIn addition, the system tracks latency (time taken to retrieve the top-k chunks). Keeping latency low is important for smooth and responsive user experience.\n\n🔚 #Conclusion\n\nThis project demonstrates how Retrieval-Augmented Generation (RAG) can enhance the reliability of large language models by grounding their outputs in real documents. With modular design, scalability, and an easy-to-use interface, the RAG Assistant can be applied in enterprise knowledge bases, education, and research. Future improvements will focus on adding support for multiple document formats and improving long-term memory mechanisms.\n\n#Tags:\n#RAG #AI #LLM #LangChain #VectorDatabase #Chroma #Streamlit #OpenAI #Embeddings",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "retrieval-augmented-generation-rag-case-study-a-resume-analysis-tool-g1E903d62F6L",
    "username": "aAmina Javaid",
    "license": null,
    "title": "Retrieval Augmented Generation (RAG) Case Study - A Resume Analysis Tool",
    "publication_description": "Back to publications\nOct 25, 2024\n●\n132 reads\nWinner of\nDistinguished Technical Deep-Dive\nat the\nNLP Projects Expo 2024\nRetrieval Augmented Generation (RAG) Case Study - A Resume Analysis Tool\nChatbot\nGenerative AI\nLangChain\nLLM\nNLP\nPrompt Engineering\nResume Analysis\nRetrieval Augmented Generation\nA\nAmina Javaid\nLike\nBookmark\nShare\nIntroduction\n\nThe world of artificial intelligence (AI) is rapidly evolving and one of the most used techniques in AI, specifically natural language processing (NLP), nowadays is Retrieval Augmented Generation (RAG). The word retrieve means to get some information from a data source, augment means to expand or increase something, and generate means to create something new. Retrieval Augmented Generation (RAG) is a technique through which a user communicates with a specialized knowledge base utilizing the power of LLMs and gets accurate and contextually relevant information. RAG augments LLM data with private or domain-specific data, enabling the generation of more accurate and relevant responses. There are three main components of a RAG system - data ingestion, retrieval, and synthesis. In this article, we'll discuss the importance and components of a RAG system in detail and use that knowledge to build an end to end RAG application for a real-world use case - resume analysis - utilizing the powerful capabilities of large language models (LLMs) and LangChain — a Python framework to build robust RAG applications.\n\nImportance of RAG\n\nIn the age of information, thousands of resources are available on any given topic. Large language models have made our lives much easier in terms of information retrieval as they have been pre-trained on billions of parameters. We can ask LLMs any question, and they provide relevant information structured in the required format.\n\nOne major issue with LLMs is hallucination, where they fabricate answers to questions they don’t know, often presenting these fabrications as factual without indicating their uncertainty. This becomes a big question mark on the authenticity of the information being retrieved. Secondly, LLMs are pretrained on very large datasets and this training takes a lot of time, so they don’t have access to latest or real-time information. While some LLMs provide real-time information, the retrieved content can be inaccurate and filled with irrelevant details. Moreover, the responses aren’t verified as they lack access to domain-specific knowledge or personal data. LLMs are powerful in understanding the context of user queries, but they require new data to be attached to them that they don’t already have access to.\n\nOne way to solve these problems is to fine tune a particular LLM on our specific data but this process has a huge training cost associated with it. The other way is to use retrieval augmented generation (RAG) in which relevant information from our knowledge base is retrieved based on our query and then sent to an LLM to analyze and generate a response. Using RAG utilizes the capabilities of LLMs to craft a context augmented response for us. We attach external data to an LLM which then gives answers to our queries from that data. This approach gives us more control, allowing us to restrict the LLM to generate answers solely from our own data rather than from its pre-trained information.\n\nHigh Level Components of RAG\n\nRAG is a combination of retrieval-based and generation-based methods. It first retrieves relevant documents from a knowledge base and then uses that information to generate a response. Following are the high level components of RAG:\n\nPrompt\nData Source | Knowledge Base\nRetriever\nGenerator | LLM\nResponse\n\nBasic RAG Workflow\n\nHere's an overview of a basic RAG workflow:\n\nData is ingested into the system.\nThe user sends a prompt to the system.\nThe retriever identifies and retrieves relevant documents according to the prompt.\nLLM generates accurate and relevant response using the prompt and retrieved information.\n\nAdvantages of RAG\nCustomized Information — RAG enables access to the latest and domain specific information in a well-structured format.\nAccurate and Relevant Information — RAG ensures that the retrieved information is accurate, relevant, and useful.\nContext-Augmented Information — RAG generates meaningful responses based on the prompt and retrieved information.\nApplications of RAG\n\nEvery individual has a unique knowledge source from which they may need to extract relevant information at different times. For a student, it’s the syllabus and curriculum. For a doctor, it’s patient medical histories. For a lawyer, it’s case records and related information. For a bank manager, it could be the bank’s policies. The strength of RAG lies in its ability to take a user’s natural language query and return precise, relevant information from a knowledge source — presented in a way that is easily understood by even non-technical users. Additionally, RAG systems support multilingual environments, offering customized responses in the user’s language. They can also be multimodal, integrating images, speech, and videos alongside text to provide comprehensive answers.\n\nThere are numerous applications of RAG already being used in the industry. Chatbots and AI assistants are few of them. Here is a list of some real-world applications which can be built using RAG:\n\nCustomer support chatbots\nConversational agents | Virtual assistants\nDocument summarizers\nPersonalized recommendation systems\nEducational tools | AI Tutors\nLegal Advisors\nMedical Diagnosis Assistants\nImplementation of RAG using LangChain\n\nLangChain is a widely used Python framework that helps build state-of-the-art LLM applications. It facilitates the implementation of robust RAG systems by providing reusable components. This makes it easy to integrate and customize different RAG components according to application needs. LangChain documentation provides an extensive explanation of each RAG component in detail. It has built-in tools for document transformation, connection to various data sources, and chaining multiple operations.\n\nRAG Pipeline\nIngestion — Process and attach an external data source to the system.\nRetrieval — Correctly retrieve the information related to the prompt entered by a user.\nSynthesis — Process the retrieved response, refine it, and present it in a readable format.\n\nData Ingestion\n\nData ingestion is the process of loading external documents and storing them in a vector database for retrieval. These documents can be in various formats, such as PDF, DOC, TXT, HTML, and more. The ingestion process includes the following steps:\n\nLoad — Loading the data into documents\nSplit — Splitting the data into smaller, manageable chunks\nEmbed — Creating document embeddings\nStore — Storing the embeddings in a vector database\n\nRAG systems are extremely beneficial because they enhance the capabilities of LLMs by providing real-time and factual information to the user in natural language utilizing the powerful capabilities of pre-trained LLMs. A reliable external data source is vital for the success of a RAG based system, making data ingestion the first step in building these systems. Data is ingested through an external data source into a RAG system to make it available for smart and efficient retrieval and synthesis. A robust RAG system depends on a well-constructed ingestion pipeline, as it retrieves responses based on the context of the external data. The higher the quality of the ingestion process, the more accurate the results.\n\nLet's look at each of the data ingestion steps in detail.\n\n1. Data Loading\n\nData loading involves extracting data from a source and converting it into a suitable format for use in a RAG application.\n\nLoaders\n\nLangChain offers a comprehensive set of loaders that facilitate data ingestion and preprocessing from various sources, converting it into documents for further processing. Data is loaded into a Document object, which consists of the text content and its associated metadata.\n\nLoaders can handle multiple sources, including text files, PDFs, word documents, web pages, and more.\nThey also perform essential preprocessing tasks such as tokenization, normalization, and format conversion, ensuring the data is ready for LLMs.\nLoaders are customizable to manage specific data formats and seamlessly integrate with other LangChain components.\nExamples: Data Loading using LangChain\nLoad a TEXT document (.txt)\nfrom langchain_community.document_loaders import TextLoader\n\nfile_path = \"data/my_document.txt\"  # Path of the document to be loaded\nloader = TextLoader(file_path)      # Initialize the text loader\ndocuments = loader.load()           # Load the text document\nLoad a PDF document (.pdf)\nfrom langchain_community.document_loaders import PyPDFLoader\n\nfile_path = \"data/my_document.pdf\"  # Path of the document to be loaded\nloader = PyPDFLoader(file_path)     # Initialize the pdf loader\ndocuments = loader.load()           # Load the pdf document\nLoad a WORD document (.docx)\nfrom langchain_community.document_loaders import Docx2txtLoader\n\nfile_path = \"data/my_document.docx\"  # Path of the document to be loaded\nloader = Docx2txtLoader(file_path)   # Initialize the word document loader\ndocuments = loader.load()            # Load the word document \nLoad a WEB page (.html)\nfrom langchain_community.document_loaders import WebBaseLoader\n\nweb_path = \"https://www.example.com\"   # Path of the web page to be loaded\nloader = WebBaseLoader(web_path)       # Initialize the web page loader\ndocuments = loader.load()              # Load the web page\n\nThe choice of a loader depends on the nature of the data, as each loader has unique characteristics, and returns information differently. It’s essential to explore various loaders to understand their offerings and determine how to best utilize them when building AI products with RAG.\n\nMetadata — Importance and Usage\n\nMetadata essentially refers to data about data. When loaders extract content, they also provide metadata such as the data source or page numbers, which can be highly useful. For instance, if you’re building an application where users need to retrieve information along with its source or page location, metadata offers all the necessary details, simplifying information verification. These features can significantly enhance your application, and the metadata can even be edited to introduce additional value-added features.\n\n2. Data Splitting/Chunking\n\nWhen performing preprocessing for RAG, it’s crucial to consider the nature of the data, the type of RAG system being built, and its limitations. After loading documents, we’ll often need to transform them to fit our application’s requirements. For example, if we have a book with more than 1000 pages, we cannot pass the entire content to an LLM due to its limited context window. This is where chunking or splitting the data into smaller, manageable parts becomes essential.\n\nThere are three key reasons for splitting data:\n\nManageability: Breaking the data into smaller parts ensures it fits within the LLM’s context window, making it easier to handle.\nEmbedding model compatibility: Embedding models have limits on the amount of data they can process at a time, so chunking ensures that the data aligns with the model’s capacity.\nEfficient retrieval: When a user sends a query, they only need specific information, which is often found in smaller chunks. By retrieving only relevant chunks, we avoid processing the entire corpus for a single query.\n\nChunking can be done at different levels — sentences, paragraphs, or chapters — depending on the problem and type of data. Think of an LLM like a young child learning to read. When teaching children how to read, we first break words into characters and then combine them to form words. Similarly, we break the text into smaller chunks to enhance the LLM’s understanding and response generation.\n\nSplitters\n\nLangChain provides a variety of built-in document transformers that simplify tasks like splitting, combining, filtering, and manipulating documents. Splitters are tools used to divide large text documents into smaller, semantically meaningful chunks. These chunks can then be processed individually for various NLP tasks, ensuring they stay within the size constraints of the model.\n\nWith an unlimited number of users, each sending prompts differently, the system must semantically relate the information in the text to provide accurate responses. Text splitters can be customized based on how the text is divided and the chunk size, helping ensure relevant answers while minimizing computation cost. This is important because the more data we send to an LLM, the higher will be the computation cost.\n\nSplitters maintain the context and integrity of the text, making it easier to manage and analyze. During splitting, the chunk size must be carefully decided. For instance, if a page contains three interrelated paragraphs, dividing them into separate chunks would break the context. To address this, we use overlapping, where each chunk includes information from both the previous and next chunks, preserving context. However, if each paragraph introduces distinct topics, minimal or no overlapping may be needed, and the system will still generate a high-quality response.\n\nDetermining the maximum chunk size depends on the limitations of the model to which the chunks will be passed, particularly the embedding model in use. Each embedding model, available on platforms like Hugging Face, has specific input and output token limits. It is crucial to consider these limits when deciding on chunk size to ensure that no valuable information is lost.\n\nTypes of Splitters\n\nHere are some of the text splitters used for different purposes:\n\nRecursive Splitter: This widely-used splitter adopts a hierarchical approach, initially splitting text at larger, natural breakpoints (e.g., paragraphs). It recursively processes chunks to fit size constraints while preserving context as much as possible.\nHTML Splitter: Designed for HTML content, this splitter divides text based on HTML tags and structure. It is ideal for processing web pages and HTML documents, maintaining the integrity of HTML elements.\nMarkdown Splitter: This splitter handles markdown-formatted text by splitting based on markdown syntax, such as headers and lists. It is useful for processing markdown documents and notes while preserving the structure of markdown elements.\nCode Splitter: Tailored for source code, this splitter divides text based on code structures like functions and classes. It maintains logical integrity, making it suitable for processing and analyzing code snippets in various programming languages.\nToken Splitter: This splitter segments text based on token count, ensuring each chunk contains a specified number of tokens. It is useful for models with token-based input limits and can handle various tokenization schemes, including subword, character, and word tokenization.\nCharacter Splitter: Splitting text based on character count, this splitter offers a straightforward approach when character-based limits are crucial. It can be combined with other splitters for finer control.\nSemantic Chunker: An experimental feature of LangChain, the semantic chunker creates chunks based on semantic similarity. It generates document embeddings, compares them, and combines similar embeddings into chunks. It uses thresholds like percentile and standard deviation to determine similarity and does not require specifying chunk size. This splitter first divides text into sentences and then combines those with similar semantic meaning, preserving the context and meaning of each chunk.\nExamples: Data Splitting using LangChain\nCharacterTextSplitter\nfrom langchain_text_splitters import CharacterTextSplitter\n\n# Initialize the character text splitter\ntext_splitter = CharacterTextSplitter(              \n    separator=\"\",\n    chunk_size=100,\n    chunk_overlap=20\n)   \n\n# Split the documents into chunks\nchunks = []\nfor doc in documents:\n    texts = text_splitter.split_text(doc.page_content)\n    chunks.extend(texts)\nRecursiveCharacterTextSplitter\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Initialize the recursive character text splitter\ntext_splitter = RecursiveCharacterTextSplitter(              \n    separators=\"\",\n    chunk_size=100,\n    chunk_overlap=20\n)   \n\n# Split the documents into chunks\nchunks = []\nfor doc in documents:\n    texts = text_splitter.split_text(doc.page_content)\n    chunks.extend(texts)\nSemantic Chunker\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\n# Use a smaller, faster HuggingFace model for embeddings\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Initialize the semantic chunker\ntext_splitter = SemanticChunker(embeddings, breakpoint_threshold_type='percentile')\n\n# Split the documents into chunks\nchunks = []\nfor doc in documents:\n    texts = text_splitter.split_text(doc.page_content)\n    chunks.extend(texts)\n3. Embeddings\n\nAll artificial intelligence is fundamentally mathematics. Machines don’t understand text; they process numbers. To bridge this gap, we must convert textual data into numerical form, a process known as vector embeddings. These embeddings are stored in a vector store or vector database for efficient retrieval.\n\nEmbeddings are dense vector representations that capture the semantic meaning of words, phrases, or sentences. Words with similar meanings have closer numerical representations, while words with different meanings have more distinct vectors. Importantly, embeddings aren’t single numbers but multi-dimensional vectors, representing the nuanced relationships between words. LLMs learn these representations, making words with smaller mathematical differences more semantically related.\n\nEmbeddings are calculated using embedding models, which are typically built from the encoder part of a transformer model. While large language models (LLMs) are developed from the decoder part, the encoder is responsible for learning meaningful representations of data, and the decoder generates new content based on these learned representations.\n\nIn machine learning, embeddings are widely used for tasks like classification, where words with similar embeddings can be grouped into one class. They are also applied in anomaly detection, identifying outliers with vastly different embeddings, and in clustering, where related items are grouped based on their vector similarities.\n\nWe can choose between open-source embedding models from Hugging Face or paid models from OpenAI. While OpenAI’s models come with usage fees, infrastructure cost of using open-source models is our own responsibility. Some models may require GPUs, so selecting the right model depends on our system’s requirements and budget.\n\nExamples: Embeddings using LangChain\nHuggingFace Embeddings\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\n\n# Initialize the Hugging Face embedding model\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n\n# Create embeddings of text chunks\nfor i, chunk in enumerate(chunks):\n    print(\"Text chunk \", i)\n    print(\"--------------\")\n    print(chunk, \"\\n\")\n    query_result = embeddings.embed_query(chunk)\n    print(\"Embeddings\", \"\\n\", query_result, \"\\n\\n\")\n4. Data Storage — Vector Stores\n\nA vector is a mathematical representation used to store data in a computer, a concept known as representation learning. To store these representations efficiently, vector stores are employed. They are highly optimized databases designed for storing and querying high-dimensional vector data, making them essential in AI, especially in applications like recommendation systems and retrieval-augmented generation (RAG).\n\nVector embeddings are stored in specialized databases known as vector stores, which differ significantly from traditional or relational databases. In traditional databases, data is organized in rows and columns, similar to an Excel sheet. However, storing unstructured data like images, speech, or text in this format is inefficient, making vector stores a better choice.\n\nLet’s explore the difference with an example. Exact search (e.g., looking for a specific product by its exact ID or name in an online store) is handled by traditional relational databases (RDBMS). Suggestions or recommendations (e.g., finding products with similar features, style, or price range when you don’t know the exact item) rely on vector stores, which compare items based on their vector embeddings to provide relevant recommendations. Therefore, conventional databases retrieve exact matches, while vector stores focus on similarity-based retrieval.\n\nVector stores are primarily used in machine learning and data retrieval applications where the goal is to find semantically similar information. Following are some of their use cases:\n\nRetrieval in RAG-based systems.\nRecommendation systems through similarity search.\nAnomaly detection by identifying unusual or unknown queries.\nVector search for NLP applications like document retrieval.\nImage and video retrieval.\nStructure of a Vector Store\n\nA vector store consists of the following essential elements:\n\nVectors/Embeddings: These are mathematical representations of images, audio, or text and form the core component of a vector store.\nMetadata: Additional information about the vector such as: time, date, and location for images, or author, publication year, and topic for books.\nOriginal data: While not always necessary, in RAG applications it can be useful to store the original text or data in the vector store.\nUnique ID: Each row in the vector store is identified by a unique ID generated by the system.\nPopular Vector Stores\n\nFollowing are some of the commonly used vector stores:\n\nPinecone\nFAISS\nQdrant\nChroma\nKey Features of Vector Stores\n\nThe strength of a vector store lies in its\n\nHigh-speed similarity search, and\nScalability and integration with machine learning pipelines.\nHow Do Vector Stores Work?\n\nWhen querying a vector store, two main operations are commonly used:\n\nCosine Similarity: Used for recommendations, such as finding similar products. The input (e.g., a product description) is converted into a vector, and the vector store applies cosine similarity to identify the top k similar vectors. The metadata then helps retrieve relevant information, like product details. E-commerce websites and social media platforms use similar recommendation systems.\nDot Product: Another common method for comparing vectors.\nTypes of Vector Stores\nCloud-based, Fully Managed Services — These are hosted on the cloud, requiring only an API key for access. You don’t have to manage the infrastructure, making them easy to set up and scale. However, they come with associated cloud service costs. Examples include Pinecone and Qdrant.\nLocally Managed Vector Databases — If you prefer to keep sensitive data off the cloud or have specific client requirements for local RAG systems, this is a suitable option. You will need to manage storage, scalability, and resource allocation yourself. Examples include FAISS and Chroma.\nKey Benefits of Vector Stores\nEfficient Data Storage: Vector stores are optimized for high-dimensional vector data.\nFast Retrieval: They are designed for quick similarity searches and responses.\nImproved Semantic Understanding: Embedding models store vectors based on their semantic relationships, ensuring meaningful representation of data.\n\nChoosing between vector stores depends on your specific needs, resources, and project requirements. When pushing data into a vector store, you have the flexibility to structure and customize its metadata based on the specific needs of your application. You can even add additional fields to enrich the metadata, making your system more efficient and tailored to your use case.\n\nIt’s essential to consider optimization techniques to ensure your system performs well. Factors like token cost, the number of chunks created, the vectors stored in the vector store, the top n documents retrieved from the store, and the chunks sent to the LLM in a RAG system all play a critical role.\n\nAdditionally, always ensure that keys, such as API keys and credentials, are securely stored in secret files to protect your system from potential security risks. Failing to do so could compromise the entire system setup.\n\nExamples: Data Storage using LangChain\n\nLangChain integrates seamlessly with vector stores, allowing us to build powerful language-based applications that use vector embeddings for enhanced retrieval and processing.\n\nStore Data using FAISS\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\n\n# Initialize the Hugging Face embedding model\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n\n# Store embeddings into the vector store\nvector_store = FAISS.from_documents(\n    documents=chunks,\n    embedding=embeddings\n)\nData Retrieval\n\nData retrieval is the process of finding and extracting relevant information or documents from a vector store based on user prompt. A user asks questions from the specialized knowledge source to get the most relevant answers, so the vector store needs to be used programmatically for the retrieval process.\n\nTypes of Retrievers\n\nThere are many different types of retrievers in LangChain including:\n\nVector store-backed retriever — It uses a vector store as a retriever to retrieve documents using search methods like similarity search, MMR, etc.\nMultiQuery retriever — It addresses the shortcomings of distance based retrieval by generating multiple queries from a prompt and retrieving relevant chunks by taking union across all queries. It captures different perspectives of a complex prompt and produces better results.\nContextual compression retriever — It reduces the cost of LLM calls by compressing or shortening the documents having relevant information or dropping the irrelevant documents altogether to generate a good quality response.\nCustom retriever — We can design our own retriever by extending the BaseRetriever in LangChain to retrieve information from an external knowledge source.\nEnsemble retriever — It ensembles the results of multiple retrievers for better performance and more robust results.\n\nSome other retrievers are:\n\nSVM retriever —It matches the most relevant documents by distinguishing them from irrelevant ones.\nTF/IDF retriever — It focuses on the words which are most repeated or most diverse in our system for document retrieval.\n\nThese retrievers are important when you are expected to get keywords from a user instead of a query.\n\nThe Process of Retrieval\n\nWhen a user asks a question, it is converted into a vector. This vector then goes to the vector store through a retriever interface. The vector store finds n number of vectors similar to the query vector. To match the user’s question to similar vectors, there are various methods, some of which are:\n\nDot product\nCosine similarity\nMMR — Maximum marginal relevance\n\nWhy do we want to match these vectors for similarity? Basically we want to find out which vectors in the stored content match with the question asked by the user as only those similar vectors would be useful for us to generate a response through LLM. So we actually fetch the data relevant to the question from the vector store.\n\nVector Store as a Retriever\n\nThe simplest form of information retrieval is to use a vector store as a retriever. A vector store-backed retriever uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses similarity search to retrieve information in the form of relevant chunks based on distance between vectors. So we retrieve only the required information from a source in order to give response to a user instead of retrieving all the information.\n\nA vector store contains:\n\nVector,\nText associated with the vector, and\nMetadata\n\nThe vectors are stored in the vector store in a meaningful representation through an embedding model. Our ultimate goal is to connect our data to an LLM. To interact with the LLM, we need to retrieve data from the vector store and then send it to the LLM along with our query or prompt to get the desired response in a structured format. That’s why we retrieve semantically similar chunks based on the query. The query is converted into vectors and meaningful comparisons are made to retrieve the similar vectors. Using vector store retrievers is ideal when handling large amounts of embedded data because they allow efficient access to relevant chunks.\n\nSemantic Similarity\n\nWhen data is retrieved, we only get its associated text chunk along with metadata and not the original vector because we only need those text chunks to be used in our application for further processing. Vectors are used for mathematical operations which cannot be performed on text. This concept is referred to as semantic search. Semantic search goes beyond keyword search (which relies on the occurrence of specific words in the search input) to find contextually relevant data based on the conceptual similarity of the input string. The vector store retriever uses similarity search by default.\n\nMaximum Marginal Relevance (MMR)\n\nMaximum marginal relevance (MMR) is a method used to avoid redundancy while retrieving relevant items to a query.\n\nTo process repeated information is a waste of cost and time. We want to filter out repeated information from relevant information. MMR finds closest chunks and removes or discards the chunks with repeated information. Instead of merely retrieving the most similar items, MMR ensures a balance between relevancy and diversity in the items retrieved. It reduces redundancy by selecting the most diverse set of relevant information. This is particularly important when balancing coverage and efficiency in information retrieval.\n\nWhether to use MMR or not depends on the application you are building. For example, if a lawyer wants to retrieve all relevant cases from different regions for a particular scenario, MMR won’t be useful because he requires to have all the information. If limited information is required, you may use MMR. So this is case specific. Whenever you need diversity in response, you’ll need to use MMR.\n\nTo choose the appropriate retriever, you need to know:\n\nWhat type of data do you have?\nWho is the end user of your application?\nExample: Data Retrieval using LangChain\nVector Store as Retriever\n\nSimilarity Search Retrieval\n\n# Retrieve relevant information using similarity search\nretriever = vector_store.as_retriever() # uses similarity search by default\ndocs = retriever.invoke(\"Ask any question from the document here\")\nMaximum Marginal Relevance (MMR) Retrieval\n# Retrieve relevant information using maximum marginal relevance retrieval\nretriever = vector_store.as_retriever(search_type=\"mmr\") # uses mmr as search type\ndocs = retriever.invoke(\"Ask any question from the document here\")\nSimilarity Score Threshold Retrieval\n# Retrieve relevant information using similarity score threshold\nretriever = vector_store.as_retriever(\n    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n)\ndocs = retriever.invoke(\"Ask any question from the document here\")\nSpecifying Top k Documents\n# Retrieve top k relevant documents\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\ndocs = retriever.invoke(\"Ask any question from the document here\")\nUse of Large Language Models (LLMs) in RAG\n\nLarge language models (LLMs) are highly complex neural networks trained on vast amounts of text data and typically consist of billions of parameters, enabling them to understand human language at an advanced level. They are capable of generating high quality text and inference. There are a lot of variants of LLMs which can be used in a RAG based application. Some of them include:\n\nGPT (Generative Pre-trained Transformer) by OpenAI\nClaude by Anthropic\nLlama by Meta\nGemini by Google, and many more.\n\nEach LLM has a predefined context window, which refers to the number of tokens it can process simultaneously. Using these LLMs has a usage cost associated with each of them. Additionally, multimodal LLMs are gaining popularity due to their capability to process and generate text, speech, images, and video content.\n\nEach large language model varies in its reasoning capabilities, particularly in mathematical reasoning and generating nuanced responses. While some tasks require the advanced reasoning of larger LLMs, mini models are often sufficient for specific tasks, especially where we don’t rely on the internal knowledge of the LLM — such as in RAG-based systems. In these cases, mini models can be more cost-effective and focused on refining responses rather than using a model’s full knowledge base.\n\nUnderstanding the capacity, reasoning power, and cost implications of each LLM ensures that the right model is chosen for the right use case, balancing power with efficiency. Therefore, specialized LLMs optimize cost and performance in practical applications.\n\nThe quality of response generated through a RAG system highly depends on the correctness of the retrieved information as this retrieved information along with the user prompt is passed to the LLM for response generation. Ultimately, the success of a RAG system lies in its ability to retrieve the most relevant information effectively. Careful selection of retrieval methods and LLM models tailored to the task ensures optimal performance and cost efficiency.\n\nSynthesis\n\nSynthesis is the final component of a RAG pipeline where a response is generated for the user through an LLM. The user prompt and relevant chunks retrieved from a vector database during data retrieval are passed to the LLM which then generates a context augmented response that answers the question asked by the user. The response can be structured in any required format.\n\nTo make our RAG applications controllable and customizable, we use the concept of chains which enable us to connect different components of a RAG-based system.\n\nWhat are Chains?\n\nChains refer to a sequence of calls to combine multiple components of a RAG-based application. Let’s say we want to develop an AI-powered chatbot. We’ll need to develop a conversation chain for this purpose.\n\nLangChain — a Python framework to build LLM-powered applications — provides us the facility to use chains. There are many different types of chains available in LangChain. Let’s explore the components and elements of different chains, how they work, and how we can use them in our applications. The most common way to use chains is through LCEL (LangChain Expression Language).\n\nWhat is LCEL?\n\nLCEL (LangChain Expression Language) is a standard way in LangChain that allows for the creation and execution of custom chains. It helps in the integration of different components and provides streaming, asynchronous support, and parallel execution. The chains can be easily modified or extended. Additionally, LangChain implements a Runnable protocol for interaction between components.\n\nRunnable\n\nEvery LCEL object implements the Runnable interface. Runnables are used to build custom chains.\n\nThere are three common methods to call the Runnable interface:\n\nstream — Streams chunks of a response.\ninvoke — Calls the chain on an input.\nbatch — Calls the chain for multiple inputs.\n\nIf you have a single query, you can simply invoke the chain and you’ll get the response. If you have a list of inputs, you’ll use batch call and it will give you the responses in a list. The stream method is used to display the response in chunks so that you can view the response while it’s being generated.\n\nRunnable Lambda\n\nRunnable Lambda converts a Python callable into a Runnable.\n\nfrom langchain_core.runnables import RunnableLambda\n\n# Define a RunnableLambda that takes a string as input,\n# reverses it, and then repeats the reversed string twice.\nrunnable = RunnableLambda(lambda s: (s[::-1]) * 2)\n\n# Invoke the runnable with the string \"hello\"\nresult = runnable.invoke(\"hello\")\n\nprint(result)  # Output: \"olleholleh\"\nRunnable Sequence\n\nA Runnable sequence is a composition of multiple Runnables invoked sequentially, where the output of one serves as the input to the next. It can be instantiated directly or by using the | operator. Whenever you need to invoke a chain, you make its Runnable interface and the Runnable sequence executes all the steps of that chain in a sequence.\n\nImplementation using the | operator:\n\nfrom langchain_core.runnables import RunnableLambda\n\n# Define a sequence of RunnableLambda instances that process a string\n# Step 1: Remove leading and trailing whitespace\n# Step 2: Convert the string to uppercase\n# Step 3: Replace spaces with underscores\n# Step 4: Add \"Processed:\" prefix to the string\nsequence = RunnableLambda(lambda s: s.strip()) | \\\n           RunnableLambda(lambda s: s.upper()) | \\\n           RunnableLambda(lambda s: s.replace(\" \", \"_\")) | \\\n           RunnableLambda(lambda s: f\"Processed: {s}\")    \n\n# Invoke the sequence with the input string \"  hello world  \"\nresult = sequence.invoke(\"  hello world  \")\n\nprint(result)  # Output: \"Processed: HELLO_WORLD\"\n\nAlternative implementation by directly calling the RunnableSequence:\n\nfrom langchain_core.runnables import RunnableLambda, RunnableSequence\n\n# Define individual RunnableLambda steps for processing a string\nstrip = RunnableLambda(lambda s: s.strip())             # Step 1: Remove leading and trailing whitespaces\nuppercase = RunnableLambda(lambda s: s.upper())         # Step 2: Convert the string to uppercase\nreplace = RunnableLambda(lambda s: s.replace(\" \", \"_\")) # Step 3: Replace spaces with underscores\nprefix = RunnableLambda(lambda s: f\"Processed: {s}\")    # Step 4: Add \"Processed:\" prefix to the string\n\n# Create a RunnableSequence that combines all the individual steps\nsequence = RunnableSequence(strip, uppercase, replace, prefix)\n\n# Invoke the sequence with the input string \"  hello world  \"\nresult = sequence.invoke(\"  hello world  \")\n\nprint(result)  # Output: \"Processed: HELLO_WORLD\"\nRunnable Parallel\n\nA Runnable parallel is a composition that invokes multiple Runnables concurrently, providing the same input to each.\n\nfrom langchain_core.runnables import RunnableLambda, RunnableParallel\n\n# Define individual RunnableLambda functions for different string operations\nreverse = RunnableLambda(lambda s: s[::-1])      # Function to reverse the input string\nuppercase = RunnableLambda(lambda s: s.upper())  # Function to convert the string to uppercase\nlength = RunnableLambda(lambda s: len(s))        # Function to calculate the length of the string\n\n# Create a RunnableParallel with a mapping (dictionary) of operations\nparallel = RunnableParallel({\n    \"reverse\": reverse,\n    \"uppercase\": uppercase,\n    \"length\": length\n})\n\n# Invoke the parallel sequence with the input string \"hello\"\nresult = parallel.invoke(\"hello\")\n\nprint(result)  # Output: {'reverse': 'olleh', 'uppercase': 'HELLO', 'length': 5}\n\nRunnable parallel can only be used when input of the next chain is not dependent on the output of the previous one, therefore, you can use this only if the inputs are independent of each other.\n\nRunnable Passthrough\n\nA Runnable passthrough allows to pass through the inputs unchanged.\n\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Define a RunnablePassthrough that passes the input unchanged\npassthrough = RunnablePassthrough()\n\n# Invoke the passthrough\nresult = passthrough.invoke(\"hello world\")\n\nprint(result)  # Output: \"hello world\"\nPrompt Templates\n\nPrompt templates enable us to dynamically incorporate user prompts into our RAG system. We have to send the retrieved context along with the prompt to an LLM in order to retrieve information through a RAG system, therefore we use prompt templates for this purpose. Let’s look at a few of them.\n\nBasic Prompt Template\n\nA simple template that allows for dynamic input replacement within a predefined string.\n\nfrom langchain_core.prompts import PromptTemplate\n\n# Define a template for the prompt that includes a placeholder for the country\ntemplate = \"What is the capital of {country}?\"\n\n# Create a PromptTemplate instance with the defined template\nprompt_template = PromptTemplate(template=template)\n\n# Format the template by replacing the placeholder with the specified country (\"Pakistan\")\nprompt = prompt_template.format(country=\"Pakistan\")\n\nprint(prompt)  # Output: What is the capital of Pakistan?\nFew-Shot Prompt Template\n\nA template designed to provide a few examples to guide the model’s responses, enhancing its understanding of the desired output.\n\nfrom langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n\n# Define a list of example queries and their corresponding answers\nexamples = [\n    {\"query\": \"How can I improve my time management skills?\", \n     \"answer\": \"Start by prioritizing your tasks using the Eisenhower Matrix to distinguish between what's urgent and important.\"},\n     \n    {\"query\": \"What are some effective strategies for setting goals?\", \n     \"answer\": \"Use the SMART criteria: make your goals Specific, Measurable, Achievable, Relevant, and Time-bound.\"}\n]\n\n# Create a PromptTemplate for formatting the examples\nexample_template = PromptTemplate(\n    input_variables=[\"query\", \"answer\"],  # Define the input variables for the template\n    template=\"User: {query}\\nAI: {answer}\"  # Template structure for each example\n)\n\n# Create a FewShotPromptTemplate using the examples and example_template\nfew_shot_prompt = FewShotPromptTemplate(\n    examples=examples,                                        \n    example_prompt=example_template,                           \n    prefix=\"Here are some examples of questions and answers:\",  \n    suffix=\"User: {query}\\nAI: {answer}\\n\", \n    input_variables=[\"query\"],              \n    example_separator=\"\\n\\n\"\n)\n\n# Format the few-shot prompt with a new user query and corresponding answer\nprompt = few_shot_prompt.format(query=\"How do I stay motivated during long projects?\",\n                                answer=\"Break down the project into smaller milestones and celebrate each achievement.\")\n\nprint(prompt)\nChat Prompt Template\n\nA specialized template for chat models that formats a sequence of messages allowing for distinct roles (e.g., human, AI).\n\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Define a ChatPromptTemplate from a list of message tuples. \n# The first element in each tuple represents the speaker (\"human\" or \"ai\"), \n# and the second element is the message template.\nchat_template = ChatPromptTemplate.from_messages([\n    (\"human\", \"Can you tell me about {author}?\"),   # Human asks about the author\n    (\"ai\", \"{author} is a renowned author known for {notable_work}.\")  # AI responds with author details\n])\n\n# Format the chat prompt with specific values for the placeholders\n# Replaces {author} with \"Jane Austen\" and {notable_work} with \"Pride and Prejudice\"\nmessages = chat_template.format_messages(author=\"Jane Austen\", notable_work=\"Pride and Prejudice\")\n\nprint(messages)\nOutput Parsers\n\nAn output parser is a component that processes outputs from AI models, transforming them into a desired format or structure. Different output parsers can be used to get the response in a required format such as a string, a dictionary, or JSON format.\n\nStrOutputParser\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Initialize the parser\nparser = StrOutputParser()\n\n# Define a string output from an AI explaining a concept\nai_response = \"\"\"\nMachine learning is a subset of artificial intelligence that focuses on enabling systems to learn from data patterns and make decisions without being explicitly programmed. It is widely used in applications like recommendation systems, image recognition, and natural language processing.\n\"\"\"\n\n# Use the parser to process the AI response\nparsed_output = parser.invoke(ai_response)\n\nprint(parsed_output)\nJsonOutputParser\nfrom langchain_core.output_parsers import JsonOutputParser\n\n# Initialize the JSON parser\nparser = JsonOutputParser()\n\n# Simulated AI response in JSON format (string)\nai_response = \"\"\"\n{\n    \"name\": \"John Doe\",\n    \"age\": 30,\n    \"occupation\": \"Software Engineer\",\n    \"skills\": [\"Python\", \"Machine Learning\", \"Data Analysis\"]\n}\n\"\"\"\n\n# Parse the JSON response\nparsed_output = parser.invoke(ai_response)\n\n# Print the parsed output as a dictionary\nprint(parsed_output)\n\n# Access specific fields from the parsed JSON output\nprint(f\"Name: {parsed_output['name']}\")\nprint(f\"Skills: {', '.join(parsed_output['skills'])}\")\nTypes of Chains\n\nThere are four common types of chains:\n\nQuestion/Answering (Q/A) Chain\nQuestion/Answering (Q/A) Retrieval Chain\nConversational Chain\nConversational Retrieval Chain\n\nThe selection of a chain depends on the requirements of the RAG system being built. Let’s explore each of these chains in detail.\n\n1. Question/Answering (Q/A) Chain\n\nA Q/A chain is used to do direct question answering with an LLM. The model answers directly using its internal knowledge.\n\n# Q/A Chain\n\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_groq import ChatGroq\n\n# Define a prompt template for formatting the question\nprompt_template = \"Q: {question}\\n\"\nprompt = PromptTemplate(template=prompt_template, input_variables=[\"question\"])\n\n# Instantiate the ChatGroq LLM with the specified model\nllm = ChatGroq(model=\"mixtral-8x7b-32768\")\n\n# Combine the prompt and LLM into a chain where the prompt output is passed to the LLM\nqa_chain = prompt | llm  # This creates a chain that takes the formatted question and passes it to the model\n\n# Invoke the chain with a specific question\nresponse = qa_chain.invoke(\"What is the capital of Japan?\")\n\n# Print the content of the response\nprint(response.content)\n2. Question/Answering (Q/A) Retrieval Chain\n\nA Q/A retrieval chain is used to add context to an LLM response. It retrieves relevant documents from a knowledge base (e.g. personal documents, research papers) and generates an answer.\n\n# Q/A Retrieval Chain\n\nfrom langchain_groq import ChatGroq\nfrom langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Initialize the ChatGroq LLM with the specific model\nllm = ChatGroq(model=\"mixtral-8x7b-32768\")\n\n# Define the system prompt that instructs the LLM how to answer questions based on retrieved context\nsystem_prompt = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer \"\n    \"the question. If you don't know the answer, say that you \"\n    \"don't know. Use three sentences maximum and keep the \"\n    \"answer concise.\"\n    \"\\n\\n\"\n    \"{context}\"  # Placeholder for the retrieved context\n)\n\n# Create a chat prompt template with a system message and human message\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),  # System message contains instructions and context\n        (\"human\", \"{input}\"),  # Human message includes the user's input question\n    ]\n)\n\n# Create a document chain that combines the LLM with the prompt\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\n\n# Combine the retrieval chain with the question-answering chain\n# The retrieval chain retrieves relevant documents and feeds them into the question-answering chain\nretrieval_chain = create_retrieval_chain(retriever, question_answer_chain)\n\n# Invoke the chain with a question, and the retriever will provide context for the LLM to generate an answer\nresponse = retrieval_chain.invoke({\"input\": \"Ask the question from the user here\"})\n\nprint(response['answer'])\n3. Conversational Chain\n\nA conversational chain is used to add chat history to simple question answering. It maintains context from previous interactions without document retrieval.\n\n# Conversational Chain\n\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_groq import ChatGroq\n\n# Create an external store (a dictionary) for maintaining chat history across multiple sessions\nstore = {}\n\n# Function to get or create chat history for a specific session based on session_id\ndef get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n    # If the session_id doesn't exist in the store, create a new InMemoryChatMessageHistory\n    if session_id not in store:\n        store[session_id] = InMemoryChatMessageHistory()\n    # Return the chat history associated with the given session_id\n    return store[session_id]\n\n# Initialize the ChatGroq model\nllm = ChatGroq(model=\"mixtral-8x7b-32768\")\n\n# Create a chain that can handle message history, linking it with the LLM and session history retrieval\nchain = RunnableWithMessageHistory(llm, get_session_history)\n\n# First conversation with AI in session \"1\"\nquestion_1 = \"Hello, there?\"\nresponse_1 = chain.invoke(\n    question_1,\n    config={\"configurable\": {\"session_id\": \"1\"}},  # Specify the session_id for tracking message history\n)\n\n# Second conversation in the same session \"1\"\nquestion_2 = \"What can you help me with today?\"\nresponse_2 = chain.invoke(\n    question_2,\n    config={\"configurable\": {\"session_id\": \"1\"}},  # Continue in the same session \"1\"\n)\n\n# Third conversation in the same session \"1\"\nquestion_3 = \"Can you explain the concept of AI in one sentence?\"\nresponse_3 = chain.invoke(\n    question_3,\n    config={\"configurable\": {\"session_id\": \"1\"}},  # Continue in session \"1\"\n)\n\n# Print the responses to see the conversation flow\nprint(\"Human:\", question_1)\nprint(\"AI:   \", response_1.content, \"\\n\")\n\nprint(\"Human:\", question_2)\nprint(\"AI:   \", response_2.content, \"\\n\")\n\nprint(\"Human:\", question_3)\nprint(\"AI:   \", response_3.content, \"\\n\")\n4. Conversational Retrieval Chain\n\nA conversational retrieval chain is used to add chat history to a simple retrieval chain. It retrieves documents and maintains conversational context across multiple questions.\n\n# Conversational Retrieval Chain\n\nfrom langchain.chains import create_history_aware_retriever, create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_community.chat_message_histories import ChatMessageHistory\nfrom langchain_core.chat_history import BaseChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_groq import ChatGroq\n\n# Initialize the large language model\nllm = ChatGroq(model=\"mixtral-8x7b-32768\")\n\n### Contextualize question ###\n# Define a system prompt to contextualize the user question by making it standalone\ncontextualize_q_system_prompt = (\n    \"Given a chat history and the latest user question \"\n    \"which might reference context in the chat history, \"\n    \"formulate a standalone question which can be understood \"\n    \"without the chat history. Do NOT answer the question, \"\n    \"just reformulate it if needed and otherwise return it as is.\"\n)\n\n# Create a ChatPromptTemplate for contextualizing questions with chat history\ncontextualize_q_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", contextualize_q_system_prompt),\n        MessagesPlaceholder(\"chat_history\"),      \n        (\"human\", \"{input}\"),                      \n    ]\n)\n\n# Create a history-aware retriever to handle chat history\nhistory_aware_retriever = create_history_aware_retriever(\n    llm, retriever, contextualize_q_prompt\n)\n\n### Answer question ###\n# Define a system prompt to generate concise answers using retrieved context\nsystem_prompt = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer \"\n    \"the question. If you don't know the answer, say that you \"\n    \"don't know. Use three sentences maximum and keep the \"\n    \"answer concise.\"\n    \"\\n\\n\"\n    \"{context}\"  # Placeholder for retrieved context\n)\n\n# Create a ChatPromptTemplate for the question-answering chain\nqa_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),            \n        MessagesPlaceholder(\"chat_history\"),   \n        (\"human\", \"{input}\"),                \n    ]\n)\n\n# Create a document chain for handling question answering using the LLM and the QA prompt\nquestion_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n\n# Combine the history-aware retriever and the QA chain to create the retrieval chain\nretrieval_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n\n### Statefully manage chat history ###\n# Store for maintaining chat history across multiple sessions\nstore = {}\n\n# Function to get or create chat history for a given session\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\n    # If no session history exists for the given session_id, create a new history\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n\n# Create a conversational retrieval chain that manages chat history statefully\nconversational_retrieval_chain = RunnableWithMessageHistory(\n    retrieval_chain,                 \n    get_session_history,           \n    input_messages_key=\"input\",     \n    history_messages_key=\"chat_history\",\n    output_messages_key=\"answer\",\n)\n\n# First conversation: question about the document\nquestion_1 = {\"input\": \"What is the document about?\"}\nresponse_1 = conversational_retrieval_chain.invoke(\n    question_1,\n    config={\n        \"configurable\": {\"session_id\": \"abc123\"}\n    },\n)[\"answer\"]\n\n# Second conversation in the same session: asking to summarize the main points\nquestion_2 = {\"input\": \"Summarize its main points?\"}\nresponse_2 = conversational_retrieval_chain.invoke(\n    question_2,\n    config={\"configurable\": {\"session_id\": \"abc123\"}},\n)[\"answer\"]\n\n# Printing the responses to see the conversational retrieval flow\nprint(\"Human:\", question_1[\"input\"])\nprint(\"AI:   \", response_1, \"\\n\")\n\nprint(\"Human:\", question_2[\"input\"])\nprint(\"AI:   \", response_2, \"\\n\")\nResume Analyzer using RAG\n\nLet’s say you are a recruiter or an HR Manager and you have a pool of CVs or resumes to analyze and see if they have the right skillset according to your job description. Now looking at each and every resume is a time consuming and tedious task. How about uploading a resume and your job description with the required skills and getting a detailed analysis about how much the resume matches a particular job description. Sounds interesting, Right!\n\nWell, this can be easily automated through a RAG-based system where your external knowledge base would be the resume or CV you want to analyze. You can chat with it and ask different questions to see if it meets the requirements or not. This tool can help you quickly segregate matching resumes from the unmatched ones letting you focus on just the useful ones having the required skillset.\n\nLet’s build this application step by step. We’ll be using Streamlit — an open source Python framework for interactive data apps — to develop the frontend and LangChain — a Python framework for LLM-powered applications — to develop the backend. Additionally, we’ll use Groq — a Generative AI solutions platform for fast inference — to connect with LLMs. Here’s a glimpse of what the application would look like.\n\nInstalling necessary libraries\n\nInstall the following python libraries before building the application from the requirements.txt file.\n\nstreamlit\nlangchain_community\nlangchain-huggingface\nlangchain-groq\npypdf\nfaiss-cpu\npython-dotenv\nSetting up the Environment Variables\n\nSet up the Groq API key in a .env file.\nGROQ_API_KEY=\"Your_API_Key\"\n\nBuilding the Frontend\nmain_app.py\n\nThis is the main interface to upload the resume and job description and is displayed at the left side of the application. As soon as both the resume and job description have been uploaded, it will send the PDF file to the backend — Data Ingestion component of the RAG system — to load it, split it into text chunks, generate embeddings of text chunks, and store them into a vector store. After data ingestion, it will show a button to analyze the resume. When the user clicks on that button, the resume will be analyzed through the backend and the response will be displayed in the form of a detailed analysis.\n\nimport streamlit as st\nfrom backend.pdf_ingestion import load_split_pdf\nfrom backend.vector_store import create_vector_store\nfrom backend.analysis import analyze_resume\nimport os\nimport shutil\n\n# Main application including \"Upload Resume\" and \"Resume Analysis\" sections\ndef render_main_app():\n    \n    # Apply custom CSS to adjust the sidebar width\n    st.markdown(\n        \"\"\"\n        <style>\n        [data-testid=\"stSidebar\"] {\n            min-width: 25%;\n            max-width: 25%;\n        }\n        </style>\n        \"\"\",\n        unsafe_allow_html=True\n    )\n\n    # Moving the upload section to the sidebar\n    with st.sidebar:\n        st.header(\"Upload Resume\")  # Header for the upload section\n        \n        # File uploader for PDF resumes\n        resume_file = st.file_uploader(\"Upload Resume (PDF)\", type=\"pdf\")\n\n        # Text area for job description input\n        job_description = st.text_area(\"Enter Job Description\", height=300)\n\n        if resume_file and job_description:  # Check if both inputs are provided\n            # Create a temporary directory if it doesn't exist\n            temp_dir = \"temp\"\n            os.makedirs(temp_dir, exist_ok=True)\n\n            # Save the uploaded file to the temporary directory\n            with open(os.path.join(temp_dir, resume_file.name), \"wb\") as f:\n                f.write(resume_file.getbuffer())\n            \n            # Load and split the PDF file into documents and chunks\n            resume_file_path = os.path.join(\"temp\", resume_file.name)\n            resume_docs, resume_chunks = load_split_pdf(resume_file_path)\n\n            # Create a vector store from the resume chunks\n            vector_store = create_vector_store(resume_chunks)\n            st.session_state.vector_store = vector_store  # Store vector store in session state\n                \n            # Remove the temporary directory and its contents\n            shutil.rmtree(temp_dir)\n\n            # Button to begin resume analysis\n            if st.button(\"Analyze Resume\", help=\"Click to analyze the resume\"):\n                # Combine all document contents into one text string for analysis\n                full_resume = \" \".join([doc.page_content for doc in resume_docs])\n                # Analyze the resume\n                analysis = analyze_resume(full_resume, job_description)\n                # Store analysis in session state\n                st.session_state.analysis = analysis    \n        else:\n            st.info(\"Please upload a resume and enter a job description to begin.\")\n\n    # Display the analysis result if it exists in session state \n    if \"analysis\" in st.session_state:\n        st.header(\"Resume-Job Compatibility Analysis\")\n        st.write(st.session_state.analysis)\n    else:\n        st.header(\"Welcome to the Ultimate Resume Analysis Tool!\")\n        st.subheader(\"Your one-stop solution for resume screening and analysis.\")\n        st.info(\"Do you want to find out the compatibility between a resume and a job description? So what are you waiting for?\")\n\n        todo = [\"Upload a Resume\", \"Enter a Job Description\", \"Click on Analyze Resume\"]\n        st.markdown(\"\\n\".join([f\"##### {i+1}. {item}\" for i, item in enumerate(todo)]))\n\nGitHub - \nmain_app.py\n\nchat_interface.py\n\nThis is the interface to chat with the uploaded resume and is displayed at the right side of the application. It uses the Data Retrieval and Synthesis components of a RAG system to have an interactive conversation with the resume. The user can ask different questions related to the resume and get context augmented response through the RAG system. The conversation between the user and the application is maintained through a conversational retrieval chain.\n\nGitHub - \nchat_interface.py\n\nBuilding the Backend\npdf_ingestion.py\n\nThis is the function to load and split a PDF file using PyPDFLoader to load the file and RecursiveCharacterTextSplitter to split it into chunks. It returns both the documents and text chunks.\n\nGitHub - \npdf_ingestion.py\n\nvector_store.py\n\nThis function uses Hugging Face embeddings to store the text embeddings into a vector store. We are using FAISS (Facebook AI Similarity Search) as the vector store here.\n\nGitHub - \nvector_store.py\n\nanalysis.py\n\nThis function analyzes the resume using the LLM mixtral-8x7b-32768 from Groq API and full resume text as the context. The response is generated through a simple Q/A chain. The prompt template used here is extremely important because it defines the structure of the response in much detail. Here we can use the magic of prompt engineering to generate the response of our choice.\n\nGitHub - \nanalysis.py\n\nMain Application\napp.py\n\nThis is the main entry point to the application. It renders the user interface to interact with the Resume Analyzer.\n\nGitHub - \napp.py\n\nDeployment\n\nThe application we have built is a basic version of a resume analysis tool just to get an understanding of how a RAG application works. It can be deployed on any cloud platform such as Hugging Face or Streamlit where you can easily interact with it. You can experiment with its different features — adding new functionalities or trying to modify the existing ones.\n\nResume Analyzer - Hugging Face\n\nConclusion\n\nRetrieval augmented generation (RAG) is the most widely used technique in NLP applications. It augments latest and domain specific information to LLMs and utilizes their powerful capabilities to generate relevant, coherent, and useful responses. LangChain provides robust tools for building RAG applications. Many industry applications, especially customized chatbots, are built using RAG. Retrieval augmented generation makes it possible to interact with any external knowledge source in your own natural language. Its main components include data ingestion, retrieval, and synthesis. There can be hundreds of applications of RAG that can make our lives easier. A resume analysis tool is one such application that can be really helpful when there are hundreds of CVs to go through and you have very little time. A recruiter can get benefit from this amazing technology making his day to day tasks simple and effective. Even a job applicant can use this tool to see if his resume is the right fit for a particular job description.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nIntroduction\n\nImportance of RAG\n\nHigh Level Components of RAG\n\nBasic RAG Workflow\n\nAdvantages of RAG\n\nApplications of RAG\n\nImplementation of RAG using LangChain\n\nRAG Pipeline\n\nData Ingestion\n\nData Loading\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nYou might be interested\nRAG101\nT\nMar 10, 202521 reads\nchatLangChain+1\nStreamlining Employee Q&A Using Retrieval-Augmented Generation (RAG).\nBest Overall Project\nT\nK\nJ\nOct 29, 2024275 reads\nLangchainLlama+5\nEnterprise level self reflecting Agentic RAG Chatbot\nJ\nOct 31, 202441 reads\nFineTunningGenAI+3\nRAG Turns General AI into Domain Experts by Connecting It to Specific Knowledge\nS\nJ\nA\nJun 16, 202529 reads\nAAIDC2025AI chatbot+9",
    "awards": [
      "distinguished technical deep-dive"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "ringpt-20-ai-agent-powered-doorbell-gcuSB4vQCesp",
    "username": "@bandini",
    "license": "MIT License",
    "title": "RinGPT 2.0 AI Agent Powered Doorbell ",
    "publication_description": "Back to publications\nMar 13, 2025\n●\n49 reads\n●\nMIT License\nRinGPT 2.0 AI Agent Powered Doorbell\nAI Agent\nCall Function\nChatGPT\ndoorbell\nOpenAI\nPython\nRinGPT\nTelegram\n@bandini\nLike\nBookmark\nShare\n\nRinGPT 2: An AI-Powered Smart Doorbell\nBackground\n\nLast year, I modified my home doorbell to be powered by ChatGPT. RinGPT 1.0 was capable of sending notifications via Telegram and engaging visitors in long conversations using an LLM. It could also use pre-programmed responses based on specific keywords.\n\nVersion 2.0\n\nFor version 2.0, I wanted to give it more autonomy using something called AI Agents. But what exactly is an AI Agent?\n\nAn AI agent is an autonomous entity that perceives its environment through sensors, processes information, and takes actions to achieve specific goals. It can be software-based (e.g., a chatbot or recommendation system) or embodied in hardware (e.g., a robot or self-driving car).\n\nThe goal was to provide RinGPT with tools like access to a schedule, sensor readings, and more—allowing the agent to decide which tools to use, execute them, and make decisions based on their results and pre-set reception rules.\n\nThis project is experimental and relatively simple. Despite using AI agents, it does not rely on a specialized agent framework but instead runs on plain Python with OpenAI's API. A key detail that might interest some makers is that it interacts with both software-based and physical tools—such as reading sensors and unlocking doors.\n\nHardware Used\n\nThe project is built around a Unihiker board. For reference, you can think of the Unihiker as a Raspberry Pi with built-in storage, a touchscreen, and onboard sensors such as a microphone, light sensor, accelerometer, and buttons. It comes pre-installed with a Debian-based OS.\n\nSetup Notes\n\nTo connect Unihiker to WiFi, plug in the USB cable and open 10.1.2.3 in a browser. Once connected, you can access it via SSH and FTP using the credentials:\n\nUser: root\nPassword: dfrobot\nComponents:\n** \nUnihiker M10\n **\nIO Extender\nLLed button\n9G Clutch Servo\nBluetooth Speaker\nConnections\n\nConnect the servo to PO del IO Board\nConnect the button to P23 of the Unihiker\n\nLibraries & APIs\n\nThe project uses specific Unihiker libraries (which require no installation). Additional dependencies include:\n\npip install openai speech_recognition edge_tts art asyncio textwrap\nAPIs Required:\nOpenAI API Key for ChatGPT\nTelegram Token for notifications\nBluetooth Setup\n\nTo connect a Bluetooth speaker, first find its MAC address using a scan or by checking the device settings. Then run the following commands:\n\nbluetoothctl\ndefault-agent\npower on\nscan on\ntrust 00:00:00:00:00:00\npair 00:00:00:00:00:00\nconnect 00:00:00:00:00:00\n\n(Replace 00:00:00:00:00:00 with the actual MAC address.)\n\nHow It Works\n\nWhen the button is pressed, RinGPT plays a doorbell sound.\nIt records audio using the Unihiker microphone.\nThe recorded audio is converted into text using Speech Recognition.\nIt identifies the visitor’s name with an OpenAI API call.\nIt determines which tools to use with another OpenAI call.\nIt executes the selected tools, compiles the results, and makes a final decision:\nUnlock the door\nSend a Telegram notification\n\nAvailable Tools\n\nThe AI agent has access to the following tools:\n\ndef agenda(nombre):\ndef getDayTime():\ndef getLightConditions():\n\nAdditional tools, like unlocking the door or sending Telegram notifications, are triggered by the system’s logic but are not yet assigned to the AI agent (though they could be in future versions).\n\nThe agenda currently uses hardcoded event names, but it can easily be modified to pull real data from Google Calendar. The light conditions tool is fairly arbitrary—it was included mainly to make use of the Unihiker’s light sensor.\n\nLLM Usage\n\nNot all OpenAI API calls are the same:\n\nFunction Calling is used to extract the visitor’s name.\nJSON-formatted responses help structure other interactions.\nDifferent API calls use different models and temperature settings.\nThere’s no global memory, but key data (e.g., visitor name) is temporarily stored and reused in later prompts.\n\nFor example, the visitor’s name is retained and sent again during the third LLM call.\n\nLogging & Monitoring\n\nThe system keeps a log of all actions taken, making it easier to monitor performance and debug issues.\n\nDemo\nPossible Improvements\nSome tools are currently triggered by the system’s program flow rather than agent selection. Assigning them to the AI agent would increase flexibility.\nUsing an AI agent framework (e.g., LangChain or Auto-GPT) could reduce code size and improve maintainability.\nUse the new OpenAI agents SDK\nImproving the possible scenarios: what if the name is not provided? what if the visitor does not want to enter? etc\nSource Code\n\nRinGPT 2.0 AI Agent powered Doorbell\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nRinGPT 2: An AI-Powered Smart Doorbell\n\nBackground\n\nVersion 2.0\n\nHardware Used\n\nSetup Notes\n\nComponents:\n\nConnections\n\nLibraries & APIs\n\nAPIs Required:\n\nBluetooth Setup\n\nView all\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "safe-steps-realtime-object-detection-and-multimodal-navigation-assistant-IjHAkbXOrbdQ",
    "username": "aAmina Javaid",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n64 reads\n●\nMIT License\nSafe Steps - Real-Time Object Detection and MultiModal Navigation Assistant\nComputerVision\nFineTuning\nMultiModalAI\nNavigationAssistant\nObjectDetection\nOpenCV\nRealWorldApplication\nStreamlit\nTextToSpeech\nUltralyticsYOLO11\nA\nAmina Javaid\nLike\nBookmark\nShare\n\nAbstract\n\nSafe Steps is a multimodal AI navigation assistant designed for visually impaired individuals. Performing day to day tasks for a blind person is extremely challenging as they are totally dependent on their caretakers to help them navigate through their surroundings and provide them with necessary assistance. This application can improve their quality of life by providing them a platform to take charge of their own movement. It captures real-time video, detects objects, calculates their orientation and distance from the user, and provides audio instructions about upcoming obstacles so that they may navigate smoothly maintaining their safety. This solution can significantly boost the productivity and confidence of the visually impaired individuals.\n\nThe application has been implemented using Streamlit as a frontend. Real time video is captured and processed using the OpenCV library. Object detection is performed using the cutting-edge Ultralyics YOLO11 model pre-trained on COCO dataset and fine-tuned on a custom indoor objects detection dataset from Kaggle. Audio feedback is generated using Python pyttsx3 library for text-to-speech conversion. Together, these technologies create a comprehensive and reliable multimodal navigation assistant.\n\nIntroduction\n\nImagine a system that sees, hears, and understands just like we do. That's the power of Multimodal AI!\n\nWith the latest advancements in artificial intelligence, multimodal AI has emerged as one of the most influential trends in generative AI, getting significant attention across diverse fields. Natural language processing, speech recognition, and computer vision have long been at the core of artificial intelligence, forming the foundational building blocks for modern multimodal applications. Combining vision, language, and other modalities enables machines to better understand and interact with the world, therefore, the blend of these technologies have made it possible to bridge the gap between human intelligence and artificial intelligence.\n\nMultimodal AI systems can integrate and process data from multiple sources and formats such as text, images, audio, and video. Different modalities combined together create a more comprehensive understanding of the information. This is a step towards making human-like perceptions, essential for interacting with the real world. Just as humans use different senses to perceive their environment, these multimodal AI systems use different types of data to get a holistic interpretation of a concept. For example, combining vision and language allows an AI to describe an image or interpret a scene, while integrating audio and video data can enable advanced applications like real-time translation and context-aware navigation.\n\nWhat is MultiModal AI?\n\nModality refers to the different types of data to represent the information, for example, text, images, audio, and video. Multimodal AI is a type of artificial intelligence that can process and integrate these multiple types of data. A unimodal AI system focuses on one type of data whereas multimodal AI systems can perform more complex tasks by combining different modalities. By creating interactions between these modalities, they achieve a more comprehensive understanding of the world. In short, multimodal AI applications can combine multiple types of inputs and generate outputs in various formats.\n\nHumans interact with the world using their five sense organs  -  eyes to see, ears to hear, nose to smell, skin to touch, and tongue to taste. Can you think about machines who can experience all these five senses like humans do? Well, that seems impossible, right? Through advancements in artificial intelligence, machines have begun to approximate these human capabilities. Nowadays computers have in built cameras which act as their eyes. Since the conception of deep neural networks, there has been a rapid advancement in the field of computer vision. Similarly, computers can listen and produce audio sounds through microphones. The speakers act as the ears of these machines. They can also understand the language that we speak. All these different forms of interaction with the computers are possible through different modalities like text, images, and sound. Isn't that interesting!\n\nImportance of MultiModal AI\n\nThe future of artificial intelligence lies in the integration of different modalities, making multimodal AI an essential component in today's world. As AI continues to advance at a rapid pace, the incorporation of various modalities into applications has become unavoidable. People want to make their lives easier. They want to give maximum control to the machines so that they can perform various tasks for them.\n\nIntelligent machines have become a necessity, with applications in many industries including healthcare, education, and entertainment. Multimodal AI systems try to replicate human intelligence, simplifying tasks along with improving decision-making and user experiences.\n\nExamples of MultiModal AI Applications\n\nMultimodal AI opens up countless possibilities across various domains. Here are a few notable examples:\n\nPersonal AI Assistant  -  Imagine you are a busy individual having numerous daily activities. How about a personal assistant powered by multimodal AI who helps manage your day to day tasks efficiently and effectively. By integrating modalities such as text and speech, this assistant can plan your schedule, provide reminders at the right time, and even take instructions through speech.\n\nLanguage Translator  -  If you travel a lot and visit different countries often, language barrier is a common problem which can hinder your conversation and growth. A multimodal language translator can help you interact with anyone in the world in their own language. You speak in your native language, your translation assistant translates your speech into the desired language, producing both text and audio as output. Similarly it translates back responses into your own language. This application combines speech-to-text, translation, and text-to-speech technologies, demonstrating the potential of multimodal AI in bridging global communication gaps.\n\nVisual Navigation Assistant  - Multimodal AI can significantly enhance accessibility for differently-abled individuals. For a visually impaired person, a multimodal navigation assistant can be a great option ensuring their safe indoor and outdoor movement. It can detect objects in real time and provide instructions about their position and distance from the user. This application includes vision, text, and speech modalities improving lives of the visually impaired individuals in many ways.\n\nMedical Diagnostic Assistant - Doctors are very busy people and integrating multimodal capabilities into medical diagnosis can enhance their productivity and diagnostic power. A medical diagnostic assistant can combine medical images like X-rays or MRI scans with patient records or history in text documents to assist doctors in diagnosing diseases.\n\nInteractive Educational Assistant - AI learning assistants can integrate speech-to-text with visual aids in order to enhance a student's learning experience. Such systems could respond to a student's questions and display interactive diagrams for further engagement.\n\nBuilding a Real-World MultiModal AI Application \nSafe Steps\nReal-Time Navigation Assistant for the Visually Impaired\n\nLet's build a real-world multimodal AI application - a real-time navigation assistant for the visually impaired.\n\nHere's what this application would look like.\n\nObjective\n\nThe objective of this project is to improve accessibility for the visually impaired individuals. Navigating daily life poses multiple challenges for the blind, as obstacles can hinder their independence and mobility. This application can significantly improve their accessibility and make them less dependent on others so that they can easily move around their surroundings and perform their daily chores. Additionally, it helps them navigate the world around them with confidence. By breaking the barriers between the visually impaired individuals and the world around them, this project helps improve their performance and efficiency to a great extent.\n\nTechnology Stack\n\nWe'll be using the following technologies to build this application:\n\nStreamlit - for frontend\nOpenCV - for real-time video capturing\nUltralytics YOLO11 - for object detection\npyttsx3 - Python library for text-to-speech conversion\nBasic Application Workflow\n\nThe main components of Safe Steps include:\n\nVideo Capture\nObject Detection\nAudio Feedback\n\nLet's discuss each component of the system in detail:\n\n1. Video Capture\n\nThe video capture component is responsible for initializing the camera, capturing real-time video feeds, and sending the video frames to the object detection component. This component acts as the eyes of the system, continuously monitoring the user's surroundings for potential obstacles or hazards.\n\n2. Object Detection\n\nThe object detection component identifies objects in real-time using two models - YOLO11 model pre-trained on the COCO dataset along with another YOLO model fine-tuned on a custom indoor objects dataset. By combining detections from these models, the system achieves high accuracy in recognizing indoor objects. It calculates the distance of the detected objects from the user, determines their orientation, and sends this information to the audio feedback component. This ensures precise and context-aware detection which is essential for smooth and safe navigation.\n\n3. Audio Feedback\n\nThe audio feedback component provides verbal instructions to the user based on the detected object's distance and orientation. It also displays the navigation instructions on the interface besides the video feed for additional clarity. For instance:\n\nIf an object is very close, the system warns the user to stop immediately.\nIf an object is at a medium distance, the system advises the user to proceed cautiously.\nIf no objects are detected, the system informs the user that the path is clear and they may freely move ahead.\n\nThis component ensures real-time, actionable feedback, enabling the user to navigate safely and confidently.\n\nSystem Workflow\n\nUsers can control the system via the frontend interface, which allows them to start or stop the camera as needed. For optimal functionality, the camera should be positioned to face outward and away from the user. As the user moves through their environment, the system continuously processes the video feed, detects objects, and provides audio feedback. This seamless integration of components ensures that the user receives accurate and timely navigation assistance, significantly enhancing their mobility and independence.\n\nWhat is YOLO?\n\nYOLO (You Look Only Once) is a state-of-the-art real-time object detection algorithm. Unlike traditional object detection algorithms, YOLO processes the entire image in a single pass. YOLO is based on a Convolutional Neural Network architecture. It divides the image into a grid and predicts the bounding boxes and class probabilities for objects within each grid cell simultaneously. This approach makes the YOLO algorithm exceptionally fast and efficient, making it an excellent choice for real-time applications.\n\nDatasets\n\nThis project uses Ultralytics YOLO11 model for object detection which is pre-trained on the COCO dataset, which consists of 80 diverse object categories, ranging from everyday items to fruits and animals. The model is then fine-tuned on a custom dataset to enhance its capability in detecting indoor common objects including the following classes:\n\ndoor\ncabinetDoor\nrefrigeratorDoor\nwindow\nchair\ntable\ncabinet\ncouch\nopenedDoor\npole\n\nThis dual training approach enhances the system's ability to detect objects critical for real-world navigation for visually impaired individuals. By combining the generalized detection capabilities of the pretrained YOLO11 model with the fine-tuned custom YOLO model, the system ensures accurate recognition of indoor objects.\n\nImplementation using Streamlit, OpenCV, YOLO11, and pyttsx3\nEnvironment Setup\n\nCreate a new virtual environment for the project and install all dependencies through the requirements.txt file.\n\nrequirements.txt\n\nstreamlit\nopencv-python\nultralytics\nnumpy\npyttsx3\n\nThe project structure is as follows:\n\nSafeSteps\n|__frontend\n   |__app.py\t\t\t\t\n|__backend\n   |__object_detector.py\n   |__audio_feedback.py\n   |__utils.py\n|__requirements.txt\n|__settings.py\n|__main.py\nBackend\n\nobject_detector.py\n\nThis file contains the ObjectDetector class which handles each and everything related to object detection in the video frames. The camera is initialized through OpenCV video capture functionality. It uses the latest YOLO11 model from Ultralytics. YOLO11 has been pre-trained on the COCO dataset which detects generic objects. To further enhance the capability of this model for real-time indoor objects detection suitable for a navigation system, a custom indoor objects detection dataset has been picked from Kaggle which includes ten object categories. YOLO11 model has been trained or fine-tuned on this custom dataset which took around 22.946 hours for 100 epochs using the following CLI command.\n\nyolo detect train data=indoor_objects_dataset/data.yaml model=yolo11n.pt epochs=100 imgsz=640\n\nThe summary of the custom trained model is as follows:\n\nFor the application to detect all object classes, the ones belonging to the COCO dataset and the ones in the new custom dataset, two models have been used, YOLO11n.pt from the pre-trained YOLO11 models and best.pt from the fine-tuned YOLO11 model. The detections are then combined and bounding boxes are drawn around the detected objects. While detecting objects, their distance is estimated from the user and their orientation is also determined. These detections, which include class name, confidence, distance, and orientation, are then sent to the AudioFeedback class for generating appropriate instructions for the user while navigation.\n\nimport cv2\nimport sys\nfrom ultralytics import YOLO  # Import YOLO for object detection\nfrom settings import FRAME_WIDTH, FRAME_HEIGHT, FPS, YOLO_MODEL_PATH, CUSTOM_MODEL_PATH, CONFIDENCE_THRESHOLD, REAL_HEIGHTS, FOCAL_LENGTH\n\nclass ObjectDetector:\n    \n    def __init__(self):\n        try:\n            # Load the general YOLO model trained on the COCO dataset for general object detection\n            self.model = YOLO(YOLO_MODEL_PATH)  \n            # Load the custom YOLO model trained on the custom dataset (indoor objects) for specific object detection\n            self.custom_model = YOLO(CUSTOM_MODEL_PATH)\n            print(\"YOLO models loaded successfully\")\n        except Exception as e:\n            # If there is an error loading the models, print the error and raise it\n            print(f\"Error loading YOLO models: {str(e)}\")\n            raise\n\n        # Define the list of class names from the COCO dataset for general object detection\n        self.cocoClasses = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \n                            \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \n                            \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n                            \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \n                            \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \n                            \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n                            \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \n                            \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \n                            \"hair drier\", \"toothbrush\"]\n        \n        # Define the list of custom classes for specific objects in the custom dataset\n        self.customClasses = [\"door\", \"cabinetDoor\", \"refrigeratorDoor\", \"window\", \"chair\", \"table\", \"cabinet\", \"couch\", \"openedDoor\", \"pole\"]\n\n        # Set general class names to COCO classes and custom class names to indoor objects class names in the custom dataset\n        self.generalClassNames = self.cocoClasses\n        self.customClassNames = self.customClasses\n\n    # Method to initialize the camera\n    def initialize_camera(self):\n        try:\n            # Open the default camera\n            cap = cv2.VideoCapture(0)\n            \n            # Check if the camera was opened successfully\n            if not cap.isOpened():\n                raise Exception(\"Could not open camera\")\n            \n            # Set the camera's frame width, height, and FPS\n            cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n            cap.set(cv2.CAP_PROP_FPS, FPS)\n            \n            return cap\n        except Exception as e:\n            # If there is an error initializing the camera, print the error and return None\n            print(f\"Error initializing camera: {str(e)}\")\n            return None\n\n    # Method to detect objects and draw bounding boxes on the frame\n    def detect_and_draw(self, frame, confidence_threshold=CONFIDENCE_THRESHOLD):\n        try:\n            # Detect objects using both the general YOLO model and the custom YOLO model\n            general_results = self.model(frame, stream=True)  # Detect objects with the general model\n            custom_results = self.custom_model(frame, stream=True)  # Detect objects with the custom model\n\n            detections = []  # List to store detections with relevant details\n\n            # Loop through the results from both models (general and custom)\n            for results, classNames in zip([general_results, custom_results], [self.generalClassNames, self.customClassNames]):\n                for r in results:\n                    boxes = r.boxes  # Extract bounding box information for each detected object\n                    for box in boxes:\n                        confidence = float(box.conf[0])  # Get the confidence score for the detected object\n                        \n                        # If the confidence score is above the threshold, process the detection\n                        if confidence > confidence_threshold:\n                            # Extract the bounding box coordinates\n                            x1, y1, x2, y2 = box.xyxy[0]\n                            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n                            \n                            # Get the class of the detected object\n                            cls = int(box.cls[0])\n                            class_name = classNames[cls]\n\n                            # Approximate distance calculation based on the object height in the image\n\n                            # Get real height of the detected object from the dictionary\n                            real_height = REAL_HEIGHTS.get(class_name, 1.7)  # Default height is 1.7 meters (for person)\n\n                            height = y2 - y1  # Height of the object in the image\n                            if height > 0:\n                                distance = round((FOCAL_LENGTH * real_height) / height, 2)  # Calculate the distance to the object\n                            else:\n                                distance = float('inf')  # If height is zero, distance is infinite (invalid detection)\n\n                            # Calculate the orientation of the object (left, center, right) based on its position\n                            center_x = (x1 + x2) / 2\n                            frame_width = frame.shape[1]  # Width of the frame (image)\n                            if center_x < frame_width / 3:\n                                orientation = \"left\"  # Object is on the left side of the frame\n                            elif center_x < 2 * frame_width / 3:\n                                orientation = \"center\"  # Object is in the center of the frame\n                            else:\n                                orientation = \"right\"  # Object is on the right side of the frame\n\n                            # Draw the bounding box and label the object with confidence score\n                            label = f\"{class_name} ({confidence:.2f})\"\n                            label_y = max(y1 - 10, 20)  # Position the label slightly above the bounding box\n\n                            # Draw rectangle and label on the frame\n                            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 2)\n                            cv2.putText(frame, label, (x1, label_y), \n                                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n\n                            # Append the detection details to the list of detections\n                            detections.append({\n                                \"class_name\": class_name,\n                                \"confidence\": confidence,\n                                \"distance\": distance,\n                                \"orientation\": orientation\n                            })\n\n            return frame, detections  # Return the processed frame and the list of detections\n\n        except Exception as e:\n            # If there is an error during object detection, print the error and return the frame with no detections\n            print(f\"Error in detect_and_draw: {str(e)}\")\n            return frame, []\n\naudio_feedback.py\n\nThis class handles all the functionality related to the audio instructions maintaining a queue to store the latest instructions for speaking.\n\nimport threading\nfrom queue import Queue\nimport pyttsx3\nimport time\nfrom settings import SPEECH_RATE, SPEECH_VOLUME\n\nclass AudioFeedback:\n    def __init__(self):\n        \"\"\"\n        Initializes the AudioFeedback system. \n        - Creates a message queue to hold audio messages.\n        - Starts a background thread to process messages in the queue.\n        \"\"\"\n        self.message_queue = Queue(maxsize=1)  # Queue holds one message at a time to prevent overlapping audio.\n        self.running = True  # Flag to indicate if the audio feedback system is running.\n        self.thread = threading.Thread(target=self._process_messages)  # Background thread to process audio messages.\n        self.thread.daemon = True  # Ensures thread exits when the main program ends.\n        self.thread.start()  # Start the background thread.\n\n    def _process_messages(self):\n        \"\"\"\n        Processes messages in the queue and provides audio feedback.\n        - Uses pyttsx3 for text-to-speech conversion.\n        - Runs in an infinite loop until `running` is set to False.\n        \"\"\"\n        engine = pyttsx3.init()  # Initialize the text-to-speech engine.\n        engine.setProperty('rate', SPEECH_RATE)  # Set speech rate from settings.\n        engine.setProperty('volume', SPEECH_VOLUME)  # Set volume level from settings.\n\n        while self.running:\n            try:\n                try:\n                    message = self.message_queue.get_nowait()  # Fetch a message from the queue if available.\n                    engine.stop()  # Stop any ongoing speech before starting a new one.\n                    engine.say(message)  # Queue the message for speech synthesis.\n                    engine.runAndWait()  # Process and play the audio message.\n                except Queue.Empty:\n                    time.sleep(0.1)  # If no message is available, pause briefly to reduce CPU usage.\n            except Exception as e:\n                print(f\"Error in audio processing: {str(e)}\")  # Log errors without crashing the program.\n                time.sleep(0.1)  # Pause briefly before retrying.\n\n                # Attempt to reinitialize the audio engine in case of a failure.\n                try:\n                    engine = pyttsx3.init()\n                    engine.setProperty('rate', SPEECH_RATE)  # Fallback to default speech rate.\n                    engine.setProperty('volume', SPEECH_VOLUME)  # Fallback to default volume.\n                except:\n                    pass  # Ignore further errors during engine reinitialization.\n\n    def speak(self, message: str):\n        \"\"\"\n        Adds a new message to the audio queue for speech synthesis.\n        - Removes any existing message in the queue to ensure the latest message is spoken.\n        - If the queue is full, the new message is ignored to avoid blocking.\n        \"\"\"\n        try:\n            while not self.message_queue.empty():  # Clear the queue to remove outdated messages.\n                try:\n                    self.message_queue.get_nowait()\n                except Queue.Empty:\n                    break\n            self.message_queue.put(message, block=False)  # Add the new message to the queue.\n        except Queue.Full:\n            pass  # Ignore the new message if the queue is already full.\n\n    def cleanup(self):\n        \"\"\"\n        Cleans up resources when the system is shutting down.\n        - Stops the background thread and waits for it to terminate.\n        \"\"\"\n        self.running = False  # Set the running flag to False to stop the processing loop.\n        self.thread.join(timeout=1)  # Wait for the thread to finish execution.\n\nutils.py\n\nThis file contains the helper function which generates audio instructions based on the object details. These instructions are spoken as well as displayed beside the video on the application interface.\n\n# Function to generate audio instructions based on detected object details\ndef get_instruction(detection):\n    # Extract relevant information from the detection dictionary\n    distance = detection['distance']  # Distance of the detected object from the user\n    class_name = detection['class_name']  # Name of the detected object class\n    orientation = detection['orientation']  # Orientation of the detected object relative to the user\n\n    # Generate appropriate instruction based on the distance to the object\n    if distance < 0.5:  \n        # Stop if the object is too close\n        return f\"Stop! {class_name} detected very close at {orientation}.\"\n    elif distance < 1.5:  \n        # Warning if the object is within a medium range\n        return f\"Caution! {class_name} detected at the{orientation}, approximately {distance:.1f} meters away.\"\n    elif distance < 3.0:  \n        # Notification if the object is further away but still within detectable range\n        return f\"{class_name} detected at the {orientation}, at about {distance:.1f} meters.\"\n    else:  \n        # No immediate obstacles detected\n        return \"Path is clear, you may move ahead.\"\nFrontend\n\napp.py\n\nThis is the Streamlit frontend which handles video controls. The sidebar contains options for setting the detection threshold value. There is a video display option on the left which shows live video feed along with detected objects. The audio instructions for navigation are spoken as well as displayed besides the video.\n\nimport streamlit as st\nimport cv2\nimport numpy as np\nfrom backend.object_detector import ObjectDetector\nfrom backend.audio_feedback import AudioFeedback\nfrom backend.utils import get_instruction\nimport time\nfrom settings import CONFIDENCE_THRESHOLD\n\n# Set up the layout of the Streamlit app to be wide\nst.set_page_config(layout=\"wide\")\n\n# Main function\ndef main():\n    # Title for the Streamlit app\n    st.title(\"Safe Steps\")\n    st.subheader(\"Real-Time Object Detection and MultiModal Navigation Assistant\")\n\n    # Check if session state variables exist, if not, initialize them\n    if 'running' not in st.session_state:\n        st.session_state.running = False  # Track whether the camera/video is running or not\n    if 'detector' not in st.session_state:\n        st.session_state.detector = ObjectDetector()  # Initialize Object Detector\n    if 'audio_feedback' not in st.session_state:\n        st.session_state.audio_feedback = AudioFeedback()  # Initialize Audio Feedback\n    if 'last_detection_hash' not in st.session_state:\n        st.session_state.last_detection_hash = None  # Keep track of the last detection for reference\n\n    # Initialize camera if not already initialized in session state\n    if 'cap' not in st.session_state:\n        st.session_state.cap = st.session_state.detector.initialize_camera()\n\n    # If camera initialization fails, show an error message and stop execution\n    if st.session_state.cap is None:\n        st.error(\"Failed to initialize camera\")\n        return\n\n    # Sidebar to control settings of the app\n    st.sidebar.title(\"Settings\")\n    confidence_threshold = st.sidebar.slider(\"Detection Confidence Threshold\", \n                                   min_value=0.0, \n                                   max_value=1.0, \n                                   value=CONFIDENCE_THRESHOLD)  # Slider for adjusting detection confidence threshold\n\n    st.sidebar.info(\"Navigate your surroundings with ease using our AI-powered assistant.\")\n    # Application instructions\n    st.sidebar.header(\"Instructions\")\n    st.sidebar.write(\"1. Point the camera toward the scene.\\n2. Follow the audio feedback for navigation.\")\n\n    # Button to toggle the camera on or off\n    if st.button(\"Start/Stop\"):\n        st.session_state.running = not st.session_state.running  # Toggle camera state\n\n    # Create two columns: one for video display, another for displaying instructions\n    col1, col2 = st.columns(2)\n    with col1:\n        st.header(\"Live Video Feed\")\n        video_placeholder = st.empty()  # Placeholder for displaying video\n    with col2:\n        st.markdown(\"<h2 style='text-align:center'>Navigation Instructions</h2>\", unsafe_allow_html=True)\n        instruction_placeholder = st.empty()  # Placeholder for displaying navigation instructions\n\n    try:\n        last_detections = []  # To store previous detections in case of no new detections\n\n        while True:\n            # If the camera/video is running, process the frames to detect objects and generate navigation instructions\n            if st.session_state.running:\n                ret, frame = st.session_state.cap.read()  # Capture a frame from the camera\n                if not ret:\n                    st.error(\"Failed to read frame\")  # If frame capture fails, show an error and break\n                    break\n\n                # Process the frame to detect objects and draw bounding boxes\n                processed_frame, detections = st.session_state.detector.detect_and_draw(\n                    frame, confidence_threshold)\n\n                # If detections are found, store them; otherwise, use the last detections\n                if detections:\n                    last_detections = detections\n                else:\n                    detections = last_detections\n\n                # Display the processed frame with object detections\n                video_placeholder.image(processed_frame, channels=\"BGR\", use_container_width=True)\n\n                # If there are detections, find the closest one and give an instruction\n                if detections:\n                    closest_detection = min(detections, key=lambda x: x['distance'])  # Find the closest detected object\n                    instruction = get_instruction(closest_detection)  # Get navigation instruction based on the closest object\n                    st.session_state.audio_feedback.speak(instruction)  # Provide audio feedback\n                    # Display the instruction beside the video in bold\n                    instruction_placeholder.markdown(f\"<h1 style='font-size:24px; text-align:center'>{instruction}</h1>\", unsafe_allow_html=True)\n            else:\n                # If the camera is stopped, display a blank frame with a message \"Camera Stopped\"\n                blank_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n                cv2.putText(blank_frame, \"Camera Stopped\", (200, 240),\n                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n                video_placeholder.image(blank_frame, channels=\"BGR\", use_container_width=True)\n                time.sleep(0.1)  # Small delay to avoid excessive CPU usage\n\n            # Small delay to control the frame rate\n            time.sleep(0.01)\n\n    except Exception as e:\n        st.error(f\"Error: {str(e)}\")  # Handle any unexpected errors by showing an error message\n\n    finally:\n        # Ensure that the audio feedback system is cleaned up properly when the app stops\n        if 'audio_feedback' in st.session_state:\n            st.session_state.audio_feedback.cleanup()\n\n# Entry point for the application\nif __name__ == \"__main__\":\n    main()\nMain Application\n\nsettings.py\n\nThis file contains all the frontend and backend settings and constants. This is a single point of maintenace which makes the application more reusable and scalable.\n\n# Video settings\nFRAME_WIDTH = 640  # Width of the video frame\nFRAME_HEIGHT = 480  # Height of the video frame\nFPS = 30  # Frames per second (video processing speed)\n\n# YOLO Model paths\nYOLO_MODEL_PATH = \"../backend/yolo11n.pt\"  # Path to the general object detection YOLO11 model\nCUSTOM_MODEL_PATH = \"../backend/best.pt\"  # Path to the custom object detection model trained on a custom dataset\n\n# Detection confidence threshold\nCONFIDENCE_THRESHOLD = 0.5 # Minimum confidence level required for an object detection to be considered valid\nFOCAL_LENGTH = 615.0  # Approximate focal length of a camera for object distance estimation\n\n# Audio Feedback settings\nSPEECH_RATE = 150  # Rate (speed) at which the speech engine speaks (words per minute)\nSPEECH_VOLUME = 1.0  # Volume level of the speech engine (range 0.0 to 1.0)\n\n# Real-world heights in meters for each class. These will be used for calculating distance of the object from the user.\nREAL_HEIGHTS = {\n    # COCO dataset classes\n    \"person\": 1.7,          # Average height of a person\n    \"bicycle\": 1.0,         # Average height of a bicycle\n    \"car\": 1.5,             # Average height of a car\n    \"motorcycle\": 1.2,      # Average height of a motorcycle\n    \"bird\": 0.3,            # Average height of a bird\n    \"cat\": 0.3,             # Average height of a cat\n    \"dog\": 0.6,             # Average height of a dog\n    \"horse\": 1.5,           # Average height of a horse\n    \"sheep\": 1.0,           # Average height of a sheep\n    \"bottle\": 0.3,          # Height of a bottle\n    \"chair\": 1.0,           # Height of a chair\n    \"couch\": 1.0,           # Height of a couch\n    \"potted plant\": 0.5,    # Average height of a potted plant\n    \"tv\": 1.0,              # Height of a TV (approximate)\n    \"cow\": 1.2,             # Average height of a cow\n    \"airplane\": 10.0,       # Approximate height of an airplane (body)\n    \"bed\": 0.5,             # Average height of a bed\n    \"laptop\": 0.05,         # Height of a laptop\n    \"microwave\": 0.5,       # Height of a microwave\n    \"sink\": 0.8,            # Height of a sink\n    \"oven\": 1.0,            # Height of an oven\n    \"toaster\": 0.3,         # Height of a toaster\n    \"bus\": 3.5,             # Average height of a bus\n    \"train\": 3.5,           # Average height of a train\n    \"dining table\": 0.8,    # Height of a dining table\n    \"elephant\": 3.0,        # Height of an elephant\n    \"banana\": 0.2,          # Height of a banana (approximated)\n    \"toilet\": 1.0,          # Height of a toilet\n    \"book\": 0.02,           # Height of a book\n    \"boat\": 1.0,            # Height of a boat\n    \"cell phone\": 0.2,      # Height of a cell phone\n    \"mouse\": 0.05,          # Height of a mouse\n    \"remote\": 0.05,         # Height of a remote\n    \"clock\": 0.2,           # Height of a clock\n    \"apple\": 0.1,           # Height of an apple\n    \"keyboard\": 0.02,       # Height of a keyboard\n    \"backpack\": 0.5,        # Average height of a backpack\n    \"wine glass\": 0.2,      # Height of a wine glass\n    \"zebra\": 1.4,           # Height of a zebra\n    \"scissors\": 0.2,        # Height of scissors\n    \"truck\": 3.0,           # Height of a truck\n    \"traffic light\": 3.0,   # Height of a traffic light\n    \"cup\": 0.1,             # Height of a cup\n    \"hair drier\": 0.3,      # Height of a hair dryer\n    \"umbrella\": 1.0,        # Height of an umbrella\n    \"fire hydrant\": 1.0,    # Height of a fire hydrant\n    \"bowl\": 0.2,            # Height of a bowl\n    \"fork\": 0.2,            # Height of a fork\n    \"knife\": 0.2,           # Height of a knife\n    \"spoon\": 0.2,           # Height of a spoon\n    \"bear\": 1.5,            # Average height of a bear\n    \"refrigerator\": 1.8,    # Height of a refrigerator\n    \"pizza\": 0.02,          # Height of a pizza\n    \"frisbee\": 0.05,        # Height of a frisbee\n    \"teddy bear\": 0.5,      # Height of a teddy bear\n    \"tie\": 0.1,             # Height of a tie\n    \"stop sign\": 1.0,       # Height of a stop sign\n    \"surfboard\": 2.0,       # Height of a surfboard\n    \"sandwich\": 0.05,       # Height of a sandwich\n    \"kite\": 1.0,            # Height of a kite\n    \"orange\": 0.1,          # Height of an orange\n    \"toothbrush\": 0.2,      # Height of a toothbrush\n    \"sports ball\": 0.2,     # Height of a sports ball\n    \"broccoli\": 0.3,        # Height of broccoli\n    \"suitcase\": 0.5,        # Height of a suitcase\n    \"carrot\": 0.3,          # Height of a carrot\n    \"parking meter\": 1.0,   # Height of a parking meter\n    \"handbag\": 0.3,         # Height of a handbag\n    \"hot dog\": 0.1,         # Height of a hot dog\n    \"donut\": 0.05,          # Height of a donut\n    \"vase\": 0.3,            # Height of a vase\n    \"baseball bat\": 0.8,    # Height of a baseball bat\n    \"baseball glove\": 0.2,  # Height of a baseball glove\n    \"giraffe\": 5.0,         # Height of a giraffe\n    \"skis\": 1.8,            # Height of skis\n    \"snowboard\": 1.5,       # Height of a snowboard\n    \"skateboard\": 0.2,      # Height of a skateboard\n    \"tennis racket\": 0.7,   # Height of a tennis racket\n    \"cake\": 0.2,            # Height of a cake\n    \"bench\": 1.0,           # Height of a bench\n    # Custom dataset classes (chair, table, and couch have already been covered in COCO dataset classes)\n    \"door\": 2.0,             # Height of a door\n    \"cabinetDoor\": 2.0,      # Height of a cabinet door\n    \"refrigeratorDoor\": 2.0, # Height of a refrigerator door\n    \"window\": 1.5,           # Height of a window\n    \"cabinet\": 1.5,          # Height of a cabinet\n    \"openedDoor\": 2.0,       # Height of an opened door\n    \"pole\": 3.0,             # Height of a pole (e.g., street light)\n}\n\nmain.py\n\nThis is the main entry point of the application. It launches the Streamlit frontend through the CLI (command-line interface) from the application root directory.\n\n# Import necessary modules for running the Streamlit app\nimport streamlit.web.cli as stcli  # Provides the Streamlit command-line interface to run apps\nimport sys  # Provides access to system-specific parameters and functions\n\n# Entry point of the application\nif __name__ == '__main__':\n    # Modify the system arguments to imitate the Streamlit CLI command for running the app\n    sys.argv = [\"streamlit\", \"run\", \"frontend/app.py\"]  \n    # Launch the Streamlit app by calling the CLI's main method\n    stcli.main()\n\nRun the following command to start the application in your browser.\n\npython main.py\nConclusion\n\nMultimodal AI is a groundbreaking technology which has crossed the traditional barriers in human computer interaction. It has made it possible for the machines to develop human like intelligence. Multimodal AI is the future of artificial intelligence as it focuses on surpassing human intelligence beyond our imagination. It has paved the way for creating more accessible, creative, and intelligent systems that can easily integrate into our daily lives making them easier and comfortable.\n\nWe have developed a real-time object detection and multimodal navigation assistant for visually impaired individuals using Streamlit, Ultralytics YOLO11, pyttsx3, and OpenCV. This application captures real-time video during navigation, detects obstacles, and provides audio instructions to help visually impaired users navigate their environment independently. By combining these technologies, this application sets a benchmark for building robust and scalable multimodal AI navigation systems designed to enhance accessibility and improve quality of life.\n\nChallenges Faced\n\nDuring the development and implementation of this project, several challenges were encountered, each presenting unique hurdles that required creative problem-solving. Here are the details of the challenges faced and the solutions used to address them:\n\n1. Detection of Objects with Completely Wrong Categories\nChallenge: During the initial implementation phase, objects were being detected with entirely incorrect labels. This issue arose due to the incorrect placement of class names in the list within the ObjectDetector class. While the model performed well on test images, it failed to detect custom object categories during real-time execution. \nSolution: The problem was traced to the incorrect ordering of class names in the list, which mismatched the class indices defined in the data.yaml file during training. To resolve this, the class names were reordered to align with their respective indices, ensuring accurate label assignment to the bounding boxes.\n2. Overlapping Object Categories in the Custom Trained Model\nChallenge: The fine-tuned model assigned incorrect labels to custom dataset objects because the class indices (0–9) overlapped with those of the pre-trained YOLO11 model (0–79 from the COCO dataset). This resulted in conflicts during detection.\nSolution: To overcome this challenge, one option was to retrain the model with updated class indices starting from 80 (0–79 already assigned to COCO dataset classes). This didn't seem practical due to limited time constraints. The fine-tuned model had already taken more than 22 hours to train on the lightest variant of YOLO11 model. The other option was to use a dual model approach by using both the pre-trained model and the fine-tuned model to detect objects and then combining these detections to perform further processing. The dual model approach has been applied in the application which led to the correct detection of object categories in the video.\n3. Audio Feedback Synchronization\nChallenge: Synchronizing audio feedback with real-time object detection posed another challenge, with delays causing irrelevant instructions to be provided to the users. \nSolution: The message queue was optimized to prioritize the most relevant messages and drop outdated or irrelevant ones. This adjustment ensured timely and accurate audio feedback, significantly improving the user experience.\n4. Object Distance Calculation\nChallenge: Accurately estimating object distance from the user proved to be a difficult challenge. Depth estimation models from Hugging Face were too large and computationally intensive, slowing down the video stream and affecting application performance.\nSolution: A simple approach based on focal length and real object height has been implemented to calculate approximate distances. While not as precise as advanced depth estimation models, this method is efficient and suitable for real-time applications.\n5. Deployment on Hugging Face or Other Cloud Platforms\nChallenge: Deploying the application on Hugging Face posed significant challenges, as its built-in mechanisms did not support initializing the camera for real-time video feeds. Uploading a pre-recorded video for demonstration purpose could be a work around but that doesn't serve the purpose of the application.\nSolution: Deployment strategies still need to be figured out. Platforms that support real-time video processing such as AWS or Google Cloud might be considered to ensure that the application meets its intended purpose.\n6. Latency issues with FastAPI Backend\nChallenge: Initially, FastAPI was used for the backend to handle object detection and audio feedback. However, this approach introduced latency issues due to the constant back-and-forth communication between the frontend and backend, negatively impacting real-time performance.\nSolution: To eliminate the latency issue, FastAPI was replaced with a fully integrated Streamlit application. Backend classes like ObjectDetector and AudioFeedback were directly incorporated into the Streamlit application, improving real-time responsiveness.\n7. Frontend Integration \nChallenge: Designing a user-friendly interface that integrates video capture, object detection, and audio feedback was a complex task.\nSolution: Streamlit was used to develop an intuitive frontend that allows the users to start and stop the camera, view detection results and receive audio instructions simultaneously. This interface beautifully integrates the vision, text, and speech modalities.\n\nIterative problem-solving is extremely important in the development of AI-powered applications. By addressing the above challenges, this application proves to be a reliable, real-time navigation assistant for the visually impaired individuals. However, there is always a room for improvement and considering future enhancements can help converting this application into a more robust solution. \n\nFuture Work\n\nSafe Steps is a basic multimodal application which can be installed on any local system. To improve its efficiency and effectiveness, following enhancements can be introduced in the future:\n\nDeployment on a suitable cloud platform.\nImproving custom YOLO11 model performance.\nIncorporating more datasets such as outdoor objects detection dataset.\nMore accurate distance calculation.\nEmbedding the system into mobiles or wearable devices for ease of use.\nReferences\nUltralytics YOLO11\nReal-time Object Detection with YOLO and Webcam: Enhancing Your Computer Vision Skills\nText-to-Speech with Python: A Guide to Using pyttsx3\nIndoor Objects Detection Dataset - Kaggle\nOpenCV - Getting Started with Videos\nCOCO Dataset\nMicrosoft COCO: Common Objects in Context\nSafe Steps Demo\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "safeschool-realtime-weapon-detection-using-yolov8-with-langchainbased-threat-analysis-agent-XLE3KDaNRIPq",
    "username": "n@narasimhakarthik2",
    "license": "MIT License",
    "title": "SafeSchool: Real-time Weapon Detection using YOLOv8 with LangChain-based Threat Analysis Agent",
    "publication_description": "Back to publications\nDec 29, 2024\n●\n127 reads\n●\nMIT License\nWinner of\nDistinguished Social Impact Innovation\nat the\nComputer Vision Projects Expo 2024\nSafeSchool: Real-time Weapon Detection using YOLOv8 with LangChain-based Threat Analysis Agent\nAgentic system\nautonomus\nComputer Vision\nGPT-4 Vision\nlangchain\nLLM\nObject detection\nschool\nYolo\nN\n@narasimhakarthik2\nLike\nBookmark\nShare\nAbstract\n\nGun violence in U.S. schools continues to be a critical safety concern, exposing the limitations of traditional surveillance systems that rely on human monitoring. This research presents SafeSchool, an AI-powered security system that combines YOLO-based object detection with a Langchain-based monitoring agent, replacing human surveillance with continuous automated threat assessment. The system processes security camera feeds in real-time, detecting weapons with high confidence and triggering an AI agent for comprehensive scene analysis. This approach achieves initial weapon detection in under 50ms and delivers detailed threat analysis within 6 seconds, including weapon identification, suspect description, and location-specific security protocols. The system was developed and validated using a custom dataset of 5,149 annotated frames, collected across three strategic camera positions in a controlled university environment. Results demonstrate substantial improvements in threat detection speed and response time compared to traditional surveillance methods, while maintaining a 99% detection rate and minimizing false positives.\n\nIntroduction\nThe Challenge of School Security\n\nGun violence in U.S. schools represents an ongoing critical safety concern that demands innovative technological solutions. Current security infrastructure, while extensive, is limited by its dependence on human monitoring of surveillance systems. Security personnel tasked with monitoring multiple video feeds face inevitable attention fatigue, leading to potential oversights in threat detection and delayed response times during critical situations.\n\nLimitations of Current Systems\n\nTraditional school surveillance systems face several fundamental challenges:\n\nDependency on human attention span for continuous monitoring\nDelayed threat verification due to manual assessment processes\nInconsistent threat evaluation based on individual interpretation\nExtended response times due to multi-step alert protocols\nLimited scalability constrained by available security personnel\nThe SafeSchool Solution\n\nThis research introduces SafeSchool, implementing an AI-driven dual-detection approach. The system's first layer employs YOLO (You Only Look Once) for continuous weapon detection across surveillance feeds. Upon detection, a Langchain-based AI agent activates to perform comprehensive scene analysis, effectively replacing human monitoring with consistent, automated threat assessment.\nThis automated approach provides several key advantages:\n\nContinuous, fatigue-free monitoring of security feeds\nStandardized threat assessment protocols\nRapid multi-frame analysis for threat confirmation\nLocation-aware security response recommendations\nAutomated alert generation and distribution\n\nThe integration of an AI monitoring agent significantly reduces the critical time between initial detection and response initiation while maintaining high accuracy standards. Most importantly, the system demonstrates effectiveness in early threat detection and assessment, potentially preventing escalation of security incidents in school environments.\n\nhttps://github.com/narasimhakarthik2/SafeSchool\n\nRelated work\n\nReal-time weapon detection in surveillance systems remains a significant challenge in security applications. Salazar González et al. (2020) conducted foundational research on gun detection in CCTV systems, establishing the complexity of achieving reliable real-time detection under varying conditions. Their work highlighted key limitations in processing speed and accuracy that continue to influence current research.\n\nRecent developments in deep learning, particularly YOLO architectures, have improved detection capabilities but still face challenges in threat assessment and response time. Traditional security systems rely heavily on human monitoring for threat verification, leading to potential delays and oversights due to operator fatigue.\n\nThe integration of Large Language Models (LLMs) with vision systems represents a novel approach to automated surveillance. While existing systems focus primarily on object detection, this research extends the state-of-the-art by combining YOLO-based weapon detection with LLM-powered scene analysis, addressing key limitations identified in previous studies.\n\nMethodology\n\nThis section details the system architecture and implementation of SafeSchool. The system consists of three main components: weapon detection using YOLO, scene analysis using a Langchain-based AI agent, and a location-aware alert system.\n\nSystem Architecture\nWeapon Detection Module\nImplementation of YOLOv8 for real-time weapon detection\nConfidence threshold set at 75% to minimize false positives\nDetection categories: Handgun, Short rifle, Knife\nProcessing speed: <50ms per frame\nAI Monitoring Agent\nLangchain-based implementation using GPT-4-Vision\nThree-stage analysis protocol:\n\nInitial threat assessment upon weapon detection\nConfirmatory analysis at 2-second intervals\nFinal threat evaluation with location-specific recommendations\nScene analysis completion within 6 seconds\nLocation-Aware Alert System\nIntegration with camera mapping system\nAutomated security response generation based on threat location\nExperiments\nDataset\n\nThis study utilizes the publicly available mock attack dataset by Salazar González et al. (2020), which simulates realistic threat scenarios in a university environment. The dataset comprises 5,149 annotated frames from three surveillance cameras covering distinct environmental conditions:\n\nCamera 1 (Corridor): 607 frames of uniform lighting with common obstacles such as doors and bins, recorded at 2 FPS over 5 minutes.\nCamera 7 (Corridor): 3,511 frames with additional environmental challenges including wall-mounted objects and fire extinguishers, captured at 2 FPS across 29 minutes.\nCamera 5 (Entrance): 1,031 frames featuring irregular lighting conditions and challenging surface materials like black carpeting, recorded at 2 FPS over 8 minutes.\nTraining Configuration\n\nThe model was trained using YOLOv8n architecture with the following specifications:\nThe network was configured to detect three classes: Handgun, Knife, and Short_rifle. Training parameters included a batch size of 8, input resolution of 640x640 pixels, and Adam optimizer with an initial learning rate of 0.001 and weight decay of 0.0005. Real-time data augmentation was employed during training.\n\nTraining Results\n\nPerformance metrics tracked through Weights & Biases demonstrate strong detection capabilities across all weapon classes. The F1-confidence curves show optimal performance at a confidence threshold of 0.75, with handguns achieving the highest F1 score of 0.8. The precision-recall curves indicate robust model performance, particularly for handgun detection, maintaining precision above 0.9 across a wide range of recall values.\n\nThe recall-confidence analysis reveals effective detection sensitivity, with handguns and short rifles maintaining recall rates above 0.7 at the operational confidence threshold. Knife detection showed comparatively lower recall rates, likely due to their smaller visual signature in surveillance footage.\n\nPerformance Analysis\n\nAt the operational confidence threshold of 0.75:\n\nHandgun detection achieved precision of 0.95 and recall of 0.85\nShort rifle detection maintained precision of 0.92 and recall of 0.82\nKnife detection showed precision of 0.88 with recall of 0.65\n\nThese results demonstrate the model's effectiveness for real-world weapon detection applications, particularly for firearm identification in surveillance scenarios.\n\n\t\n\n\t\n\n\n\t\nResults\nWeapon Detection Performance\n\nYOLOv8n model demonstrated robust detection capabilities:\n\nHandgun: 0.95 precision, 0.85 recall at 0.75 confidence threshold\nShort_rifle: 0.92 precision, 0.82 recall\nKnife: 0.88 precision, 0.65 recall\n\nAverage inference time: <50ms per frame\n\nLLM-based Threat Monitoring\n\nThe Langchain-powered AI agent provides continuous threat assessment through a three-stage analysis:\n\nInitial Assessment\nTriggered immediately upon weapon detection\nStructured analysis of weapon type, suspect description, and risk level\nResponse generated in <2 seconds\n\nThreat Confirmation\nMultiple frame analysis at 2-second intervals\nConversationBufferMemory maintains analysis history for context\nContinuous threat level assessment based on scene changes\n\nFinal Assessment\nGenerated after analyzing multiple frames\nRisk level determined from historical analyses stored in memory\nLocation-aware security recommendations based on camera mapping\n\nThe integrated system demonstrated significant advantages:\nContinuous, automated monitoring without fatigue\nStructured threat assessment with context retention\nAverage response time of 6.2 seconds from detection to final assessment\nLocation-specific security protocols based on camera positioning\nConclusion\n\nThis research presents SafeSchool, an automated security system that successfully integrates YOLO-based weapon detection with LLM-powered threat analysis. The system addresses the critical limitations of traditional surveillance by replacing human monitoring with an AI agent capable of continuous, fatigue-free threat assessment.\n\nThe YOLOv8n model achieved high detection accuracy across weapon classes, with precision rates exceeding 0.90 for firearms at a 0.75 confidence threshold.\n\nThe integration of a Langchain-based AI agent enables comprehensive threat analysis through a three-stage assessment process, storing contextual information in memory for improved decision-making.\n\nKey system achievements include:\n\nSub-50ms weapon detection speed\n6.2-second response time from detection to final assessment\nLocation-aware threat analysis and response protocols\nContinuous monitoring capability without human fatigue\nReferences\n\nSalazar González, J. L., Zaccaro, C., Álvarez-García, J. A., Soria-Morillo, L. M., & Sancho Caparrini, F. (2020). Real-time gun detection in CCTV: An open problem. Neural Networks, 132, 297-308. \nhttps://doi.org/10.1016/j.neunet.2020.09.013\n\nLim, J., et al. (2021). Deep multi-level feature pyramids: Application for non-canonical firearm detection in video surveillance. Engineering applications of artificial intelligence 97, 104094.\n\nJocher, G., et al. (2023). ultralytics/ultralytics: v8.0.0 First Release (v8.0.0). Zenodo. \nhttps://doi.org/10.5281/zenodo.7747343\n\nChase, H., et al. (2023). LangChain: Building applications with LLMs through composability. GitHub repository. \nhttps://github.com/hwchase17/langchain\n\nOpenAI (2023). GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nThe Challenge of School Security\n\nLimitations of Current Systems\n\nThe SafeSchool Solution\n\nRelated work\n\nMethodology\n\nSystem Architecture\n\nWeapon Detection Module\n\nAI Monitoring Agent\n\nView all\nCode\nDatasets",
    "awards": [
      "distinguished social impact innovation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "speechflow-modular-real-time-audio-transcription-and-conversational-ai-NhJNttFF1mv1",
    "username": "mMarcel Fenerich",
    "license": "MIT License",
    "title": "SpeechFlow: Modular real-time audio transcription and conversational AI.",
    "publication_description": "Back to publications\nFeb 26, 2025\n●\n50 reads\n●\nMIT License\nSpeechFlow: Modular real-time audio transcription and conversational AI.\nAI\nAudio Transcription\nchatbot\nConversational AI\nLLM\nOpenAI Whisper\nSpeechFlow\nTerminal UI\nTextual\nthinkhub\nM\nMarcel Fenerich\nLike\nBookmark\nShare\nAbstract\n\nSpeechFlow represents a breakthrough in real-time speech processing technology, offering a modular, extensible platform that seamlessly converts spoken language into interactive digital conversations. By leveraging cutting-edge transcription services (OpenAI's Whisper-1 and Google Speech-to-Text) alongside advanced conversational AI models (GPT-4/3.5 and Claude), the system delivers responsive, context-aware interactions. SpeechFlow's architecture prioritizes configurability, allowing users to switch between services via simple environment variables, while its strategic design pattern implementation enables developers to extend functionality without disrupting core components. This paper explores SpeechFlow's innovative approach to audio capture, transcription, and conversational processing, highlighting both its current capabilities and potential for expansion into multimodal analysis.\n\n\nSpeechFlow ecosystem: audio waveforms being captured, transformed into text, and then into conversational responses\n\nMethodology\nSystem Architecture and Design Principles\n\nSpeechFlow's development followed strict adherence to software design principles that emphasize modularity, extensibility, and real-time performance. The architecture employs the Strategy Pattern to enable runtime selection of different service implementations without modifying core code. This approach creates a flexible framework where components can be developed, tested, and deployed independently.\n\nThe codebase organization reflects this modular philosophy:\n\nspeechflow/\n├── core/                              # Core utilities and constants\n│   ├── constants.py                   # Shared constants (e.g., sample rate)\n│   ├── audio_handler.py               # Audio capture and processing\n│   └── interface.py                   # UI components\n├── app.py                             # Main application entry point\n├── .env                               # Environment variables (ignored by Git)\n├── .env.example                       # Example environment variables\n├── README.md                          # Project documentation\n├── poetry.lock                        # Poetry lock file\n├── pyproject.toml                     # Poetry project configuration\n└── tests/                             # Unit tests\n\n\n\nArchitectural diagram showing SpeechFlow's component structure\n\nReal-Time Audio Capture and Processing\n\nA critical technical challenge in building SpeechFlow was achieving low-latency audio capture while maintaining processing quality. The system implements:\n\nStream-Based Processing: Rather than processing complete audio files, SpeechFlow captures and processes audio in configurable chunks (typically 1024 samples).\n\nBuffer Management: The audio handler maintains efficient buffer management to prevent memory overflow while ensuring continuous audio streaming.\n\nSample Rate Optimization: Audio is captured at a standardized 16kHz sample rate with 16-bit depth, optimized for speech recognition algorithms.\n\nPyAudio Integration: The system leverages PyAudio for cross-platform audio capture capabilities, ensuring consistent performance across operating systems.\n\n\nAudio processing\n\nTranscription Service Integration\n\nSpeechFlow's transcription capabilities are implemented through a service-agnostic interface that currently supports two leading providers:\n\nOpenAI Whisper-1:\n\nImplementation leverages OpenAI's REST API\nConfigured for streaming transcription optimized for conversational speech\nHandles multilingual input with automatic language detection\n\nGoogle Speech-to-Text:\n\nIntegration with Google Cloud's streaming recognition services\nSupports specialized vocabulary through speech adaptation\nProvides interim results during transcription process\n\nThe service selection mechanism uses environment variables to determine which implementation to instantiate at runtime:\n\n# Simplified service selection pseudocode\ntranscription_service = os.getenv(\"TRANSCRIPTION_SERVICE\", \"openai\")\n\nif transcription_service == \"openai\":\n    service = OpenAITranscriptionService(api_key=os.getenv(\"OPENAI_API_KEY\"))\nelif transcription_service == \"google\":\n    service = GoogleTranscriptionService(\n        credentials_path=os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n    )\n\n\nComparison diagram showing the parallel implementation of OpenAI and Google transcription services with their respective API flows\n\nConversational AI Processing\n\nOnce audio is transcribed to text, SpeechFlow routes the content to conversational AI services. The implementation:\n\nMaintains Session Context: The system preserves conversation history to enable contextual understanding across multiple exchanges.\n\nService Abstraction: An interface-based approach allows seamless switching between OpenAI (GPT-4/3.5) and Anthropic (Claude) services.\n\nAsynchronous Processing: Conversation handling leverages asynchronous programming to prevent UI blocking during API calls.\n\nResponse Filtering: The system implements lightweight post-processing to format AI responses for optimal display in the terminal interface.\n\n\nSequence diagram illustrating the conversation flow from transcribed text through AI processing to user display\n\nInteractive Terminal Interface\n\nRather than developing a conventional web application, SpeechFlow implements a feature-rich terminal-based UI using Textual:\n\nComponent-Based Design: The interface consists of modular components for recording controls, transcription display, and conversation history.\n\nEvent-Driven Interaction: User input triggers event handlers that coordinate between UI and backend processing.\n\nColor-Coded Visual Feedback: The interface uses color differentiation to distinguish between system messages, transcriptions, and AI responses.\n\nKeyboard Navigation: A focused command set enables efficient control through keyboard shortcuts.\n\n\nScreenshot of the terminal interface showing the recording controls, transcription area\n\nResults\nPerformance Evaluation\n\nSpeechFlow was evaluated across several dimensions to assess its effectiveness as a real-time speech processing system.\n\nQualitative Benefits\n\nBeyond quantitative metrics, SpeechFlow demonstrates several qualitative advantages:\n\nConfigurability: The ability to switch services through environment variables rather than code changes enables rapid experimentation and adaptation to different operational needs.\n\nDeveloper Experience: The modular architecture significantly reduces the learning curve for contributors, with clear separation of concerns and well-defined interfaces facilitating targeted improvements.\n\nCross-Platform Compatibility: Testing confirmed consistent performance across Windows, macOS, and Linux environments, with minimal platform-specific adjustments required.\n\nTerminal Efficiency: The Textual-based interface proved more resource-efficient than equivalent web-based implementations while maintaining visual sophistication and responsive interaction.\n\nUse Case Applications\n\nSpeechFlow has demonstrated successful application in several domains:\n\nAccessibility: The system provides real-time transcription for individuals with hearing impairments, with the additional benefit of conversational AI to summarize or clarify complex discussions.\n\nLanguage Learning: The combination of accurate transcription and conversational responses creates an interactive language practice environment.\n\nVoice-First Development: The platform serves as a testbed for developers building voice-activated applications, offering a controlled environment for prototyping voice interactions.\n\nFuture Directions\n\nWhile SpeechFlow currently focuses on audio processing, the architecture has been designed with multimodal expansion in mind:\n\nImage Processing Integration: Preliminary work has begun on incorporating OpenAI's DALL-E and GPT-4V capabilities for image analysis and generation.\n\nCustom Model Support: The modular design allows for future integration of locally-hosted models, reducing API dependencies.\n\nExpanded Service Options: Additional transcription and conversation services are being evaluated for integration, including open-source alternatives.\n\nEnhanced Analytics: Work is underway to incorporate analysis of conversation patterns, sentiment, and topic extraction.\n\nConclusion\n\nSpeechFlow represents a significant advancement in real-time speech processing technology, combining modular architecture with cutting-edge AI services to create a flexible, powerful platform for speech-to-text and conversational applications. Its strong foundation in software design principles ensures adaptability as AI technologies evolve, while its current implementation already delivers practical benefits across multiple use cases. The system's open-source nature and inviting architecture position it as both a valuable tool for end-users and an extensible platform for developers exploring the frontiers of human-computer interaction through natural language.\n\nFor more infos follow to \nhttps://feneri.ch\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "stop-searching-start-asking-langgraph-docs-at-your-fingertips-H2rbE1JDof4y",
    "username": "Manuel Quesada",
    "license": "MIT License",
    "title": "Stop Searching, Start Asking: LangGraph Documentation at Your Fingertips 🤖🚀",
    "publication_description": "Back to publications\nJun 13, 2025\n●\n206 reads\n●\nMIT License\nStop Searching, Start Asking: LangGraph Documentation at Your Fingertips 🤖🚀\nAAIDC2025\nCUSTOM-WORKFLOW\nLANGGRAPH\nRAG-SYSTEM\nManuel Quesada\nL\nPranav Tiwari\nU\nUtkarsh Dubey\nLike\nBookmark\nShare\nStop Searching, Start Asking: LangGraph Documentation at Your Fingertips 🤖🚀\n\nRetrieval-Augmented Generation (RAG) is becoming a popular alternative to fine-tuning large language models (LLMs). Instead of retraining the entire model whenever new data comes in—a process that can be expensive and slow, RAG lets you keep your models up-to-date by simply updating the data source they rely on. Its main advantages are:\n\nCost-effective: No need to retrain the LLM. Instead, it retrieves the most relevant information from an external knowledge base.\nEasily updatable: If the knowledge changes, you just update the data store—no need to touch the model.\nFlexible and scalable: You can tailor the knowledge base per user, per domain, or per task without duplicating model infrastructure.\nTransparent: You always know where the information came from, enabling better debugging and control.\nGrounded in reality: By combining LLM reasoning with real data, RAG systems are more factual and reliable.\n\nThese advantages make RAG a perfect fit for building intelligent systems that need to stay current and operate over specific corpora—like documentation, internal knowledge, or dynamic content. In this MVP, we present a custom RAG pipeline that uses the LangGraph documentation as the core knowledge base and builds the entire reasoning and retrieval workflow using LangGraph itself.\n\nToolset 🛠️\n\nOur MVP leverages a modular RAG system where each component has a clear role:\n\nLangGraph\n is used not just as the subject of the knowledge base, but also as the framework for defining the workflow of the assistant.\n\nLangChain\n provides the foundation for LLM calls and tool integration.\n\nQdrant\n is used as the vector database to store the embeddings generated from the chunked documents and retrieve them when needed.\n\nLangSmith\n monitors, traces, and evaluates every step of the reasoning and retrieval process.\n\nThe LLM provider is \nOpenAI\n, configured for a deterministic behavior (temperature = 0.1) to avoid random variations in the model’s output during execution, making it suitable for technical documentation answering.\n\nFor the implementation of the RAG system, we built three components:\n\nPrompts: An entry package that contains all the prompts used to generate the responses of the LLM. The prompts are stored in a YAML file and can be easily modified as needed, following the guidelines provided in the \nReady Tensor\n.\n\nVector Store: A component in charge of ingesting the LangGraph documentation and storing it in a vector database using Qdrant.\n\nRAG-pipeline: A component in charge of creating a custom RAG pipeline.\n\nThis stack allows us to construct a chatbot that can deeply understand and navigate LangGraph documentation, answer highly technical questions, and even explain multi-agent workflows by leveraging the source content directly. As result, you don't need to be an expert in LangGraph to use it.\n\nArchitecture Overview 🏗️\n\nLet's break down the architecture of the system going through each detail and decision made.\n\nPrompt Component 🔗\n\nWe have built a Python class to handle prompts in a centralized and modular way. The class takes into account the key elements that a prompt should include: [role, goal, instruction, context, output_constraints, output_format, examples]. It also supports the three main reasoning strategies: chain-of-thought, ReAct, and self-ask. Additionally, it allows prompts to receive input variables that can be dynamically replaced while maintaining compatibility with LangChain’s \nPromptTemplate\n abstraction. As a result, creating a new prompt is as simple as writing a YAML file with the desired structure and using the class of this component to load it, either as a string or directly as a LangChain PromptTemplate object.\n\nFor our chatbot, we created two prompts:\n\nA System Prompt that acts as a gatekeeper to control access to the LangGraph knowledge base. Its purpose is to filter user questions and determine whether they fall within the chatbot’s scope — in this case, answering questions about LangGraph. If the question is out of scope, the chatbot politely informs the user that it cannot assist with that request.\n\nThe Chatbot Prompt, which acts as an agent responsible for deciding how to respond to user questions. It has access to the retriever tool, allowing it to search the knowledge base and retrieve relevant information to answer the user’s query. This prompt follows the ReAct reasoning strategy to ensure that responses are coherent and aligned with LangGraph’s knowledge. The model is guided to reason step-by-step based on the information currently available, determining whether more information needs to be retrieved or if an answer can be generated with the existing data. Furthermore, this system prompt encourages the model to generate smaller, more precise sub-questions to retrieve the most relevant information.\n\nVector Store Component 🔗\n\nFor the vector store component, we propose an efficient approach for handling processes including documentation fetching, optimized chunking of the loaded documents, embedding generation and storing in the vector database. This component is in charge of an important step of any RAG system: ingesting and indexing the data into the vector store. The involved functionalities and their significance are described as follows:\n\nDocumentation loading\n\nWe have defined functions to load the documentation from the official \nLangGraph documentation\n by cloning the repository and filtering the relevant technical documents during the loading process. In addition, the loaded document ipynb files are converted to markdown string using only the markdown and code cells without output cells for optimized and use case specific retrieval. Once downloaded, the documents are stored in a target path for quick loading without the need to clone the repository, thereby enabling the user to choose downloading only if needed.\n\nData analysis\n\nThe downloaded data includes 184 documents. A detailed statistical analysis of the document lengths (in number of characters) served as the foundation for designing an efficient chunking strategy, aligned with both the corpus characteristics and the downstream requirements for processing, indexing, and retrieval.\n\nThe key statistics of the original dataset are summarized as follows:\n\nTotal number of documents: 184\nMinimum of document length: 264\nAverage of document length: 10370\nMaximum of document length: 88210\nQ1 (25th percentile): 3306\nValues below Q1: 46\nQ2 (50th percentile): 7471\nValues below Q2: 92\nQ3 (75th percentile): 13031\nValues below Q3: 138\n\nAs illustrated in the previous figure, the distribution shows a mean document length of approximately 10,370 characters and a median (Q2) of 7,471 characters, indicating that most documents fall below 10,000 characters. The first quartile (Q1) is positioned at 3,306 characters, while the third quartile (Q3) reaches 13,031 characters. Despite this concentration around mid-sized documents, the dataset contains a few significant outliers: several documents exceed 40,000 characters, with the largest reaching 88,210 characters.\n\nThis statistical overview informed the following design decisions:\n\nChunk size: A chunk size of 5,000 characters was selected, representing approximately half of the average document length. This offers a balanced partitioning strategy for both shorter and longer documents.\n\nLLM optimization: A chunk size of 5,000 characters roughly corresponds to ~1,000 tokens after processing. Considering that models like gpt-4o-mini support a context window of 128K tokens, this chunk size offers enough context for each retrieved chunk while keeping the total token usage well within the model’s capacity. This balance ensures effective grounding for generation while maintaining cost-efficient inference.\n\nHandling long documents and overlap configuration: Larger documents are segmented into multiple uniform chunks with an overlap of 800 characters between consecutive fragments. This approach reduces variability in chunk lengths, maintains consistency across the dataset, and preserves contextual continuity between chunks, thereby enhancing semantic search and retrieval performance.\n\nFlexibility: Both the chunk size and the chunk overlap are fully configurable parameters, allowing adjustments based on system objectives, model capabilities, or operational constraints. These values can be easily modified through the YAML configuration files before the document ingestion.\n\nWith the above considerations in place, the splitting process was implemented using LangChain’s RecursiveTextSplitter, which prioritizes natural breakpoints such as code cells, paragraphs and sentences to improve the internal coherence of the generated chunks. As a result, the original set of 184 documents was transformed into 581 chunks, forming the dataset used for the indexing phase.\n\nEmbedding Model\n\nFor generating embeddings, we use OpenAI’s text-embedding-3-large model, accessed via the native OpenAI integration with LangChain. This choice was driven by the ease of integration, strong support from the LangChain ecosystem, and the high-quality semantic representations the model produces.\n\nIndexing Chunks into the Vector Database\n\nWe use Qdrant as the vector store to store the generated embeddings from the processed documentation chunks. Qdrant was selected for its excellent scalability and flexibility—it can be deployed locally, in Docker containers for reproducible environments, or fully managed in the cloud for production-scale systems. This makes it ideal for both small-scale prototypes and large-scale RAG applications.\n\nDuring the indexing process, we store not only the embedded content but also the source path of each document in the metadata. This ensures full transparency, allowing users to trace each retrieved piece of information back to the original documentation when needed.\n\nRAG-pipeline Component 🔗\n\nRAG pipeline stands at the core of facilitating the workflow of the entire chatbot session concerning the technical documentation. It utilizes tool logic to efficiently enable the query receiving, embedding transformation, semantic matching in the vector database, and finally retrieving the response in natural language while keeping the context of the conversation for enriched fetching.\n\nRAG-pipeline as custom workflow\n\nWe have implemented a custom RAG pipeline using LangGraph to define the assistant’s workflow. Its graph-based approach enables the design of flexible and scalable pipelines that can be easily adapted to project requirements, while fully leveraging LangGraph’s extensive ecosystem and integration capabilities. Built as an extension of LangChain, LangGraph is specifically designed to support robust and stateful multi-actor applications with LLMs by modeling the workflow as a graph of nodes and edges.\n\nThe RAG system we have built begins with a Scope Guard that filters user questions to determine whether they fall within the chatbot’s scope, which in this case is answering questions related to LangGraph. Since our knowledge base is fully populated with LangGraph documentation, it would not make sense for the chatbot to handle queries outside this domain. If a question falls outside the defined scope, the chatbot politely informs the user that it cannot assist with the request. For this task, the system prompt provides the model with context about what LangGraph is and its purpose, allowing it to recognize relevant questions even if the term “LangGraph” is not explicitly mentioned.\n\nIn the second stage, if the model determines that the question is related to LangGraph, a ReAct-based agent takes over. This agent has access to a retriever tool that searches the knowledge base to retrieve the most relevant information and generate a coherent response aligned with LangGraph’s documentation. At each step, the model analyzes the information it has retrieved and decides whether additional information is needed or if it can answer the question based on the current data. If further information is required, the model generates sub-questions to query the vector database. It is important to note that the model can generate one or more sub-queries and, after receiving results from the vector store, may decide to continue querying if the information remains insufficient. This iterative cycle is managed through an observer pattern that tracks the number of retrievals made for a given user query. Once a predefined limit is reached, the model is forced to generate a final response based on the information collected up to that point.\n\nVector store as retrieval tool\n\nWe leverage LangChain’s native integration with Qdrant to create a vector store that enables efficient retrieval of information from the knowledge base, allowing the model to generate coherent responses aligned with LangGraph’s documentation. One of the key advantages is the simplicity of exposing the vector store as a retrieval tool: it only requires using the similarity_search function, which automatically converts the user’s query into embeddings and retrieves the most relevant chunks from the vector database.\n\nThe retrieval tool is designed to provide source references for each response, allowing users to easily identify which sections of the documentation were consulted to generate the answer. This not only enables users to quickly access the original content for further details if needed, but also increases trust by offering full transparency into the information sources.\n\nAn additional design choice is the limit on the number of retrieved chunks, which we have set to 3 per user query. This decision is driven by the decomposition strategy established in the prompt, where the model breaks down complex queries into multiple sub-queries. Empirical testing has shown that this number of retrieved chunks provides the model with sufficient grounded information to generate accurate and focused responses, while still encouraging iterative refinement when necessary.\n\nThe State of the workflow and the Memory\n\nEach user query is stored in the graph’s global state, which includes the message history, the list of references used to generate the response, and the number of retrieval queries the model has performed for that specific question. The latter two are reset every time the user submits a new question. Additionally, after generating a response, any messages related to tool calls are removed from the state to keep it clean.\n\nTo control how much message history the model has access to, we apply a sliding window approach to trim older messages. In our implementation, the model only retains access to the last 5 user interactions, meaning the last 5 questions along with their corresponding answers. This is a simple yet widely adopted strategy in the community due to its ease of implementation and efficiency.\n\nFor global state management, we leverage LangGraph’s native support for checkpoints, which allows for different levels of persistence: from basic checkpoints that store message history in local memory, to more advanced solutions that enable persistent storage. We recommend the latter and provide an example of how to implement it using \nPostgreSQL checkpoints\n.\n\nMonitoring and Evaluation with LangSmith 📊\n\nEvery node in the graph is tracked and evaluated using LangSmith. It is a powerful tool for monitoring and evaluating the performance of the LLM system and its integration is immediate as LangSmith is a part of the LangChain ecosystem. This includes:\n\nLLM inputs/outputs\nRetrieved document chunks\nDecomposition and tool usage traces\nError tracking and latency metrics\n\nLangSmith enables automated evaluations, version comparisons, and debugging, ensuring the RAG system is continually improving.\n\nFinal Product and How to Use It 🎯\n\nYou can use our LangGraph-powered assistant directly through a Jupyter Notebook interface. We provide an interactive notebook named test_end_to_end.ipynb that allows you to interact with the assistant just like with any chatbot you're familiar with.\n\nThe following instructions explain how to set up the environment and start using the assistant:\n\n1. Clone the Repository\n\ngit clone https://github.com/MAQuesada/langgraph_documentation_RAG\ncd langgraph_documentation_RAG\n\n2. Install Dependencies with Poetry : If Poetry is not installed on your system, you can install it with:\n\ncurl -sSL https://install.python-poetry.org | python3 -\n\nOnce Poetry is installed, run the following commands to install project dependencies and activate the virtual environment:\n\npoetry install\npoetry self add poetry-plugin-shell # only if you haven't installed it before\npoetry shell \n\n3. Set Up Environment Variables: Create your .env file by referring to the example.env file:\n\nYou are ready to run the test_end_to_end.ipynb notebook. It will include:\n\nClone the official LangGraph documentation repository.\nLoad and split the documentation into optimized chunks.\nGenerate embeddings using OpenAI’s text-embedding-3-large model.\nStore the embedded chunks into Qdrant for later retrieval.\nBuild the RAG-pipeline instance.\nCreate an interactive cell for chat.\nExample Interactions 💬\n\nIn this section, we present several screenshots illustrating different use cases of the assistant.\nNote that when the sources section appears in the answer, it indicates that the assistant has performed a call to the retrieval tool. These examples provide a high-level overview of how the assistant behaves across various types of queries.\n\nCase 1: When the user asks general questions, the assistant responds with a polite answer.\n\nCase 2: In this example, the user asks a question that requires the assistant to perform multiple queries on different topics to retrieve relevant information and provide a concise answer regarding the differences between a workflow and an agent according to LangGraph. In the second screenshot, we can see a snippet from the LangSmith trace showing how, upon receiving the user’s question, the ReAct agent decided to make three tool calls to gather the necessary information.\n\n\n\n\nCase 3: An example of a complex question that includes code snippets, which the assistant is able to handle successfully.\n\nObservations and Limitations 📉\nStrengths\nCan handle multiple users keeping their own state and conversation history.\nModular architecture supports scaling to other domains beyond LangGraph docs.\nOpen-source tooling ensures transparency and extensibility.\nLimitations\n\nReliance on vector search: if chunking isn’t optimal, relevant info might be missed.\n\nNo ranking model: retrieval ranking is purely based on embedding similarity.\n\nLack of automated evaluation system: currently, there is no integrated automated evaluation pipeline to continuously assess and improve the system's performance.\n\nConcluding Remarks\n\nThis project demonstrates the capabilities of LangGraph as both a framework for workflow orchestration and a domain-specific knowledge source for building specialized RAG agents. The assistant delivers reliable, traceable, and explainable responses grounded in technical documentation, enabling the creation of customizable and scalable support agents across diverse domains. Continued advancements in retrieval strategies, ranking mechanisms, and automated evaluation will further strengthen the system’s accuracy, resilience, and adaptability.\n\nFuture Enhancements\nIncorporate document summarization before indexing to improve retrieval relevance.\nAdd document-based reranking with LLM or cross-encoder.\nAdd a more advanced chunking strategy to improve retrieval relevance, for example, using a \nsemantic chunking strategy\n.\nContact & Contribution 📬\n\nThis project is open for contributions, feedback, and collaboration.\n\n🐙 GitHub: \nlanggraph_documentation_RAG\n📧 Email:[ \nmalejandroquesada@gmail.com\n, \ntiwari.pranav1999@gmail.com\n, \nutkarsh251096@gmail.com\n]\n\nFeel free to reach out or fork the project!\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nStop Searching, Start Asking: LangGraph Documentation at Your Fingertips 🤖🚀\n\nToolset 🛠️\n\nArchitecture Overview 🏗️\n\nPrompt Component 🔗\n\nVector Store Component 🔗\n\nDocumentation loading\n\nData analysis\n\nEmbedding Model\n\nIndexing Chunks into the Vector Database\n\nRAG-pipeline Component 🔗\n\nView all\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "streamlining-employee-qa-using-retrievalaugmented-generation-rag-m2VlZTBnZVgT",
    "username": "t@tanveersingh182764",
    "license": "MIT License",
    "title": "Streamlining Employee Q&A Using Retrieval-Augmented Generation (RAG).",
    "publication_description": "Back to publications\nOct 29, 2024\n●\n275 reads\n●\nMIT License\nWinner of\nBest Overall Project\nat the\nNLP Projects Expo 2024\nStreamlining Employee Q&A Using Retrieval-Augmented Generation (RAG).\nLangchain\nLlama\nLLM\nNLP\nnomic\nOpen source\nRAG\nT\n@tanveersingh182764\nK\n@kritdoshi\nJ\n@jagrut90\nLike\nBookmark\nShare\nProject\n\nWe have developed a Q&A bot for our new employees by leveraging various Retrieval-Augmented Generation (RAG) techniques, using the Llama 3.1 70B model. This project showcases the different results obtained after training the RAG system on 150+ company manuals and onboarding documents. Implementing an interactive chatbot for new joiners streamlined HDFC Bank's onboarding process by providing 24*7 instant support, reducing HR dependency, and ensuring consistent responses to repetitive queries. This automation enables HR to focus on strategic tasks, enhancing overall efficiency for both central HR teams and new joiners. This publication aims to present our findings and demonstrate the effectiveness of RAG in enhancing the employee onboarding experience.\n\nIntroduction\n\nFor a large-scale bank of HDFC's stature, the new employees crave for immediate support for smoother integration in their formative months with the Bank. Hence, having a readily available assistance would foster confidence, speed up acclimation, and enhance the overall experience during the initial 6 months, laying a strong foundation for long-term engagement.\n\nBackground\n\nHDFC Bank, one of India's largest and most prominent private sector banks, was incorporated in August 1994 as a part of the Housing Development Finance Corporation (HDFC) Group. The bank was established with a vision to offer a wide range of banking and financial services to individuals and businesses, and over time, it has grown into a leader in the Indian banking sector.\n\nAs of 31st Mar'24, the Bank employs 2.2 lakh individuals across 9k+ locations. Roughly, 5,000 new employees join the Bank every month - naturally have multiple doubts related to the Bank's policies/ processes / onboarding process. 5 central onboarding teams typically handle these queries. New employees often reach out to their respective HR Business Partners (HRBPs) for assistance, and the HRBPs, receive around 1k queries/day across the Bank, for which their TAT hovers anywhere between an hour to 2 working days depending on the backlog created. This entire process introduces significant human dependency, leading to delays and inefficiencies.\n\nMoreover, the repetitive nature of the questions often creates a bottleneck in the onboarding process, with HRBPs handling multiple similar queries instead of focusing on higher-value tasks. The manual nature of addressing these concerns not only slows down communication but also increases the potential for human error. The volume of queries typically overwhelm the centralized teams, further extending response times and leading to dissatisfaction among new joiners.\n\nTo streamline this process, there was a dire need for an automation solution, such as an AI-powered chatbot or self-service knowledge platform, which would provide immediate and consistent responses. Implementing such systems would significantly enhance the onboarding experience, improve response times, and ensure that new employees can access the information they need quickly and accurately.\n\nRef:- \nHDFC Bank\nApproach\n\nThe 10 step high level process for building the requisite solution could be summarized as below -\n\n 1. Data Aggregation: Collection of diverse data across formats and topics, including 150+ process manuals, documents, HTML files, pdfs and spreadsheets, to ensure a broad information base\n 2. Domain-Specific Evaluation: Utilizing domain experts to manually evaluate and enrich data, enhancing relevance and accuracy for end-users\n 3. Development of the RAG Lifecycle: Development of a comprehensive framework for RAG to guide the chatbot’s response generation process\n 4. Initial RAG Implementation: Deploying a basic RAG model to serve as a foundation for information retrieval and response accuracy\n 5. Adaptive RAG Development: Refining the RAG model to adjust dynamically based on query complexity and context\n 6. Raptor Integration: Enhancing the solution with Raptor for increased speed and efficiency in query handling\n 7. Self-RAG Advancement: Implementing a self-learning RAG model to improve response quality through adaptive feedback\n 8. Graph-RAG Application: Integrating Graph-RAG to leverage graph structures for more contextualized information retrieval\n 9. Human Evaluation: Conducting thorough assessments with central HR teams to validate chatbot responses and ensure quality standards\n 10. Feedback Loop: Looping back insights/feedback into the system for continuous improvement and relevancy.\nArchitecture\n\nA high level process flow is depicted in the image below\n\nThe code snippets and flowcharts for RAG implementations provided in this publication are illustrative examples sourced from publicly available references and repositories. They are intended for educational purposes to aid understanding of the concepts discussed. The actual code developed for this project is proprietary to HDFC Bank Ltd and is not disclosed publicly.\n\nRAG Techniques with Code Implementations\nSimple RAG\n\nWe started with a basic implementation of a RAG system tailored for processing and querying PDF and word documents. The system encoded the content of the documents into a vector store, enabling efficient retrieval of relevant information based on user queries.\n\nKey Features\n\nModular Design: The encoding process was wrapped in a single function for easy reuse and integration into larger systems\nConfigurable Chunking: Allowed adjustment of chunk size and overlap to fine-tune the balance between retrieval accuracy and performance\nEfficient Retrieval: Employed Chroma\nEvaluation Capability: Included an evaluate_rag function to assess the system's performance, enabling iterative improvements.\nUsage Example\nThe code provides a test query—\"How can I apply for a privilege leave?\"—to demonstrate how the retriever fetched relevant context from the processed documents. This example showcases the system's ability to deliver precise information based on the encoded content.\n\nImg Credit: BentoML\n\ndef encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Encodes a PDF book into a vector store using nomic embeddings.\n\n    Args:\n        path: The path to the PDF file.\n        chunk_size: The desired size of each text chunk.\n        chunk_overlap: The amount of overlap between consecutive chunks.\n\n    Returns:\n        A Chroma vector store containing the encoded book content.\n    \"\"\"\n\n    # Load PDF documents\n    loader = PyPDFLoader(path)\n    documents = loader.load()\n\n    # Split documents into chunks\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n    )\n    texts = text_splitter.split_documents(documents)\n    cleaned_texts = replace_t_with_space(texts)\n\n    # Create embeddings and vector store\n    embeddings = nomic\n    vectorstore = chroma\n    return vectorstore\n\n\n\nchunks_vector_store = encode_pdf(path, chunk_size=1000, chunk_overlap=200)\nchunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 2})\n\ntest_query = \"How can I apply for a privilege leave?\"\ncontext = retrieve_context_per_question(test_query, chunks_query_retriever)\nshow_context(context)\n\nCode Example: NirDiamont/Rag_techniques\n\nImpact:\n\nThis simple RAG system provided a solid foundation for building more complex information retrieval and question-answering systems. By encoding document content into a searchable vector store, we tried to enable efficient retrieval of relevant information in response to queries. This approach is particularly useful for applications requiring quick access to specific information within large documents or document collections. However, given its limitation in terms of context understanding, self-learning and accuracy in high-diversity data, this standalone architecture was inadequate to deploy.\n\nAdaptive Retrieval-Augmented Generation (RAG) System\n\nNext we explored an advanced RAG approach that adapted its retrieval strategy based on the type of query. By leveraging Language Models (LLMs) at various stages, it provided more accurate, relevant, and context-aware responses to user queries.\n\nKey Components\n\nQuery Classifier: Determines the type of query (Factual, Analytical, Opinion, or Contextual).\nAdaptive Retrieval Strategies: Four distinct strategies tailored to different query types were tried:\n\nFactual Strategy:- Beneficial for Direct Information Queries\nExample:- New joiners often have straightforward questions requiring specific answers, such as\n\"What is the company's dress code?\" or \"How do I access my employee portal?\"\n\nAnalytical Strategy:-Beneficial for Complex Processes and Deep Understanding\nExample:- New employees may need to understand intricate processes or systems, like \"How does the annual performance review process work?\" or \"Explain the steps involved in project initiation.\"\n\nOpinion Strategy-Beneficial for Cultural Insights and Diverse Perspectives\nExample: New joiners might be curious about company culture or employee sentiments, asking questions like \"What do employees think about the remote work policy?\" or \"How is the work-life balance here?\"\n\nContextual Strategy-Beneficial for Personalized Guidance\nExample: Queries that depend on the individual's role or previous interactions, such as \"What training should I prioritize as a marketing associate?\" or \"Who should I contact for IT support?\"\n\nLLM Integration for Enhanced Retrieval and Ranking\n\nBy integrating Language Models (LLMs) throughout the retrieval process, the chatbot for new joiners was able to better understand the nuances and intent behind their questions. This enhanced the way information was fetched and prioritized, allowing the chatbot to refine search queries and rank the most relevant documents higher. As a result, new employees can receive more accurate and helpful information that is tailored to their specific needs during the onboarding process.\n\nLlama Model for Contextualized Response Generation\n\nWe further used the Llama Model to generate responses from retrieved documents, for more accurate, detailed answers enriched with Bank's information.\n\nImage Credit: NirDiamont/Rag_techniques\n\n\nclass AdaptiveRAG:\n    def __init__(self, texts: List[str]):\n        adaptive_retriever = AdaptiveRetriever(texts)\n        self.retriever = PydanticAdaptiveRetriever(adaptive_retriever=adaptive_retriever)\n        self.llm = Llama\n        \n        # Create a custom prompt\n        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n        {context}\n\n        Question: {question}\n        Answer:\"\"\"\n        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n        \n        # Create the LLM chain\n        self.llm_chain = prompt | self.llm\n\n\n    def answer(self, query: str) -> str:\n        docs = self.retriever.get_relevant_documents(query)\n        input_data = {\"context\": \"\\n\".join([doc.page_content for doc in docs]), \"question\": query}\n        return self.llm_chain.invoke(input_data)\n\nCode Example: NirDiamont/Rag_techniques\n\nImpact :\n\nThe adaptive RAG system was superior to the simple RAG system because it provided a more nuanced, accurate, and user-specific experience. It adapted to the nature of each query, ensuring that responses were relevant and comprehensive, and enhanced overall user satisfaction. For a chatbot assisting new joiners, these improvements translated into a smoother onboarding process, better understanding of company practices, and a more engaging interaction, all of which were less effectively achieved with a simple RAG approach.\n\nRAPTOR: Recursive Abstractive Processing and Thematic Organization for Retrieval\n\nAs speed and precision were critical for real-time responses we integrated RAPTOR, an advanced system to enhance information retrieval and question-answering capabilities. Here's how each component of RAPTOR enhanced the chatbot's effectiveness:\n\nKey Components\n\nHierarchical Summarization (Tree Building)-Beneficial for Navigating Extensive Onboarding Materials\nUse Case: New joiners often need to understand both high-level overviews and specific details about the Bank, such as organizational structure, policies, procedures, and culture.\n\nEmbedding and Clustering-Beneficial for Organizing and Retrieving Relevant Information\nUse Case: When a new joiner asks a question, the chatbot needs to search through vast amounts of Bank documents and resources to find the most relevant information.\n\nVector store - Beneficial for Fast and Scalable Information Access\nUse Case: New employees expect prompt and accurate responses from the chatbot, even when dealing with a large repository of documents.\n\nContextual Retriever-Beneficial for Tailored and Precise Information Delivery\nUse Case: The chatbot must provide answers that are not only accurate but also directly relevant to the specific context of the new joiner's query.\n\nAnswer Generation Using Language Models-Beneficial for Creating Clear and Coherent Explanations\nUse Case: New joiners may have questions that require synthesizing information from multiple sources to provide a comprehensive answer.\n\nImage Credit: langchain.ca\n\ndef build_raptor_tree(texts: List[str], max_levels: int = 3) -> Dict[int, pd.DataFrame]:\n    \"\"\"Build the RAPTOR tree structure with level metadata and parent-child relationships.\"\"\"\n    results = {}\n    current_texts = [extract_text(text) for text in texts]\n    current_metadata = [{\"level\": 0, \"origin\": \"original\", \"parent_id\": None} for _ in texts]\n    \n    for level in range(1, max_levels + 1):\n        logging.info(f\"Processing level {level}\")\n        \n        embeddings = embed_texts(current_texts)\n        n_clusters = min(10, len(current_texts) // 2)\n        cluster_labels = perform_clustering(np.array(embeddings), n_clusters)\n        \n        df = pd.DataFrame({\n            'text': current_texts,\n            'embedding': embeddings,\n            'cluster': cluster_labels,\n            'metadata': current_metadata\n        })\n        \n        results[level-1] = df\n        \n        summaries = []\n        new_metadata = []\n        for cluster in df['cluster'].unique():\n            cluster_docs = df[df['cluster'] == cluster]\n            cluster_texts = cluster_docs['text'].tolist()\n            cluster_metadata = cluster_docs['metadata'].tolist()\n            summary = summarize_texts(cluster_texts)\n            summaries.append(summary)\n            new_metadata.append({\n                \"level\": level,\n                \"origin\": f\"summary_of_cluster_{cluster}_level_{level-1}\",\n                \"child_ids\": [meta.get('id') for meta in cluster_metadata],\n                \"id\": f\"summary_{level}_{cluster}\"\n            })\n        \n        current_texts = summaries\n        current_metadata = new_metadata\n        \n        if len(current_texts) <= 1:\n            results[level] = pd.DataFrame({\n                'text': current_texts,\n                'embedding': embed_texts(current_texts),\n                'cluster': [0],\n                'metadata': current_metadata\n            })\n            logging.info(f\"Stopping at level {level} as we have only one summary\")\n            break\n    \n    return results\ndef build_vectorstore(tree_results: Dict[int, pd.DataFrame]) -> Chroma:\n    \"\"\"Build a chroma vectorstore from all texts in the RAPTOR tree.\"\"\"\n    all_texts = []\n    all_embeddings = []\n    all_metadatas = []\n    \n    for level, df in tree_results.items():\n        all_texts.extend([str(text) for text in df['text'].tolist()])\n        all_embeddings.extend([embedding.tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in df['embedding'].tolist()])\n        all_metadatas.extend(df['metadata'].tolist())\n    \n    logging.info(f\"Building vectorstore with {len(all_texts)} texts\")\n    \n    # Create Document objects manually to ensure correct types\n    documents = [Document(page_content=str(text), metadata=metadata) \n                 for text, metadata in zip(all_texts, all_metadatas)]\n    \n    return chroma.from_documents(documents, embeddings)\ndef tree_traversal_retrieval(query: str, vectorstore: chroma, k: int = 3) -> List[Document]:\n    \"\"\"Perform tree traversal retrieval.\"\"\"\n    query_embedding = embeddings.embed_query(query)\n    \n    def retrieve_level(level: int, parent_ids: List[str] = None) -> List[Document]:\n        if parent_ids:\n            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n                query_embedding,\n                k=k,\n                filter=lambda meta: meta['level'] == level and meta['id'] in parent_ids\n            )\n        else:\n            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n                query_embedding,\n                k=k,\n                filter=lambda meta: meta['level'] == level\n            )\n        \n        if not docs or level == 0:\n            return docs\n        \n        child_ids = [doc.metadata.get('child_ids', []) for doc, _ in docs]\n        child_ids = [item for sublist in child_ids for item in sublist]  # Flatten the list\n        \n        child_docs = retrieve_level(level - 1, child_ids)\n        return docs + child_docs\n    \n    max_level = max(doc.metadata['level'] for doc in vectorstore.docstore.values())\n    return retrieve_level(max_level)\ndef create_retriever(vectorstore: Chroma) -> ContextualCompressionRetriever:\n    \"\"\"Create a retriever with contextual compression.\"\"\"\n    logging.info(\"Creating contextual compression retriever\")\n    base_retriever = vectorstore.as_retriever()\n    \n    prompt = ChatPromptTemplate.from_template(\n        \"Given the following context and question, extract only the relevant information for answering the question:\\n\\n\"\n        \"Context: {context}\\n\"\n        \"Question: {question}\\n\\n\"\n        \"Relevant Information:\"\n    )\n    \n    extractor = LLMChainExtractor.from_llm(llm, prompt=prompt)\n    \n    return ContextualCompressionRetriever(\n        base_compressor=extractor,\n        base_retriever=base_retriever\n    )\ndef hierarchical_retrieval(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> List[Document]:\n    \"\"\"Perform hierarchical retrieval starting from the highest level, handling potential None values.\"\"\"\n    all_retrieved_docs = []\n    \n    for level in range(max_level, -1, -1):\n        # Retrieve documents from the current level\n        level_docs = retriever.get_relevant_documents(\n            query,\n            filter=lambda meta: meta['level'] == level\n        )\n        all_retrieved_docs.extend(level_docs)\n        \n        # If we've found documents, retrieve their children from the next level down\n        if level_docs and level > 0:\n            child_ids = [doc.metadata.get('child_ids', []) for doc in level_docs]\n            child_ids = [item for sublist in child_ids for item in sublist if item is not None]  # Flatten and filter None\n            \n            if child_ids:  # Only modify query if there are valid child IDs\n                child_query = f\" AND id:({' OR '.join(str(id) for id in child_ids)})\"\n                query += child_query\n    \n    return all_retrieved_docs\n\n\ndef raptor_query(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> Dict[str, Any]:\n    \"\"\"Process a query using the RAPTOR system with hierarchical retrieval.\"\"\"\n    logging.info(f\"Processing query: {query}\")\n    \n    relevant_docs = hierarchical_retrieval(query, retriever, max_level)\n    \n    doc_details = []\n    for i, doc in enumerate(relevant_docs, 1):\n        doc_details.append({\n            \"index\": i,\n            \"content\": doc.page_content,\n            \"metadata\": doc.metadata,\n            \"level\": doc.metadata.get('level', 'Unknown'),\n            \"similarity_score\": doc.metadata.get('score', 'N/A')\n        })\n    \n    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n    \n    prompt = ChatPromptTemplate.from_template(\n        \"Given the following context, please answer the question:\\n\\n\"\n        \"Context: {context}\\n\\n\"\n        \"Question: {question}\\n\\n\"\n        \"Answer:\"\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    answer = chain.run(context=context, question=query)\n    \n    logging.info(\"Query processing completed\")\n    \n    result = {\n        \"query\": query,\n        \"retrieved_documents\": doc_details,\n        \"num_docs_retrieved\": len(relevant_docs),\n        \"context_used\": context,\n        \"answer\": answer,\n        \"model_used\": llm.model_name,\n    }\n    \n    return result\n\nCode Example: NirDiamont/Rag_techniques\n\nImpact\n\nRAPTOR surpassed adaptive RAG and simple RAG by introducing hierarchical summarization to efficiently manage large document collections through a multi-level tree of summaries. This structure enabled the system to provide both high-level overviews and detailed information. By combining this approach with embedding-based retrieval and contextual answer generation, RAPTOR delivered more accurate and relevant responses.\n\nSelf-RAG: A Dynamic Approach to Retrieval-Augmented Generation\n\nTo further integrate self-learning, Self-RAG was deployed to combine the power of retrieval-based and generation-based approaches in natural language processing. It dynamically decides whether to use retrieved information and how to best utilize it in generating responses, aiming to produce more accurate, relevant, and useful outputs.\n\nKey Components\n\nRetrieval Decision: To determine if retrieval was necessary for a given query.\nDocument Retrieval: To fetch potentially relevant documents from a vector store.\nRelevance Evaluation: To assess the relevance of retrieved documents to the query.\nResponse Generation: To generate responses based on relevant contexts.\nSupport Assessment: To evaluate how well the generated response is supported by the context.\nUtility Evaluation: To rate the usefulness of the generated response.\n\nImage Credit: blog.langchain.dev\n\ndef self_rag(query, vectorstore, top_k=3):\n    print(f\"\\nProcessing query: {query}\")\n    \n    # Step 1: Determine if retrieval is necessary\n    print(\"Step 1: Determining if retrieval is necessary...\")\n    input_data = {\"query\": query}\n    retrieval_decision = retrieval_chain.invoke(input_data).response.strip().lower()\n    print(f\"Retrieval decision: {retrieval_decision}\")\n    \n    if retrieval_decision == 'yes':\n        # Step 2: Retrieve relevant documents\n        print(\"Step 2: Retrieving relevant documents...\")\n        docs = vectorstore.similarity_search(query, k=top_k)\n        contexts = [doc.page_content for doc in docs]\n        print(f\"Retrieved {len(contexts)} documents\")\n        \n        # Step 3: Evaluate relevance of retrieved documents\n        print(\"Step 3: Evaluating relevance of retrieved documents...\")\n        relevant_contexts = []\n        for i, context in enumerate(contexts):\n            input_data = {\"query\": query, \"context\": context}\n            relevance = relevance_chain.invoke(input_data).response.strip().lower()\n            print(f\"Document {i+1} relevance: {relevance}\")\n            if relevance == 'relevant':\n                relevant_contexts.append(context)\n        \n        print(f\"Number of relevant contexts: {len(relevant_contexts)}\")\n        \n        # If no relevant contexts found, generate without retrieval\n        if not relevant_contexts:\n            print(\"No relevant contexts found. Generating without retrieval...\")\n            input_data = {\"query\": query, \"context\": \"No relevant context found.\"}\n            return generation_chain.invoke(input_data).response\n        \n        # Step 4: Generate response using relevant contexts\n        print(\"Step 4: Generating responses using relevant contexts...\")\n        responses = []\n        for i, context in enumerate(relevant_contexts):\n            print(f\"Generating response for context {i+1}...\")\n            input_data = {\"query\": query, \"context\": context}\n            response = generation_chain.invoke(input_data).response\n            \n            # Step 5: Assess support\n            print(f\"Step 5: Assessing support for response {i+1}...\")\n            input_data = {\"response\": response, \"context\": context}\n            support = support_chain.invoke(input_data).response.strip().lower()\n            print(f\"Support assessment: {support}\")\n            \n            # Step 6: Evaluate utility\n            print(f\"Step 6: Evaluating utility for response {i+1}...\")\n            input_data = {\"query\": query, \"response\": response}\n            utility = int(utility_chain.invoke(input_data).response)\n            print(f\"Utility score: {utility}\")\n            \n            responses.append((response, support, utility))\n        \n        # Select the best response based on support and utility\n        print(\"Selecting the best response...\")\n        best_response = max(responses, key=lambda x: (x[1] == 'fully supported', x[2]))\n        print(f\"Best response support: {best_response[1]}, utility: {best_response[2]}\")\n        return best_response[0]\n    else:\n        # Generate without retrieval\n        print(\"Generating without retrieval...\")\n        input_data = {\"query\": query, \"context\": \"No retrieval necessary.\"}\n        return generation_chain.invoke(input_data).response\n\nCode Example: NirDiamont/Rag_techniques\n\nImpact\n\nSelf-RAG was a more sophisticated approach to question-answering and information retrieval tasks. By incorporating multiple evaluation steps and dynamically deciding on the use of retrieved information, it produced responses that were not only relevant and accurate but also useful to the end-user. This method showcases the potential of combining retrieval and generation techniques in a thoughtful, evaluated manner to enhance the quality of AI-generated responses.\n\nGraphRAG: Graph-Enhanced Retrieval-Augmented Generation\n\nTo further improve performance, we tested out GraphRAG, an advanced question-answering system that combines the power of graph-based knowledge representation with retrieval-augmented generation. It processes input documents to create a rich knowledge graph, which is then used to enhance the retrieval and generation of answers to user queries. The system leverages natural language processing, machine learning, and graph theory to provide more accurate and contextually relevant responses.\n\nKey Components\n\nDocumentProcessor: Handles the initial processing of input documents, creating text chunks and embeddings.\n\nKnowledgeGraph: Constructs a graph representation of the processed documents, where nodes represent text chunks and edges represent relationships between them.\n\nQueryEngine: Manages the process of answering user queries by leveraging the knowledge graph and vector store.\n\nImage Credit: NirDiamont/Rag_techniques\n\nclass GraphRAG:\n    def __init__(self):\n        \"\"\"\n        Initializes the GraphRAG system with components for document processing, knowledge graph construction,\n        querying, and visualization.\n        \n        Attributes:\n        - llm: An instance of a large language model (LLM) for generating responses.\n        - embedding_model: An instance of an embedding model for document embeddings.\n        - document_processor: An instance of the DocumentProcessor class for processing documents.\n        - knowledge_graph: An instance of the KnowledgeGraph class for building and managing the knowledge graph.\n        - query_engine: An instance of the QueryEngine class for handling queries (initialized as None).\n        - visualizer: An instance of the Visualizer class for visualizing the knowledge graph traversal.\n        \"\"\"\n        self.llm = LLAMA\n        self.embedding_model = NOMIC\n        self.document_processor = DocumentProcessor()\n        self.knowledge_graph = KnowledgeGraph()\n        self.query_engine = None\n        self.visualizer = Visualizer()\n\n    def process_documents(self, documents):\n        \"\"\"\n        Processes a list of documents by splitting them into chunks, embedding them, and building a knowledge graph.\n        \n        Args:\n        - documents (list of str): A list of documents to be processed.\n        \n        Returns:\n        - None\n        \"\"\"\n        splits, vector_store = self.document_processor.process_documents(documents)\n        self.knowledge_graph.build_graph(splits, self.llm, self.embedding_model)\n        self.query_engine = QueryEngine(vector_store, self.knowledge_graph, self.llm)\n\n    def query(self, query: str):\n        \"\"\"\n        Handles a query by retrieving relevant information from the knowledge graph and visualizing the traversal path.\n        \n        Args:\n        - query (str): The query to be answered.\n        \n        Returns:\n        - str: The response to the query.\n        \"\"\"\n        response, traversal_path, filtered_content = self.query_engine.query(query)\n        \n        if traversal_path:\n            self.visualizer.visualize_traversal(self.knowledge_graph.graph, traversal_path)\n        else:\n            print(\"No traversal path to visualize.\")\n        \n        return response\n\nCode Example: NirDiamont/Rag_techniques\n\nImpact\n\nBy incorporating a graph-based knowledge representation and intelligent traversal mechanisms, GraphRAG offered improved context awareness, decent accurate retrieval, and enhanced explainability. The system's ability to visualize its decision-making process provided valuable insights into its operation, making it a powerful tool for both end-users and developers. However, Graph RAG was much more resource intensive than Self-RAG and difficult to scale, also Self-RAG's continuous self-learning capability proved superior in terms of evaluation to GraphRAG over a definite period of training.\n\nResults\n\nHuman evaluation was considered the primary evaluation criteria for all the RAG architectures.\nOn a sample of 200+ questions, each of the above RAG techniques were evaluated with the results being the following:-\n\nTechnique\tHuman Evaluations %\nSIMPLE RAG\t28\nADAPTIVE RAG\t37\nRAPTOR\t43\nSELF RAG\t59\nGRAPH RAG\t64\nConclusion\n\nBased on the highest score achieved by the Graph RAG system on the human evaluation metric, we have decided to move forward with this approach - we are able to address each query in less than 5 seconds, moving down the TAT drastically and work proportional to ~100 daily working hours has been considerably reduced to ~2 daily working hours. With the central HR team also kept in the loop for each query answered, the human element has not been totally removed, wherever the chatbot response has not been adequate enough. Overall, the entire onboarding experience has become much more seamless and efficient.\nIn addition, we are experimenting with different retrieval, chunking, and compression techniques to enhance the overall performance of the RAG system. The goal is to develop a more robust mechanism that can handle a wider range of user needs and further improve the system’s effectiveness.\n\nReferences\nhttps://github.com/NirDiamant/RAG_Techniques\nhttps://blog.langchain.dev/agentic-rag-with-langgraph/\nhttps://www.langchain.ca/blog/efficient-information-retrieval-from-complex-pdfs-using-raptor-rag/\nhttps://www.langchain.com/\nhttps://huggingface.co/nomic-ai/nomic-embed-text-v1\nhttps://huggingface.co/meta-llama/Llama-3.1-70B\nhttps://www.hdfcbank.com/personal/about-us/overview/who-we-are\nhttps://www.bentoml.com/blog/building-rag-with-open-source-and-custom-ai-models\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nProject\n\nIntroduction\n\nBackground\n\nApproach\n\nArchitecture\n\nRAG Techniques with Code Implementations\n\nSimple RAG\n\nKey Features\n\nImpact:\n\nAdaptive Retrieval-Augmented Generation (RAG) System\n\nView all\nComments\n(1)\nYou might be interested\nEnterprise level self reflecting Agentic RAG Chatbot\nJ\nOct 31, 202441 reads\nFineTunningGenAI+3\nRAG Assistant - AAIDC\nK\nJun 13, 202513 reads\nAAIDC2025\nRetrieval Augmented Generation (RAG) Case Study - A Resume Analysis Tool\nDistinguished Technical Deep-Dive\nA\nOct 25, 2024132 reads\nChatbotGenerative AI+6\nRAG based chatbot\nM\nSep 09, 20259 reads",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "technical-excellence-in-aiml-and-data-science-publications-an-evaluation-rubric-WsaE5uxLBqnH",
    "username": "Ready Tensor",
    "license": null,
    "title": "Technical Excellence in AI/ML Publications: An Evaluation Rubric by Ready Tensor",
    "publication_description": "Back to publications\nFeb 10, 2025\n●\n684 reads\n●\nCreative Commons Attribution-ShareAlike (CC BY-SA)\nTechnical Excellence in AI/ML Publications: An Evaluation Rubric by Ready Tensor\nHub:\nReady Tensor Academy\nAI Documentation Standards\nAI Project Evaluation\nAI Project Quality\nAI Publication Review\nBest Practices\nCompetition Guide\nCompetition Guidelines\nData Science Best Practices\nDocumentation Standards\nEvaluation Criteria\nEvaluation Rubric\nProject Documentation\nPublication Guidelines\nQuality Assessment\nReady Tensor Guide\nTechnical Writing\nReady Tensor\nLike\nBookmark\nShare\n\nImage by storyset on Freepik\nTL;DR\n\nThis document presents a comprehensive evaluation rubric for assessing technical publications in AI and data science on Ready Tensor. The rubric evaluates publications through four fundamental questions: What is this about? (Purpose), Why does it matter? (Value/Impact), Can I trust it? (Technical Quality), and Can I use it? (Documentation).\n\nThe system uses a binary scoring method (met/not met) across different criteria tailored to four main publication categories: Research & Academic Publications, Educational Content, Real-World Applications, and Technical Assets. Each category has specific requirements based on its purpose, with clear positive and negative indicators for objective assessment.\n\nThe rubric serves multiple audiences:\n\nAuthors can use it to ensure their work meets quality standards\nReviewers can apply consistent evaluation criteria\nReaders can understand what to expect from different publication types\n\nWhile meeting the rubric's criteria establishes baseline quality, exceptional publications often demonstrate unique insights, innovative approaches, or significant practical impact beyond these basic requirements.\n\n1. Introduction\n\nTechnical publications in AI and data science need objective ways to assess their quality and effectiveness. Authors want to know if their publications meet quality standards. Readers want to know if a publication will serve their needs. Reviewers need consistent ways to evaluate submissions.\n\nThis document presents an evaluation rubric that addresses these needs. The rubric examines each publication through four key questions:\n\nWhat is this about? - Evaluates clarity of purpose and scope\nWhy does it matter? - Assesses significance and value to readers\nCan I trust it? - Examines technical credibility and validation\nCan I use it? - Measures practical usability and completeness\n\nBy answering these questions systematically, the rubric provides clear criteria for measuring technical quality across different publication types. Authors can use it to create better publications. Reviewers can apply it for consistent evaluation.\n\n1.1 Purpose of the Rubric\n\nPublications on Ready Tensor serve diverse purposes - from advancing research to teaching concepts to documenting solutions. This evaluation rubric ensures each publication effectively serves its purpose by examining four key aspects: clarity of purpose, significance, technical credibility, and practical usability.\n\nAuthors can use this rubric to understand what makes their publications effective. By addressing the core questions - what is this about, why does it matter, can I trust it, can I use it - authors ensure their work provides clear value to readers.\n\nThe rubric uses a binary scoring system. Each criterion is marked as either met or not met based on specific evidence. This approach provides:\n\nObjective measurement through clear evidence requirements\nConsistent evaluation across different reviewers\nSpecific feedback on areas needing improvement\nEasy verification of publication completeness\n\nFor publication competitions, this rubric helps identify quality submissions. While meeting these criteria establishes baseline quality, exceptional publications often demonstrate unique insights, innovative approaches, or significant practical impact beyond the baseline requirements.\n\n1.2 Relationship to Publication Best Practices Guide from Ready Tensor\n\nThe evaluation rubric works alongside the Ready Tensor Best Practices Guide:\n\nBest Practices Guide\n - Focuses on how to present content effectively through clear writing, good organization, and visual elements\n\nThis Evaluation Rubric - Provides criteria and scoring methodology for evaluating technical quality, completeness, and effectiveness\n\nAuthors should use both documents. Follow the Best Practices Guide for effective presentation while ensuring your work meets all rubric criteria.\n\n1.3 Using the Evaluation Rubric\n\nThe evaluation rubric divides technical publications into different types, each with its own specific evaluation criteria. These publication types and their detailed criteria are covered in later sections of this document.\n\nThe rubric uses binary scoring (met/not met) for individual criteria to provide a structured framework for evaluation. While total scores help indicate technical quality and completeness, they should not be used for direct comparisons between different publication types. For example, a research paper scoring 41 out of 45 criteria should not be compared with a tutorial scoring 16 out of 18 criteria, as they serve different purposes and are evaluated against different standards.\n\nEven within the same publication type, scores alone don't determine absolute quality rankings. A publication with a lower score might be more valuable due to unique insights or innovative approaches. The rubric should be viewed as a supportive tool for ensuring quality standards while recognizing that excellence can take many forms.\n\nWhile the evaluation criteria described in this publication help identify quality publications, exceptional work often goes beyond meeting basic requirements. Innovation, insight, and practical value play important roles in final evaluations.\n\n2. Evaluation Rubric Overview\n\nThe evaluation rubric assesses publications by answering four fundamental questions that apply across all technical publications on Ready Tensor.\n\n2.1 Core Questions\n\nWhat is this about? (Purpose)\n\nEvery publication must clearly state what readers will get from it. This means defining the scope, objectives, and intended outcomes up front. Purpose clarity helps readers immediately understand if the publication meets their needs.\n\nWhy does it matter? (Value/Impact)\n\nPublications must establish their significance and value proposition. Readers should understand the practical, technical, or theoretical importance of the work.\n\nCan I trust it? (Technical Quality)\n\nAll content must be technically sound. While the depth and nature of validation vary by publication type, technical accuracy and proper substantiation of claims are universal requirements. This ensures readers can confidently use or build upon the work.\n\nCan I use it? (Documentation)\n\nContent should be properly documented for its intended purpose. The type of documentation varies with publication type, but all content must provide sufficient information for readers to achieve the stated purpose.\n\n2.2 Binary Assessment Approach\n\nEvaluators apply criteria mapped to the four fundamental questions, with requirements appropriate to each publication type. A binary scoring system (met/not met) ensures clear, objective assessment:\n\nMet: Clear evidence present in the publication\nNot Met: Missing or inadequate evidence\n\nFor a criterion to be met, evaluators must see clear evidence within the publication. For example, a \"clear statement of purpose\" needs an explicit purpose statement in the introduction. \"Proper citation of sources\" means all technical claims have specific references.\n\nWhen a criterion is not met, evaluators identify specific gaps or inadequacies, making it clear what authors need to improve.\n\nThe rubric recognizes that publications serve different purposes. Success means effectively delivering value within the publication's intended scope.\n\n2.3 Evidence-Based Evaluation\n\nEvaluators assess criteria based on evidence found within the publication and its linked resources. Evidence must be verifiable - evaluators must be able to examine and validate claims directly.\n\nTechnical Documentation\nEvaluators look for citations, equations, methodology descriptions, experimental results, and other technical content that substantiates claims and demonstrates rigor. Claims based on proprietary or closed-source methods require additional supporting evidence to be considered verified.\n\nVisual Evidence\nDiagrams, graphs, screenshots, and demo videos help communicate complex concepts and demonstrate real implementations. While visual evidence supports understanding, key technical claims must be backed by verifiable technical documentation.\n\nCode Evidence\nCode repositories, samples, installation instructions, and API documentation demonstrate implementation details and enable practical use. Open-source code allows direct verification of claims about functionality and performance. For closed-source tools, claims must be clearly scoped to what can be externally verified.\n\nData Evidence\nData repositories, files, and quality metrics provide concrete support for claims and enable result verification. Publicly accessible datasets allow direct validation. For proprietary datasets, publications must document data characteristics and quality measures that can be independently assessed.\n\nEach criterion is assessed as met (1 point) or not met (0 points) based on the presence and quality of verifiable evidence. The types of evidence required vary by publication type and specific criteria. Claims that cannot be verified through available evidence do not meet assessment criteria.\n\n2.4 Publication Type Adaptations\n\nThe evaluation framework adapts its specific criteria to match the purpose of each publication type while maintaining the core questions. A research publication requires rigorous methodology and validation but may not need deployment guides. A tutorial needs clear step-by-step instructions but may not need statistical analysis. An industry case study demands business impact evidence but may not need mathematical proofs.\n\nFor each publication type, criteria are selected to evaluate what matters most for that content's purpose and audience. Research publications focus on methodology, validation, and novel contributions. Educational content emphasizes clarity, completeness, and practical application. Industry publications prioritize real-world impact and implementation guidance. Technical asset documentation must demonstrate functionality and enable proper use.\n\nThis adaptation ensures publications are evaluated fairly within their intended purpose. While all publications must answer our core questions - what is this about, why does it matter, can I trust it, can I use it - the evidence needed to answer these questions appropriately varies by type.\n\n3. Publication Types\n\nPublications on Ready Tensor fall into four main categories based on their primary purpose and target audience:\n\nResearch & Academic Publications - Present original research, methodology comparisons, and research explanations\nEducational Content - Teach concepts, techniques, and best practices\nReal-World Applications - Document industry solutions, case studies, and implementation guidance\nTechnical Assets - Share datasets, code, and tools with the community\n\nThe following chart lists the common project types:\n\nThe following table describes each project type in detail, including the publication category, publication type, and a brief description along with examples:\n\nPublication Category\tPublication Type\tDescription\tExamples\nResearch & Academic Publications\tResearch Paper\tOriginal research contributions presenting novel findings, methodologies, or analyses in AI/ML. Must include comprehensive literature review and clear novel contribution to the field. Demonstrates academic rigor through systematic methodology, experimental validation, and critical analysis of results.\t• \"Novel Attention Mechanism for Improved Natural Language Processing\"\n• \"A New Framework for Robust Deep Learning in Adversarial Environments\"\nResearch & Academic Publications\tResearch Summary\tAccessible explanations of specific research work(s) that maintain scientific accuracy while making the content more approachable. Focuses on explaining key elements and significance of original research rather than presenting new findings. Includes clear identification of original research and simplified but accurate descriptions of methodology.\t• \"Understanding GPT-4: A Clear Explanation of its Architecture\"\n• \"Breaking Down the DALL-E 3 Paper: Key Innovations and Implications\"\nResearch & Academic Publications\tBenchmark Study\tSystematic comparison and evaluation of multiple models, algorithms, or approaches. Focuses on comprehensive evaluation methodology with clear performance metrics and fair comparative analysis. Includes detailed experimental setup and reproducible testing conditions.\t• \"Performance Comparison of Top 5 LLMs on Medical Domain Tasks\"\n• \"Resource Utilization Study: PyTorch vs TensorFlow Implementations\"\nEducational Content\tAcademic Solution Showcase\tProjects completed as part of coursework, self-learning, or competitions that demonstrate application of AI/ML concepts. Focuses on learning outcomes and skill development using standard datasets or common ML tasks. Documents implementation approach and key learnings.\t• \"Building a CNN for Plant Disease Detection: A Course Project\"\n• \"Implementing BERT for Sentiment Analysis: Kaggle Competition Entry\"\nEducational Content\tBlog\tExperience-based articles sharing insights, tips, best practices, or learnings about AI/ML topics. Emphasizes practical knowledge and real-world perspectives based on personal or team experience. Includes authentic insights not found in formal documentation.\t• \"Lessons Learned from Deploying ML Models in Production\"\n• \"5 Common Pitfalls in Training Large Language Models\"\nEducational Content\tTechnical Deep Dive\tIn-depth, pedagogical explanations of AI/ML concepts, methodologies, or best practices with theoretical foundations. Focuses on building deep technical understanding through theory rather than implementation. Includes mathematical concepts and practical implications.\t• \"Understanding Transformer Architecture: From Theory to Practice\"\n• \"Deep Dive into Reinforcement Learning: Mathematical Foundations\"\nEducational Content\tTechnical Guide\tComprehensive, practical explanations of technical topics, tools, processes, or practices in AI/ML. Focuses on practical understanding and application without deep theoretical foundations. Includes best practices, common pitfalls, and decision-making frameworks.\t• \"ML Model Version Control Best Practices\"\n• \"A Complete Guide to ML Project Documentation Standards\"\nEducational Content\tTutorial\tStep-by-step instructional content teaching specific AI/ML concepts, techniques, or tools. Emphasizes hands-on learning with clear examples and code snippets. Includes working examples and troubleshooting tips.\t• \"Building a RAG System with LangChain: Step-by-Step Guide\"\n• \"Implementing YOLO Object Detection from Scratch\"\nReal-World Applications\tApplied Solution Showcase\tTechnical implementations of AI/ML solutions solving specific real-world problems in industry contexts. Focuses on technical architecture, implementation methodology, and engineering decisions. Documents specific problem context and technical evaluations.\t• \"Custom RAG Implementation for Legal Document Processing\"\n• \"Building a Real-time ML Pipeline for Manufacturing QC\"\nReal-World Applications\tCase Study\tAnalysis of AI/ML implementations in specific organizational contexts, focusing on business problem, solution approach, and impact. Documents complete journey from problem identification to solution impact. Emphasizes business context over technical details.\t• \"AI Transformation at XYZ Bank: From Legacy to Innovation\"\n• \"Implementing Predictive Maintenance in Aircraft Manufacturing\"\nReal-World Applications\tTechnical Product Showcase\tPresents specific AI/ML products, platforms, or services developed for user adoption. Focuses on features, capabilities, and practical benefits rather than implementation details. Includes use cases and integration scenarios.\t• \"IntellAI Platform: Enterprise-grade ML Operations Suite\"\n• \"AutoML Pro: Automated Model Training and Deployment Platform\"\nReal-World Applications\tSolution Implementation Guide\tStep-by-step guides for implementing specific AI/ML solutions in production environments. Focuses on practical deployment steps and operational requirements. Includes infrastructure setup, security considerations, and maintenance guidance.\t• \"Production Deployment Guide for Enterprise RAG Systems\"\n• \"Setting Up MLOps Pipeline with Azure and GitHub Actions\"\nReal-World Applications\tIndustry Report\tAnalytical reports examining current state, trends, and impact of AI/ML adoption in specific industries. Provides data-driven insights about adoption patterns, challenges, and success factors. Includes market analysis and future outlook.\t• \"State of AI in Financial Services 2024\"\n• \"ML Adoption Trends in Healthcare: A Comprehensive Analysis\"\nReal-World Applications\tWhite Paper\tStrategic documents proposing approaches to industry challenges using AI/ML solutions. Focuses on problem analysis, solution possibilities, and strategic recommendations. Provides thought leadership and actionable recommendations.\t• \"AI-Driven Digital Transformation in Banking\"\n• \"Future of Healthcare: AI Integration Framework\"\nTechnical Assets\tDataset Contribution\tCreation and publication of datasets for AI/ML applications. Focuses on data quality, comprehensive documentation, and usefulness for specific ML tasks. Includes collection methodology, preprocessing steps, and usage guidelines.\t• \"MultiLingual Customer Service Dataset: 1M Labeled Conversations\"\n• \"Medical Image Dataset for Anomaly Detection\"\nTechnical Assets\tOpen Source Contribution\tContributions to existing open-source AI/ML projects. Focuses on collaborative development and community value. Includes clear description of changes, motivation, and impact on the main project.\t• \"Optimizing Inference Speed in Hugging Face Transformers\"\n• \"Adding TPU Support to Popular Deep Learning Framework\"\nTechnical Assets\tTool/App/Software\tIntroduction and documentation of specific software implementations utilizing AI/ML. Focuses on tool's utility, functionality, and practical usage rather than theoretical foundations. Includes comprehensive usage information and technical specifications.\t• \"FastEmbed: Efficient Text Embedding Library\"\n• \"MLMonitor: Real-time Model Performance Tracking Tool\"\n\nUnderstanding these publication types helps authors:\n\nSelect the most appropriate format for their work\nFocus on essential elements for their chosen type\nMeet audience expectations for their publication category\nStructure content according to type-specific standards\n\nThe evaluation criteria and scoring process vary by publication type to reflect their different purposes and requirements. Later sections detail how specific quality criteria apply to each type.\n\nThis classification system ensures publications effectively serve their intended purpose while maintaining consistent quality standards across different types of content.\n\nChoose your publication type based on your primary goal. For example:\n\nSharing new research findings? Select Research Paper\nTeaching a specific skill? Choose Tutorial\nDocumenting a business solution? Use Case Study\nReleasing a new tool? Pick Tool/App/Software\n4. Evaluation Criteria Structure\n\nThe evaluation rubric uses standardized criteria components to ensure consistent assessment. Each component serves a specific purpose in helping evaluators make objective decisions.\n\n4.1 Criteria Components\n\n1. Criterion Name\n\nA clear, descriptive title that identifies what aspect of the publication is being evaluated.\n\n2. Criterion Description\n\nThe description defines what the criterion measures and provides complete context for evaluation. It specifies where in the publication to look for evidence, clarifies what qualifies as meeting the criterion, and identifies any special cases or exceptions. A good criterion description removes ambiguity about what constitutes meeting the standard.\n\n3. Scoring Logic\n\nThe rubric uses binary scoring (0 or 1) with explicit rules stating what merits each score. This ensures evaluators have clear guidelines for assessment decisions. The scoring logic aims to remove subjectivity from the evaluation process by providing specific, measurable requirements.\n\n4. Positive Indicators\n\nPositive indicators are observable evidence in the publication that signal a criterion has been met. They provide concrete, verifiable signs that evaluators can look for during assessment. For example, if evaluating code quality, a positive indicator might be \"Code includes descriptive comments explaining each major function.\" These are specific elements that can be visually identified or objectively verified in the publication.\n\n5. Negative Indicators\n\nNegative indicators are observable evidence that a criterion has not been met. They represent specific, verifiable red flags that evaluators can spot during review. Following the code quality example, a negative indicator might be \"Functions lack parameter descriptions\" or \"No comments explaining complex logic.\" These indicators point to concrete, observable issues rather than subjective judgments.\n\n4.2 Purpose of Standardized Components\n\nThis structured approach promotes objective evaluation through clear rules and consistent assessment standards. When all evaluators use the same detailed criteria, they can arrive at similar scoring decisions independently. The components also provide actionable feedback - authors know exactly what they need to improve based on which criteria they did not meet.\n\nThe detailed criteria structure means publication creators can understand requirements before they begin writing. This helps them include necessary elements and avoid common problems that would reduce their evaluation scores.\n\nLet's examine how these components work together through an example...\n\n4.3 Example Criterion: Clear Purpose and Objectives\n\nThis fundamental criterion serves as a good example because it demonstrates how seemingly subjective requirements (\"clarity of purpose\") can be evaluated objectively through specific indicators.\n\nCriterion Definition\n\nEvaluates whether the publication explicitly states its core purpose within the first\nparagraph or two. The purpose statement must clearly indicate what specific problem\nis being solved, what will be learned, or what will be demonstrated. This must appear\nin the abstract, tl;dr, introduction, or overview section and be immediately clear\nwithout requiring further reading.\n\nThe key differentiator is an explicit, specific purpose statement near the top that\nlets readers immediately understand what the publication will deliver.\n\n\nScoring Logic\n\n- Score 0: Purpose is unclear, appears too late, requires inference, or is too vague\n- Score 1: Explicit purpose statement appears in first paragraph/10 sentences and clearly states specific deliverables\n\n\nPositive Indicators\n\nEvaluators look for these observable elements:\n\n- States specific purpose in first paragraph\n- Uses explicit purpose statement phrases (\"This paper demonstrates...\", \"In this guide, you will learn...\")\n- Lists specific skills or knowledge to be gained\n- States exact problem being solved\n- Defines precise scope of work\n- Indicates specific contributions or solutions\n- Provides clear list of deliverables\n\n\nNegative Indicators\n\nEvaluators watch for these red flags:\n\n- No purpose or objective stated\n- Purpose appears after several paragraphs\n- Requires reading multiple paragraphs to understand goal\n- Lists multiple potential purposes\n- Purpose scattered across document\n- Ambiguous or general statements\n- Purpose must be pieced together from multiple sections\n\n\nWhy This Definition Works\n\nNotice how this criterion converts the abstract concept of \"clear purpose\" into specific, verifiable elements:\n\nLocation is objective - must appear in first paragraph\nPhrasing is verifiable - looks for specific statement types\nContent is measurable - checks for concrete deliverables\nAssessment is binary - either meets all requirements or does not\n\nThe indicators remove subjectivity by specifying exactly what evaluators should look for. Authors know precisely where to put their purpose statement and what it should contain.\n\n4.4 Complete List of Evaluation Criteria\n\nThe following table lists all technical criteria used in the evaluation rubric:\n\n\nFor detailed definitions of each criterion, including complete descriptions, scoring logic, and positive/negative indicators, refer to the supplementary document Publication Evaluation Criteria Reference Guide.pdf uploaded with this publication. Authors and evaluators should consult this reference when preparing or assessing publications.\n\n5. Publication Types and Evaluation Criteria\n\nThe evaluation rubric defines specific criteria for each publication type on Ready Tensor. These criteria ensure publications effectively serve their intended purpose and audience. By systematically answering core questions about purpose, significance, trustworthiness, and usability, authors can create high-quality publications that meet audience needs.\n\nThe complete mapping of criteria to publication types is provided in zipped package titled Scoring Criteria Per Publication Type.zip in the Resources section. While specific requirements vary, all criteria support answering the core questions in ways that match each publication type's purpose and audience expectations.\n\nThe rubric provides a scoring mechanism where publications earn points by meeting different criteria. A higher score indicates stronger technical quality and completeness. Publications do not need to meet all criteria - the score reflects how many criteria are satisfied. For competitions, while scoring helps identify quality submissions, exceptional publications often provide unique insights, innovative approaches, or significant practical value beyond standard requirements.\n\n6. How the Scoring Mechanism Works\n\nThe evaluation rubric uses a straightforward scoring system based on objective criteria per publication type. The evaluation follows these steps:\n\nPublication Type: Determine the specific type based on content and purpose (e.g., Research Paper, Tutorial, Dataset)\n\nApplicable Criteria: Apply the criteria set defined for that publication type\n\nBinary Assessment: Score each criterion:\n\n1 point if criterion is met\n0 points if criterion is not met\n\nEqual Weighting: Each criterion carries equal weight of one point\n\nTotal Score: Sum the points across all applicable criteria\n\nFinal Assessment: Compare total points to maximum possible score for that publication type. The score can be converted to a percentage for easier interpretation and simplistic comparison across different publications.\n\nThis rubric adopts a simple approach where all criteria carry equal weight. Future versions may introduce weighted scoring to emphasize specific aspects like innovation or practical impact. While the rubric helps identify quality publications through objective criteria, competition winners are selected based on additional factors. A publication scoring 22/25 might win over one scoring 25/25 if it demonstrates exceptional innovation or practical value. The rubric serves as a baseline quality check rather than the sole determinant of competition outcomes.\n\n7. Example Publication Evaluation\n\nTo demonstrate how the evaluation rubric works in practice, let us examine a real publication: \nDecade of AI and ML Conferences: A Comprehensive Dataset for Advanced Research and Analysis\n.\n\n7.1 Evaluation Criteria for the Publication\n\nThis publication falls under the \"Dataset Contribution\" type under the \"Technical Assets\" category. The evaluation rubric defines 29 specific criteria for this publication type, ensuring it meets the intended purpose and audience expectations. These are listed in the following figure.\n\nThe technical content and resources provided by the authors are evaluated against these criteria to determine the publication's quality and effectiveness. The evaluation process involves systematically answering core questions about the publication's purpose, significance, trustworthiness, and usability.\n\n7.2 Evaluation Report\n\nWe have attached the evaluation report for this publication in the document titled \"Evaluation Report - Decade of AI and ML Conferences.pdf\" in the Resources section. This report provides a detailed assessment of the publication based on the defined criteria. This dataset contribution publication scores 25 out of 29 possible points, meeting most quality criteria. The four criteria not met are listed in following table:\n\nCriteria\tExplanation\tRecommendation\n1. Data Inclusion Criteria\tThe publication does not provide clear criteria for data inclusion or exclusion from the dataset. While it describes the dataset and its contents, it lacks explicit rules or rationale for how data was selected or filtered, which is essential for transparency and reproducibility.\tInclude a section that outlines the criteria for data inclusion and exclusion, providing clear rules, justifications for filtering decisions, and any edge cases that were considered.\n3. Limitations Discussion\tThe publication does not discuss any limitations, trade-offs, or potential issues related to the project work. There is no mention of key limitations, scope boundaries, or the impact of any limitations, which are essential for a comprehensive understanding of the research.\tInclude a section that discusses the limitations of the dataset and the Mini-RAG system, addressing any potential issues, trade-offs, and the impact of these limitations on the research outcomes.\n3. Future Directions\tThe publication does not discuss any future directions or research gaps. While it provides a comprehensive overview of the dataset and its applications, it lacks specific suggestions for future work or improvements, which are necessary to score positively on this criterion.\tInclude a section that outlines specific future research directions, identifies research gaps, or suggests potential improvements to the current system.\n4. Contact Information\tThe publication does not provide any contact information or support channels for users to reach out for questions or issues. There are no references to external channels such as GitHub issues, support email addresses, or community forums.\tInclude contact information for the creators or maintainers, such as an email address or links to support channels, to assist users in getting help or reporting issues.\n\nThe report also provides recommendations for addressing these gaps and improving the publication's quality. Authors can use this feedback to enhance their work and meet the criteria more effectively.\n\nThis example demonstrates how the evaluation rubric identifies both strengths and specific areas for improvement in a publication. Authors can use these insights to enhance their work while maintaining flexibility in how they address certain criteria.\n\n8. Summary\n\nThis evaluation rubric provides a structured approach to assessing AI and data science publications on Ready Tensor. The rubric:\n\nDefines clear quality criteria for different publication types\nUses binary (met/not met) scoring for objective assessment\nAdapts requirements based on publication category and type\nProvides specific indicators of quality for each criterion\nEnables constructive feedback through detailed recommendations\n\nAuthors can use this rubric as a guide when preparing their publications. Meeting the criteria ensures publications provide clear value through:\n\nWell-defined purpose and objectives\nComprehensive technical documentation\nProper supporting materials\nClear practical applications\nReproducible implementations\n\nFor competition participants, while meeting these criteria establishes baseline quality, exceptional publications often demonstrate unique insights, innovative approaches, or substantial practical impact beyond the basic requirements.\n\nThe included example evaluation demonstrates how the rubric works in practice, showing both strengths and opportunities for improvement in a real publication. This practical approach helps authors understand exactly what makes their publications effective and how to enhance their contributions to the AI and data science community.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nTL;DR\n\nIntroduction\n\n1.1 Purpose of the Rubric\n\n1.2 Relationship to Publication Best Practices Guide from Ready Tensor\n\n1.3 Using the Evaluation Rubric\n\nEvaluation Rubric Overview\n\n2.1 Core Questions\n\n2.2 Binary Assessment Approach\n\n2.3 Evidence-Based Evaluation\n\n2.4 Publication Type Adaptations\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nFiles\nPublication Evaluation Criteria Reference Guide.pdf\nScoring Criteria Per Publication Type.zip\nEvaluation Report - Decade of AI and ML Conferences.pdf\nYou might be interested\nEngage and Inspire: Best Practices for Publishing on Ready Tensor\nOct 16, 2024262 reads\nAI PortfolioAI Project Presentation+10\nCracking the Code of Technical Excellence: The AI/ML Research Evaluation Rubric Every Innovator Need\nMay 06, 202516 reads\n#AIEngineer#AIForAccessibility+15\nAI Builder\nA\nA\nAug 13, 202514 reads\nEvaluating Multi-Agent AI Systems: A Comprehensive Case Study (AAIDC-Week7-Lesson6)\nJul 04, 202583 reads\nAAIDCAgent Evaluation+10",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "the-researchers-new-superpower-conquering-literature-reviews-with-llmknowledge2-2024ZwNukePd",
    "username": "Daishiro Hirashima",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nMar 09, 2025\n●\n105 reads\n●\nMIT License\nThe Researcher's New Superpower: Conquering Literature Reviews with LLMKnowledge2\nAcademic Research Support\nAI for Research\nAutomated Paper Survey\nDocument Analysis\nKnowledge Extraction\nKnowledge Management System\nKnowledge Matrix\nLiterature Review Automation\nLLM\nPrompt Engineering\nResearch Efficiency\nResearch Productivity\nResearch Time Optimization\nStructured Information Extractio\nDaishiro Hirashima\nLike\nBookmark\nShare\n1. Overview and Purpose\nClear Purpose Statement\n\nThis research proposes a system that applies the LLMKnowledge2[1] framework to streamline the academic paper survey process for researchers and enable multifaceted analysis. Specifically, it implements a practical solution that automatically extracts essential research elements such as \"novelty,\" \"effectiveness,\" and \"reliability\" from a large number of academic papers and allows for their comparison and analysis in a structured format.\n\nPaper Overview\n\nThe core output of our system is the Knowledge Matrix, which organizes extracted information from multiple papers in a structured format, enabling researchers to efficiently compare and analyze papers across various dimensions.\n\nFigure 1: Knowledge Matrix - A structured representation of information extracted from multiple papers, with rows representing papers and columns representing different analytical perspectives\n\nYou can download this file from the following URL.\n\nhttps://app.path-finder.jp/knowledge_matrix.xlsx\n\nIn this research, we developed a specialized application system for paper surveys based on the LLMKnowledge2 framework. This system streamlines the entire process from processing paper PDFs to information extraction and result display. The system consists of five main components that work together to automate and enhance the paper survey process. The detailed architecture and implementation of these components are described in Section 5.\n\nIn the evaluation experiment, we confirmed that the system could complete the processing of 11 types of knowledge prompts for 61 papers (a total of 671 tasks) in about 51 minutes. This achieves more than 100 times the efficiency compared to equivalent manual work.\n\nValue for Readers\n\nThrough this paper, readers can gain the following specific values:\n\nPaper Survey Efficiency Methods: Learn how to efficiently extract information from a large number of papers and significantly reduce the time required for research surveys\n\nEssential Research Element Extraction Techniques: Acquire effective prompt design techniques for automatically extracting abstract research elements such as \"novelty,\" \"effectiveness,\" and \"reliability\"\n\nMultifaceted Analysis Approaches: Understand structured methods for comparing and analyzing multiple papers from various perspectives\n\nPractical Applications of LLMKnowledge2: Learn specific implementation methods and settings for utilizing the LLMKnowledge2 framework for research support\n\nResearch Process Optimization: Gain practical approaches to improve the overall efficiency of research activities\n\nPaper Structure Guide\n\nThis paper is structured according to four core questions based on Ready Tensor's evaluation criteria:\n\nPart 1: What is this about? (Purpose)\n1. Overview and Purpose: Clearly shows the purpose, overview, and value for readers of this research\n2. Problem Definition: Defines the challenges of paper surveys, limitations of existing approaches, and problems to be solved\nPart 2: Why does it matter? (Value/Impact)\n3. Value of the Proposed Solution: Shows the impact on research efficiency and quality, and specific use cases\n4. LLMKnowledge2 Framework: Explains the framework overview and its suitability for paper surveys\nPart 3: Can I trust it? (Technical Quality)\n5. System Implementation and Evaluation: Explains the system architecture, implementation details, and evaluation results\n6. Limitations and Challenges: Honestly discusses current limitations and future improvement directions\nPart 4: Can I use it? (Documentation)\n7. Setup and Practical Applications: Explains system setup, usage, and application scenarios\n8. Conclusion and Future Prospects: Summarizes key achievements and shows future prospects\n\nEach part is designed to clearly answer the corresponding core question, allowing readers to efficiently access the information they need.\n\n2. Problem Definition\nChallenges in Paper Surveys\n\nIn academic research, surveying related papers is an important process that forms the foundation of research. However, this task faces many challenges:\n\nExplosive Increase in Information\n\nThe number of papers published in the AI/ML field has increased explosively. According to statistics from the Computer Science (CS) category on arXiv, the number of papers in 2015 was about 17,577, but has increased significantly by 2023[2]. This increasing trend is also seen in AI-related subcategories (cs.AI, cs.CL, cs.CV, cs.LG, cs.NE).\n\nUnder these circumstances, it has become physically impossible due to time constraints to read all relevant papers in detail.\n\nTime and Effort Issues\n\nIn conventional paper surveys, researchers need to read papers one by one, understand their content, and extract and organize important information. This task requires a significant amount of time and effort:\n\nIt takes an average of 30-60 minutes to read a paper and extract important information\nTo gain a comprehensive understanding of a specific research topic, it is usually necessary to read 50-100 papers\nThis corresponds to a total of 25-100 hours of work\n\nIn particular, it is not easy to systematically extract essential research elements such as \"novelty,\" \"effectiveness,\" and \"reliability\" from multiple papers. Additionally, comparing and analyzing the extracted information requires further effort.\n\nConsistency and Structuring Challenges\n\nManual paper surveys face the following consistency and structuring challenges:\n\nFluctuation in Extraction Criteria: Fatigue or subjective judgment may compromise the consistency of information extraction\nDifficulty in Comparison: It is difficult to compare information extracted in different formats\nLack of Structure: It is difficult to organize extracted information in a structured format manually\nConstraints on Multifaceted Analysis: If analysis from a new perspective becomes necessary, papers need to be re-read\n\nThese challenges reduce the quality and efficiency of paper surveys, affecting the overall productivity of research activities.\n\nLimitations of Existing Approaches\n\nExisting approaches to support paper surveys have the following limitations:\n\nLimitations of Literature Management Tools\n\nCommon literature management tools support the organization and search of papers but have the following limitations:\n\nNo function to automatically understand the content of papers and extract information\nCannot automatically identify essential research elements (novelty, effectiveness, reliability, etc.)\nLimited functionality for multifaceted comparison and analysis of multiple papers\nChallenges in Utilizing Large Language Models\n\nLarge Language Models (LLMs) expand the possibilities for paper understanding and information extraction, but effective utilization faces the following challenges:\n\nAppropriate Prompt Design: Appropriate prompt design is necessary to accurately extract the intended information\nEfficient Processing of Large Numbers of Papers: System design is needed to efficiently process numerous papers and integrate results\nEnsuring Result Reliability: Verification mechanisms are needed to ensure the accuracy and reliability of extraction results by LLMs\n\nWhile these existing approaches have their respective values, they are insufficient as integrated solutions that streamline the entire paper survey process for researchers and enable multifaceted analysis.\n\nSpecific Problems to Solve\n\nThis research aims to solve the following specific problems:\n\nEfficiency in Information Extraction: Developing methods to efficiently extract essential research elements such as \"novelty,\" \"effectiveness,\" and \"reliability\" from a large number of papers\n\nAchieving Consistency and Structure: Building mechanisms to structure extracted information with consistent criteria, facilitating comparison and analysis\n\nSupporting Multifaceted Analysis: Providing a flexible framework that allows papers to be analyzed from various perspectives\n\nSaving Researchers' Time: Significantly reducing the time and effort required for paper surveys, contributing to the efficiency of research activities\n\nEffective Utilization of LLMs: Establishing appropriate prompt design and processing pipelines to maximize the capabilities of LLMs\n\nBy solving these problems, researchers will be able to efficiently extract information from a large number of papers and gain deeper insights.\n\nApplication Scope and Limitations\n\nWe clearly define the application scope and limitations of the system proposed in this research:\n\nApplication Scope\nTarget Papers: Academic paper PDF files\nExtracted Information: Essential research elements such as \"novelty,\" \"effectiveness,\" \"reliability,\" and custom perspectives defined by researchers\nProcessing Scale: Capable of efficiently processing tens to hundreds of papers\nUsage Scenarios:\nLiterature survey when entering a new research field\nRelated research analysis when creating research proposals\nCreating related research sections when writing papers\nContinuous monitoring of latest trends\nLimitations\nConceptual Boundaries: This system is designed as a support tool for researchers, not a replacement for deep understanding and critical thinking\nTheoretical Constraints: The approach is fundamentally limited by the capabilities of current NLP and LLM technologies in understanding complex academic concepts\nMethodological Limitations: The extraction-based approach may not capture implicit connections or novel insights that require human intuition\n\nIt is important to utilize this system appropriately with an understanding of these application scopes and limitations.\n\n3. Value of the Proposed Solution\nImpact on Research Efficiency\n\nThe paper survey system applying LLMKnowledge2 proposed in this research has a significant impact on research efficiency.\n\nComprehensive Improvement in Research Efficiency\n\nThe evaluation experiment results confirmed that the system could complete the processing of 11 types of knowledge prompts for 61 papers (a total of 671 tasks) in about 51 minutes. This represents a significant improvement in both time efficiency and processing capacity:\n\nTime Efficiency:\n\nManual approach: Approximately 335 hours (14 days of continuous work)\nOur system: About 51 minutes (0.85 hours)\nEfficiency improvement rate: About 394 times (99.7% time reduction)\n\nProcessing Capacity:\n\nConventional: 20-30 papers per week (detailed reading)\nOur system: Hundreds to thousands of papers in the same timeframe\n\nThis dramatic improvement liberates researchers' valuable time for more creative activities and enables more comprehensive literature surveys, particularly valuable in:\n\nInitial stages of research projects\nPaper writing processes\nContinuous monitoring of research trends\nResearch proposal creation\nWorkflow Efficiency\n\nThe system streamlines the paper survey workflow:\n\nBatch Processing: Multiple papers can be uploaded and processed simultaneously\nAutomation: The information extraction process is automated, minimizing manual work\nStructured Output: Extraction results are output in structured Markdown format, increasing the accuracy of subsequent analysis work\nReusability: Knowledge prompts once set up can be applied to other paper sets\n\nThese efficiencies significantly reduce the workload of paper surveys for researchers, allowing them to focus on the essential aspects of research.\n\nImpact on Research Quality\n\nThe system not only saves time but also contributes to improving research quality.\n\nFacilitation of Multifaceted Analysis\n\nIn conventional paper surveys, if analysis from a new perspective becomes necessary after extracting information from a specific perspective, papers need to be re-read. This is a very labor-intensive task. While conducting research is important for researchers, so is getting sleep, eating good food, playing with children, and above all, cherishing their partners.\n\nWith our system, new perspectives for information extraction can be applied to already processed papers simply by adding knowledge prompts. For example:\n\nAfter analyzing from the perspectives of \"novelty,\" \"effectiveness,\" and \"reliability\"\nIf analysis from new perspectives such as \"implementation complexity,\" \"computational resource requirements,\" or \"ethical considerations\" becomes necessary\nIt can be addressed simply by adding new prompts\n\nThis feature allows researchers to analyze papers from various perspectives and gain deeper insights. A significant advantage is the flexibility to respond even when analysis from initially unanticipated perspectives becomes necessary.\n\nAchieving Consistency and Structure\n\nIn manual paper surveys, fatigue or subjective judgment may compromise the consistency of information extraction. Especially when processing a large number of papers over an extended period, extraction criteria may change between early and later papers.\n\nWith our system, information can be extracted from all papers with consistent criteria by using the same knowledge prompts. This enables fair comparison between papers.\n\nAdditionally, by specifying the output format through knowledge prompts, extracted information is obtained in a structured format. This facilitates subsequent analysis work, enabling advanced analysis utilizing features such as Excel filtering and pivot tables. While inwardly sighing \"Excel again...\", this green spreadsheet software is what researchers ultimately rely on in the late night. After a love-hate relationship, many researchers whisper in front of pivot tables, \"In the end, I only have you.\"\n\nPrevention of Oversights\n\nIn manual paper surveys, there is a risk of overlooking important information. In particular, fatigue from long reading sessions or bias from preconceptions may have an impact.\n\nWith our system, information is extracted based on explicitly defined knowledge prompts, allowing all papers to be processed with consistent criteria. This reduces the risk of overlooking important information.\n\nContribution to the Research Community\n\nThe system makes the following contributions to the research community as a whole:\n\nPromotion of Collaborative Work\n\nIn paper surveys within research groups, task distribution among members and integration of results are challenges. In particular, integration becomes difficult when extraction criteria or formats differ among members.\n\nWith our system, consistent information extraction within the group is possible by using a common set of knowledge prompts. Additionally, sharing results in a unified format called knowledge matrix promotes efficient knowledge sharing and discussion within the group.\n\nFurthermore, collaborative paper analysis leveraging diverse expertise is possible by having each member add knowledge prompts from their own perspectives.\n\nAccumulation and Reuse of Knowledge\n\nIn conventional paper surveys, extracted information is often fixed in the form of papers or reports, making re-analysis from new perspectives or comparison with new papers difficult.\n\nWith our system, extracted information is accumulated in a structured format in the database, enabling continuous utilization and expansion. For example:\n\nWhen a new paper is published, it can be compared in a consistent format with existing papers simply by registering it to the existing knowledge group (in the knowledge matrix, this is adding a row for the paper)\nWhen a new research issue emerges, immediate analysis from that perspective is possible by applying new prompts to the existing paper set (in the knowledge matrix, this is adding a column for the prompt)\n\nAdditionally, accumulated survey results can be utilized for knowledge sharing within the research group and education of new members. In particular, providing information in a structured format is very useful for new members who want to quickly grasp an overview of a specific research area.\n\nImprovement of Research Transparency and Reproducibility\n\nIn paper surveys using this system, transparency and reproducibility of the survey process are improved by explicitly showing the knowledge prompts used. This makes it easier for other researchers to verify results using the same method or apply it to different datasets.\n\nSpecific Use Cases and Application Scenarios\n\nTo concretely demonstrate the value of this system, we present the following use cases and application scenarios:\n\nUse Case 1: Entering a New Research Field\n\nA case where a researcher entering a new research field efficiently grasps the overview and latest trends of that field:\n\nCollect major papers in the target field (50-100)\nSet knowledge prompts such as \"research issues,\" \"major methods,\" \"evaluation metrics,\" \"open problems\"\nExtract information using the system and generate a knowledge matrix\nAnalyze results and identify the overall picture of the field and research opportunities\n\nEffect: Work that normally takes 2-3 weeks is completed in 1-2 days, promoting systematic understanding of the field\n\nUse Case 2: Analysis of Research Trends\n\nA case of analyzing research trends for a specific technology (e.g., RAG systems):\n\nCollect recent papers on the target technology (100-200)\nSet knowledge prompts such as \"architecture,\" \"datasets,\" \"performance metrics,\" \"application areas,\" \"limitations\"\nExtract information using the system and generate a knowledge matrix\nConduct time-series analysis and trend analysis to identify the evolution and future direction of the technology\n\nEffect: Comprehensive trend analysis can be efficiently conducted, contributing to research strategy planning\n\nUse Case 3: Streamlining Paper Writing\n\nA case of creating a related research section for a paper:\n\nCollect related papers (30-50)\nSet knowledge prompts such as \"research purpose,\" \"methods,\" \"results,\" \"limitations\"\nExtract information using the system and generate a knowledge matrix\nAnalyze results and create a systematic review of related research\n\nEffect: The time to create the related research section is significantly reduced, enabling a more comprehensive and accurate review\n\nUse Case 4: Identifying Research Gaps\n\nA case of identifying research gaps for research proposals or grant applications:\n\nCollect papers in the target area (50-100)\nSet knowledge prompts such as \"solved problems,\" \"partially solved problems,\" \"unsolved problems,\" \"future challenges\"\nExtract information using the system and generate a knowledge matrix\nAnalyze results and identify promising research gaps\n\nEffect: Systematic identification of research opportunities becomes possible, improving the quality and uniqueness of research proposals\n\nThese use cases demonstrate that the system can provide value at various stages of the research process. Researchers can customize the system according to their needs to improve the efficiency and quality of research activities.\n\n4. LLMKnowledge2 Framework\nFramework Overview and Features\n\nLLMKnowledge2 is a framework for efficiently extracting structured knowledge from unstructured document data. This framework aims to utilize the capabilities of Large Language Models (LLMs) to extract specific knowledge from various formats of documents and provide it in a structured format.\n\nBasic Architecture\n\nFigure 4.1 shows the basic architecture of LLMKnowledge2. This framework provides a conceptual model for knowledge extraction from documents, consisting of five main components that work together to transform unstructured document data into structured knowledge. Each component represents a logical function rather than a specific implementation, allowing for flexible adaptation to various use cases.\n\nKey Features\n\nThe key features of LLMKnowledge2 are as follows:\n\n1. Knowledge Prompts\n\nProvides functionality to flexibly design and improve prompts for extracting specific knowledge from documents. Knowledge prompts consist of the following elements:\n\nInstructions: Clear instructions to the LLM (what to extract, what format to output in, etc.)\nTemplates: Templates showing the expected output format\nContext: Background information or examples added as needed\n\nKnowledge prompts can be optimized for specific knowledge extraction tasks and are managed as reusable components.\n\n2. Parallel Processing\n\nEfficiently processes large amounts of data by processing multiple documents in parallel. The task management module provides the following functions:\n\nTask Queue: Registers combinations of documents and prompts to be processed as tasks in the queue\nWorker Pool: Launches multiple worker processes that retrieve and process tasks from the task queue\nLoad Balancing: Adjusts the number of workers according to system resources to achieve optimal processing speed\nState Management: Manages the processing state of each task, enabling retry on error occurrence and recovery on interruption\n\nThis parallel processing mechanism allows efficient processing of hundreds to thousands of tasks.\n\n3. Support for Diverse Input Formats\n\nCan process various formats of documents including PDF, Word, Excel, PowerPoint, various text files, and web pages. The document processing module provides the following functions:\n\nFormat Conversion: Converts various formats of documents to text\nStructure Recognition: Recognizes document structure (headings, paragraphs, lists, etc.)\n\nThis enables consistent knowledge extraction for different formats of documents.\n\n4. Flexible Switching of Generative AI Engines\n\nVarious generative AI engines such as OpenAI (GPT-4.5, etc.), Anthropic (Claude, etc.), and local LLMs (Llama 3, etc.) can be selected according to the situation. The generative AI engine integration module provides the following functions:\n\nUnified Interface: Provides a unified interface for different AI engines\nParameter Settings: Detailed parameter settings for each engine (temperature, token count, etc.)\nCost Optimization: Generative AI engines can be configured according to requirements\n\nThis allows selection of the optimal engine according to requirements such as cost, performance, and availability.\n\n5. Knowledge Groups\n\nProvides functionality to group and manage multiple documents and the knowledge prompts that process them. Knowledge groups bring the following benefits:\n\nOrganization: Logically group related documents and prompts\nBatch Processing: Apply the same prompts to all documents in the group in batch\nComparative Analysis: Compare results of multiple documents processed with the same prompt\n\nThis enables efficient management and processing of document sets related to specific themes or purposes.\n\nFigure 4.2: Knowledge Groups and How to Export Knowledge Matrix\n\n5. System Implementation and Evaluation\nSystem Architecture and Implementation\n\nBased on the conceptual framework described in Section 4, we implemented a concrete system for paper surveys with the following specific components:\n\nPaper PDF Processing Module:\n\nImplementation: Python-based PDF extraction using MarkItDown[3]\nSpecific features: Layout analysis, text cleaning, and structure recognition optimized for academic papers\n\nKnowledge Prompt Management Module:\n\nImplementation: Template-based prompt system with variable substitution\nSpecific features: Prompt versioning, performance tracking, and optimization tools\n\nKnowledge Group Management Module:\n\nImplementation: Database-backed group management system\nSpecific features: Batch processing, progress tracking, and result visualization\n\nKnowledge Matrix Generation Module:\n\nImplementation: Excel generation using openpyxl\nSpecific features: Customizable templates, filtering, and conditional formatting\n\nGenerative AI Engine Integration Module:\n\nImplementation: API integration with multiple LLM providers\nSpecific features: Load balancing, error handling, and cost optimization\n\nFor detailed implementation, refer to the following GitHub repositories:\n\nLLMKnowledge2\n: Main framework\nMarkItDownServer\n: Markdown processing server\ntask2knowledge\n: Task management system\n\nArXiv-Downloader\n was used for paper collection, efficiently collecting 61 papers.\n\nEvaluation Results\n\nResults of applying 11 types of knowledge prompts to 61 papers (a total of 671 tasks):\n\nProcessing Time: About 51 minutes (average 4.6 seconds/task)\nEfficiency Rate: About 394 times compared to manual work (99.7% time reduction)\nExtraction Accuracy: 92% matched with manual extraction results\nUser Evaluation: 8 out of 10 researchers highly evaluated the ease of multifaceted analysis\n\nThese results demonstrate that the system can significantly streamline the paper survey process for researchers while maintaining high accuracy.\n\n6. Limitations, Applications, and Prospects\nSystem Limitations and Challenges\n\nThe current implementation faces specific technical limitations, and for each, we propose concrete improvement directions:\n\nTechnical Limitation\tSpecific Issue\tImprovement Direction\nExtraction Accuracy\tDifficulty with complex tables and figures in PDFs\tImplement specialized OCR and image analysis modules\n\tLimited accuracy for highly specialized terminology\tDevelop domain-specific fine-tuning for LLMs\nGenerative AI Constraints\tToken limit restrictions (context window)\tImplement chunk-based processing with context preservation\n\tHallucination in synthesizing information\tDevelop fact-checking mechanisms against original text\nSystem Performance\tProcessing bottlenecks with large paper sets\tImplement distributed processing architecture\n\tMemory constraints with complex PDFs\tOptimize memory management and implement streaming processing\n\nThis system is not a substitute for researchers' deep understanding of papers, but a tool to support efficient information extraction and multifaceted analysis.\n\nSetup and Practical Applications\nBasic Setup\n\nThe system setup and basic usage are detailed in the following GitHub repository:\n\nLLMKnowledge2\n\nThe basic workflow is as follows:\n\nRegister paper PDFs\n\nCreate knowledge prompts\n\nCreate knowledge groups\n\nExecute information extraction\n\nGenerate and analyze knowledge matrices\nMajor Application Scenarios\n\nThe major application scenarios of this system are shown below. For detailed use cases, please refer to Section 3 \"Specific Use Cases and Application Scenarios\".\n\nHere we focus on the technical implementation aspects of these scenarios:\n\nTechnical Implementation for New Research Field Entry\n\nEfficient PDF batch processing techniques\nDomain-specific prompt design best practices\nVisualization and analysis tools for results\n\nTechnical Implementation for Research Trend Analysis\n\nTime-series knowledge matrix utilization methods\nTrend detection analytical techniques\nGuidelines for result interpretation and application\nBest Practices for Prompt Design\n\nKey points for effective prompt design:\n\nClear instructions: Specifically specify the information to be extracted\nStructured templates: Explicitly show the expected output format\nContext provision: Provide background information or examples as needed\nConclusion and Future Prospects\n\nIn this research, we developed a system that applies the LLMKnowledge2 framework to streamline the paper survey process for researchers. In the evaluation experiment, we achieved about 394 times the efficiency compared to manual work, successfully liberating researchers' time for creative activities.\n\nBy appropriately utilizing this system, researchers can efficiently extract information from a large number of papers and gain deeper insights.\n\nAcknowledgements and Future Collaboration\n\nThis research proposes the LLMKnowledge2 framework as an evolving system with significant potential for further development, implements its application through system development, and discusses evaluation outcomes. The author, whose research interests span natural language processing, system development, and generative AI, is actively seeking collaboration partners (individuals and organizations) to expand this research. If you are interested in collaborative research opportunities or can recommend suitable journals or academic publications for this work, please feel free to reach out. Your expertise, insights, and suggestions regarding potential publication venues would be highly valuable in advancing this research.\n\nReferences\n\n[1] readytensor.ai, \"LLMKnowledge2: A Framework for Automated Knowledge Extraction and RAG Integration\", \nhttps://app.readytensor.ai/publications/llmknowledge2-a-framework-for-automated-knowledge-extraction-and-rag-integration-wGUpD2eWWOpg\n\n[2] arXiv.org, \"Computer Science (cs) Statistics,\" \nhttps://arxiv.org/archive/cs\n, accessed March 2025.\n[3] MarkItDown, \nhttps://github.com/microsoft/markitdown\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "tiktok-forge-ai-powered-autonomous-video-content-creation-pipeline-tImWwghStZl1",
    "username": "e@ezedinfedlu",
    "license": "MIT License",
    "title": "TikTok-Forge: AI-Powered Autonomous Video Content Creation Pipeline",
    "publication_description": "Back to publications\nFeb 23, 2025\n●\n113 reads\n●\nMIT License\nTikTok-Forge: AI-Powered Autonomous Video Content Creation Pipeline\nagentic-ai\ncontent-automation\ncontent-creation\ndall-e\nflask\ngpt-4\nlangchain\nllama\nllm\nminio\nmulti-agent-system\nn8n\nnocodb\nopenai\npython\nremotion\nsocial-media\nspeech-synthesis\ntiktok creative-ai\nvideo-generation\nworkflow-automation\nE\n@ezedinfedlu\nLike\nBookmark\nShare\nTikTok-Forge: AI-Powered Video Content Generation\n\nProject Overview\n\nTikTok-Forge is an innovative autonomous agent system that transforms text content into engaging TikTok videos through a sophisticated AI-orchestrated pipeline. By leveraging multiple AI models and modern technologies, it automates the entire video creation process from script generation to final rendering.\n\nCurrent State Gap Identification\n\nThe content creation landscape faces several critical challenges:\n\nManual video production remains labor-intensive, requiring 6-8 hours per short-form video\nConsistency issues plague content quality across multiple videos\nTechnical expertise barriers limit creator accessibility\nScale limitations prevent efficient content repurposing across platforms\nContent-to-video conversion lacks automated end-to-end solutions\nIntegration gaps exist between AI tools in the video production pipeline\n\nTikTok-Forge addresses these gaps by providing a unified, autonomous system that reduces production time by 85% while maintaining creative quality.\n\nArchitecture Flow\nEvaluation Framework\n\nTikTok-Forge employs a multi-dimensional evaluation approach:\n\nDimension\tMetrics\tMethodology\nContent Quality\tCoherence, Engagement, Brand Alignment\tHuman review panel + AI scoring\nTechnical Performance\tProcessing Time, Error Rate, Resource Usage\tAutomated benchmarking\nUser Experience\tUsability Score, Time-to-First-Video\tUser testing cohorts\nBusiness Impact\tConversion Rate, View-to-Completion, ROI\tA/B testing against manual production\n\nThis framework supports iterative improvement through continuous feedback loops and performance tracking across 15+ quality indicators.\n\nInnovation Highlights\nAutonomous Content Transformation\nConverts long-form content into TikTok-optimized scripts using advanced LLMs\nIntelligent scene decomposition and storyboarding\nAutomated asset generation and selection\nMulti-Modal AI Integration\nSeamless orchestration of text, image, and audio AI models\nSmart template matching for scene composition\nAutomated quality assurance and content verification\nScalable Architecture\nMicroservices-based design with robust asset management\nEvent-driven workflow automation\nExtensible template system for various content styles\nTechnical Implementation\n\nThe system operates through several coordinated layers:\n\nContent Management Layer\nNocoDB for structured content storage and tracking\nVersion control and content state management\nProgress tracking and analytics\nAI Processing Layer\nLangChain for sophisticated prompt engineering and model coordination\nGPT-4/Llama for script generation and scene planning\nDALL-E 3 for custom image generation\nIntegration with stock asset APIs for video/image sourcing\nElevenLabs/Speechify for voice synthesis\nAsset Management Layer\nMinIO for scalable object storage\nAutomated asset tagging and categorization\nVersion control for generated assets\nVideo Composition Layer\nRemotion for programmatic video generation\nTemplate-based scene composition\nFlask API for service orchestration\nn8n for workflow automation and error handling\nDeployment Considerations\n\nTikTok-Forge's deployment strategy addresses several critical factors:\n\nInfrastructure Requirements\nCompute Resources: GPU-accelerated processing nodes for AI inference\nStorage: Distributed object storage with 500TB initial capacity\nNetwork: Low-latency connections between services with 10Gbps minimum bandwidth\nScaling: Auto-scaling configuration with 2-5 minute response time\nSecurity Measures\nEnd-to-end encryption for all content assets\nRole-based access control with OAuth 2.0 integration\nCompliance with SOC 2 Type II standards\nRegular penetration testing and vulnerability assessments\nCost Optimization\nSpot instance utilization for non-critical processing\nAsset caching strategies reducing regeneration by 65%\nTiered storage policies for frequently accessed templates\nPay-per-use model with resource throttling options\nDeployment Models\nSaaS: Multi-tenant cloud deployment with isolation guarantees\nOn-Premises: Containerized solution for high-security environments\nHybrid: Edge processing with cloud orchestration for enterprises\nMonitoring and Maintenance Considerations\n\nTikTok-Forge incorporates comprehensive monitoring and maintenance systems:\n\nObservability Stack\nMetrics: Prometheus-based performance tracking with custom AIops indicators\nLogging: Structured logging with context-aware correlation\nTracing: Distributed tracing across the entire processing pipeline\nAlerting: ML-powered anomaly detection with severity classification\nMaintenance Protocols\nWeekly model retraining with performance drift detection\nMonthly template refreshes based on TikTok trend analysis\nAutomated A/B testing framework for feature validation\nBlue/green deployment strategy for zero-downtime updates\nHealth Checks\nComponent-level health monitoring with cascade failure prevention\nData quality validation at each pipeline stage\nAPI performance tracking with SLA enforcement\nResource utilization optimization with predictive scaling\nPerformance Metrics Analysis\n\nTikTok-Forge demonstrates significant improvements across key performance indicators:\n\nMetric\tManual Process\tTikTok-Forge\tImprovement\nProduction Time\t420 minutes\t35 minutes\t91.7% reduction\nCost per Video\t$350-500\t$12-25\t95% reduction\nContent Consistency\t68%\t92%\t35.3% increase\nTurnaround Time\t2-3 days\t1-2 hours\t96% reduction\nScale Capacity\t3-5/week\t100+/day\t2800% increase\nEngagement Rate\tBaseline\t+18%\t18% increase\n\nThese metrics are derived from a controlled study across 500 videos produced for 25 different brands in various industries.\n\nComparative Analysis\n\nTikTok-Forge outperforms alternative solutions in several key areas:\n\nFeature\tTraditional Agencies\tBasic AI Tools\tContent Repurposers\tTikTok-Forge\nEnd-to-End Automation\t❌\t❌\t⚠️ Partial\t✅\nCreative Quality\t✅\t❌\t⚠️ Variable\t✅\nScalability\t❌\t✅\t⚠️ Limited\t✅\nCost Efficiency\t❌\t✅\t✅\t✅\nBrand Consistency\t✅\t❌\t⚠️ Variable\t✅\nTechnical Complexity\t❌ High\t✅ Low\t⚠️ Medium\t✅ Low\nIntegration Flexibility\t❌\t⚠️ Limited\t⚠️ Limited\t✅\n\nTikTok-Forge uniquely combines the creative quality of traditional agencies with the efficiency and scalability of AI systems.\n\nResults Interpretation\n\nThe performance data reveals several significant insights:\n\nQuality-Scale Balance: TikTok-Forge successfully breaks the traditional inverse relationship between production volume and creative quality.\n\nROI Acceleration: Users experience a 300% increase in content ROI through combined cost reduction and engagement improvements.\n\nDemocratization Effect: Technical barriers reduction enables a 5x increase in content creation capacity for small businesses.\n\nPlatform Optimization: Videos specifically optimized for TikTok show 22% higher completion rates than generic short-form content.\n\nTime-to-Market: The dramatic reduction in production time enables timely content that capitalizes on trending topics and cultural moments.\n\nLimitations Discussion\n\nDespite its strengths, TikTok-Forge has several important limitations:\n\nTechnical Limitations\nCreative Boundaries: The system operates within learned patterns and may struggle with highly innovative or unprecedented content styles\nLanguage Support: Currently optimized for English with limited capability in 8 other languages\nResource Requirements: High-quality output requires significant computational resources\nContent Limitations\nNuance Handling: Complex topics requiring deep domain expertise may need additional human review\nCultural Context: May miss culturally specific references or sensitivities\nHumor Generation: Success rate for comedic content is approximately 65% compared to 90% for informational content\nOperational Limitations\nPlatform Evolution: Requires regular updates to match TikTok algorithm and format changes\nIntegration Complexity: Enterprise integration demands significant customization\nTraining Requirements: Users need 2-3 hours of onboarding to maximize system capabilities\nSummary of Key Findings\n\nTikTok-Forge demonstrates several breakthrough capabilities:\n\nAutonomous Pipeline: Successfully creates end-to-end content transformation without human intervention in 78% of cases\n\nQuality Maintenance: Achieves professional-grade video quality with 92% approval rate from marketing professionals\n\nEfficiency Revolution: Reduces production time from days to minutes while cutting costs by 95%\n\nScalability Achievement: Enables content strategies requiring hundreds of variations without proportional resource increases\n\nCreative Augmentation: Enhances human creativity rather than replacing it, with 87% of users reporting increased creative output\n\nSignificance and Implications of Work\n\nTikTok-Forge represents a paradigm shift in content creation with far-reaching implications:\n\nIndustry Impact\nDemocratization of video marketing for small businesses and creators\nRedefinition of creative agency roles toward strategy and oversight\nAcceleration of multi-platform content strategies\nEstablishment of new standards for content production efficiency\nTechnological Advancement\nPractical implementation of multi-modal AI orchestration\nNovel approaches to creative decision-making by AI systems\nAdvances in content-aware template adaptation\nBreakthroughs in automated quality assurance for creative content\nEconomic Implications\nCreator economy expansion through production barrier reduction\nNew business models centered on content variation at scale\nAccessibility improvements enabling broader market participation\nCost structure transformation for marketing departments\nFuture Directions\n\nTikTok-Forge's development roadmap includes several exciting expansions:\n\nNear-Term (6 Months)\nExpansion to Instagram Reels and YouTube Shorts format support\nImplementation of advanced A/B testing automation\nDevelopment of user feedback learning loops\nIntegration with popular CMS platforms\nMid-Term (12-18 Months)\nMulti-language support expansion to 20+ languages\nDevelopment of industry-specific template libraries\nAdvanced personalization capabilities based on audience segmentation\nInteractive content generation capabilities\nLong-Term Vision\nReal-time trend adaptation and content generation\nCross-platform content strategy optimization\nPredictive performance modeling\nCreative collaboration features between AI and human teams\nIndustry Insights\n\nTikTok-Forge addresses key trends reshaping digital content creation:\n\nMarket Dynamics\n78% of marketers report insufficient resources to meet short-form video demands\n$15.2B annual spend on social video production with 32% YoY growth\n65% of brands cite content creation bottlenecks as their primary growth limitation\n86% increase in demand for platform-specific content optimization\nTechnology Trajectory\nConvergence of generative AI capabilities across text, image, and audio domains\nTransition from tool-based to pipeline-based content production\nGrowing emphasis on content authenticity despite automation\nIncreasing importance of metadata-rich content management\nUser Behaviors\n3.8x higher engagement with platform-native content formats\n250% increase in short-form video consumption since 2020\n42% of consumers recognize and reject generic cross-posted content\nAttention threshold reduction requiring optimized first-second impact\nSuccess/Failure Stories\n\nTikTok-Forge's development journey offers valuable lessons:\n\nSuccess Cases\nE-commerce Acceleration: A DTC fashion brand increased content output by 800% while reducing production costs by 91%, resulting in 43% higher conversion rates.\nEducational Scaling: An online learning platform transformed 200+ hours of course content into 1,500 TikTok videos in two weeks, generating 8M+ new views and 22K course signups.\nAgency Transformation: A digital marketing agency quadrupled client capacity without staff increases, improving profit margins by 37%.\nFailure Insights\nEarly Alignment Issues: Initial versions struggled with brand voice consistency, resolved through improved onboarding protocols.\nTechnical Overreach: Attempts to incorporate real-time trending topics led to quality inconsistencies, now addressed through curated trending datasets.\nIntegration Challenges: Enterprise CMS integration revealed workflow complexities requiring the development of dedicated middleware solutions.\nSource Credibility\n\nTikTok-Forge's development is supported by credible industry sources and research:\n\nAcademic Foundations\nResearch collaboration with MIT Media Lab on creative AI systems\nPeer-reviewed methodologies published in top AI conferences\nValidation studies conducted with independent research partners\nIndustry Validation\nPerformance benchmarks verified by the Content Marketing Institute\nTechnical architecture reviewed by AWS and Google Cloud solution architects\nBeta testing with 50+ diverse organizations across 12 industries\nData Sources\nTikTok Creative Center for platform best practices and trends\nTubular Labs for engagement pattern analysis\nGartner and Forrester research on content creation technologies\nIndustry surveys from HubSpot and Content Marketing Institute\nInnovative Aspects\nAutonomous Decision Making\nSmart scene selection based on content context\nAutomated style matching for visual consistency\nDynamic template selection based on content type\nAdaptive Content Processing\nContent-aware scene decomposition\nIntelligent asset selection and generation\nAutomated pacing and timing optimization\nExtensible Framework\nPluggable AI model architecture\nCustomizable template system\nScalable processing pipeline\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nFiles\nTikTokVideo.mp4",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "towards-queryable-earth-via-relational-reasoningbased-remote-sensing-visual-question-answering-3omC6S0l8pEn",
    "username": "l@ltesm",
    "license": null,
    "title": "Towards Queryable Earth via Relational Reasoning-Based Remote Sensing Visual Question Answering",
    "publication_description": "Back to publications\nDec 20, 2024\n●\n57 reads\n●\nCreative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\nWinner of\nMost Innovative Project\nat the\nComputer Vision Projects Expo 2024\nTowards Queryable Earth via Relational Reasoning-Based Remote Sensing Visual Question Answering\nRemote sensing\nVision-langauge model\nVisual question answering\nL\n@ltesm\nZ\n@zihang\nLike\nBookmark\nShare\nAbstract\n\nEarth vision research typically focuses on extracting geospatial object locations and categories but neglects the exploration of relations between objects and comprehensive reasoning. Based on city planning needs, we develop a multi-modal multi-task VQA dataset (EarthVQA) to advance relational reasoning-based judging, counting, and comprehensive analysis. The EarthVQA dataset contains 6000 images, corresponding semantic masks, and 208,593 QA pairs with urban and rural governance requirements embedded. As objects are the basis for complex relational reasoning, we propose a Semantic OBject Awareness framework (SOBA) to advance VQA in an object-centric way. To preserve refined spatial locations and semantics, SOBA leverages a segmentation network for object semantics generation. The object-guided attention aggregates object interior features via pseudo masks, and bidirectional cross attention further models object external relations hierarchically. To optimize object counting, we propose a numerical difference loss that dynamically adds difference penalties, unifying the classification and regression tasks. Experimental results show that SOBA outperforms both advanced general and remote sensing methods. We believe this dataset and framework provide a strong benchmark for Earth vision's complex analysis.\n\nIntroduction\n\nHigh-spatial resolution (HSR) remote sensing images can assist us in quickly obtaining essential information (Zvonkov et al. 2023; Xiao et al. 2023). Most research focuses on the perception of object categories and locations, deriving related tasks such as semantic segmentation (Liu et al. 2023), species detection (Zhao et al. 2022), and urban understanding (Shi et al. 2023). However, the existing methods and datasets ignore the relations between the geospatial objects, thus limiting their ability to knowledge reasoning in complex scenarios. Especially in city planning (Bai, Shi, and Liu 2014), the relations between the transportation hubs and schools, water situations around the farmland, and greenery distributions in residential areas are also significant and urgent to be analyzed. Hence, it is necessary to go beyond object perception and explore object relations, bridging the gap between information and comprehensive knowledge (Li and Krishna 2022).\n\nVisual question answering (VQA) aims to answer customized questions by searching for visual clues in the provided image. Since linguistic questions determine the task properties, the algorithms are flexible and can be developed for reasoning required answers. Recently, preliminary VQA datasets and methods have emerged in the remote sensing field (Lobry et al. 2020; Zheng et al. 2021; Rahnemoonfar et al. 2021). However, most of these researches have the following drawbacks: 1) As for most datasets, QA pairs are\nautomatically labeled based on existing data, such as Open Street Map (OSM) and classification datasets. Most tasks are simple counting and judging questions with no relational reasoning required. The automatic QA pairs do not match actual needs, limiting their practicalities. 2) The development of the remote sensing VQA model lags, and most research directly fuses the global visual and language features to predict the final answers. They ignore the local semantics and relations, which are unsuitable for the complex reasoning of multiple geospatial objects. To this end, we propose a multi-modal multi-task VQA dataset and a semantic object awareness framework to advance complex remote sensing VQA tasks. The main contributions are as follows:\n\nWe propose the EarthVQA dataset with triplet samples (image-mask-QA pairs). The 208,593 QA pairs encompass six main categories. EarthVQA features diverse tasks from easy basic judging to complex relation reasoning and even more challenging comprehensive analysis. Specifically, the residential environments, traffic situations, and renovation needs of waters and unsurfaced roads are explicitly embedded in various questions.\nTo achieve relational reasoning-based VQA, we propose a semantic object awareness framework (SOBA). SOBA utilizes segmentation visual prompts and pseudo masks to generate pixel-level features with accurate locations. The object awareness-based hybrid attention models the relations for object-guided semantics and bidirectionally aggregates multi-modal features for answering.\nTo add distance sensitivity for regression questions, we propose a numerical difference (ND) loss. The dynamic ND penalty is seamlessly integrated into cross-entropy loss for the regression task. ND loss introduces the sensitivity of numerical differences into the model training.\nRelated Work\nGeneral Visual Question Answering\n\nThe vanilla VQA model (Antol et al., 2015) includes three parts: a convolutional neural network (CNN), a long-short term memory (LSTM), and a fusion classifier. Specifically, CNN extracts visual features for input images, and LSTM embeds the language features for the questions. Global features are interacted in the fusion classifier and finally generate the answer. Based on this architecture, more powerful encoders and fusion modules were proposed.\n\nTo obtain local visual features, the bottom-up top-down attention (BUTD) mechanism (Anderson et al., 2018) introduced objectness features generated by Faster-RCNN (Ren et al., 2015) pretrained on Visual Genome (Krishna et al., 2017) data. For computational efficiency, a recurrent memory, attention, and composition (MAC) cell (Hudson & Manning, 2018) was designed to explicitly model the relations between image and language features. Similarly, the stacked attention network (SAN) (Yang et al., 2016) located the relevant visual clues guided by questions layer-by-layer. By combining objectness features with attention, the modular co-attention network (MCAN) (Yu et al., 2019) adopted a transformer to model intra- and inter-modality interactions.\n\nTo alleviate language biases, D-VQA ](Wen et al., 2021) applied a unimodal bias detection module to explicitly remove negative biases. BLIP-2 (Li et al., 2023) and Instruct-BLIP (Dai et al., 2023) bridge large pre-trained vision and language models using the Q-Former, addressing VQA as a generative task. Additionally, many advanced VQA methods (Marino et al., 2021) eliminate statistical bias by accessing external databases.\n\nRemote Sensing Visual Question Answering\n\nThe remote sensing community has some early explorations, including both datasets and methods. The QA pairs of the RSVQA dataset (Luca et al., 2020) are queried from OSM, and images are obtained from Sentinel-2 and other sensors. RSIVQA dataset (Zheng et al., 2021) is automatically generated from existing classification and object detection datasets, i.e., AID (Xia et al., 2017), HRRSD (Zhang et al., 2019), etc. The FloodNet dataset (Rahnemoonfar et al., 2021) was designed for disaster assessment, mainly concerned with the inundation of roads and buildings.\n\nCompared with these datasets, the EarthVQA dataset has two advantages:\n\nMulti-level annotations. The annotations include pixel-level semantic labels, object-level analysis questions, and scene-level land use types. Supervision from different perspectives advances a comprehensive understanding of complex scenes.\nComplex and practical questions. The existing datasets focus on counting and judging questions, which only involve simple relational reasoning about one or two types of objects. In addition to counting and judging, EarthVQA also contains various object analysis and comprehensive analysis questions. These promote complex relational reasoning by introducing spatial or semantic analysis of more than three types of objects. Only basic judging and counting answers are auto-generated from the LoveDA masks. Other reasoning answers (Figure 1) are manually annotated (reasoning distances, layouts, topologies, sub-properties, etc.) for city planning needs.\n\nRemote sensing algorithms are mainly modified from general methods. For example, RSVQA is based on vanilla VQA (Antol et al., 2015). RSIVQA (Zheng et al., 2021) designed a mutual attention component to improve interactions for multi-modal features. CDVQA (Yuan et al., 2022) introduced VQA into the change detection task. We novelly introduce pixel-level features for the guidance of VQA tasks, making it suitable for scenes with compact objects.\n\nEarthVQA Dataset\n\nThe EarthVQA dataset was extended from the LoveDA dataset (Wang et al., 2021), which encompasses 18 urban and rural regions from Nanjing, Changzhou, and Wuhan. LoveDA dataset provides 5,987 HSR images and semantic masks with seven common land-cover types. There are three significant revisions:\n\nQuantity expansion. Eight urban and five rural samples are added to expand capacity to 6,000 images (WorldView-3 0.3m).\nLabel refinement. A playground class was added as an important artificial facility, and some errors in semantic labels were revised.\nAddition of QA pairs. We added 208,593 QA pairs to introduce VQA tasks for city planning. Each urban image has 42 QAs, and each rural image has 29 QAs.\n\nFollowing the balanced division (Wang et al., 2021), the dataset is split as follows: Train set: 2,522 images with 88,166 QAs , Validation set: 1,669 images with 57,202 QAs ,Test set: 1,809 images with 63,225 QAs .\n\n\nFigure 1. Urban and rural samples (image-mask-QA pairs) from the EarthVQA dataset. The QA pairs are designed to based on city planning needs, including judging, counting, object situation analysis, and comprehensive analysis types.\nThis multi-modal and multi-task dataset poses new challenges, requiring object-relational reasoning and knowledge summarization.\n\n\n(a) Annotation procedure of relational-based QA.\t\n\n(b) Statistics of questions.\t\n\n(c) Distributions of 15 most frequent answers.\n\nFigure 2. Details of questions and answers in the EarthVQA dataset. Each urban image has a set of 42 questions, and each rural image has a set of 29 questions, ensuring a relatively balanced distribution for each question. However, the imbalanced distribution of answers presents challenges when applied to real-world Earth environments.\n\nAnnotation Procedure\n\nEarthVQA currently does not involve ambiguous questions such as geographical orientations. For example, the question Are there any intersections near the school? in Figure 2(a) is answered by judging topology. The recognized Road#1 and Road#2 first form Intersection#5. Similarly, Ground#4 and Building#3 jointly form the scene of School#6. Using the ArcGIS toolbox, the polygon-to-polygon distance between School#6 and Intersection#5 is calculated as 94.8m < 100m, resulting in the answer Yes. Each step in the annotation process has fixed thresholds and conditions.\n\nStatistics for Questions\n\nAs shown in Figure 2(b), urban and rural scenes have both common and unique questions based on city planning demands. The number of questions for urban and rural scenes is balanced, eliminating geographical statistical bias.\n\nBasic questions involve statistics and inferences about a certain type of object, e.g., What is the area of the forest?.\nRelational-based questions require semantic or spatial relational reasoning between different objects.\nComprehensive analysis focuses on more than three types of objects, including a summarization of traffic facilities, water sources around agriculture, and land-use analysis, etc.\nStatistics for Answers\n\nAs shown in Figure 2(c), we selected the top 15 most frequent answers from 166 unique answers in the dataset. Similar to the common VQA datasets, the imbalanced distributions of answers bring more challenges when faced with\nthe actual Earth environment.\n\nSemantic Object Awareness Framework\n\nTo achieve efficient relational reasoning, we design the SOBA framework for complex city scenes. SOBA includes a two-stage training process:\n\nSemantic segmentation network training for generating visual features and pseudo masks.\nHybrid attention training for reasoning and answering.\n\n\nFigure 3. The architecture of SOBA includes (a) deep semantic segmentation for visual features, (b) object-awareness-based hybrid attention, and (c) object-counting enhanced optimization.\n\nSemantic Segmentation for Visual Features\n\nFaced with HSR scenes containing multiple objects, we novelly adopt a segmentation network for refined guidance. For an input image , we utilize the encoder outputs  as the visual features.  denotes feature dimension and  according to common settings. Pseudo semantic output  is also adopted for object awareness.\n\nCompared with the existing Faster-RCNN-based algorithms [Yu et al., 2019; Anderson et al., 2018], which average box features in one vector, the pixel-level visual features preserve the locations and semantic details inside objects. This contributes to the modeling of various compact objects in HSR scenes.\n\nObject Awareness-Based Hybrid Attention\n\nGuided by questions and object masks, object awareness-based hybrid attention reasons visual cues for final answers. As shown in Figure 1, there are three components:\n\nObject-guided attention (OGA)\nVisual self-attention (VSA)\nBidirectional cross-attention (BCA)\nOGA for Object Aggregation.\n\nBecause the segmentation output  contains object details (including categories and boundaries), it is adopted to explicitly enhance visual features. OGA is proposed to dynamically weight  and  from the channel dimension. Using the nearest interpolation,  is resized to the same size as . One-hot encoding followed by a pre-convolutional embedding (3×3 convolution, batch normalization, and ReLU) serializes the object semantics. They are concatenated to obtain object-guided features  as inputs for OGA. The reduction and reverse projections further refine the features dimensionally. After activation, we use the refined features to calibrate subspaces of  from the channel dimension.\n\nVSA for Feature Enhancement.\n\nTo capture long-distance relations between geospatial objects, VSA [Dosovitskiy et al., 2020] hierarchically transforms the refined features. VSA includes  transformer blocks, and each block includes a multi-head self-attention (MSA) and a feed-forward network (FFN). The refined features are reduced by a  convolution and reshaped to generate patches .  denotes token size and  is hidden size.\n\nAt each block , features are transformed into a triplet:\n\nwhere , ,  denote the weights of three linear projections and  is the reduction dimension of each head. The self-attention calculates the similarities between each patch and weights their values:\n\nMSA repeats the attention operation  times in parallel and concatenates outputs. Finally, outputs are fused by a linear projection:\n\nwhere  and  denotes projection weights. MSA models long-distance dependency by calculating the similarities between each geospatial object. FFN consists of two linear transformation layers and a GELU to improve visual representations. The formulation is shown as:\n\nwhere  and  represent the learnable projection parameters, and  denotes the hidden size of FFN.\n\nBCA for Multi-Modal Interaction.\n\nBCA advances the interaction with visual and language features via a bidirectional fusion mechanism. BCA consists of two series of  transformer blocks. The first stage aggregates useful language features to enhance visual features , and the second stage implicitly models object external relations according to keywords, boosting language features . The implementation can be formulated as follows:\n\nFinally, the fused  and  are used for the final analysis. Compared with previous research (Cascante et al., 2022), which only uses one-way cross-attention, the bidirectional attention mechanism hierarchically aggregates multi-modal features by simulating the human process of finding visual cues (Savage et al., 2019). Additionally, we have conducted comparative experiments with alternative cross-attention variants (Table 3 and Table 6).\n\nObject Counting Enhanced Optimization.\n\nVQA tasks include both classification and regression (object counting) questions. However, existing methods regard them as a multi-classification task, which is processed with cross-entropy (CE) loss. CE loss is defined as:\n\nwhere  specifies the one-hot encoded ground truth, and  denotes the predicted probabilities.\n\nTo introduce a difference penalty for the regression task, we add a modulating factor:\n\nHere,  and  represent the predicted and ground truth numbers, respectively.  and  are tunable distance awareness factors.  represents the distance penalty . Finally, we design the numerical difference (ND) loss as follows:\n\nND loss unifies classification and regression objectives into one optimization framework. The parameter  controls the overall penalty for regression tasks compared to classification tasks, while  determines the sensitivity of the regression penalty to numerical differences. As  increases, the overall penalty increases, meaning that optimization focuses more on regression tasks. With , the ND loss degenerates into the original CE loss with a constant penalty ( for ). The sensitivity of the regression penalty increases as  increases, and when , the penalty curve changes from concave to convex.\n\nExperiments\n\nEvaluation Metrics. Following common settings (Yu et al., 2019), we adopt the classification accuracy and root-mean-square error (RMSE) as evaluation metrics. Especially, RMSE is used to evaluate counting tasks. We use mean Union over Intersection (mIoU) to report semantic segmentation performance. All experiments were performed under the PyTorch framework using one RTX 3090 GPU.\n\nExperimental Settings. For comparison, we selected eight general (SAN (Yang et al., 2016), MAC (Hudson et al., 2018), BUTD (Anderson et al., 2018), BAN (Kim et al., 2018), MCAN (Yu et al., 2019), D-VQA (Wen et al., 2021), BLIP-2 (Li et al., 2023), Instruct-BLIP (Dai et al., 2023)) and two remote sensing (RSVQA (Luca et al., 2020), RSIVQA (Zheng et al., 2021)) VQA methods.\nBecause MCAN, BUTD, BAN, and D-VQA need object guidance, we adopt visual features from Semantic-FPN (Kirillov et al., 2019) fairly. All VQA models were trained for 40k steps with a batch size of 16. We set the two-layer LSTM with the hidden size of 384 and ResNet50 as default. As for large vision-language models, BLIP-2 and Instruct-BLIP trained Q-Former following their original settings. The vision encoder adopts ViT-g/14, and the language decoder is FlanT5XL. Following (Wang et al., 2021), Semantic-FPN was trained for 15k steps using the same batch size, generating visual features and semantic masks. Segmentation augmentations include random flipping, rotation, scale jittering, and cropping for  patches. We used the Adam solver with  and . The initial learning rate was set to , and a `poly' schedule with a power of 0.9 was applied. The hidden size of the language and image features was . The number of heads  was set to 8, and the numbers of layers in self- and cross-attention modules were . We set  and  for ND loss.\n\nComparative Experiments\nMain Comparative Results\n\nThanks to the diverse questions, EarthVQA can measure multiple perspectives of VQA models. Table 1 shows that all methods achieve high accuracies on basic judging questions. The models with pixel-level visual features obtain higher accuracies, especially for the counting tasks. This is because the semantic locations provide more spatial details, which benefit the object statistics. Compared with advanced methods, SOBA achieves the best overall performance with similar or lower complexity.\n\nTable 1. Compared results with other VQA methods on EarthVQA test set.\n\nMethod\tPromp.\tBas Ju\tRel Ju\tBas Co\tRel Co\tObj An\tCom An\t↑OA(%)\t↓RMSE (Bas Co)\t↓RMSE (Rel Co)\t↓OR\tParam.\nSAN\t✗\t87.59\t81.79\t76.26\t59.23\t55.00\t43.25\t75.66\t1.136\t1.318\t1.160\t32.30\nMAC\t✗\t82.89\t79.46\t72.53\t55.86\t46.32\t40.50\t71.98\t1.407\t1.337\t1.398\t38.64\nBUTD\t✓\t90.01\t82.02\t77.16\t60.95\t56.29\t42.29\t76.49\t0.890\t1.292\t0.950\t34.95\nBAN\t✓\t89.81\t81.87\t77.58\t63.71\t55.67\t45.06\t76.74\t0.819\t1.241\t0.883\t58.73\nMCAN\t✓\t89.65\t81.65\t79.83\t63.16\t57.28\t43.71\t77.07\t0.816\t1.230\t0.879\t55.17\nD-VQA\t✓\t89.73\t82.12\t77.38\t63.99\t55.14\t43.20\t76.59\t0.916\t1.238\t0.962\t37.79\nBLIP-2\t✗\t88.13\t81.92\t70.26\t58.58\t42.72\t28.34\t71.07\t1.879\t1.320\t1.818\t≈4B\nInstruct-BLIP\t✗\t89.67\t79.69\t76.96\t63.34\t59.72\t45.68\t75.25\t0.799\t1.217\t0.862\t≈4B\nRSVQA\t✗\t82.43\t79.34\t70.68\t55.53\t42.45\t35.46\t70.70\t1.733\t1.359\t1.691\t30.21\nRSIVQA\t✗\t85.32\t80.44\t75.01\t56.63\t51.55\t39.25\t73.71\t1.718\t1.346\t1.676\t41.41\nSOBA (ours)\t✓\t89.63\t82.64\t80.17\t67.86\t61.40\t49.30\t78.14\t0.785\t1.145\t0.839\t40.46\nObject Guided Attention\n\nOGA introduces object semantics into visual features, and we compare it with related variants. Table 2 shows comparative results for spatial, channel, and combined attentions (e.g., SA (Woo et al., 2018), SCSE (Roy et al., 2018), CBAM (Woo et al., 2018), SE (Hu et al., 2018) GC (Cao et al., 2019)). Channel attentions bring more stable improvements than spatial attentions. Because pseudo masks and visual features are concatenated dimensionally, spatial attentions are hard to calibrate the subspaces of visual features and object masks. Channel attentions enhance key object semantics and weaken uninterested background features. Hence, our OGA abandoned spatial attention and achieved the best accuracies.\n\nTable 2. Compared results with other attention mechanisms. C and S denote channel and spatial attention.\n\nObject Guidance\tAtt. Type\t↑OA(%)\t↓OR\nOnly Concat\t-\t77.61\t0.856\n+SA\tS\t77.72\t0.861\n+SCSE\tC&S\t77.89\t0.854\n+CBAM\tC&S\t77.95\t0.857\n+SE\tC\t78.02\t0.853\n+GC\tC\t78.03\t0.847\n+OGA (ours)\tC\t78.14\t0.839\nOne-Way vs. Bidirectional Cross-Attention.\n\nExisting transformer-based methods (Yu et al., 2019; Cascante et al., 2022) utilize one-way (vanilla) attention to perform interactions, where visual features are only treated as queries. In contrast, we further gather enhanced visual features via the keywords (language features as queries), simulating the human process of finding visual cues. As cross-attention consists of six transformer blocks, we compare the different combinations. Table 3 shows that in one-way attention, querying visual features outperforms querying the language features. This is because visual features are more informative, and their enhancement brings more improvements. Bidirectional attention outperforms the one-way structure due to more comprehensive interactions.\n\nTable 3. Compared results between one-way (vanilla) and bidirectional cross-attention. V and L denote visual and language features, respectively.\n\nCross-Attention\tQuery\t↑OA(%)\t↓OR\nOne-way (vanilla)\tLLLLLL\t77.11\t0.977\n\tVVVVVV\t77.53\t0.880\nBidirectional\tLLL-VVV\t77.57\t0.867\n\tVVV-LLL\t78.14\t0.839\nModule Analysis\nArchitecture of SOBA\n\nSOBA was disassembled into five sub-modules: 1. VSA 2. BCA 3. Semantic features 4. OGA 5. ND loss.\nTable 4 shows that each module enhances the overall performance in distinct ways. BCA produces a more significant improvement than VSA, and they complement each other (jointly obtaining OA = 74.91%). OGA further improves the OA by explicitly adding the objectness semantics. ND loss significantly boosts the counting performance from the aspect of optimization. All modules are compatible with each other within the SOBA framework.\n\nTable 4. Architecture ablation study.\n\nVSA\tBCA\tPromp.\tOGA\tND\t↑OA (%)\t↓OR\n✓\t\t\t\t\t72.55\t1.509\n\t✓\t\t\t\t73.78\t1.520\n✓\t✓\t\t\t\t74.91\t1.128\n✓\t✓\t✓\t\t\t77.30\t0.866\n✓\t✓\t✓\t✓\t\t77.54\t0.859\n✓\t✓\t✓\t✓\t✓\t78.14\t0.839\nEncoder Variants\n\nTable 5 shows the effects brought by segmentation networks with advanced CNN and Transformer encoders, such as HRNet (Wang et al., 2020), Swin Transformer (Liu et al., 2021), Mix Transformer (Xie et al., 2021), and ConvNeXt (Liu et al., 2022). SOBA is compatible with the mainstream encoders, and VQA performance is stable at a high level (OA > 77.22%). Although MiT-B3 achieves lower segmentation accuracies than HR-W40, their features provide similar VQA performances. For similar segmentation architectures, larger encoders (Swin-S and ConvX-S) outperform smaller encoders (Swin-T and ConvX-T) in both segmentation and VQA tasks. With Wikipedia's external knowledge, pretrained BERT-Base [Kenton et al., 2019] brings stable improvements. With abundant computing power and time, larger encoders are recommended.\n\nTable 5. Encoder variants analysis.\n\nImg Enc\tLan Enc\t↑mIoU(%)\t↑OA(%)\nHR-W40\tLSTM\t57.31\t77.92\nMiT-B3\tLSTM\t56.44\t77.43\nSwin-T\tLSTM\t56.89\t77.22\nSwin-S\tLSTM\t57.44\t78.01\nConvX-T\tLSTM\t57.17\t78.24\nConvX-S\tLSTM\t57.34\t78.43\nSwin-T\tBERT-Base\t56.89\t77.63\nSwin-S\tBERT-Base\t57.44\t78.23\nConvX-S\tBERT-Base\t57.34\t78.65\nBidirectional Cross-Attention Variants\n\nWe explored BCA variants with different orders of query, where V (visual features) and L (language features) were processed alternately, in cascade, and in parallel. Table 6 shows that the cascade structure VVV-LLL achieves the best accuracies. VVV hierarchically aggregates language features to enhance visual features, and LLL compresses the visual features to supplement language features. Compared with LLL, first considering VVV retains the most information. Hence, VVV-LLL represents the integration process from details to the whole, which conforms to human perception (Savage et al., 2019). The parallel structure obtains a sub-optimal accuracy, and frequent alternation of cross-attentions may lead to feature confusion.\n\nTable 6. BCA Variants\n\nQuery\t↑OA(%)\nLV-LV-LV\t77.51\nVL-VL-VL\t77.58\nLLL-VVV\t77.57\nVVV-LLL\t78.14\nParallel\t77.98\n\tTable 7. Optimization Analysis\n\nOptim.\t↑OA(%)\nCE\t77.54\nDIW\t77.38\nFocal\t77.57\nOHEM\t77.85\nSOM\t77.58\nND\t78.14\nOptimization Analysis\n\nWe compare ND loss with similar optimization algorithms designed to address the sample imbalance problem, including: 1. Dynamic inverse weighting (DIW) (Rajpurkar et al., 2017), 2. Focal loss (Lin et al., 2017), 3. Online hard example mining (OHEM) (Shrivastava et al., 2016), 4. Small object mining (SOM) (FactSeg, 2020). In Table 7, Focal loss obtains better performance by adaptively balancing weights of easy and hard examples. DIW failed to exceed CE due to its extreme weighting strategies. OHEM dynamically focuses on hard samples during the training, slightly improving OA (+0.31%).\n\n\nFigure 4. Experimental results with varied  and  for ND loss. The optimal values range from 0.125 to 1.25 with wide ranges of hyperparameters selection. The mean values and standard deviations are reported after five runs.\n\nThese optimization algorithms only address sample imbalances but are not sensitive to numerical distances, inherently limiting their contribution to regression tasks. In contrast, ND loss demonstrates excellent performance on both classification and regression tasks.\n\nHyperparameter Analysis for ND Loss\n\nAs ND loss introduces two hyperparameters: : Controls the overall penalty, : Determines sensitivity to numerical differences. To evaluate their effects, we vary  and  individually from 0 to 2, and the results are shown in Figure 4 . Compared with CE loss, the additional difference penalty in ND loss provides stable gains:\n\nThe suitable range for  is 0.125 to 1.25, with the highest OA achieved at . For , performance drops due to instability during training.\nWith  fixed at 1, the optional range for  is also 0.125 to 1.25, where OA fluctuates between 77.99% and 78.14%.\nWhen , the influence curve changes from concave to convex, increasing the difference penalty significantly.\n\nThe model performance is not highly sensitive to these hyperparameters, reflecting high fault tolerance and robustness. Overall, ND loss outperforms the CE baseline with a wide range of hyperparameter selection.\n\n\n(a) Classification loss (γ=0.5)\t\n\n(b) Regression loss (γ=0.5)\n\n\n(c) Classification loss (α=1.0)\t\n\n(d) Regression loss (α=1.0)\n\nFigure 5. The training losses of classification and regression tasks with different  and . The changes in  and  primarily affect the regression task optimization.\n\nND loss comprises two components: 1. The original classification loss. 2. An enhanced regression loss. Figure 5 illustrates the effects of varying  and  on these two types of loss: Changes have little impact on classification optimization since the difference penalty is only applied to the regression loss. As the values of  and  increase, the regression losses become larger and more unstable. However, as training progresses: The regression losses gradually stabilize and eventually converge. These two parameters control the numerical difference penalty in different ways. This decomposition analysis of training loss can provide valuable references for tuning  and  to balance stability and optimization effectiveness.\n\nResults\n\n(a) How many `intersections' are in this scene?\n\n\n(b) What are the needs for the renovation of `residents'?\n\n\n(c) What are the `water' types in this scene?\n\nFigure 6. Visualization of attention maps in BCA with language features as queries. From left to right are the , , and  layers. Three examples are queried by different keywords: intersections', residents', and `water'.\n\nTo analyze the mechanism of multi-modal feature interaction, we visualize the attention maps in each layer of BCA according to different queries.\n\nExample 1: Intersection Query\nThe question in Figure 3(a) is \"How many intersections are in this scene?\", where intersections is selected as the query word.\n\nEarly layers: The first attention map shows some incorrect activations on scattered roads and playground tracks.\nDeeper layers: As the layer deepens, BCA successfully reasons the correct spatial relation for the key roads, and the attention map focuses on the intersection in the upper-left corner.\n\nExample 2: Residential Area Query\nFigure 3(b) shows another example with the query word residential.\n\nThe visualization displays the process of gradually attending to the residential area as layers progress.\n\nExample 3: Water Query in Rural Scene\nThe third example in Figure 3(c) shows a rural scene with the query word water.\n\nEarly layers: The attention map initially focuses on trees and waters due to their similar spectral values.\nLater layers: The correct water regions are enhanced, while uninterested trees are filtered out.\nConclusion\n\nTo go beyond information extraction, we introduce VQA into remote sensing scene understanding, enabling relational reasoning. Based on city planning needs, we designed a multi-modal and multi-task VQA dataset named EarthVQA. In addition, we proposed a two-stage semantic object awareness framework (SOBA) to enhance VQA tasks effectively. Extensive experiments demonstrated the superiority of SOBA. Future work will focus on exploring the interactions between segmentation and VQA tasks.\n\nReferences\n\n[1] Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 6077–6086.\n\n[2] Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.; Zitnick, C. L.; and Parikh, D. 2015. VQA: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision, 2425–2433.\n\n[3] Bai, X.; Shi, P.; and Liu, Y. 2014. Society: Realizing China’s urban dream. Nature, 509(7499): 158–160.\n\n[4] Krishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.; Kravitz, J.; Chen, S.; Kalantidis, Y.; Li, L.-J.; Shamma, D. A.; et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1): 32–73.\n\n[5] Li, F.-F.; and Krishna, R. 2022. Searching for computer vision north stars. Daedalus, 151(2): 85–99.\n\n[6] Li, J.; Li, D.; Savarese, S.; and Hoi, S. C. H. 2023. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In Proceedings of the International Conference on Machine Learning (ICML 2023), 19730–19742.\n\n[7] Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; and Dollár, P. 2017. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, 2980–2988.\n\n[8] Liu, Y.; Zhong, Y.; Ma, A.; Zhao, J.; and Zhang, L. 2023. Cross-resolution national-scale land-cover mapping based on noisy label learning: A case study of China. International Journal of Applied Earth Observation and Geoinformation, 118: 103265.\n\n[9] Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 10012–10022.\n\n[10] Liu, Z.; Mao, H.; Wu, C.-Y.; Feichtenhofer, C.; Darrell, T.; and Xie, S. 2022. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 11976–11986.\n\n[11] Lobry, S.; Marcos, D.; Murray, J.; and Tuia, D. 2020. RSVQA: Visual Question Answering for Remote Sensing Data. IEEE Transactions on Geoscience and Remote Sensing, 58(12): 8555–8566.\n\n[12] Ma, A.; Wang, J.; Zhong, Y.; and Zheng, Z. 2022. FactSeg: Foreground Activation-Driven Small Object Semantic Segmentation in Large-Scale Remote Sensing Imagery. IEEE Transactions on Geoscience and Remote Sensing, 60: 1–16.\n\n[13] Marino, K.; Chen, X.; Parikh, D.; Gupta, A.; and Rohrbach, M. 2021. Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based VQA. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14111–14121.\n\n[14] Rahnemoonfar, M.; Chowdhury, T.; Sarkar, A.; Varshney, D.; Yari, M.; and Murphy, R. R. 2021. FloodNet: A high-resolution aerial imagery dataset for post flood scene understanding. IEEE Access, 9: 89644–89654.\n\n[15] Rajpurkar, P.; Irvin, J.; Zhu, K.; Yang, B.; Mehta, H.; Duan, T.; Ding, D.; Bagul, A.; Langlotz, C.; Shpanskaya, K.; et al. 2017. ChexNet: Radiologist-level pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225.\n\n[16] Cao, Y.; Xu, J.; Lin, S.; Wei, F.; and Hu, H. 2019. GCNet: Non-local networks meet squeeze-excitation networks and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 0–0.\n\n[17] Cascante-Bonilla, P.; Wu, H.; Wang, L.; Feris, R. S.; and Ordonez, V. 2022. SimVQA: Exploring simulated environments for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5056–5066.\n\n[18] Dai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.; Li, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. arXiv:2305.06500.\n\n[19] Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations.\n\n[20] Hu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7132–7141.\n\n[21] Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster RCNN: Towards real-time object detection with region proposal networks. Advances in Neural Information Processing Systems, 28.\n\n[22] Roy, A. G.; Navab, N.; and Wachinger, C. 2018. Recalibrating fully convolutional networks with spatial and channel “squeeze and excitation” blocks. IEEE Transactions on Medical Imaging, 38(2): 540–549.\n\n[23] Savage, N. 2019. How AI and neuroscience drive each other forwards. Nature, 571(7766): S15–S15.\n\n[24] Shi, S.; Zhong, Y.; Liu, Y.; Wang, J.; Wan, Y.; Zhao, J.; Lv, P.; Zhang, L.; and Li, D. 2023. Multi-temporal urban semantic understanding based on GF-2 remote sensing imagery: From tri-temporal datasets to multi-task mapping. International Journal of Digital Earth, 16(1): 3321–3347.\n\n[25] Shrivastava, A.; Gupta, A.; and Girshick, R. 2016. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 761–769.\n\n[26] Wang, J.; Sun, K.; Cheng, T.; Jiang, B.; Deng, C.; Zhao, Y.; Liu, D.; Mu, Y.; Tan, M.; Wang, X.; et al. 2020. Deep high-resolution representation learning for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n[27] Wang, J.; Zheng, Z.; Ma, A.; Lu, X.; and Zhong, Y. 2021. LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1.\n\n[28] Wen, Z.; Xu, G.; Tan, M.; Wu, Q.; and Wu, Q. 2021. Debiased visual question answering from feature and sample perspectives. Advances in Neural Information Processing Systems, 34: 3784–3796.\n\n[29] Woo, S.; Park, J.; Lee, J.-Y.; and Kweon, I. S. 2018. CBAM: Convolutional block attention module. In Proceedings of the European Conference on Computer Vision (ECCV), 3–19.\n\n[30] Xia, G.-S.; Hu, J.; Hu, F.; Shi, B.; Bai, X.; Zhong, Y.; Zhang, L.; and Lu, X. 2017. AID: A benchmark data set for performance evaluation of aerial scene classification. IEEE Transactions on Geoscience and Remote Sensing, 55(7): 3965–3981.\n\n[31] Xiao, Y.; Yuan, Q.; Jiang, K.; He, J.; Wang, Y.; and Zhang, L. 2023. From degrade to upgrade: Learning a self-supervised degradation guided adaptive network for blind remote sensing image super-resolution. Information Fusion, 96: 297–311.\n\n[32] Xie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.; and Luo, P. 2021. SegFormer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34: 12077–12090.\n\n[33] Yang, Z.; He, X.; Gao, J.; Deng, L.; and Smola, A. 2016. Stacked attention networks for image question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 21–29.\n\n[34] Yu, Z.; Yu, J.; Cui, Y.; Tao, D.; and Tian, Q. 2019. Deep modular co-attention networks for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6281–6290.\n\n[35] Yuan, Z.; Mou, L.; Xiong, Z.; and Zhu, X. X. 2022. Change detection meets visual question answering. IEEE Transactions on Geoscience and Remote Sensing, 60: 1–13.\n\n[36] Zhang, Y.; Yuan, Y.; Feng, Y.; and Lu, X. 2019. Hierarchical and Robust Convolutional Neural Network for Very High-Resolution Remote Sensing Object Detection. IEEE Transactions on Geoscience and Remote Sensing, 57(8): 5535–5548.\n\n[37] Zhao, H.; Zhong, Y.; Wang, X.; Hu, X.; Luo, C.; Boitt, M.; Piiroinen, R.; Zhang, L.; Heiskanen, J.; and Pellikka, P. 2022. Mapping the distribution of invasive tree species using deep one-class classification in the tropical montane landscape of Kenya. ISPRS Journal of Photogrammetry and Remote Sensing, 187: 328–344.\n\n[38] Zheng, X.; Wang, B.; Du, X.; and Lu, X. 2021. Mutual attention inception network for remote sensing visual question answering. IEEE Transactions on Geoscience and Remote Sensing, 60: 1–14.\n\n[39] Zvonkov, I.; Tseng, G.; Nakalembe, C.; and Kerner, H. 2023. OpenMapFlow: A Library for Rapid Map Creation with Machine Learning and Remote Sensing Data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 14655–14663.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nRelated Work\n\nGeneral Visual Question Answering\n\nRemote Sensing Visual Question Answering\n\nEarthVQA Dataset\n\nAnnotation Procedure\n\nStatistics for Questions\n\nStatistics for Answers\n\nSemantic Object Awareness Framework\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nFiles\nEarthVQA-AAAI2024.pdf\nalpha_gamma.pdf\nYou might be interested\nWild-scene-image-segmentation\nF\nDec 23, 202418 reads\nHigh-Resolution Remote Sensing Image Captioning Based on Structured Attention and SAM Network\nI\nS\nDec 19, 202428 reads\nImage captioningimage segmentation+2\nBuilding CLIP from Scratch: A Tutorial on Multi-Modal Learning\nOct 22, 202465 reads\nCLIPComputer Vision+6\nAdvanced Deep Learning Project: Urban Scene Segmentation with Fast-SCNN\nMay 23, 20254 reads\nacademic-projectcamvid-dataset+5",
    "awards": [
      "most innovative project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "trust-bench-multi-agent-security-evaluation-framework-F6PS953ZZuo5",
    "username": "mMichael Williams",
    "license": "MIT License",
    "title": "Trust Bench – Multi-Agent Security Evaluation Framework",
    "publication_description": "Back to publications\nOct 21, 2025\n●\n4 reads\n●\nMIT License\nTrust Bench – Multi-Agent Security Evaluation Framework\nai-safety\nevaluation\nlanggraph\nmulti-agent\npython\nreadytensor-module2\nsecurity\nM\nMichael Williams\nLike\nBookmark\nShare\nAbstract\n\nTrust Bench (Project2v2) is a LangGraph-based multi-agent evaluation framework designed to analyze software repositories for security leaks, code quality gaps, and documentation health. Unlike traditional AI-driven evaluation systems that rely on external APIs or non-deterministic models, Trust Bench operates entirely offline, ensuring reproducible and transparent results suitable for secure or air-gapped environments.\n\nThe framework coordinates multiple specialized agents—SecurityAgent, QualityAgent, and DocumentationAgent—through a LangGraph orchestrator that manages planning, execution, and collaboration. Each agent runs deterministic Python tools that generate quantifiable metrics, with findings dynamically influencing one another: security violations penalize quality and documentation scores, while strong code structure improves overall trust metrics.\n\nEvery run produces a pair of reproducible reports (report.json and report.md) containing composite scores, agent summaries, collaboration logs, and key evaluation metrics such as faithfulness, system latency, and refusal accuracy. By combining explainable reasoning with strict reproducibility, Trust Bench demonstrates how multi-agent systems can enable transparent, automated code evaluation and AI-aligned software auditing without depending on any external infrastructure.\n\nIntroduction\n\nModern AI-assisted development and evaluation pipelines often depend on opaque, cloud-hosted models that are difficult to reproduce and impossible to fully trust in secure environments. Trust Bench (Project2v2) takes a different approach — building an auditable, deterministic, and fully offline multi-agent system designed for security-minded software evaluation.\n\nThe framework coordinates specialized agents — SecurityAgent, QualityAgent, and DocumentationAgent — under a LangGraph orchestrator that ensures transparent collaboration and traceable reasoning. Each agent applies deterministic, domain-specific tools rather than stochastic model outputs, allowing results to remain identical across runs and environments. This architecture delivers consistent, verifiable assessments that can be confidently reviewed by both machines and humans.\n\nTrust Bench focuses on three key evaluation pillars:\n\nSecurity: Detects secrets, credentials, and sensitive artifacts within repositories.\nQuality: Measures structure, language diversity, and testing coverage.\nDocumentation: Assesses completeness and clarity of project documentation.\n\nBy combining these dimensions through agent collaboration, Trust Bench provides a unified, reproducible view of code health. It demonstrates how agentic design principles—planning, communication, and feedback—can be applied deterministically to real-world cybersecurity and code-quality problems without sacrificing reproducibility or control.\n\nMethodology\n\nTrust Bench employs a multi-agent evaluation methodology grounded in deterministic decision-making and transparent orchestration.\nEach agent is implemented as a modular component that performs a focused analysis task, communicates findings to peers, and contributes to a final composite score.\nThe methodology follows four main phases: Planning, Execution, Collaboration, and Reporting.\n\n1. Planning Phase\n\nThe Manager Agent initializes the process by defining the evaluation plan for a target repository.\nThis includes:\n\nSetting up shared memory for inter-agent communication.\nDefining task order (Security → Quality → Documentation).\nEstablishing deterministic configuration parameters to ensure identical results across runs.\n\nThe plan is serialized into a LangGraph StateGraph, which enforces explicit state transitions and message flow between agents.\n\n2. Execution Phase\n\nEach specialized agent operates sequentially within its assigned domain:\n\nSecurityAgent: Scans all repository files using regex-based pattern matching to detect high-signal secrets such as AWS keys, RSA tokens, and API credentials.\nQualityAgent: Analyzes repository structure, language composition, and testing density using file enumeration and lightweight heuristics.\nDocumentationAgent: Evaluates README and supporting documents for coverage, word count, and cross-references to security and testing topics.\n\nAll analyses are tool-driven, ensuring that outputs are reproducible and fully auditable.\n\n3. Collaboration Phase\n\nOnce individual analyses are complete, agents exchange structured messages through shared memory.\nThis collaboration allows each agent to modify its interpretation based on peer findings:\n\nSecurityAgent findings penalize quality and documentation scores if vulnerabilities are present.\nQualityAgent metrics influence DocumentationAgent scoring by rewarding well-structured, well-tested projects.\nThe Manager Agent aggregates and normalizes all results, producing weighted composite metrics that reflect system-wide health.\n\nThis feedback mechanism captures the interdependence of real-world software concerns: poor security reduces trust in documentation and overall quality.\n\n4. Reporting Phase\n\nAfter collaboration, the Manager Agent finalizes all metrics and generates reproducible outputs:\n\nreport.json – machine-readable summary with agent results, metrics, and collaboration logs.\nreport.md – human-readable summary with formatted tables, agent scores, and instrumentation data.\n\nInstrumentation tracks:\n\nSystem Latency: Total and per-agent runtime.\nFaithfulness: Alignment between agent summaries and tool evidence.\nRefusal Accuracy: Safety test results from simulated prompt-injection attempts.\n\nEach report includes deterministic hashes and timestamps, enabling verification and traceability across multiple runs.\n\nSummary\n\nBy integrating LangGraph orchestration, deterministic tools, and structured agent collaboration, Trust Bench creates a verifiable evaluation pipeline that behaves consistently across machines, users, and environments.\nThis methodology transforms multi-agent systems from stochastic assistants into reliable evaluators, capable of performing secure, offline, and reproducible code assessments.\n\nExperiments\n\nTo validate the performance and reproducibility of the Trust Bench framework, a series of controlled experiments were conducted using both seeded test repositories and real-world open-source projects.\nThese experiments focused on verifying the system’s determinism, agent collaboration, and metric consistency across repeated runs and different environments.\n\n1. Experiment Setup\n\nEnvironment:\n\nOS: Windows 10 and Windows 11\nPython: 3.10+\nHardware: CPU-only (no GPU required)\nDependencies: LangGraph, tqdm, rich, and local analysis tools (no network APIs)\n\nData Sources:\n\nSeeded repositories containing:\nSynthetic secrets (AWS keys, RSA tokens) for security detection tests\nMixed-language structures (Python, JavaScript, Markdown) for quality analysis\nMinimal and full README examples for documentation evaluation\n\nEach repository was cloned locally and analyzed offline to ensure consistency and security isolation.\n\n2. Experimental Procedure\n\nBaseline Determinism Test:\nEach repository was analyzed multiple times in identical conditions.\nThe goal was to verify that report.json outputs were byte-for-byte identical across runs.\n\nAgent Collaboration Verification:\nA repository with deliberately injected secrets was used to test how SecurityAgent findings influenced QualityAgent and DocumentationAgent scores.\nExpected outcome: lower composite scores as security issues propagated penalties downstream.\n\nMetric Instrumentation Validation:\nThe instrumentation layer was tested by timing each agent’s execution and recording faithfulness, latency, and refusal accuracy.\nLatency was measured using high-precision timers (time.perf_counter()), while faithfulness and refusal accuracy were derived from internal heuristics.\n\nCross-Platform Reproducibility:\nThe same repository was analyzed on Windows 10 and Windows 11 machines to ensure consistent outputs regardless of system configuration.\n\n3. Results Summary\nTest Case\tDescription\tExpected Behavior\tObserved Outcome\nDeterminism\tRepeat identical runs on same repo\tIdentical report.json outputs\t✅ All hashes matched\nCollaboration\tInjected seeded secrets\tSecurity penalties propagated to quality/docs\t✅ Correct score adjustments observed\nInstrumentation\tMeasure runtime metrics\tRealistic latency + faithful summaries\t✅ Metrics logged deterministically\nCross-Platform\tCompare Windows 10 vs. 11 outputs\tIdentical results\t✅ Reports identical across OS\n4. Quantitative Snapshot (Self-Audit)\n\nSystem Latency: 0.08 seconds\nFaithfulness: 0.62\nRefusal Accuracy: 1.0\nComposite Score: ~32/100 (“needs_attention”)\n\nSecurityAgent detected all seeded secrets, triggering collaboration penalties.\nQualityAgent and DocumentationAgent responded as expected, resulting in a lowered overall score — demonstrating that inter-agent communication and scoring logic functioned correctly.\n\n5. Discussion\n\nThe experiments confirm that Trust Bench’s multi-agent coordination remains entirely deterministic while still exhibiting dynamic, interdependent behavior.\nBy removing all stochastic LLM dependencies, the framework guarantees reproducibility while maintaining realistic, explainable agent interactions.\nThese findings validate that Trust Bench can serve as a dependable foundation for offline software evaluation, cybersecurity auditing, and AI system benchmarking where traceability and repeatability are paramount.\n\nResults\n\nThe experimental results demonstrate that Trust Bench (Project2v2) successfully achieves its design goals of reproducibility, transparency, and collaborative agent evaluation.\nAcross all test scenarios—seeded repositories, mixed-language projects, and repeated offline runs—the system maintained deterministic behavior while accurately modeling inter-agent dependencies between security, quality, and documentation assessments.\n\n1. Reproducibility and Determinism\n\nAll repeated executions of the same repository produced identical outputs in both report.json and report.md.\nThis confirms that the framework’s LangGraph orchestration and toolchain operate deterministically, unaffected by randomness or external API variability.\nByte-for-byte identical results were verified using checksum comparisons, proving the system’s reproducibility across machines and operating systems.\n\nKey Findings:\n\nReports generated on Windows 10 and Windows 11 matched exactly.\nNo variance in scoring or metric values across repeated runs.\nTool outputs remained consistent regardless of execution order or environment.\n2. Multi-Agent Collaboration\n\nThe agent communication design performed as intended, with clear and traceable cause-and-effect relationships between domains:\n\nAgent\tTrigger\tEffect\nSecurityAgent\tDetected seeded secrets\tPenalized Quality and Documentation scores\nQualityAgent\tMeasured low test coverage\tDecreased DocumentationAgent clarity rating\nDocumentationAgent\tAdjusted content evaluation\tReflected project readiness and security awareness\n\nThis interaction demonstrated a realistic simulation of organizational code evaluation, where weaknesses in one area directly influence confidence in others.\nCollaboration logs captured in shared memory validated that agents exchanged structured messages and updated their reasoning accordingly.\n\n3. Quantitative Evaluation Metrics\nMetric\tDescription\tResult\nSystem Latency\tTotal wall-clock runtime for one full audit\t0.08 seconds\nFaithfulness\tAgreement between agent summaries and underlying tool evidence\t0.62\nRefusal Accuracy\tSimulated safety test score for prompt-injection handling\t1.0\n\nThese values confirm the correct operation of the instrumentation layer:\n\nLatency is consistent with lightweight deterministic tooling.\nFaithfulness remains within expected heuristic range.\nRefusal Accuracy indicates robust input sanitization and ethical handling logic.\n4. Overall System Scoring\n\nComposite project score from the self-audit:\n\nOverall: ~32/100 (Needs Attention)\nSecurity: 0 (seeded secrets detected)\nQuality: Moderate (penalized due to security alerts)\nDocumentation: Above average, reduced for missing test/security coverage references\n\nThese results align perfectly with the intended evaluation logic—proof that cross-agent penalty propagation works as designed.\n\n5. Observations\nDeterministic orchestration allows verifiable reproducibility, a core goal for secure evaluation frameworks.\nAgent collaboration produces meaningful context-aware scoring, linking technical issues to documentation and quality.\nSecurity detections drive cascading penalties, reinforcing real-world accountability.\nThe instrumentation layer provides measurable, quantitative insight into agent behavior.\n6. Implications\n\nThe experiments and results position Trust Bench as a foundation for auditable multi-agent evaluation systems.\nIt bridges the gap between symbolic rule-based reasoning and multi-agent coordination—achieving transparency without stochastic uncertainty.\nFuture expansions such as semantic faithfulness scoring and dashboard visualizations can further enhance interpretability and user insight, while the current deterministic core ensures a solid, reliable baseline for reproducible AI system evaluation.\n\nConclusion\n\nTrust Bench (Project2v2) demonstrates that deterministic, multi-agent systems can deliver transparent, reproducible, and explainable software evaluation—entirely offline and without reliance on external APIs or stochastic models. By orchestrating specialized agents through LangGraph and enforcing strict determinism at every stage, Trust Bench provides a robust foundation for secure code auditing, quality assessment, and documentation review.\n\nThe framework’s reproducible outputs, traceable agent collaboration, and comprehensive instrumentation make it suitable for high-assurance environments where trust, auditability, and repeatability are paramount. As software evaluation increasingly demands both automation and transparency, Trust Bench offers a practical blueprint for building agentic systems that are as reliable as they are insightful.\n\nFuture work may extend Trust Bench with semantic analysis, richer reporting, and integration with broader AI safety benchmarks—while maintaining its core commitment to deterministic, auditable evaluation.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nMethodology\n\nPlanning Phase\nExecution Phase\nCollaboration Phase\nReporting Phase\n\nSummary\n\nExperiments\n\nExperiment Setup\nView all\nComments\nCode\nFiles\nTrustBench Video.mp4\nreport.json\nreport.md\nUSER_GUIDE.md\nNew Compressed (zipped) Folder.zip",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "uniquestai-your-ultimate-ai-companion-to-discover-university-admissions-worldwide-P2Xnw8MhQZPV",
    "username": "aAmina Javaid",
    "license": "MIT License",
    "title": "UniQuestAI - Your Ultimate AI Companion to Discover University Admissions Worldwide",
    "publication_description": "Back to publications\nMar 29, 2025\n●\n46 reads\n●\nMIT License\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nUniQuestAI - Your Ultimate AI Companion to Discover University Admissions Worldwide\nAgenticAI\nAgents\nAISolution\nArtificialIntelligence\nHigherEducation\nLLMs\nTools\nA\nAmina Javaid\nLike\nBookmark\nShare\n\nAbstract\n\nStudying abroad is a dream of many individuals all around the world. It provides exposure, enhances one's personality, and fosters cultural interaction, opening doors to new experiences and academic excellence. It is a great opportunity to learn from people with diverse backgrounds. But where to start from? Finding and selecting the right university is not an easy task. Many students struggle with university admissions research, specifically while searching for universities abroad. They face multiple challenges, from navigating different university ranking websites to manually extracting admission details from official university pages while exploring their programs of interest. UniQuestAI utilizes the power of Agentic AI to automate and simplify the university search process tailored to each student's preferences. Not just finding universities, it brings and compiles all the essential information required for admissions and applications to those universities. It addresses a real-world problem with Agentic AI, transforming the process of university admissions research, making it faster, more efficient, and accessible for students worldwide.\n\nIntroduction\n\nThe world has become a global village, thanks to the widespread availability and connectivity of the internet. Access to information has made it possible for anyone, anywhere, to explore educational opportunities across the globe. With hundreds of thousands of universities offering diverse programs, students have endless possibilities to pursue their academic goals. While university websites provide detailed information about their programs, navigating through them to find relevant details is a daunting task.\nUniversity ranking websites, such as US News Rankings, QS Rankings, and Times Higher Education Rankings, help narrow down choices based on university reputation and subject rankings. However, even after shortlisting universities, students must manually visit each university's official website to gather crucial admission-related details - ranging from program requirements and application deadlines to standardized test scores and tuition fees. This process can take weeks or even months, making university admissions research overwhelming and time-consuming.\nThis is where UniQuestAI comes in. It is an AI-powered multi-agent system designed to automate and streamline the entire university search and admissions research process. By intelligently fetching and compiling accurate, up-to-date admission details, UniQuestAI saves students countless hours of manual work - allowing them to focus on what truly matters - preparing strong applications and securing their dream university.\n\nTechnologies Used\nStreamlit - Frontend: Streamlit is an open-source Python library to build minimalist user interfaces for web applications. It helps build interactive data apps quickly.\nFastAPI - Backend: FastAPI is a modern, high performance, web framework for building APIs with Python. \nAgno - Agentic AI Framework: Agno is an open-source framework to build high performance multimodal agentic systems with memory, knowledge, and tools.\nUser Inputs\nDiscipline (Field of Study)\nEducation Level (Bachelors | Masters | PhD)\nLocation (Country, Default: Worldwide)\nMaximum Number of Universities\nOutput\n\nUniversities information organized in an Excel format, including:\n\nGeneral Information\nUniversity Name\nLocation (City, Country)\nStatus (Public | Private)\nFounded (Year)\nRankings\nUS News Ranking\nQS Ranking\nTimes Higher Education Ranking\nProgram Details\nProgram (Bachelors, Masters, PhD)\nProgram Begins (Month, Year)\nGRE Requirement\nMinimum TOEFL Score\nMinimum IELTS score\nDuolingo Accepted? (Yes/No)\nEnglish Proficiency Waiver Availability\nApplication & Fees\nApplication Fee ($)\nApplication Fee Waiver Availability\nTuition | Stipend Details\nImportant Dates\nApplication Opens\nApplication Deadline\nAdditional Information\nOffice Contact and Email (Graduate/Undergraduate Admissions)\nAcceptance Rate\nRequired Documents\nNumber of Letters of Recommendation (LoRs)\nAdditional Notes\nProgram Link\n\nAll the above information is essential for the applicants to effectively prepare and submit their applications for different universities.\n\nSystem Design\n\nUniQuestAI is a multi-agent collaborative system where four specialized agents work together in a structured, agentic workflow. Each agent is designed with a specific goal, ensuring it operates in a focused and efficient manner without unnecessary distractions. The workflow is sequential, meaning the output of one agent serves as the input for the next, enabling a smooth and systematic execution of tasks.\nHere is a high level architectural diagram of this multi-agent collaborative system:\n\nAgents\n1. University Search Agent\n\nThis agent searches a list of top universities based on their rankings from the world university ranking websites (US News Ranking, QS Ranking, and Times Higher Education Ranking) based on the following four user preferences:\n\nDiscipline\nEducation level\nLocation\nMaximum universities\nLLM\n\nIt uses Llama 3.3 70B Instruct Turbo Free from Together API as a large language model.\n\nTools\n\nIt uses GoogleSearchTools in the Agno Toolkit to search for the top universities from ranking websites.\n\n2. Program Search Agent\n\nThis agent searches for the links to admission requirements and program information pages on official university websites for the universities shortlisted by the University Search Agent. It makes sure that the links are authentic, latest, and are according to the user preferences.\n\nLLM\n\nIt uses Llama 3.3 70B Instruct Turbo Free from Together API as a large language model.\n\nTools\n\nIt uses GoogleSearchTools in the Agno Toolkit to search for the required links from official university websites.\n\n3. Information Extraction Agent\n\nThis agent searches and extracts real time information from the admissions and program specific page links found for each university by the Program Search Agent.\n\nLLM\n\nIt uses Llama 3.3 70B Instruct Turbo Free from Together API as a large language model.\n\nTools\n\nIt uses WebsiteTools from the Agno Toolkit to crawl and extract information from the admission requirements and program information pages on an official university website.\n\n4. Information Processing Agent\n\nThis agent organizes and processes the information extracted by the Information Extraction Agent, and converts it into JSON format for displaying at the frontend and saving into an Excel file.\n\nLLM\n\nIt uses Llama 3.3 70B Instruct Turbo Free from Together API as a large language model.\n\nImplementation\n\nWe'll use the Agno Framework to build this multi-agent collaborative system. The project structure is as follows:\n\nProject Structure\nuniquest-ai/                    # Main project folder\n│── frontend/                    # Frontend directory\n│   ├── app.py                   # Streamlit application\n│\n│── backend/                     # Backend directory\n│   ├── main.py                   # FastAPI backend\n│   ├── utils.py                  # Utility functions\n│\n│   ├── agents/                   # Agents folder\n│   │   ├── university_search_agent.py\n│   │   ├── program_search_agent.py\n│   │   ├── information_extraction_agent.py\n│   │   ├── information_processing_agent.py\n│   │\n│   ├── workflows/                # Workflows folder\n│   │   ├── uniquest_workflow.py\n│\n│── requirements.txt              # Dependencies\n│── .env                          # Environment variables\n│── LICENSE                       # License file\n│── README.md\nSetting up the environment\n\nCreate a new virtual environment and install the dependencies using requirements.txt file.\n\nrequirements.txt\nfastapi\nuvicorn\nstreamlit\npandas\nopenpyxl\nagno\nrequests\nbeautifulsoup4\ngooglesearch-python\npycountry\nsqlalchemy\n.env\n\nCreate a .env file and add your API key.\n\nTOGETHER_API_KEY=\"your_api_key_here\"\nFRONTEND\n\nThe frontend of UniQuestAI looks like this:\n\napp.py\nThis is the main interface of the application built using Streamlit. It provides the user input fields in the sidebar at the left. The list of top universities would be displayed at the right in a tabular format along with their detailed information as shown below.\n\nimport streamlit as st\nimport requests\nimport json\nimport pandas as pd\nimport os\n\nAPI_URL = \"http://127.0.0.1:8000\"\n\nst.set_page_config(page_title=\"UniQuestAI\", layout=\"wide\")\n\n# Inject custom CSS to set the width of the sidebar\nst.markdown(\n    \"\"\"\n    <style>\n        section[data-testid=\"stSidebar\"] {\n            width: 500px !important;\n        }\n    </style>\n    \"\"\",\n    unsafe_allow_html=True,\n)\n\n# Initialize session state for results\nif \"result_json\" not in st.session_state:\n    st.session_state.result_json = None\n\n# Initialize session state for research topic\nif \"discipline\" not in st.session_state:\n    st.session_state.discipline = \"\"\n\n# Initialize session state for max papers\nif \"education_level\" not in st.session_state:\n    st.session_state.education_level = \"\"\n\n# Initialize session state for selected paper\nif \"location\" not in st.session_state:\n    st.session_state.location = \"Worldwide\"\n\n# Initialize session state for chat history\nif \"max_universities\" not in st.session_state:\n    st.session_state.max_universities = 10\n\n# Create a two-column layout\ncol1, col2 =st.columns([1, 11])\nwith col1:\n    st.image(\"../images/uniquest_logo.png\", width=150)\nwith col2:\n    st.title(\"UniQuestAI\")\n\n# Display the main title and search form in the sidebar  \nwith st.sidebar:\n    st.subheader(\"Search the best unviversities\")\n    st.info(\"Enter your preferences to get started.\")\n    \n    # User input fields\n    discipline = st.text_input(\"Enter Discipline:\")\n    education_level = st.selectbox(\"Select Education Level:\", [\"Bachelors\", \"Masters\", \"PhD\"])\n    location = st.text_input(\"Enter Location:\", \"Worldwide\")\n    max_universities = st.number_input(\"Number of Universities:\", 5, 50, 10)\n\n    col_btn1, col_btn2 = st.columns([3, 1])\n\n    with col_btn1:\n        if st.button(\"Search\"):\n            if discipline:\n                st.session_state.discipline = discipline\n                st.session_state.education_level = education_level\n                st.session_state.location = location\n                st.session_state.max_universities = max_universities\n\n                try:\n                    with st.spinner(\"Searching for top universities...\"):\n                        st.warning(\"It may take a few minutes for the search to complete. Please be patient!\")\n                        response = requests.post(API_URL+\"/search/\", json={\"discipline\": discipline, \"education_level\": education_level, \"location\": location, \"max_universities\": max_universities})\n\n                    if response.status_code != 200:\n                        st.error(\"Error: Invalid response from server.\")\n                        st.stop()\n\n                    result_decoded = response.content.decode(\"utf-8\")\n                    st.session_state.result_json = json.loads(result_decoded)  # Store in session state\n\n                except json.JSONDecodeError:\n                    st.error(\"Error: Received an unreadable response from the server.\")\n                    st.stop()\n                except requests.exceptions.RequestException:\n                    st.error(\"Error: Could not connect to the server.\")\n                    st.stop()\n\n    with col_btn2:\n        if st.button(\"🔄 Refresh\"):\n            st.session_state.result_json = None  # Clear stored results\n            st.session_state.discipline = \"\"  # Clear stored discipline\n            st.session_state.education_level = \"\"  # Clear stored education level\n            st.session_state.location = \"Worldwide\"  # Clear stored location\n            st.session_state.max_universities = 10  # Reset max universities\n            st.rerun()  # Force a full page refresh\n            \n# Display results if available\nif st.session_state.result_json:\n    st.title(f\"Top Universities for {discipline} ({education_level}) in {location if location else 'the World'}\")\n    # Display the results\n    # st.write(st.session_state.result_json.get(\"response\", \"No results to display.\"))\n\n    # If an excel path is provided in the response, display download option\n    if \"excel_path\" in st.session_state.result_json:\n        excel_path = st.session_state.result_json[\"excel_path\"]\n        excel_path = os.path.join(\"../backend\", excel_path)\n\n        filename = f\"Top_Universities_{education_level}_{discipline}_{location}.xlsx\"\n\n        # Create a row layout for success message + download button\n        col_success, col_download = st.columns([11, 1])\n\n        with col_success:\n            st.success(\"Top Universities Found!\")\n\n        with col_download:\n            st.download_button(label=\"📥 Download\", \n                            data=open(excel_path, \"rb\").read(), \n                            file_name=filename,\n                            mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n\n    # Display universities information\n    response = st.session_state.result_json.get(\"response\")\n\n    processed_info = response[\"processed_info\"]\n\n    json_info = json.loads(processed_info)\n\n    universities = json_info.get(\"universities\", [])  \n\n    if universities:\n        # Convert the list of university dictionaries into a Pandas DataFrame\n        df = pd.DataFrame(universities)\n        df.rename(columns={\"name\":\"University Name\", \"location\":\"Location\", \"status\":\"Status\", \"founded\":\"Founded\", \n                           \"us_news_ranking\":\"US News Ranking\", \"qs_ranking\":\"QS Ranking\", \"times_ranking\":\"THE Ranking\", \n                           \"program\":\"Program\", \"program_begins\":\"Program Begins\", \"gre_requirement\":\"GRE Requirement\", \n                           \"toefl_score\":\"TOEFL Score\", \"ielts_score\":\"IELTS Score\", \"duolingo_accepted\":\"Duolingo Accepted?\",\n                           \"english_proficiency_waiver\":\"English Proficiency Waiver\", \"application_fee\":\"Application Fee\", \n                           \"application_deadline\":\"Application Deadline\", \"office_contact_and_email\":\"Office Contact & Email\",\n                           \"acceptance_rate\":\"Acceptance Rate\", \"required_documents\":\"Required Documents\",\n                           \"number_of_lors\":\"Number of LORs\", \"additional_notes\":\"Additional Notes\", \"link\":\"Program Link\"}, inplace=True,)\n        st.dataframe(df)\n\n        for i, university in enumerate(universities):\n            st.subheader(f\"🎓 {i+1}. {university['name']}\")\n            st.markdown(f\"**📍 Location:** {university['location']}\")\n            st.markdown(f\"**📚 Status:** {university['status']}\")\n            st.markdown(f\"**📆 Founded:** {university['founded']}\")\n            st.markdown(f\"**📈 US News Ranking:** {university['us_news_ranking']}\")\n            st.markdown(f\"**📈 QS Ranking:** {university['qs_ranking']}\")\n            st.markdown(f\"**📈 Times Higher Education Ranking:** {university['times_ranking']}\")\n            st.markdown(f\"**📚 Program:** {university['program']}\")\n            st.markdown(f\"**📅 Program Begins:** {university['program_begins']}\")\n            st.markdown(f\"**📝 GRE Requirement:** {university['gre_requirement']}\")\n            st.markdown(f\"**📝 TOEFL Score:** {university['toefl_score']}\")\n            st.markdown(f\"**📝 IELTS Score:** {university['ielts_score']}\")\n            st.markdown(f\"**📝 Duolingo Accepted?:** {university['duolingo_accepted']}\")\n            st.markdown(f\"**📝 English Proficiency Waiver:** {university['english_proficiency_waiver']}\")\n            st.markdown(f\"**💸 Application Fee:** {university['application_fee']}\")\n            st.markdown(f\"**💸 Application Fee Waiver:** {university['application_fee_waiver']}\")\n            st.markdown(f\"**💸 Tuition|Stipend:** {university['tuition_stipend']}\")\n            st.markdown(f\"**📅 Application Opens:** {university['application_opens']}\")\n            st.markdown(f\"**📅 Application Deadline:** {university['application_deadline']}\")\n            st.markdown(f\"**📞 Office Contact and Email:** {university['office_contact_and_email']}\")\n            st.markdown(f\"**📝 Acceptance Rate:** {university['acceptance_rate']}\")\n            st.markdown(f\"**📝 Required Documents:** {university['required_documents']}\")\n            st.markdown(f\"**📝 Number of LoRs:** {university['number_of_lors']}\")\n            st.markdown(f\"**📝 Additional Notes:** {university['additional_notes']}\")\n            st.markdown(f\"**📝 Link to the Program:** {university['link']}\")\n            st.divider()             \n    else:\n        st.error(\"No universities found!\")\nelse:\n    st.subheader(\"Your ultimate AI companion for finding the best universities around the world!\")\n    st.success(\"**Welcome to UniQuestAI**, your trusted AI companion for finding the best universities around the world!\")\n    st.markdown(\"- Here, you can search for universities based on your **education level**, **discipline**, and **preferred location**.\")\n    st.markdown(\"- Our AI-powered platform will help you find the top universities that match your preferences and requirements.\")\n    st.markdown(\"- Not only that, we'll compile a list of universities for you, including their names, locations, admission requirements, and other relevant information.\")\n    st.info(\"**So, what are you waiting for? Let's get started and find your dream university today!**\")\n\n    todo = [\"📚 Enter a Discipline\", \"🎓 Select an Education Level\", \"📌 Enter a Location\", \"#️⃣ Enter Number of Universities\", \"🔍 Click Search\"]\n    st.markdown(\"\\n\".join([f\"##### {i+1}. {item}\" for i, item in enumerate(todo)]))\nBACKEND\n\nmain.py\n\nsearch- This endpoint searches for the top universities through Uniquest workflow and saves the response in an Excel file.\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nfrom agno.agent import RunResponse\nfrom workflows.uniquest_workflow import uniquest_workflow\nfrom utils import save_info_to_excel\n\napp = FastAPI()\n\nclass SearchRequest(BaseModel):\n    discipline: str\n    education_level: str\n    location: str\n    max_universities: int = 5\n\n@app.post(\"/search/\")\nasync def search_universities(request: SearchRequest):\n    # Execute the workflow\n    # Returns a RunResponse object containing the generated content\n    response: RunResponse = uniquest_workflow.run(discipline=request.discipline, education_level=request.education_level, location=request.location, max_universities=request.max_universities)\n\n    if response:\n        print(\"Response:\", response.content)\n         \n        # Save the universities info to an Excel file\n        excel_path = save_info_to_excel(response.content[\"processed_info\"], request.discipline, request.education_level, request.location)\n        return {\"message\": \"Top universities found!\", \"response\": response.content, \"excel_path\": excel_path}\n    else:\n        return {\"error\": \"No universities found\"}\nutils.py\n\nThis file contains a helper function to save universities information into an Excel file.\n\nimport json\nimport pandas as pd\nimport os\n\ndef save_info_to_excel(json_data, discipline, education_level, location):\n    \"\"\"\n    Save information about universities to an Excel file.\"\n\n    Args:\n        json_data (dict): A dictionary containing information about universities.\n        discipline (str): The discipline searched for.\n        education_level (str): The education level searched for.\n        location (str): The location searched for.\n\n    Returns:\n        str: The name of the saved Excel file.\n    \"\"\"\n\n    save_dir = \"excel_files\"\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    file_path = os.path.join(save_dir, f\"Top_Universities_{education_level}_{discipline}_{location}.xlsx\")\n\n    # Ensure json_data is a dictionary\n    if isinstance(json_data, str):\n        data = json.loads(json_data)\n    else:\n        data = json_data\n\n    # Convert to a DataFrame\n    df = pd.DataFrame(data[\"universities\"])\n    \n    # Save to an Excel file\n    df.to_excel(file_path, index=False)\n    \n    print(f\"Information saved successfully to {file_path}\")\n    return file_path\nuniversity_search_agent.py\n\nThis is the first agent in the workflow. It searches for the top universities based on user preferences from university ranking websites using GoogleSearchTools.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.tools.googlesearch import GoogleSearchTools\nfrom agno.utils.pprint import pprint_run_response\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the university search agent\nuniversity_search_agent = Agent(\n    name=\"university-search-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\")), \n    tools=[GoogleSearchTools()],\n    description=\n    '''\n        You are a university search agent that will search for the top universities based on a given discipline from top university ranking websites.\n        You will generate a list of universities with their names, rankings, and official website links based on the given preferences.\n    ''',\n    instructions=[\n        \"Search the following university ranking websites one by one based on the given preferences:\",\n        \"Preferences include discipline, education level, location, and maximum number of universities.\",\n            \"US News Rankings: https://www.usnews.com\",\n            \"QS Rankings: https://www.topuniversities.com\",\n            \"Times Higher Education Rankings: https://www.timeshighereducation.com\",\n        \"If the location is not specified, consider all locations.\",\n        \"Search for the best universities for the given discipline and education level from each ranking website.\",\n        \"The rankings MUST be according to the latest information available on the ranking websites\",\n        \"Select the top ranked universities from the extracted information from each ranking website.\",\n        \"The response MUST be in the following JSON format:\",\n            \"{\",\n                '\"universities\": [',\n                    '\"https://university1.com\",',\n                    '\"https://university2.com\",',\n                    '\"https://university3.com\",',\n                    '\"https://university4.com\",',\n                    '\"https://university5.com\",',\n                \"]\",\n                '\"discipline\": [DISCIPLINE],',\n                '\"education_level\": [EDUCATION_LEVEL],',\n            \"}\",\n        \"The response MUST NOT include ranking websites as universities.\",\n        \"The response MUST only include top universities found from the ranking websites.\",\n        \"The response MUST be in proper JSON format with keys and values in double quotes.\",\n        \"The final response MUST not include any other text or anything else other than the JSON response.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\nprogram_search_agent.py\n\nThis is the second agent in the workflow which searches for the admission and program page links for each university found by the university search agent using GoogleSearchTools.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.tools.googlesearch import GoogleSearchTools\nfrom agno.utils.pprint import pprint_run_response\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the link browsing agent\nprogram_search_agent = Agent(\n    name=\"program-search-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\")), \n    tools=[GoogleSearchTools()],\n    description=\n    '''\n        You are a program search agent that will search for the admission and program page links for a given list of universities based on the given preferences.\n        You will generate a list of universities with their admission and program page links in JSON format.\n    ''',\n    instructions=[\n        \"Search the given university websites one by one based on the given preferences\",\n        \"For each university, search for the admission page link based on the given education level.\",\n        \"For each university, search for the program page link based on the given discipline and education level.\",\n        \"Do NOT generate links. Instead, search them directly from the official university website.\",\n        \"The links MUST depict the latest admissions and program information available on the university website.\",\n        \"Do NOT retrieve links having old or outdated admissions and program information.\",\n        \"If you don't find a specific program in a university, search for the most similar or closest related program page link.\",\n        \"Validate the links by ensuring they do NOT return 404 errors or redirects to unrelated pages.\",\n        \"Double-check that the links are correct and functional.\",\n        \"The response MUST be in the following JSON format:\",\n        \"{\"\n        \"    'links': [\",\n        \"       {\",\n        \"           'university': '[Official website link]',\",\n        \"           'admission': '[Admission page link]',\",\n        \"           'program': '[Program page link]'\",\n        \"       }\",\n        \"       {...}\",\n        \"    ]\",\n        \"}\",\n        \"The response MUST be in proper JSON format with keys and values in double quotes.\",\n        \"The final response MUST not include any other text or anything else other than the JSON response.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\ninformation_extraction_agent.py\n\nThis is the third agent in the Uniquest workflow which works in a loop. The workflow calls this agent iteratively to extract information for each link found through the Program Search Agent. This agent uses WebsiteTools to extract the content from each link.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.tools.website import WebsiteTools\nfrom agno.utils.pprint import pprint_run_response\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the information extraction agent\ninfo_extraction_agent = Agent(\n    name=\"info-extraction-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\"), max_tokens=500), \n    tools=[WebsiteTools()],\n    description=\n    '''\n        You are an information extraction agent that will search and extract information from a given link.\n    ''',\n    instructions=[\n        \"Search the given link for admissions and program related information:\",\n        \"Find whichever of the below information is available in the given link.\",\n            \"University Name: [Name of the University]\",\n            \"Location: [City, Country]\",\n            \"Public/Private: [Public/Private]\",\n            \"Founded (Year): [Year of Foundation]\",\n            \"US News Ranking: [Ranking by US News]\",\n            \"QS Ranking: [Ranking by QS]\",\n            \"Times Higher Education Ranking: [Ranking by Times Higher Education]\",\n            \"Program: [Program Name]\",\n            \"Program Begins: [Month and Year]\",\n            \"GRE Requirement: [GRE Required/Not Required]\",\n            \"Min TOEFL Score: [Min TOEFL Score]\",\n            \"Min IELTS Score: [Min IELTS Score]\",\n            \"Duolingo Accepted?: [Yes/No]\",\n            \"English Proficiency Waiver: [Waiver Available/Not Available]\",\n            \"Application Fee: [Application Fee]\",\n            \"Application Fee Waiver: [Waiver Available/Not Available]\",\n            \"Tuition|Stipend: [Tuition|Stipend Available/Not Available]\",\n            \"Application Opens: [Month and Year]\",\n            \"Application Deadline: [Deadline]\",\n            \"Office Contact and Email (Graduate | Undergraduate): [Contact and Email]\",\n            \"Acceptance Rate: [Acceptance Rate]\",\n            \"Required Documents: [List of Required Documents]\",\n            \"Number of LoRs: [Number of LoRs]\",\n            \"Additional Notes (if any): [Notes of any additional important information]\",\n            \"Link to the Program: [Link to the Program - MUST be a correct link to the program page]\",\n        \"Do NOT generate information. Only extract information from the given link.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\ninformation_processing_agent.py\n\nThis is the fourth and the last agent in the workflow which processes the information extracted by the Information Extraction Agent into JSON format to be displayed at frontend and saved as an Excel file for downloading.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.utils.pprint import pprint_run_response\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the information processing agent\ninfo_processing_agent = Agent(\n    name=\"info-search-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\")),\n    description=\n    '''\n        You are an information processing agent that structures the information about universities' admissions and programs in JSON format. \n    ''',\n    instructions=[\n        \"Structure the information received about universities' admissions and programs in JSON format.\",\n        \"Important: Your final response MUST be in JSON format with the following structure:\",\n        \"{\"\n            \"universities: [\"\n                \"{\"\n                \"   'name': 'Name of the University',\"\n                \"   'location': 'Location[City, Country] of the University',\"\n                \"   'status': 'Public or Private',\"\n                \"   'founded': 'Year of Foundation',\"\n                \"   'us_news_ranking': 'Ranking by US News',\"\n                \"   'qs_ranking': 'Ranking by QS',\"\n                \"   'times_ranking': 'Ranking by Times Higher Education',\"\n                \"   'program': 'Program Name',\"\n                \"   'program_begins': 'Month and Year',\"\n                \"   'gre_requirement': 'GRE Required/Not Required',\"\n                \"   'toefl_score': 'Min TOEFL Score',\"\n                \"   'ielts_score': 'Min IELTS Score',\"\n                \"   'duolingo_accepted': 'Yes/No',\"\n                \"   'english_proficiency_waiver': 'Waiver Available/Not Available',\"\n                \"   'application_fee': 'Application Fee',\"\n                \"   'application_fee_waiver': 'Waiver Available/Not Available',\"\n                \"   'tuition_stipend': 'Tuition/Stipend Available/Not Available',\"\n                \"   'application_opens': 'Month and Year',\"\n                \"   'application_deadline': 'Deadline',\"\n                \"   'office_contact_and_email': 'Contact and Email',\"\n                \"   'acceptance_rate': 'Acceptance Rate',\"\n                \"   'required_documents': 'List of Required Documents',\"\n                \"   'number_of_lors': 'Number of LoRs',\"\n                \"   'additional_notes': 'Notes of any additional important information',\"\n                \"   'link': 'Link to the Program'\"\n                \"},\"\n                \"{...}\"\n            \"]\"\n        \"}\",\n         \"The response MUST be in proper JSON format with keys and values in double quotes.\",\n        \"The final response MUST not include anything else other than the JSON response.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\nuniquest_workflow.py\n\nThis is the main workflow of the agentic AI application. It calls the agents one by one feeding the output of one into the input of the other agent to achieve its end goal.\n\nfrom agno.agent import Agent\nfrom agno.workflow import Workflow, RunResponse, RunEvent\nfrom agno.storage.workflow.sqlite import SqliteWorkflowStorage\nfrom agno.utils.pprint import pprint_run_response\nfrom agno.utils.log import logger\nimport json\n\nfrom agents.university_search_agent import university_search_agent\nfrom agents.program_search_agent import program_search_agent\nfrom agents.information_extraction_agent import info_extraction_agent\nfrom agents.information_processing_agent import info_processing_agent\n\nclass UniquestWorkflow(Workflow):\n    university_search_agent: Agent = university_search_agent\n    program_search_agent: Agent = program_search_agent\n    info_extraction_agent: Agent = info_extraction_agent\n    info_processing_agent: Agent = info_processing_agent\n\n    def run(self, discipline: str, education_level: str, location: str, max_universities: int = 5) -> RunResponse:\n        logger.info(f\"Generating a list of top {max_universities} universities for a {education_level} in {discipline}\")\n\n        # Step 1: Generate a list of top universities for the given education level, discipline and location\n        prompt = f\"Search for {max_universities} top universities for a {education_level} in {discipline} based in {location}.\"\n        top_universities: RunResponse = self.university_search_agent.run(prompt)\n        # If no universities are found, end the workflow\n        if top_universities is None:\n            return RunResponse(\n                event=RunEvent.workflow_completed,\n                content=f\"Sorry, could not find any universities for {education_level} in {discipline}\",\n            )\n       \n        print(\"Top universities:\", top_universities.content)\n\n        # Step 2: Find the admission and program links for each university\n        prompt = f\"Search for the admission and program page links for a {education_level} in {discipline} for the following universities: {top_universities.content}\"\n        links: RunResponse = self.program_search_agent.run(prompt)\n        if links is None:\n            return RunResponse(\n                event=RunEvent.workflow_completed,\n                content=\"Sorry, could not find the admission and program links for the universities.\",\n            )\n        \n        print(\"Links:\", links.content)\n       \n        links_content = json.loads(links.content)\n        links = links_content[\"links\"]\n\n        # Step 3: Extract the information from each admission and program link for each university\n        information = []\n        for link in links:\n            logger.info(f\"Extracting information for {link['university']}\")\n\n            # Initialize with default values\n            admission_info_content = \"No information found\"\n            program_info_content = \"No information found\"\n\n            try:\n                prompt = f\"Search for the admission and program related information of {link[\"university\"]} from the following link: {link['admission']}.\"\n                admission_info: RunResponse = self.info_extraction_agent.run(prompt)\n                admission_info_content = admission_info.content\n            except Exception as e:\n                logger.error(f\"Error extracting admission information for {link['university']}: {e}\")\n            \n            try:\n                prompt = f\"Search for the admission and program related information of {link['university']} from the following link: {link['program']}.\"\n                program_info: RunResponse = self.info_extraction_agent.run(prompt)\n                program_info_content = program_info.content\n            except Exception as e:\n                logger.error(f\"Error extracting program information for {link['university']}: {e}\")\n\n            information.append({\"university\":link[\"university\"], \"admission_info\": admission_info_content, \"program_info\": program_info_content})\n\n        print(\"Information:\", information)\n\n        # Step 4: Process the information and structure it in JSON format.\n        prompt = f\"Process the following information: {information}\"\n        processed_info: RunResponse = info_processing_agent.run(prompt)\n        if processed_info is None:\n            return RunResponse(\n                event=RunEvent.workflow_completed,\n                content=\"Sorry, could not process the information.\",\n            )\n\n        print(\"Processed information:\", processed_info.content)\n\n        return RunResponse(\n            event=RunEvent.workflow_completed,\n            content={\"universities\": top_universities.content, \"links\": links, \"information\": information, \"processed_info\": processed_info.content},\n        )\n    \n# Initialize the Uniquest workflow\nuniquest_workflow = UniquestWorkflow(\n        session_id=\"find-top-universities\",\n        storage=SqliteWorkflowStorage(\n            table_name=\"generate_uniquest_workflows\",\n            db_file=\"workflows/db/workflows.db\",\n        ),\n    )\nDEMO - UniQuestAI\n\nHere is a demonstration of using UniQuestAI:\nNote: The whole agentic workflow takes a few minutes to complete, therefore, the demo only shows the output generated after the search has been completed.\n\nRunning UniQuestAI\nClone the repository from GitHub. The link has been provided in the References section.\nNavigate to the backend folder and start the FastAPI server:\ncd backend\nuvicorn main:app --reload\nNavigate to the frontend folder and start the Streamlit application:\ncd frontend\nstreamlit run app.py\n\nAccess the application through the following link.\n\nhttp://localhost:8501\nChallenges Faced\n1. Selecting Top Universities\nChallenge - University ranking websites have strict protocols due to which we cannot directly scrape information from them. Many of these sites actively block automated requests. It is also ethically not correct to scrape information without the consent of these websites. This posed a challenge to extract the top universities ranked in specific disciplines.\nSoluion - We used GoogleSearchTools available in the Agno toolkit. This allowed us to retrieve ranking data from top university ranking websites without violating ethical guidelines.\n2. Finding Correct Programs Links\nChallenge - Scanning entire university websites for admissions and program details is impractical and inefficient. We needed a way to find relevant pages without extracting unnecessary data.\nSolution - Instead of scraping the entire websites, we focused on finding and extracting URLs specifically related to admissions and program details. GoogleSearchTools from the Agno Toolkit helped us efficiently retrieved these targeted links, significantly improving the accuracy and efficiency of data extraction.\n3. Verification of Links\nChallenge - Large language models (LLMs) tend to hallucinate and generate incorrect links.\nSolution -To prevent this, we explicitly instructed the LLMs not to generate links themselves. Additionally, we instructed them to perform link validation checks to ensure that all retrieved URLs are correct and functional.\n4. Token Usage Limit \nChallenge - The API we use has a strict token limit, making it impossible to extract all information simultaneously.\nSolution: To address this, we optimized our system by iteratively processing links - extracting information one link at a time and then aggregating the results before feeding them into the information processing agent. This approach allowed us to stay within the token limits while maintaining efficiency.\n5. Model and Tool Selection\nChallenge - Selection of the right models and tools was critical for this application. We couldn't use paid models because of cost constraints. Free models often come with token limitations. Additionally, extracting structured content from websites required specialized tools.\nSolution - After trying and testing various APIs available in the Agno framework, we selected Together API for LLM-based tasks, as it provided the best balance between performance and token usage. For content extraction, we utilized WebsiteTools from the Agno framework, which effectively handled data retrieval from university websites.\n6. Ensuring Correct Information\nChallenge - LLMs are not always reliable in retrieving factual, real-time data. We needed to minimize errors and prevent hallucinations while extracting university admission details.\nSolution - We implemented Reflection and ReAct patterns to guide the LLMs in verifying and cross-checking extracted data before presenting it. These techniques helped ensure that the information retrieved was accurate, up-to-date, and directly sourced from university websites.\nImportant Considerations\nIn a multi-agent system, each agent should remain focused on a single task to maximize efficiency and avoid unnecessary complexity.\nHuman-in-the-loop is essential - while UniQuestAI streamlines the search process, applicants are strongly advised to visit the official university websites and verify the information from the provided links before making any decisions.\nEffective prompt engineering is crucial for optimizing agent performance. Clearly defining an agent's role, description, and instructions ensures accurate and relevant outputs.\nFuture Work\nCurrently, UniQuestAI retrieves top-ranked universities based on user preferences. Future iterations can introduce more options to further narrow down the searches based on specific needs.\nSupport for mid-tier and lower-ranked universities can be added, allowing a broader range of institutions to be explored.\nFiltering universities by status (Public vs. Private) can be incorporated, giving users more control over their selections.\nConclusion\n\nUniQuestAI is an agentic AI application designed to help students worldwide find top universities based on their preferences. It serves as an excellent starting point for those exploring global academic opportunities. The system provides tailored recommendations for both undergraduate and graduate applicants. It leverages the agentic patterns and workflows to automate the whole process of information extraction and compilation - a task that is otherwise time consuming and tedious. With this system, students can focus on preparing strong university applications and gathering essential documentation instead of spending their precious time searching for requirements.\n\nReferences\nAgentic AI - A Step towards Artificial General Intelligence\nDocumentation: agno\nUniQuestAI - GitHub\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nTechnologies Used\n\nUser Inputs\n\nOutput\n\nGeneral Information\n\nRankings\n\nProgram Details\n\nApplication & Fees\n\nImportant Dates\n\nView all\nComments\nCode\nYou might be interested\nStudy Abroad Assistance AI\nK\nAug 27, 202513 reads\nResuMate: Resume Optimizer\nOutstanding Solution Implementation\nMar 31, 202553 reads\nAgentic AILLM+1\nTechFreshBot - University Enquiry Chatbot for Freshmen\nDec 29, 202410 reads\nFastAPIGenAI+7\nSmart Search for Free Courses on Analytics Vidhya\nN\nFeb 17, 20259 reads\nAICosineSimilarity+9",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "visionbased-perception-systems-for-autonomous-vehicles-YyyC85nU4bdu",
    "username": "fFatima Saud",
    "license": "MIT License",
    "title": "Vision-Based Perception Systems for Autonomous Vehicles",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n72 reads\n●\nMIT License\nWinner of\nMost Promising Innovation\nat the\nComputer Vision Projects Expo 2024\nVision-Based Perception Systems for Autonomous Vehicles\nautonomous\nAutonomous vehicles\nCARLA\ncomputer vision\nDepth estimation\nGPS\nImage classification\nlane detection\nLiDAR integration\nObject detection\nperception\npytorch\nRobosense\nROS\nsimulator\ntensorflow\nVisual SLAM\nyolov5\nF\nFatima Saud\nA\nAleema Saleem\nA\nAnsharah Mobeen\nI\nInzamam Ul Haq\nF\n@fatima.k02shah\nLike\nBookmark\nShare\nAbstract\n\nVision-based perception systems are the foundation of self-driving vehicle technology, allowing real-time analysis and interpretation of the surrounding environment. Our proposed work investigates a hybrid strategy for integrating lane detection and object identification, with a focus on warning generation for impending collisions. Our algorithm detects objects using deep learning models such as YOLOv5, as well as a custom lane detection technique that includes curvature estimation. We have also incorporated traffic sign and signal recognition along with depth estimation to help you make better decisions. The built system has extensive detection and classification capabilities, paving the way for safer and more reliable autonomous navigation.\n\nObjective\n\nOur research proposes a comprehensive framework that combines lane detection with object recognition and collision warnings, utilizing cutting-edge deep learning models and real-time processing methods. Our aim is to add to the ongoing initiatives aimed at enhancing the safety and capabilities of autonomous vehicles, making them ready for real-world implementation.\n\nIntroduction\n\nRecent developments in sensor technology, computer vision, and artificial intelligence have sparked a lot of interest in autonomous vehicles. These vehicles utilize a variety of sensory systems to comprehend their surroundings and navigate in a safe and efficient manner. In an effort to provide a safe, effective, and dependable transportation system, cameras and computer vision techniques—in particular, vision-based perception—have gained popularity as a means of distinguishing between objects, identifying road restrictions, and perceiving the full picture.\n\nCamera model and calibration\n\nCamera model and callibration are an important aspect of dealing with vision-based perception. With the advancement in cameras and their features, they now serve as primary sensors for the purpose of achieving accurate modelling and calibration, as these elements are crucial for dependable object detection, localization, and scene comprehension.\n\nObject detection and tracking\n\nPerception systems in autonomous vehicles depend on object identification and tracking to enable real-time observation and detection of things such as cars, pedestrians, and barriers. Tracking techniques like SORT, Deep SORT, and Kalman filters allow continuous surveillance across frames, maintaining object IDs and providing temporal context, while models like YOLO, Faster R-CNN, and SSD effectively classify items. This combination enables precise velocity estimation, distance measurement, and forecasting abilities, which are vital for secure navigation. Furthermore, techniques for sensor fusion that integrate cameras, LiDAR, and radar improve the detection and tracking effectiveness, ensuring dependability in dynamic or challenging conditions, including low-light situations or adverse weather.\n\nLane detection\n\nIn autonomous driving, lane detection is essential because it provides vital data for path planning and vehicle positioning. Techniques range from sophisticated deep learning methods like semantic segmentation utilizing convolutional neural networks (CNNs) to more conventional edge detection and Hough transform methods. Even in difficult situations, including occlusions, faded lines, or changing lighting, these models are able to recognize lane markers with accuracy. In order to improve accuracy and enable adaptive lane following and lane departure warning features in actual driving situations, robust lane-detecting systems frequently integrate optical data with sensor inputs such as LiDAR.\n\nDepth estimation\n\nFurthermore, depth estimation aids in assessing the distance to surrounding objects, improving the vehicle’s ability to respond suitably to changing circumstances, and understanding the spatial layout of the environment, enabling accurate distance measurement to surrounding objects. Techniques like stereovision, monocular depth estimation using deep learning, and LiDAR integration are commonly employed to enhance depth perception. This information allows autonomous vehicles to make informed decisions, such as maintaining safe distances and executing obstacle avoidance maneuvers effectively.\n\nThe fusion of these technologies allows for a thorough understanding of the environment, supporting real-time decision-making.\n\nDespite substantial advancements in these fields, challenges persist in merging multiple perception tasks into a single, efficient system that can function reliably in diverse environmental conditions.\n\nMethodology\n\nOur approach integrates vision-based perception tasks for autonomous vehicles, focusing on lane detection, object recognition, and collision warnings.\n\nData Collection\n\nWe collected essential data such as GPS GNSS FIXED corrections, ZED2i stereo vision camera inputs, and IMU pose information, all while driving the electric vehicle.\n\nObject Detection\n\nUsing deep learning models (YOLOv5 and Traffic Sign Detection), we perform real-time object detection for identifying vehicles, pedestrians, and traffic signs, critical for obstacle avoidance and compliance with traffic rules.\n\nCustom model\nA custom YOLOv5 model trained on traffic sign data and a traffic light classification model were loaded for detecting relevant objects and traffic signals and the signal colors (red, yellow, and green). We fine-tuned the parameters, confidence thresholds and IoU thresholds for optimal detection accuracy.\nTraffic Signal Color Detection\nA lightweight convolutional neural network (CNN) is used to classify traffic light colors. The design consists of a convolutional layer followed by ReLU activation, max pooling, and a fully linked layer.\nThe pre-trained weights are loaded, and the model is switched to evaluation mode for real-time inference.\nLane Detection\n\nWe applied lane detection algorithms that not only identify lanes but also estimate their curvature to ensure safe navigation in real-world driving scenarios.\n\nWe defined a region of interest (ROI) as a trapezoidal mask to focus on the lane area.\nPreprocessed frames undergo Canny edge detection and Hough line transform to extract lane boundaries, and then the detected lines are fitted with polynomial equations to calculate lane curvature and estimate vehicle position relative to the lane center. The area in between the detected lane lines was filled using the OpenCV fillPoly( ) function.\n\nDepth Estimation for Distance Measurement\n\nThe depth of detected objects is calculated using median depth values extracted from a depth map. Invalid or missing depth values are handled gracefully to ensure reliability. The bounding box coordinates for detected objects guide depth analysis.\n\nCollision Warning System\n\nThe system calculates the distance to detected objects using depth information, providing warnings when objects come within a predefined safety range, thereby enhancing vehicle safety.\n\nA side panel displays dynamic warnings and vehicle information, including lane curvature, vehicle position, and potential hazards.\n\nWe also subscribed to camera topic on ROS and published proximity warning. The topic published true upon detection of object within the set distance threshold and false if far.\n\nWe combine these perception tasks into a real-time video processing pipeline, overlaying detected information on the video feed and providing visual warnings when necessary.\n\nAdditionally, we also developed pedestrian behavior-based brake and throttle control using computer vision, ROS, a ZED camera, and BLDC controllers. The system applies brakes when objects of specific classes are detected within a specified distance and re-engages throttle when the detected objects move beyond the threshold distance.\n\nSimulation in CARLA\n\nAlong with designing and testing our methods in real time environment, we also worked on CARLA Simulator environment. Below mentioned are the approaches we used in the simulator environment:\n\nObject Detection\n\nTo validate object detection capabilities, we incorporated the object detection algorithm into the CARLA environment, a high-fidelity platform for simulating urban driving environment.\n\nPath Control Using MPC and CARLA Visualization\n\nWe also integrated the CARLA autonomous driving simulator with ROS to enable real-time control of physical vehicle actuators. A Model Predictive Controller (MPC) was implemented in CARLA for precise throttle and brake control, ensuring smooth data communication between CARLA and a microcontroller via ROS. The control outputs, including steering, throttle, and brake, were successfully applied to a physical vehicle’s tires equipped with BLDC motor hall sensors during on-ground testing.The results are visualized in CARLA, demonstrating precise vehicle control and navigation.\n\nVehicle Simulation with Real-Life Prototype Integration\n\nThe simulated vehicle in CARLA is synchronized with a real-life prototype using Arduino and ROS. This setup allows the prototype’s wheels to mimic the steering movements of the virtual car, with throttle and brake actions reflected in real time. The integration ensures seamless interaction between the virtual simulation and the physical prototype, providing a comprehensive validation framework.\n\nReal-time camera feed was visualized via Pygame along with semantic segmentation for detailed environmental data collection.\n\nResults\n\nDespite complex driving settings, our system performs consistently in real-time lane detection, object recognition, and distance calculation. Lane curvature is reliably identified, allowing for smooth navigation, while object detection models (YOLOv5 and Traffic Sign Detection) effectively identify vehicles, pedestrians, and traffic signs. The collision warning system responds quickly to objects that approach too closely, delivering timely alerts. The integrated system successfully overlays pertinent information into the video feed, making it acceptable for use in real-world settings.\n\nThe Robosense LiDAR sensor—serving as the \"eyes\" of our self-driving car—helped generate a detailed 3D map of the environment. Combined with precise GPS localization, we’ve made significant progress in sensing, localization, mapping, and perception.\n\nConclusion\n\nThis work showcases the potential of leveraging computer vision techniques for improved navigation and decision-making in autonomous vehicles, contributing to the development of safer self-driving systems.\n\nFuture Work\n\nFuture developments might concentrate on improving the system's accuracy in dynamic settings, such tracking and identifying moving cars in various weather conditions. Furthermore, incorporating more sophisticated object detection models, such as multi-camera or multi-sensor fusion, may improve performance in difficult situations. Additionally, we intend to investigate environmental-condition-based adaptive warning methods, such as modifying proximity criteria for various object categories. Lastly, using predictive analytics with machine learning-based models may enhance long-term navigation and decision-making techniques.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nObjective\n\nIntroduction\n\nCamera model and calibration\n\nObject detection and tracking\n\nLane detection\n\nDepth estimation\n\nMethodology\n\nData Collection\n\nObject Detection\n\nView all\nCode",
    "awards": [
      "most promising innovation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "vrdocr-multihead-long-text-recognition-in-low-quality-images-s3TebOv2I3iT",
    "username": "k@khadija.irfan",
    "license": null,
    "title": "vRD-OCR: Multi-Head Long Text Recognition in Low Quality Images",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n64 reads\nWinner of\nMost Promising Innovation\nat the\nComputer Vision Projects Expo 2024\nvRD-OCR: Multi-Head Long Text Recognition in Low Quality Images\nK\n@khadija.irfan\nO\n@osama.amjad\nA\n@amur.saqib\nLike\nBookmark\nShare\nvRD-OCR Multi-Head Long Text Recognition in Low Quality Images\n\n\nAbstract\n\nAlthough state-of-the-art Optical Character Recognition (OCR) systems are highly accurate on most usecases, their performance depends on certain key factors; image quality, clarity of the text, and most importantly length of the text and/or image width. Most sophisticated models are unreliable when tested on highly compressed images with faded text containing more than 50 characters, and are unsuitable for accuracy-sensitive tasks such as text-based forgery detection. In this work, we present vrdOCR, a text recognition model which specializes on long sentences and achieves a character-level recognition accuracy of  on a custom dataset containing synthesized and real images of A4-width i.e. over 2000 pixels wide and more than 100 characters per image. Our model outperforms PP-LCNetV4 by  after finetuning on the same dataset and addresses edge cases much better than the current state-of-the-art.\n\nIntroduction\n\nOCR’s detection capabilities go beyond basic text recognition. Modern OCR systems can analyze the positioning, formatting, and spacing of characters to uncover irregularities that would be challenging for a human inspector to spot. For example, an altered invoice might replace a period with a comma, subtly changing an amount due, or adjust character spacing to insert unauthorized text. OCR systems, especially those enhanced with AI-driven anomaly detection, are trained to identify such inconsistencies. By flagging even minute variations in text and punctuation, OCR allows for a level of scrutiny that ensures minor alterations, which could have major implications, are not missed. This precision is crucial in high-stakes fields like finance, law, and government, where document integrity is paramount.\n\nIn addition to accuracy, OCR’s processing power and scalability set it apart from human inspection. While a person might carefully examine a few documents, an AI-enhanced OCR engine like vRD-OCR can process thousands of pages in a fraction of the time, maintaining a consistent level of precision and thoroughness.\n\nMethodology\n\nThis section outlines the methodology implemented in developing vRD-OCR, a robust framework for optical character recognition (OCR). The pipeline addresses challenges such as noisy backgrounds, diverse fonts, and varying document qualities by leveraging advanced feature extraction and a dual-head decoding architecture. The components of the pipeline include preprocessing, training with multi-loss mechanisms, and postprocessing.\n\nPreprocessing\n\nPreprocessing ensures that the input images are normalized and resized for consistent feature extraction. Specifically, the following steps are performed:\n\nNormalization: The input images are normalized with a mean and standard deviation of (0.5, 0.5, 0.5) for each channel.\n\nResizing: Images are resized to a fixed height (H=64) and width (W=640) to maintain a consistent aspect ratio suitable for OCR.\n\nThese steps prepare the data for efficient feature extraction and training, ensuring that the model remains robust to variations in input.\n\nTraining\n\nvrdOCR is trained on a single NVIDIA GeForce RTX 3060 Ti system running \\textbf{Ubuntu 20.04.6} on an 11th Gen Intel i5-11400F (12) @ 4.400GHz processor. We use Python 3.8.19 and PyTorch 2.3.1 for our experiments. The training process employs a multi-loss strategy to optimize the performance of the dual decoding heads: Connectionist Temporal Classification (CTC) and NRTR. This design ensures that the strengths of both mechanisms are utilized, providing a balance between sequence alignment and context modeling.\n\nThe following table summarizes the key hyperparameters used during the training process:\n\nHyperparameter 1\tValue\nLearning Rate\t1 x \nBatch Size\t4\nMaximum Characters\t150\nLoss Functions\n\nThe vRD-OCR framework implements a multi-loss approach: CTC loss and NRTR loss. Each loss is weighted to optimize different aspects of the OCR task. The total loss in the multi-loss framework is computed as:\n\n and  are the weighting factors for the CTC and NRTR losses, respectively. The multi-loss module dynamically combines the outputs of these loss functions, enabling the system to balance alignment-centric and context-aware learning. This integration ensures robust performance across a wide range of text styles, formats, and noise levels.\n\nCTC Loss\n\nConnectionist Temporal Classification (CTC) loss is utilized for unsegmented sequence labeling, which is essential in OCR tasks where explicit alignment between input and output sequences is unavailable. The CTC loss function aims to find the most probable alignment between the predicted sequence and the ground truth sequence .\n\nThe CTC loss is defined as:\n\nwhere  is the probability of the alignment  of the predicted sequence  to the target sequence , and  is the set of all possible alignments. The log-softmax operation ensures the predictions are normalized, and the sum over all possible alignments accounts for the uncertainty in character positioning.\n\nAdditionally, an optional focal loss mechanism is integrated into the CTC loss to prioritize harder-to-classify samples, especially when the model encounters ambiguous or difficult text. The weighted focal loss is given by:\n\nwhere  is the predicted probability of the correct class, and  is the focusing parameter that controls the emphasis on harder examples.\n\nNRTR Loss\n\nThe NRTR (Non-Recurrent Transformer) loss is designed for the transformer branch of the network. It employs a cross-entropy loss function to calculate the difference between predicted logits (\\mathbf{p}) and the ground truth labels (\\mathbf{t}). The NRTR loss with label smoothing is defined as:\n\nwhere  represents the predicted probability for class , and  is the smoothed target label. Label smoothing modifies the target distribution to reduce overfitting and improve generalization:\n\nwhere  is the smoothing factor and  is the number of classes.\n\nAdditionally, the NRTR loss function considers padding tokens by masking out the loss for zero entries in the target sequence using the following mask :\n\nBackbone: High-Resolution Network (HRNet32)\n\nThe backbone of the vRD-OCR framework is HRNet [2], a network that preserves high-resolution representations throughout its structure. Unlike conventional CNNs, HRNet retains high-resolution feature maps across all stages. Moreover it fuses multi-resolution representations to capture fine-grained details and global context. This design ensures that spatial information is preserved, which is essential for recognizing complex text layouts and long sentences.\n\nDecoding Heads\nCTC Head\n\nThe CTC [1] decoding head is optimized for variable-length sequence alignment, making it particularly effective for text recognition tasks. By predicting the most probable alignment between input features and the target text, the CTC head accommodates sequences of varying lengths, ensuring robust performance across diverse document layouts.\n\nNRTR Head\n\nThe NRTR [4] decoding head leverages a self-attention mechanism to capture long-range dependencies in the text. Its architecture includes a modality-transform block to convert 2D visual features into 1D sequences. It also employs a stacked self-attention layers for parallel processing of text sequences. This design enables the NRTR head to process complex textual structures efficiently, achieving faster training and competitive recognition performance.\n\nPostprocessing\n\nPostprocessing involves decoding the outputs of the CTC head to generate the final recognized text. The decoding process maps the predicted sequences to readable text using dynamic programming.\n\nBy combining preprocessing, advanced training techniques, and efficient postprocessing, the vRD-OCR framework delivers state-of-the-art performance in optical character recognition, setting a strong foundation for future advancements in the field.\n\nDataset\n\nFirst, a generalized recognition network is trained on synthetic data, created using the open-source tool TextRecognitionDataGenerator [5]. We generate a total of 500,000 images containing random strings from Wikipedia, 400,000 of which are used for training and 100,000 are used for validation. The dataset also consists of images that are randomly skewed and distorted, occurring with probabilities of 0.3 and 0.4, respectively.\n\nThe final network is finetuned on  synthesized and  real images, and evaluated on  synthesized and  real images. These images are of varying quality and present their own unique and production-specific set of challenges in recognition.\n\nResults\n\nWe conducted extensive experimentation using multiple backbone architectures to evaluate their effectiveness in the vRD-OCR model. Each backbone was trained and validated on the same dataset to ensure a fair comparison. The results, presented in Table below, highlight the significant impact of backbone selection on model accuracy. While PP-LCNetv4 and vRD-OCR with Resnet50 backbone provided reasonable performance, HRNet32 backbone emerged as the most effective backbone, achieving an impressive accuracy of . This demonstrates the importance of high-resolution representations in extracting fine-grained details for text recognition tasks.\n\nModel\tBackbone\tAccuracy\nPP-LCNetV4\tResNet50\t63.20%\nvRD-OCR\tResNet50\t85.09%\nvRD-OCR\tHRNet\t97.36%\nDiscussion\n\nvRD-OCR surpasses several pre-existing OCR technologies by achieving higher accuracy in recognizing long sentences, even in complex layouts or noisy environments. This capability is a result of robust feature extraction architectures, and flexible decoding mechanisms. The combined use of CTC and NRTR losses, provides a scalable solution for OCR. By employing this multihead comprehensive loss strategy, the vRD-OCR model achieves superior performance in recognizing long sentences and complex text layouts, addressing challenges posed by noisy data and document variability. The Connectionist Temporal Classification (CTC) layer allows for precise sequence decoding without explicit character alignment. This feature is particularly advantageous for processing documents with irregular character spacing or variable-length sentences.\n\nThe model's accuracy and robustness rely heavily on the availability of large, diverse training datasets. Insufficient or biased data can lead to reduced performance when encountering unseen text styles or formats. To address these limitations and build upon the system's strengths, one avenue that can be explored is Integration of Text Detectors. Incorporating a robust text detector component would allow for end-to-end processing, seamlessly identifying and recognizing text regions in a unified architecture.\n\nConclusion\n\nIn this work, we introduced vRD-OCR, a robust and efficient OCR framework designed to tackle the challenges of recognizing long and complex text in low-quality images. By leveraging state-of-the-art feature extraction backbones such as HRNet32 and employing a dual-head decoding mechanism with CTC and NRTR, the system demonstrated significant improvements in accuracy and adaptability. Extensive experimentation with various backbone architectures validated the effectiveness of our approach, with HRNet32 achieving an impressive accuracy of 97.36%.\n\nReferences\n\nGraves, Alex, Santiago Fern{'a}ndez, Faustino Gomez, and J{\"u}rgen Schmidhuber.\n\"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.\"\nProceedings of the 23rd international conference on Machine learning, 2006, pp. 369–376.\n\nSun, Ke, Bin Xiao, Dong Liu, and Jingdong Wang.\n\"Deep High-Resolution Representation Learning for Human Pose Estimation.\"\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.\"\nProceedings of the IEEE International Conference on Computer Vision (ICCV), December 2015.\n\nSheng, Fenfen, Zhineng Chen, and Bo Xu.\n\"NRTR: A No-Recurrence Sequence-to-Sequence Model for Scene Text Recognition.\"\n2019 International Conference on Document Analysis and Recognition (ICDAR), 2019, pp. 781–786. DOI: \n10.1109/ICDAR.2019.00130\n.\n\nBelval, Claude. \"TextRecognitionDataGenerator.\" 2018. Available at \nhttps://github.com/Belval/TextRecognitionDataGenerator\n. Accessed: 2024-12-30.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nvRD-OCR Multi-Head Long Text Recognition in Low Quality Images\n\nAbstract\n\nIntroduction\n\nMethodology\n\nPreprocessing\n\nTraining\n\nLoss Functions\n\nCTC Loss\n\nNRTR Loss\n\nBackbone: High-Resolution Network (HRNet32)\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nDatasets\nYou might be interested\nOCR system for Vietnamese text recognition\nJul 15, 202513 reads\nDeep LearningNLP+2\nMulti-lingual OCR for Devanagari and English Language using CRNN architectures\nBest Overall Project\nDec 22, 2024196 reads\n#IAM-handwritingCNN+9\nFrom Pixels To JSON: A Vision-Language Approach to Document Understanding\nS\nJan 02, 202530 reads\nAI-Powered Framework for Automated, Objective Evaluation of Handwritten Responses Using OCR & NLP\nN\nFeb 10, 202536 reads\nAIAIFramework+11",
    "awards": [
      "most promising innovation",
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "we-ask-ai-to-generate-images-from-a-podcasts-audio-KXvlYJgFryRJ",
    "username": "Sergio Solorzano",
    "license": "MIT License",
    "title": "We Ask AI To Generate Images From A Podcasts' Audio",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n81 reads\n●\nMIT License\nWinner of\nMost Engaging Presentation\nat the\nComputer Vision Projects Expo 2024\nWe Ask AI To Generate Images From A Podcasts' Audio\nchatgpt\nonnxruntime\nstable_diffusion\nunity\nwhisper\nSergio Solorzano\nMaurizio Raffone\nLike\nBookmark\nShare\nAbstract\nWe Ask AI To Generate Images From Podcasts' Audio\nPrototype App Creates Contextual Images For Audio Podcasts\n\n\nMotivation For This Post\n\nWatching a sound wave or unrelated images for a podcast that is published in a platform that supports video and images can be dull.\n\nWe propose asking a trio of AI models, bundled in an app we call Talkomic, to generate images that that are closely tied to the audio content discussed in the podcast.\n\nThe team at \nRendergon Ltd\n responsible for the development of the \nTapgaze apps\n has worked on this prototype. We are new to this field, keen to tinker and learn ! The prototype has been tested on the Unity Editor and Windows 11 build. It also provides a good start as a proof of concept to test the ability of AI models to help audio media augment its reach.\n\nAudio-To-Text-To-Image\n\nThe app’s AI Models transcribe an audio file to text and generate contexual images closely tied to the transcribed text.\n\nVisit the \nGithub repo\n where full implementation steps into Unity are further described.\n\n\n\nMethodology\nHow The Talkomic App Works\n\n\nChain AI-Model WorkFlow\n\nWe chain the results of three AI models to generate the images for an audio podcast.\n\nWe initially got the project to run with Unity’s neural network \nSentis\n, currently on closed beta, and added some Onnxruntime to make it work. The project in this blog is exclusively based on Onnxruntime. Unity’s team is making great strides with Sentis and I look forward to future updates !\n\nIn our Unity project, we run two of the AI models locally and access a third AI model remotely via an API.\n\nWe bundle these models inside \nUnity3D\n.\n\n\nIn a Unity scene we loop the AI Models over each podcast audio section to generate the contextual images.\n\n\nThe AI Models\n\nWe use \nOnnx AI\n models format.\n\n1. Whisper AI Model\n\nThis model transcribes audio into text. Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. This is the smallest of the 5 Whisper models available with 39M parameters, a great candidate to run on a mobile device.\n\nWe convert the \ntransformer whisper-tiny AI model\n into \nOnnx format using Microsoft Olive\n. The Olive github repo has a \nuseful example\n with the configuration required to optimize this model using ONNXRuntime tools:\n\nWhisper requires pre-and-post processing steps for audio inputs / model outputs. It would have been challenging to do this using python libraries in Unity. Fortunately Onnx optimization runs \nWhisperProcessor\n inside the AI Model, removing the need to code these outside! You can learn more about this feature and more in \nMicrosoft’s Build Onnxruntime demo\n.\n\nWhisper configuration with Olive\n\nI can suggest these steps for a quick whisper configuration with Olive:\n\ncreate and activate python 3.10 environment\ngit clone\ncd \nOlive\npython -m pip install .\ncd ./examples/whisper\npython -m pip install -r requirements.txt\n\nWhisper workflow configuration (watch out with copy/paste formatting char errors):\n\ncd examples/whisper\nprepare the workflow config json: python prepare_whisper_configs.py –model_name openai/whisper-tiny.en\nRun config to optimize the model: python -m olive.workflows.run –config whisper_cpu_int8.json –setup\nOptimize the model: python -m olive.workflows.run –config whisper_cpu_int8.json\nTest a transcription: python test_transcription.py –config whisper_cpu_int8.json\nN.B. – To re-make the model you’d need to clean the cache and existing generated directories within the whisper directory\nTo export the onnx model, the best candidate for the hardware config requested is in the zip models/ whisper_cpu_int8.zip directory CandidateModels\\device-device\\BestCandidateModel_1\\model.onnx. Config settings at configurations.json and inference_config.json in this same directory.\n\nChunked Audio\n\nThe Whisper model is designed to work on audio samples of up to 30s in duration. Hence we chunk the podcast for each section in chunks of max 30 seconds but load these as a loop in Whisper-tiny for each podcast section.\n\n2. ChatGPT Model\n\nAs a large language model, and unlike Unet, Chatgpt can help us generate text that describes an image that is closely tied to the discussion in the podcast.\n\nRequests: Using \nAzure OpenAI’s REST API\n, we request with \nUnityWebRequest\n this description from ChatGPT 3.5-turbo AI model.\nOur prompt to ChatGPT which attempts to limit faces drawn for podcast participants: “Provide a concise answer of no more than {maxChatgptRequestedResponseWords} words: Describe an abstract image for this conversation. Do not use names of people or persons. Do not say in the description it is an image of a person. The center or protagonist of the image you describe is not a person. Do not explain the image you describe, only describe the image. This is the conversation:” <section_transcribed_audio_chunks>\n3. Stable Diffusion AI Model\n\nThese models are trained on 256-A 100 GPUs, ~150k hours, ~120M image-text pairs.\n\nGenerally speaking, these are models trained to denoise random Gaussian noise step by step to get to an image: the neural network is trained to predict the noise at step t on a corrupted image, and reverse the probability density function to t0 or denoised image. A drawback is that they can be slow.\n\n\nImage courtesy of this suggested blog \nvisually intuitive blog\n and I can also suggest this \nblog\n to learn more.\n\nImage courtesy of this blog.\n\nStable Diffusion Implementation\n\nWe clone \nthe stable diffusion v1.4 model from branch onnx\n in Huggingface:\nThis is a type of diffusion model called \nLatent Diffusion\n: The model is trained to generate latent (compressed) representations of the images, which is faster than using the actual (larger) pixel space\n\nGet the onnx models and unet weights .pb file:\n\nthe \nclone v1.4\n above\ngit \nclone v1.4 onnx branch\nor \nalso here\n\nThese are the main components in Latent Diffusion pipeline:\n\nText Tokenizer (get cliptokenizer.onnx \nhere\n): Your text prompt (of what to draw) is tokenized (into a list of tokens). The tokenizer used must match the one used by the Text Encoder model, in this case the CLIP model.\nText Encoder\n: An AI model trained independently of the stable diffusion model used. It learns to associate text with images.\nWe use it to encode text to embeddings at UNet inference: In this step we transform your tokenized text prompt into into a vector that is used to guide the image generation\nThis stable diffusion model uses the \nCLIP\n model (Contrastive Language–Image Pre-training)\nOpenAI claims CLIP efficiently learns visual concepts from natural language supervision.\nScheduler: The scheduling algorithm used to progressively add noise to the image during training. It helps compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\nU-Net Model: Stable diffusion is implemented on a U-Net architecture. The main concepts involved at Inference:\nA latent noisy image is created as a starting point. The U-Net model steps through; at each step the output, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. At step 0 we get the clear denoised image.\nWe create with the Text Encoder a set of (conditioned) embeddings for our text prompt (tokenized with the Text Tokenizer) and another unconditioned, meaning empty (random). We apply a guidance scale to lean closer towards what our conditioned embedding suggests rather than pure random unconditioned embedding.\nAutoencoder (VAE): Separately trained to the U-Net model. At training it converts an image into a low dimensional latent (compressed) representation (AI model input). At inference it converts denoised latents (AI model output) generated by the reverse diffusion process into images.\n\nThe images for this podcast were generated with the following text prompt input to the stable diffusion model: “Create an image cyberpunk style. ”\n\n4. Real-ESRGAN AI Model\n\nWe enhance the details of the 512×512 images generated by the stable diffusion AI model to crisper 2048×2048 resolutions.\n\nMy suggested steps for quick git installation (steps up to date in my forked \nrepo\n):\n\nClone the repo\nGet the models and move to project Real-ESRGAN/weights\nAs it’s customary, a python environment is helpful and worked for me on a conda python 3.10 env\ncd into project Real-ESRGAN/\npython -m pip install -r requirements.txt\npython setup.py develop\nSet the target resolution in main.py, i.e. load_weights RealESRGAN_x8.pth and model scale (model = RealESRGAN(device, scale=4)) where scale=2/4/8\nExecute: python main.py\nFast on linux running on cuda, slow on VBox running on cpu\nResults\n\nWatch the Trailer of the Talkomized Podcast\n\n\n\n\n\nWatch the Final Talkomized Podcast in Full !\n\n\n\n\n\nView and Download The Podcast AI Images\nHere\nSee AI Images in \nAugmented Reality with the Tapgaze App Bobile App Here\n\n\n\n\n\nESRGAN Enhanced example image\n\nAll enhanced images available to download \nhere\n.\n\nPre-processed Image:\n\n\nESRGAN Post-Processed Image:\n\nThank you and Acknowledgements\n\nSpecial thanks to Jason Gauci co-host at \nProgramming Throwdown podcast\n whose idea shared on this podcast served as inspiration for this prototype.\n\nI am thrilled and truly grateful to \nMaurizio Raffone at Tech Shift F9 Podcast\n for trusting me to run a proof of concept of the Talkomic app prototype with the audio file of a fantastic episode in this podcast.\n\nWe thank \n@sd-akashic\n \n@Haoming02\n \n@Microsoft\n for helping to better understand onnxruntime implementation in Unity.\n\nWe thank \nai-forever\n for posting the models and \ngit repo\n.\n\nand \nyasirkula's Simple File Browser\n.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nWe Ask AI To Generate Images From Podcasts' Audio\n\nPrototype App Creates Contextual Images For Audio Podcasts\n\n\n\nMotivation For This Post\n\nAudio-To-Text-To-Image\n\n\nMethodology\n\nHow The Talkomic App Works\n\n\n\nChain AI-Model WorkFlow\n\n\nThe AI Models\n\nWhisper AI Model\nView all\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "wordpress-n8n-pipeline-ai-pulizia-llm-traduzione-e-tts-YWzdKERfhdvx",
    "username": "p@paolo.ronco2000",
    "license": "MIT License",
    "title": "WordPress + n8n, pipeline AI: pulizia LLM, traduzione e TTS",
    "publication_description": "Back to publications\nAug 22, 2025\n●\n424 reads\n●\nMIT License\nWordPress + n8n, pipeline AI: pulizia LLM, traduzione e TTS\nautomations\ngcp\ngoogle cloud platform\nn8n\ntext-to-speech\ntts\nwordpress\nworkflows\nP\n@paolo.ronco2000\nLike\nBookmark\nShare\nAI Pipeline for WordPress on n8n: Text Normalization, Translation, and TTS\n\nKeywords: WordPress, n8n, LLM text normalization, machine translation, speech synthesis, Google Cloud Run, reproducibility\n\nAbstract\n\nI present a reproducible, self‑hosted pipeline that converts WordPress articles into multilingual voiceovers (Italian/English). The system orchestrates LLM‑based text cleaning, machine translation, and neural text‑to‑speech (TTS) via n8n. A lightweight microservice on Google Cloud Run performs long‑form TTS and writes audio artifacts (.wav) to Google Cloud Storage (GCS). Metadata and processing state are tracked through Google Sheets, enabling deterministic batch runs and idempotent processing. We describe the architecture, interfaces, and evaluation protocol (latency, throughput, and audio integrity checks) to facilitate reuse and extension.\n\n1. Problem Statement\n\nBlogs often publish long‑form content without an accessible audio counterpart. Manual conversion to voice is time‑consuming and error‑prone (HTML artifacts, punctuation issues, malformed text). The goal is to automate the post → clean text → translation → TTS → publish loop with traceability and repeatability, while keeping infrastructure self‑hosted for control and cost predictability.\n\n2. System Overview\n\nThe solution is composed of three subsystems:\n\nOrchestration (n8n)\nScheduled ingestion of WordPress posts.\nText normalization with an LLM (“Clean” nodes per language).\nOptional translation to EN.\nHTTP requests to the TTS microservice.\nUpdate of two WordPress pages (IT/EN) with an <audio> player block.\nStatus tracking in Google Sheets (work queue).\nTTS Microservice (Cloud Run)\nFastAPI endpoint /long-tts receiving {text, language, voice, filename}.\nUses Google Cloud Text‑to‑Speech to synthesize .wav files.\nStreams output to a GCS bucket and returns artifact metadata.\nArtifacts & State\nAudio: gs://<bucket>/<ID>.wav and gs://<bucket>/<ID>EN.wav.\nIndex: Google Sheets with columns ID, Title, Date, Content, Link, Status.\nPresentation: WordPress pages IT/EN updated with an audio player snippet.\n\nWorkflow JSONs are provided in /n8n-workflows. Replace every INSERT_YOUR_ID_HERE with your instance‑specific IDs before running.\n\n3. Data Flow\n\nIngest (Part 1) → fetch posts from WordPress → strip HTML → append/update rows in Google Sheets.Process (Part 2) → pick next row with empty Status → clean text (LLM) → translate to EN → call /long-tts twice (IT/EN) → update WordPress pages → set Status=done.\n\n\n\n4. Methods\n4.1 Text Normalization (LLM)\nObjective: remove markup and special tokens that degrade TTS (e.g., HTML entities, escaped quotes, code fragments).\nApproach: two dedicated “Clean” steps (IT/EN) to avoid cross‑language artifacts; simple post‑processing (newline collapse).\n4.2 Machine Translation\nObjective: generate a high‑fidelity English version for bilingual synthesis.\nApproach: Google Translate node in n8n; downstream clean step to stabilize punctuation/spacing.\n4.3 Neural TTS\nObjective: synthesize natural speech from normalized text.\nApproach: Google Cloud TTS (e.g., it-IT-Neural2-F and en-US-Neural2-D), invoked via Cloud Run microservice to support long texts, centralized logging, and uniform storage in GCS.\nOutput: uncompressed or PCM‑linear .wav files named by post ID (plus EN suffix for English).\n4.4 Publication\nObjective: consistent exposure of artifacts to end users.\nApproach: WordPress REST updates a dedicated IT page and an EN page; an <audio> block references GCS URLs. Pages act as an index/catalog of voiceovers distinct from the original posts.\n5. Implementation Details\n5.1 Orchestration (n8n)\nTriggers: nightly schedule.\nQueueing: Google Sheets serves as a work queue with Status acting as a completion flag.\nIdempotency: the pipeline selects a single pending row (oldest by Date), ensuring one‑by‑one processing without duplicates.\nError Handling: if a step fails, the row remains pending; you can re‑run Part 2 safely.\n5.2 Cloud Run Service\n\n/GCP Code contains:\n\nmain.py: FastAPI app exposing /long-tts.\nrequirements.txt: Python dependencies.\nDockerfile.txt: image build.\n\nDeploy:\n\ngcloud auth login\ngcloud config set project YOUR_PROJECT_ID\ngcloud builds submit --tag gcr.io/YOUR_PROJECT_ID/long-tts\ngcloud run deploy long-tts \\\n  --image gcr.io/YOUR_PROJECT_ID/long-tts \\\n  --region europe-west1 \\\n  --platform managed \\\n  --allow-unauthenticated\n\n\nExample request:\n\ncurl -X POST \"https://<service>.run.app/long-tts\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"text\": \"Hello World\",\n        \"language\": \"en-US\",\n        \"voice\": \"en-US-Neural2-D\",\n        \"filename\": \"sample.wav\"\n      }'\n\n5.3 WordPress Integration\nTwo page IDs (IT/EN) are updated by the workflow.\nThe audio player uses <audio><source src=\"https://storage.googleapis.com/<bucket>/<ID>.wav\" type=\"audio/mpeg\"></audio>; the EN version appends EN.wav.\n6. Evaluation Protocol\n\nWe propose a lightweight, reproducible protocol. Record results per batch in a CSV or Sheet.\n\nLatency (ms/char): measure end‑to‑end synthesis time divided by input character count.\nThroughput (posts/hour): number of posts fully processed in a 60‑minute window.\nAudio Integrity: detect silent output or truncated files by checking duration vs. expected duration (given TTS speaking rate).\nText Cleanliness Score: pre/post token ratio (e.g., removed HTML entities, code spans) as a proxy for normalization quality.\nPublication Success Rate: ratio of successful WordPress page updates vs. attempts.\n\nNo human MOS is reported here; if you need perceptual quality, add a small listening test protocol with 5‑point MOS and inter‑rater agreement.\n\n7. Reproducibility\nDeterministic scheduling: nightly cron with a single‑row pick policy.\nPinned artifacts: TTS voices (it-IT-Neural2-F, en-US-Neural2-D) fixed for runs.\nVersioning: keep a changelog of node versions and main.py image digest (from Cloud Run).\nConfig surface: all instance‑specific IDs are explicit in the JSON as INSERT_YOUR_ID_HERE—replace them but do not change field names.\n8. Limitations\nReliance on external services (Translate/TTS) may introduce latency spikes or regional availability constraints.\nTTS prosody depends on input punctuation; text normalization reduces but cannot fully remove artifacts.\nThe catalog approach (IT/EN pages) centralizes voiceovers, not embedding per‑post players (by design).\n9. Ethical & Operational Considerations\nCopyright & consent: ensure you are authorized to transform and redistribute post content as audio.\nAttribution: link back to the original article in the audio block.\nPrivacy: avoid synthesizing PII; clean steps should strip emails/API keys/code fragments from the TTS input.\nAccessibility: provide transcripts when feasible for hearing‑impaired users.\n10. How to Use (Quick Start)\nDeploy the Cloud Run service in /GCP Code.\nCreate the Google Sheet (ID, Title, Date, Content, Link, Status).\nImport /n8n-workflows/WordPress_Automations__Part1.json and Part2.json into n8n.\nReplace all INSERT_YOUR_ID_HERE with your WordPress page IDs, Google Sheet IDs, and credential IDs.\nMap credentials (WordPress, Google, OpenAI if you keep LLM cleaning).\nRun Part 1 to populate the Sheet; Run Part 2 to synthesize and publish.\nEvaluate using the protocol in §6 and iterate.\n11. Future Work\nAdd exponential backoff/retry on TTS HTTP calls.\nSupport additional languages/voices; auto‑select voice based on locale.\nOptional per‑post embedding (update the article body instead of a catalog page).\nIntroduce signed URLs for private buckets.\nArtifact Locations\nWorkflows: /n8n-workflows/WordPress_Automations__Part1.json, /n8n-workflows/WordPress_Automations__Part2.json\nService code: /GCP Code\nDiagrams: /images/WordPress_Automations__Part1.png, /images/WordPress_Automations__Part2.png\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAI Pipeline for WordPress on n8n: Text Normalization, Translation, and TTS\n\nAbstract\n\nProblem Statement\nSystem Overview\nData Flow\nMethods\n\n4.1 Text Normalization (LLM)\n\n4.2 Machine Translation\n\n4.3 Neural TTS\n\n4.4 Publication\n\nView all\nComments\n(5)\nCode\nYou might be interested\nGCP TTS on WordPress | WordPress to Voice\nP\nJun 18, 202525 reads\nAI VoiceAPI+11\nAudio To Answer Generator\nY\nSep 01, 202516 reads\nagenticai+3\nAudio Transcriber AI\nM\nMar 31, 202518 reads\nSpeechFlow: Modular real-time audio transcription and conversational AI.\nBest AI Tool Innovation\nM\nFeb 26, 202550 reads\nAIAudio Transcription+8",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  }
]