[
  {
    "id": "advanced-image-captioning-transformer-with-reinforcement-learning-optimization-MPM1xO3YGPuo",
    "username": "dDaniel Plotkin",
    "license": null,
    "title": "Advanced Image Captioning Transformer with Reinforcement Learning Optimization",
    "publication_description": "Back to publications\nDec 22, 2024\n●\n233 reads\n●\nCreative Commons Attribution (CC BY)\nWinner of\nMost Promising Innovation\nat the\nComputer Vision Projects Expo 2024\nAdvanced Image Captioning Transformer with Reinforcement Learning Optimization\nbeam search\ncomputer vision\nimage captioning\nmulti-modal learning\nnlp\npolicy gradients\nreinforcement learning\nself-critical sequence training\ntransformers\nD\nDaniel Plotkin\nLike\nBookmark\nShare\n1. Abstract\n\nGenerating high-quality captions for images is a critical challenge in AI, requiring advancements at the intersection of computer vision and natural language processing. This study introduces a novel image captioning pipeline that integrates the CPTR (Full Transformer Network) architecture with Self-critical Sequence Training (SCST) for optimization. By using a two-step training process, we demonstrate substantial improvements in caption quality, as measured by the METEOR score. This work highlights the potential of combining state-of-the-art transformers with reinforcement learning techniques to address complex computer vision tasks.\n\n2. Introduction\n\nImage captioning, the task of generating textual descriptions for visual content, bridges the gap between vision and language understanding. While traditional models have achieved notable success in optimizing differentiable objectives, they often fall short when it comes to non-differentiable evaluation metrics such as METEOR and BLEU. These metrics, which better reflect human evaluation criteria, remain challenging to optimize directly with standard training approaches. Addressing this limitation is essential for improving alignment between model-generated captions and human judgment.\n\nThis study presents a two-step training pipeline that leverages the strengths of:\n\nCPTR, a transformer-based architecture with a Vision Transformer (ViT) encoder and transformer decoder.\nSCST, a reinforcement learning technique that directly optimizes non-differentiable metrics.\n\nWe hypothesize that reinforcement learning can significantly enhance alignment between model-generated captions and human evaluation metrics. The contributions of this work are as follows:\n\nImplementation of a CPTR-based baseline model for image captioning.\nOptimization of caption quality using SCST with METEOR as the reward signal.\nEvaluation on the Flickr8K dataset, showing noticeable improvements in METEOR score.\n3. Related Work\n3.1 Transformer Architectures in Image Captioning\n\nThe transformer model, initially proposed by Vaswani et al. (2017), has become a cornerstone of NLP and computer vision tasks. CPTR [1], a fully transformer-based image captioning model, eliminates the need for convolutional backbones by utilizing a Vision Transformer encoder.\n\n3.2 Reinforcement Learning for Captioning\n\nSCST [2], introduced by Rennie et al., revolutionized image captioning by enabling direct optimization of evaluation metrics. Unlike supervised learning, SCST trains models to generate captions that maximize rewards such as CIDEr, BLEU, or METEOR.\n\n4. Methodology\n4.1 CPTR Architecture\n\nOur baseline model employs CPTR, which integrates:\n\nA Vision Transformer (ViT) encoder that processes images into sequences of visual tokens.\nA Transformer decoder that generates captions by attending to these visual tokens.\n\n4.2 SCST Optimization\n\nIn the second training phase, we apply SCST to optimize captions using the METEOR score as a reward signal. The SCST loss function is defined as:\n\nwhere:\n\n: model parameters,\n: sampled sequence,\n: reward (METEOR score),\n: baseline reward (e.g., greedy-decoded sequence).\n\nSCST encourages the model to generate captions with higher rewards than the baseline.\n\n5. Loss Function\n5.1 Baseline Loss: Zero-masked Categorical Cross Entropy\n\nThe baseline training uses:\n\nwhere  is the conditional probability of the target token  given previous tokens and image features.\n\n5.2 SCST Loss\n\nSCST optimizes METEOR directly, as defined in Section 4.2.\n\n6. Dataset\n\nWe evaluate our pipeline on the \nFlickr8K dataset\n, which contains:\n\nTraining: 23,890 image-caption pairs.\nValidation: 5,975 image-caption pairs.\nTesting: 7,470 image-caption pairs.\n7. Experimental Setup\n7.1 Platform and Tools\nHardware: Colab L4 GPU.\nPretrained Components:\nTokenizer: distilbert-base-uncased\nVision Encoder: google/vit-base-patch16-384.\n7.2 Training Details\nBaseline Training\nWarmup-cosine decaying learning rate schedule.\nEarly stopping based on validation loss.\nEpochs: 15, stopped at epoch 10.\n\n\nFigure: Baseline Training and Validation Loss by Epoch\n\n\nFigure: Baseline Training and Validation Accuracy by Epoch\n\nSCST Optimization\nTrain for 8 epochs.\nInitial learning rate: . We then decay the learning rate by 0.5 for the remaining epochs.\nBeam search (beam size: 3) for caption generation.\n8. Results\n8.1 Baseline Performance\n\nWe use a batched single-reference meteor score to evaluate our baseline and post-SCST captions on our test set.\n\nMetric\tScore\nSingle METEOR\t0.276\n8.2 Post-SCST Optimization\nMetric\tBefore SCST\tAfter SCST\nSingle METEOR\t0.276\t0.301\n8.3 Final Evaluation Metrics (Beam Search)\n\nWe use beam search to decode our captions for final evaluation and generation, with a beam size of 3. A simple normalized function is used as the score function for beam search. It is defined as:\n\nMETEOR\tBLEU 1\tBLEU 2\tBLEU 3\tBLEU 4\n0.4659\t0.5528\t0.4006\t0.2714\t0.1743\n9. Discussion\n9.1 Strengths\nAlignment with Human Evaluation: Optimizing METEOR ensures captions better reflect human judgment.\nArchitecture Flexibility: CPTR allows seamless integration of transformer advancements.\n9.2 Limitations\nHardware Constraints: Limited by computational power and memory on machine, limiting the number of training epochs and decoder layers.\nDecoder Training: A pre-trained decoder with a deeper architecture could improve results and training time.\nLarger Dataset: Training on Flickr30k or MSCOCO would improve training size and generalization.\n9.3 Future Work\nTrain on larger datasets with high-performance hardware.\nIncorporate pre-trained decoders to boost performance.\nGain access to stronger computational hardware to allow longer SCST training.\n10. Conclusion\n\nThis study demonstrates the synergy between transformers and reinforcement learning in image captioning. By aligning model outputs with human evaluation metrics, the proposed pipeline achieves significant improvements in caption quality, providing a foundation for further research.\n\n11. References\n\n[1] Wei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu, and Jing Liu. \"CPTR: Full Transformer Network for Image Captioning.\" CoRR, vol. abs/2101.10804, 2021. Available: \nhttps://arxiv.org/abs/2101.10804\n.\n\n[2] Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. \"Self-critical Sequence Training for Image Captioning.\" CoRR, vol. abs/1612.00563, 2016. Available: \nhttp://arxiv.org/abs/1612.00563\n.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nAbstract\nIntroduction\nRelated Work\n\n3.1 Transformer Architectures in Image Captioning\n\n3.2 Reinforcement Learning for Captioning\n\nMethodology\n\n4.1 CPTR Architecture\n\n4.2 SCST Optimization\n\nLoss Function\n\n5.1 Baseline Loss: Zero-masked Categorical Cross Entropy\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nYou might be interested\nTransformVision: AI-Powered Caption Generation\nDec 10, 202449 reads\n#AttentionMechanisam#FastAPI+3\nImage Captioning with Multiple Decoder Architectures\nO\nDec 23, 202449 reads\nAIComputer Vision+5\nPLEDGE: Paragraph LEvel image Description GEneration\nS\nDec 24, 202412 reads\nCross-modalityEnd-to-end Model+2\nImage Captioning Model using CNN and LSTM\nZ\nDec 28, 202419 reads\nCNNDeep Learning+3",
    "awards": [
      "most promising innovation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "advancing-autonomous-driving-deep-learning-innovations-for-enhanced-perception-and-decisionmaking-EFEFMQSUNvvm",
    "username": "sPol Navarro Solà",
    "license": null,
    "title": "Advancing Autonomous Driving: Deep Learning Innovations for Enhanced Perception and Decision-Making",
    "publication_description": "Back to publications\nDec 27, 2024\n●\n121 reads\n●\nApache 2.0\nWinner of\nBest Overall Project\nat the\nComputer Vision Projects Expo 2024\nAdvancing Autonomous Driving: Deep Learning Innovations for Enhanced Perception and Decision-Making\nAI\nComputer Vision\nDeep Learning\nML\nMultiModal\nVLM\nS\nPol Navarro Solà\nLike\nBookmark\nShare\nAdvancing Autonomous Driving: Deep Learning Innovations for Enhanced Perception and Decision-Making\n\nFigure 0. Real Time detection\n\nAbstract\n\nAutonomous driving systems rely heavily on robust object detection and perception to ensure safety and efficiency. Traditional methods often struggle to address the challenges posed by dynamic environments and varying conditions. This publication introduces a comprehensive approach leveraging state-of-the-art deep learning techniques to enhance perception and decision-making capabilities in autonomous systems.\n\nKey contributions include the application of YOLOv8 for 2D object detection and VoxelNeXt for 3D object detection, alongside the integration of simulation tools like the CARLA simulator for real-time evaluation. These innovations address core challenges, such as object recognition in complex scenarios, by utilizing tailored datasets like Mapillary Traffic Sign and ONCE. Results demonstrate significant advancements in detection precision and real-world applicability.\n\nThis work sets a foundation for future enhancements in Advanced Driver Assistance Systems (ADAS) and autonomous vehicles, emphasizing scalability, real-time performance, and integration of multimodal data.\n\n1. Introduction\n\nThe field of autonomous driving has witnessed remarkable advancements through the integration of deep learning. However, challenges like real-time object detection, dynamic environments, and scalability persist. This research aims to address these issues through innovative approaches in 2D and 3D object detection and visualization.\n\nKey innovations of this study include:\n\nDevelopment and training of YOLOv8 on the Mapillary Traffic Sign dataset for robust 2D object detection.\nImplementation of VoxelNeXt within the OpenPCDet framework, leveraging the ONCE dataset for accurate 3D object recognition.\nCreation of real-time simulation visualizations using the CARLA simulator to evaluate model performance dynamically.\nVisualization of deep learning concepts through Manim animations to enhance understanding and engagement.\nGeneration of comprehensive plots and metrics for detailed performance analysis.\n\nThese contributions demonstrate the potential of advanced deep learning architectures to improve perception in autonomous driving, paving the way for safer and more efficient systems.\n\n2. Related Works\n\nObject detection and perception in autonomous driving is a critical area of research, focusing on automating decision-making processes and enhancing safety. Traditional methods initially relied on rule-based image processing techniques, but the rise of machine learning and deep learning has significantly advanced the field. This section explores prior research efforts, comparing traditional methods, state-of-the-art architectures, and their applications in 2D and 3D detection tasks.\n\nTraditional Image Processing Techniques\n\nEarly approaches to object detection relied on classical image processing techniques, such as edge detection, thresholding, and feature extraction. These methods were computationally lightweight but often struggled with environmental variability, such as occlusions and lighting conditions. Techniques like histogram equalization and contour-based region detection offered incremental improvements but lacked generalization across diverse scenarios.\n\nAdvances in Deep Learning for Object Detection\n\nThe introduction of deep learning revolutionized object detection, enabling robust performance in complex environments. Several architectures have been pivotal:\n\nYOLO (You Only Look Once):\nRedmon et al. (2016) introduced YOLO, a unified model capable of real-time detection.\nSuccessive iterations, including YOLOv8, have focused on balancing speed and accuracy, achieving state-of-the-art performance in 2D detection tasks.\nVoxel-based 3D Detection:\nVoxelNet and its successors, like VoxelNeXt, leverage voxelized representations of LiDAR data for accurate spatial recognition.\nIntegration within frameworks like OpenPCDet has facilitated scalable and high-performing 3D object detection.\nHybrid Approaches:\nMethods combining 2D and 3D data streams, such as PointPillars, optimize perception pipelines by fusing RGB and LiDAR inputs.\nEmerging multi-sensor fusion techniques demonstrate improved detection under challenging conditions.\nEvaluation Metrics and Comparative Analysis\n\nStandard metrics, including Precision, Recall, IoU, and F1 Score, are employed to evaluate detection models. Comparative analyses reveal:\n\nYOLO-based models excel in real-time applications, achieving up to 90% precision on datasets like Mapillary Traffic Sign.\nVoxelNeXt achieves superior performance in 3D tasks, with IoU thresholds exceeding 0.7 in benchmark tests using the ONCE dataset.\nChallenges and Future Directions\n\nWhile deep learning offers substantial improvements, challenges such as scalability, real-time processing, and multimodal integration remain. Research continues to explore lightweight architectures and efficient training methodologies to address these limitations.\n\n3. Methodology\n3.1 Mapillary Dataset Description\n\nThe dataset utilized in this study consists of 52,000 fully annotated images captured from various driving scenarios. Each 2D image is paired with bounding box annotations specifically for traffic signs.\n\nThe dataset includes over 300 traffic sign classes, each with bounding box annotations, making it highly suitable for traffic sign detection tasks. It has a global geographic reach, with images and traffic sign classes covering six continents, ensuring a broad representation of real-world driving conditions. Additionally, the dataset features a variety of weather conditions, seasons, times of day, as well as diverse camera types and viewpoints, offering comprehensive coverage of various environmental and situational factors.\n\nDataset Summary:\n\nCharacteristic\tDetail\nTotal Images\t100,000\nFully Annotated Images\t52,453\nPartially Annotated Images\t47,547\nResolution\t1080p+ (High-resolution images)\nTotal Classes (Traffic Signs)\t401\nTotal Bounding Boxes\t257,543\n\nFigure 1. Mapillary Dataset Class Labelling\n\nExample Annotations: This visual representation highlights the meticulous annotation process that characterizes the Mapillary Traffic Sign Dataset (MTSD), which is critical for training accurate and reliable object detection models. The dataset’s extensive and varied data make it an indispensable resource for advancing traffic sign detection capabilities, ultimately contributing to the development of safer and more reliable autonomous driving systems.\n\n3.2 Mapillary Data Preprocessing\n\nInitially, the Mapillary Dataset posed a challenge due to its non-standard format, making it incompatible with pre-trained models. To address this issue, a script was created to convert the dataset into the YOLOv8 format, commonly used in object detection tasks. This conversion allowed the dataset to be properly integrated into the model training process, ensuring compatibility with YOLOv8.\n\nClass Imbalance and Data Augmentation\n\nUpon inspecting the dataset, a significant class imbalance was identified, particularly the dominance of the “other-sign” class, which accounted for more than half of the total 200,000 distinct annotations. This imbalance resulted in a model with high recall (correctly identifying many signs) but low precision (misclassifying many signs as “other-sign” due to its overrepresentation).\n\nFigure 2. Mapillary Dataset Histogram\n\nTo address this, the following techniques were applied:\n\nData Augmentation:\n\nMosaic and Mixup were employed to increase the diversity of samples in the Convolutional Neural Network (CNN) training process. These augmentation techniques generate new training examples by combining different images or regions of images.\nMosaic combines multiple images into one, allowing the model to learn from more varied contexts.\nMixup blends two images and their labels, providing a more diverse set of training examples, helping the model generalize better.\nThese techniques were expected to help reduce class imbalance, especially for the \"other-sign\" class.\n\nRegion Cropping and Focused Augmentation:\n\nTo further improve balance, an image cropping algorithm was designed to crop the regions defined by the bounding boxes for each traffic sign class. This method allowed the focus to be on individual objects (traffic signs) rather than the entire image, resulting in more balanced and meaningful training samples.\nHowever, cropping the images often led to the cropped objects occupying the entire frame, which could prevent the model from learning to detect objects in more complex, real-world scenes. This presented a new challenge: ensuring the model still learned to detect objects within diverse scenes.\n\nAspect Ratio Preservation:\n\nAspect ratio preservation is crucial for traffic signs, as their shapes are key to identification. To ensure the model could maintain the correct proportions of the objects, the images were scaled while retaining their original aspect ratios.\nAny extra space created by scaling was filled with a black background, ensuring that the traffic signs remained centered and properly proportioned. This also allowed the model to learn both object classification (identifying what the object is) and localization (determining where the object is) while preserving the integrity of the sign shapes.\nThe labels were adjusted to exclude the black background, ensuring that only the relevant portions of the image were considered during training.\n\nData Normalization:\n\nNormalization of the pixel values may have been applied to ensure that the image data was standardized, with pixel values typically scaled to a range such as [0, 1]. This step ensures that all images contribute equally to the model training, helping the model converge faster and improving overall performance.\n\nFinal Adjustments and Training Preparation\n\nAfter applying data augmentation, cropping, and aspect ratio normalization, the dataset was stratified and rebalanced to ensure the model had a more even distribution of traffic sign classes. This helped mitigate the class imbalance and provided a better basis for training.\n\nThe final dataset, with its properly scaled images, rebalanced classes, and focused augmentation, was then ready for training. By addressing the initial dataset issues and ensuring that the model could learn both to classify and localize objects, the overall performance of the model in recognizing and detecting traffic signs in real-world environments significantly improved.\n\n3.3 ONCE Dataset Description\n\nThe ONCE (One Million Scenes) Dataset was selected for this research due to its comprehensive collection of autonomous driving scenarios, aimed at training and evaluating 3D perception models. This dataset offers rich, multi-modal data, including point clouds from LiDAR, camera images, and radar signals, making it a highly valuable resource for advancing autonomous vehicle technology.\n\nWith a focus on 3D scene understanding, the ONCE dataset provides high-quality data from real-world driving scenarios, supporting the development of models that accurately perceive and react to complex road environments. The dataset’s detailed annotations are critical for 3D object detection, segmentation, and tracking, which are essential for ensuring the safety and reliability of autonomous driving systems.\n\nDataset Summary:\n\nCharacteristic\tDetail\nTotal Scenes\t1,000,000\nAnnotations\t3D Bounding Boxes\nSensors\tLiDAR, Camera, Radar\nObject Categories\tCars, Pedestrians, Cyclists, Trucks, etc.\nEnvironmental Diversity\tUrban, Highway, Rural, Various Weather, Day/Night\n\nFigure 3. Example Scene from ONCE Dataset\n\nExample Annotations: The ONCE dataset includes 3D bounding boxes that highlight the locations of various objects like cars, pedestrians, and cyclists. These precise annotations enable the training of models capable of understanding spatial relationships and dynamics in complex environments, which are vital for the development of autonomous systems.\n\n3.4 ONCE Data Preprocessing\n\nThe ONCE Dataset, with its rich multi-modal data, required preprocessing steps to integrate its diverse data types and format them for effective model training. The following preprocessing steps were applied:\n\n1. Data Fusion:\n\nThe ONCE dataset includes multiple sensor modalities (LiDAR, camera, radar), which were fused to create a unified representation of each scene. The fusion process ensures that the model can leverage complementary information from all sensors to enhance object detection accuracy.\n\n2. Voxelization:\n\nTo leverage the 3D data provided by LiDAR, voxelization was applied to convert the raw point cloud data into a structured voxel grid. This transformation allows the model to process 3D spatial information more efficiently and enables accurate object detection in 3D space.\n\n3. Bounding Box Transformation:\n\nThe 3D bounding boxes were used to annotate the objects in the dataset. These boxes were adjusted to maintain the integrity of object shapes while ensuring compatibility with the voxelized representation. This step was essential to ensure that the model could learn both the object classification and the precise 3D localization of each object.\n\n4. Data Augmentation:\n\nRandom Rotation and Flipping: These augmentation techniques were applied to increase the variety of training samples and help the model generalize better to different viewpoints and orientations.\nNoise Addition: LiDAR point clouds were augmented by introducing controlled noise to simulate different environmental conditions, helping the model become more robust to real-world challenges.\nScaling and Translation: To ensure that the model could handle objects at various distances, the dataset underwent scaling and translation operations to simulate objects at different sizes and locations within the 3D space.\n\n5. Data Normalization:\n\nThe pixel values for camera images and the LiDAR data were normalized to a consistent range to ensure uniformity across the dataset. This step helps the model converge faster during training and improves overall performance.\n\nFinal Adjustments and Training Preparation:\n\nAfter preprocessing, the dataset was split into training, validation, and test sets to evaluate the model’s performance in different stages.\nThe data was carefully balanced, particularly ensuring that less-represented classes had adequate representation through augmentation techniques. This step is crucial for addressing any potential class imbalance that could affect model accuracy.\n3.5 Model Architectures\nYOLOv8 for 2D Detection\n\nYOLOv8 is selected for real-time 2D object detection tasks primarily due to its lightweight architecture and fast inference times, making it highly suitable for applications like autonomous driving, where real-time performance is crucial. YOLOv8 is the latest iteration of the You Only Look Once (YOLO) family of models, designed to address the need for both speed and accuracy in detecting objects within images. The model’s efficiency and high accuracy are essential when detecting traffic signs, where real-time analysis is required to ensure safe and effective navigation.\n\nSpeed and Efficiency\nYOLO has always been known for its real-time performance, making it a preferred choice for autonomous systems. YOLO processes images in a single pass, significantly faster than traditional two-stage models like Faster R-CNN, which split the detection process into separate proposal and classification steps. The ability to perform both object localization and classification in a single network makes YOLOv8 highly efficient and suitable for environments that require low latency, such as autonomous driving and real-time surveillance.\n\n \n\nIn addition, YOLOv8 offers improved speed compared to previous YOLO versions, capable of processing images at 45 to 155 frames per second (fps) depending on the model version and hardware. This capability ensures that the model can operate in real-time, an essential feature when deploying object detection models for fast-moving objects like traffic signs and vehicles.\n\nLightweight Architecture with High Accuracy\nThe YOLOv8 architecture has been optimized for both speed and accuracy. The key components of YOLOv8 include:\n\nBackbone: CSPDarkNet - a lightweight yet powerful backbone designed to efficiently extract features from images without compromising performance.\nNeck: PANet (Path Aggregation Network) - a feature aggregation network that enhances the ability to capture context across different layers, improving the model’s ability to handle objects of various scales.\nHead: YOLOv8’s head handles bounding box and class predictions, making it effective for 2D detection tasks like traffic sign detection.\n\nYOLOv8’s flexible design enables it to work across different hardware platforms, from low-power edge devices to high-performance GPUs. This adaptability is particularly important when deploying in varying environments with different hardware constraints.\n\nSimplified Detection Pipeline\nUnlike traditional models that require multiple stages for object detection, YOLOv8 performs detection in a single, unified pipeline. This integrated approach reduces complexity and computational overhead, which leads to faster inference times. The traditional multi-stage models often involve separate components for region proposal generation, classification, and bounding box regression. However, YOLO’s streamlined method directly predicts bounding box coordinates and class probabilities in a single step, making it more efficient for real-time applications.\n\nGrid-based Approach\nYOLOv8 divides an input image into a grid of cells, where each cell predicts bounding boxes and class probabilities for the objects it contains. This grid-based spatial approach simplifies the detection task, allowing each grid cell to focus on a smaller region of the image, reducing overall processing time. Although the grid-based approach could struggle with detecting small objects or objects spanning multiple cells, YOLOv8 mitigates this challenge by incorporating anchor boxes and using multiple grid scales, improving the model’s ability to handle various object sizes.\n\nDirect Bounding Box Regression\nAnother advantage of YOLOv8 is its direct bounding box regression, meaning the model directly predicts bounding box coordinates and class labels from the grid cells. This contrasts with models like Faster R-CNN, which require an additional region proposal network to hypothesize potential bounding box locations before classification. By simplifying the detection pipeline and predicting bounding boxes alongside class probabilities, YOLOv8 achieves faster inference times and more consistent predictions.\n\nReal-Time Performance and Versatility\nThe most compelling reason for choosing YOLOv8 is its real-time performance. YOLOv8 is capable of processing images at speeds that make it suitable for time-sensitive applications such as autonomous driving, where quick decision-making is critical. While other models like Faster R-CNN may provide slightly higher accuracy, they typically process images at slower rates, making them unsuitable for environments where speed is a priority.\n\n \n\nMoreover, YOLOv8’s versatility allows for the adaptation of the model to various platforms with different computational capabilities. It offers different model sizes, allowing it to scale depending on the hardware available, making it suitable for both edge devices with limited resources and high-performance servers.\n\nAdaptability to Real-World Scenarios\nGiven the complexity and variability of real-world environments, YOLOv8’s flexibility in handling different object scales, various lighting conditions, and backgrounds makes it an ideal choice for traffic sign detection. With the right training and dataset augmentation, YOLOv8 can effectively handle diverse traffic sign scenarios in complex driving environments.\n\nYOLOv8: Key Features Summary\nSingle-Stage Detection: YOLOv8 performs both object localization and classification in a single pass, allowing for fast and efficient real-time processing.\nUnified Detection Pipeline: By combining all detection tasks in a single network, YOLOv8 reduces complexity and training overhead compared to traditional multi-stage models.\nGrid-Based Spatial Constraints: The grid-based approach simplifies the detection task, while multi-scale anchor boxes improve its ability to handle different object sizes.\nReal-Time Performance: YOLOv8 is optimized for real-time inference, making it ideal for applications like autonomous driving.\nVersatility and Flexibility: YOLOv8 adapts to different hardware configurations and can scale depending on the resources available.\nVoxelNeXt for 3D Detection\n\nVoxelNeXt is an advanced deep learning architecture designed for 3D object detection using voxelized representations of LiDAR point cloud data. This approach leverages the 3D nature of the data, making it ideal for applications in autonomous driving, where understanding the spatial relationships between objects is critical.\n\nVoxelization: Point cloud data is first converted into a voxel grid, enabling the model to process the data efficiently by reducing the dimensionality while maintaining essential spatial information.\nVoxel Feature Encoding: VoxelNeXt uses specialized encoding layers to extract features from each voxel, capturing both the spatial structure and object information.\n3D Convolutional Neural Networks (CNNs): After voxel feature encoding, 3D CNN layers process the voxel grid to detect objects by learning spatial hierarchies from the data.\nSparse Convolutions: To enhance computational efficiency, sparse convolutions are employed, focusing processing on non-empty voxels, which reduces overhead while maintaining accuracy.\nRegion Proposal Network (RPN): The RPN generates candidate regions of interest where objects are likely to be located, which are refined in subsequent stages.\nBounding Box Regression and Classification: The final stages of the network refine object proposals by predicting 3D locations and classifying objects (e.g., car, pedestrian, cyclist).\nKey Features of VoxelNeXt:\nVoxel-Based Representation: VoxelNeXt uses voxel grids to represent point cloud data, which is more efficient for large-scale 3D data processing compared to traditional point-based methods.\nSparse Convolutions: These layers reduce computation by focusing only on non-empty voxels, essential for real-time applications like autonomous driving.\n3D Convolutional Layers: These layers enable the model to understand complex spatial relationships in 3D, crucial for detecting objects and understanding their positions relative to others.\nRegion Proposal Network (RPN): This feature improves the model’s detection accuracy and speed by efficiently narrowing down areas where objects are likely to be located.\nScalability: VoxelNeXt is highly scalable, capable of processing large, dense point clouds, making it suitable for real-time applications in autonomous driving.\n4. Simulation\n4.1 Simulate Reality\n\nWhen evaluating models, traditional metrics such as accuracy, precision, recall, and mean average precision (mAP) provide a quantitative assessment of performance in controlled environments. These metrics help in comparing different models and tracking improvements. However, real-world performance often differs due to unpredictable factors like environmental conditions, sensor noise, and hardware limitations. To truly assess how well a 3D object detection model will function, testing it in a realistic setting is crucial.\n\nChallenges of Real-World Testing\n\nWhile it might seem ideal to set up sensors on a vehicle and drive around to collect real-world data, this approach is impractical and ethically risky. Testing an untested model in a high-risk environment, such as a populated area, can be dangerous and could result in accidents or unintended consequences. Therefore, before live testing, ensuring a model performs well in controlled conditions is essential.\n\nOne possible option is using closed test tracks, where vehicles with sensors can operate in safer, contained environments. However, this method is costly for many individuals and smaller teams, as it requires substantial investment in vehicles, sensors, and specialized equipment. Even large corporations may find frequent physical tests inefficient, wasting both time and money.\n\nThe Role of Simulators\n\nThis is where simulators become invaluable. A simulator provides a virtual environment that mimics real-world complexities in a safe and controlled manner. High-fidelity simulators allow models to be tested under various scenarios, such as different weather conditions, times of day, or traffic levels, without any physical risk or the need for expensive equipment. Through simulation, we can introduce environmental factors that would be difficult or dangerous to recreate in real life, such as vehicle detection in extreme weather or simulating high-speed driving in dense urban traffic.\n\nAdvantages of Simulation\n\nOne significant advantage of simulation is the ability to accelerate time. Instead of spending hours navigating city traffic to evaluate a model's performance, a simulator can compress time, enabling hours of real-world driving to be simulated in a fraction of the time. This efficiency allows developers to run more tests, gather data faster, and iterate on models more quickly.\n\nMoreover, simulators offer a consistent, reproducible environment for testing, which is invaluable for debugging and fine-tuning models. In real-world tests, replicating identical conditions for each test can be nearly impossible. However, in simulation, every aspect of the environment can be controlled, enabling precise comparisons between different model configurations or versions.\n\n4.2 Carla Simulator\n\nCARLA Simulator was chosen for this project because it is open-source, offering flexibility for customization and integration into our research. Unlike proprietary systems with high licensing fees, CARLA allows us to modify its code to meet the specific needs of our project, making it ideal for academic research and experimental development.\n\nFigure 4. Carla Simulator Logo\n\nCARLA Features\n\nRealistic Urban Simulation:\nCARLA simulates urban environments, making it perfect for testing autonomous vehicle models. It uses the Unreal Engine for accurate visuals and physics, including gravity, collisions, and road friction. This ensures that vehicles behave realistically, similar to how they would in the real world.\n\nActors:\nIn CARLA, actors are all the entities in the simulation, like vehicles, pedestrians, and traffic signs. These actors can follow traffic rules, interact with each other, and simulate real-world driving behaviours, making it ideal for testing object detection models.\n\nMaps and Customization:\nCARLA provides detailed maps of urban and suburban areas, which can be customized to fit specific testing scenarios. Users can create new environments that mimic real-world locations, enhancing the model's ability to generalize in various conditions.\n\nSensor Suite:\nCARLA simulates essential sensors used in autonomous vehicles, such as cameras, LiDAR, radar, and GPS. These sensors provide synthetic data that closely matches real-world sensor inputs, which is crucial for training and testing 3D object detection models.\n\nTraffic Simulation:\nCARLA also simulates traffic systems, including vehicles and pedestrians following traffic laws. This creates realistic conditions for testing how models handle busy intersections, lane changes, and other complex traffic scenarios.\n\nTime Acceleration:\nOne key advantage of CARLA is the ability to speed up time during testing, allowing hours of real-world driving to be condensed into a much shorter period. This accelerates the development process and allows for rapid model iteration. Additionally, testing in a simulator eliminates the ethical risks of real-world testing, where untested models could cause accidents.\n\n4.3 Client-Server Architecture\n\nCARLA Simulator operates on a scalable client-server architecture, which is crucial for its flexibility and performance. The server handles all core tasks of the simulation, including rendering sensors, calculating physics, updating the environment and actors, and more. For optimal performance, especially when using machine learning models, it's recommended to run the server on a dedicated GPU. This helps process computationally demanding tasks, such as rendering detailed 3D environments and handling large sensor data (e.g., from LiDAR and cameras) without slowing down the system.\n\nFigure 5. Carla API workflow\n\nThe client manages the logic of the actors (e.g., vehicles, pedestrians) and sets the conditions of the world. Clients communicate with the server using the CARLA API, available in both Python and C++, allowing users to control the simulation, manipulate the environment, and retrieve data from sensors. The API is regularly updated, making CARLA highly adaptable for autonomous driving research.\n\nKey CARLA Components\n\nTraffic Manager:\nThis built-in system controls all vehicles in the simulation except the ego vehicle (the one being tested or trained). It ensures that vehicles behave realistically, following traffic rules and responding to events like intersections and pedestrian crossings.\n\nSensors:\nCARLA offers a rich set of sensors, including RGB cameras, depth cameras, LiDAR, radar, and GPS. These sensors mimic real-world autonomous vehicle sensors and can be attached to vehicles in the simulation. The data collected can be streamed or stored for later analysis, making it easier to train and evaluate models.\n\nRecorder:\nThe recorder feature tracks the state of every actor in the simulation, enabling users to replay events frame by frame. This is especially useful for debugging, as it allows users to trace actions and interactions during the simulation.\n\nROS Bridge and Autoware Integration:\nCARLA supports integration with Robot Operating System (ROS) and Autoware, an open-source autonomous driving stack. These integrations allow CARLA to interact with other simulation tools and real-time environments, broadening testing capabilities.\n\nOpen Assets:\nCARLA includes a variety of assets, such as urban maps, weather conditions, and actor blueprints. These assets are customizable, allowing users to create tailored environments. The ability to control weather and lighting conditions adds realism, enabling simulations of diverse driving scenarios, including rain, fog, or night driving.\n\nScenario Runner:\nCARLA includes predefined driving scenarios, such as urban routes and common traffic situations. These scenarios are used in the CARLA Challenge, an open competition where participants test their autonomous driving solutions. Scenario Runner automates test setup, allowing vehicles to repeatedly encounter specific situations to improve their responses.\n\n4.3.1 Server-Side Simulation\n\nTown 10 is the default map for server-side simulation. It combines suburban and urban areas with multiple intersections, providing a realistic testing environment. The map includes:\n\nFigure 6. Town 10 view\n\n50 vehicles using autopilot, following a waypoint algorithm to simulate everyday traffic with actions like stopping, turning, and interacting with intersections.\nPedestrians are introduced at five times the number of vehicles, simulating modern cities where foot traffic is common. Pedestrians follow random paths but can also be programmed for specific behaviours, like crossing during a red light.\n\nTo ensure accurate sensor data, the simulation is set to synchronous mode, aligning all actions and sensor readings at fixed time intervals. This setup guarantees reliable data for testing and model training.\n\n4.3.2 Client-Side Simulation\n\nThe ego vehicle in the simulation is an Audi A2, a compact hatchback commonly seen in Europe. It’s equipped with LiDAR and four RGB cameras, which provide critical data for object detection and navigation.\n\nLiDAR Sensor Specifications\nRange: 100 meters (with a 75-meter detection model buffer for smoother transitions).\nRotation Frequency: 60 FPS, synchronized with the world FPS, ensuring real-time data collection.\nChannels: 64 channels for balanced vertical resolution.\nPoints per Second: 1 million points for detailed point cloud data.\nFields of View:\nUpper FOV: 10° for detecting elevated objects like traffic lights and bridges.\nLower FOV: -30° for detecting obstacles on or below the road surface.\nHorizontal FOV: 360° for full 360-degree coverage around the vehicle.\nRGB Camera Sensor Specifications\nResolution: 1920x1080 pixels for high-definition image quality.\nPosition: 2 meters above the vehicle’s centre for optimal field of view.\nOrientation: Four cameras (front, rear, left, right) for 360-degree visual coverage, similar to real-world autonomous vehicles.\n\nThese sensors provide the necessary data for object detection and scene understanding, ensuring the simulation accurately reflects real-world driving conditions.\n\n4.4 Real-time Processing in 3D\n\nReal-time processing in autonomous vehicle simulations presents a significant challenge, primarily due to the need to parse and compute vast amounts of data within a very limited time frame. Achieving real-time performance requires optimizing each step of the data pipeline to minimize execution time, ensuring that sensor data can be processed, interpreted, and visualized quickly enough to make decisions in real-time. This involves not only efficient data handling but also high-performance visualization tools to interpret complex data outputs, such as 3D point clouds and camera feeds, all within milliseconds.\n\nTo meet these requirements, the simulation and data transformation processes are handled using Python, a high-level language known for its flexibility. Python enables easy integration with high-performance code written in lower-level languages like C, C++, or Rust, where necessary, to maximize efficiency while retaining Python’s user-friendly nature. The simulation leverages both the CARLA Simulator API and the PyTorch API to handle the simulation and machine learning inference in real time.\n\nFigure 7. Real time system architecture\n\nThe CARLA Simulator API allows for direct communication between the simulation environment and the vehicle’s sensors. However, in addition to controlling the simulation, real-time results need to be processed from the machine learning models driving the perception system. This is achieved by directly interfacing with the PyTorch API, which allows the inference model to be called and applied to the live sensor data. PyTorch is responsible for running the object detection model, taking in sensor data (such as camera images or LiDAR point clouds), and outputting the detected objects, classifications, and bounding boxes.\n\nBy calling the PyTorch API from within the Python environment, sensor data from CARLA can be directly fed into the object detection model for processing. This allows real-time inferences to be made on the incoming data streams, delivering immediate feedback on what the model detects in the environment. The flexibility of PyTorch enables fast computation on both CPU and GPU, ensuring that the processing pipeline remains optimized for performance. This approach eliminates the need for post-processing delays, as the model inference happens in sync with the simulation, allowing for continuous data flow and decision-making.\n\nSince CARLA Simulator separates the client-side data processing from the server-side simulation, it becomes critical to visualize the data in real-time. Humans rely heavily on visual inputs for interpretation, so displaying sensor data as it’s captured by the vehicle is crucial for understanding how the system responds to its environment. The integration of the PyTorch API allows for this data to be processed and rendered in real-time, giving immediate insight into how the model is interpreting the sensor data.\n\nA Flask Application serves as the core for handling real-time data outputs from the autonomous driving model. After some preprocessing, the data is adapted for rendering in the front end. The Flask application manages the data flow between the model and the visual interface, offering a lightweight but efficient framework for serving real-time data. This modular approach allows for flexibility in data handling and processing, separating the simulation data gathering from its visualization.\n\nThe camera data from CARLA is initially received in BGR format (Blue, Green, Red), which is the standard image format returned by the simulator. This data must then be processed and transformed into RGB format to align with standard display requirements, ensuring correct color representation. Each camera feed produces an array of shape (1080, 1920, 3), corresponding to the height, width, and three color channels (red, green, and blue). By processing these camera feeds in real-time, it becomes possible to visualize multiple views simultaneously, which is critical for understanding the vehicle’s surroundings from various perspectives.\n\nAdditionally, LiDAR data is transformed from its raw float32 format, which represents the position and intensity of each point in the cloud, into an integer format of shape (N, 4). This transformation compresses the data into a manageable form for rendering and analysis, where each point consists of its X, Y, Z coordinates, and intensity value. Handling LiDAR data efficiently is key to ensuring the vehicle has a precise understanding of its environment in real-time, especially in dense urban settings where point cloud data must be processed quickly.\n\nFor the 3D visualization, the Flask application uses Plotly.js as the backend to render the real-time 3D data. Plotly.js is a robust library that supports high-performance 3D plotting, enabling users to interact with the data through zooming, panning, and rotating the view without suffering rendering slowdowns. This interactivity is essential for evaluating the performance of the autonomous vehicle model in a complex 3D environment, providing insights into how the vehicle processes point cloud data and detects objects. Websockets are used to facilitate real-time communication between the Flask application and the 3D rendering, ensuring that updates are delivered with minimal latency.\n\nThe use of websockets enables seamless, two-way communication between the data processing module and the rendering interface. This separation of concerns allows the data processing to happen in one module, while the visualization runs independently, providing a smooth, fluid user experience. Data is sent asynchronously between the server (which runs the simulation) and the client (which handles the real-time visualization), ensuring that sensor data is immediately available for interpretation.\n\nThe following three visual outputs illustrate the same simulation frame from different perspectives:\n\nCamera Grid: A grid display of the four RGB camera feeds from the vehicle, providing comprehensive visual coverage of the vehicle’s surroundings. The grid view helps visualize what the car’s cameras are capturing in real-time, offering insights into how the vehicle sees the road, pedestrians, and other obstacles.\n\nFigure 8. Camera views on real time app\n\n3D Plot of LiDAR Data: A real-time 3D plot displaying point cloud data from the LiDAR sensor. This visual can demonstrate how objects are detected and represented in 3D space, including the processed voxels and the identification of objects surrounding the vehicle. This is essential to visualize how the environment is interpreted in 3D, with objects like pedestrians and vehicles plotted in real time.\n\nFigure 9. LiDAR point cloud view on real time app\n\nSimulator Screenshot: A screenshot taken directly from the CARLA simulation, showing the vehicle in its environment on the server side. This gives context to how the vehicle navigates through the virtual world, interacting with other actors like cars and pedestrians in real time.\n\nFigure 10. Screenshot of server side simulation scene\n\n4.5 Scalability and Resource Optimization\n\nThe modular design of the simulation framework, which leverages both the CARLA Simulator API and PyTorch for machine learning inference, allows for significant scalability and resource optimization. This approach simplifies the development process by separating different components (such as simulation, sensor data processing, and machine learning model inference), facilitating expansion and optimization without requiring major structural changes.\n\nScalability\n\nScalability is achieved through the modularity of the system, enabling individual components, like data collection, inference, and visualization, to be distributed across different systems or scaled up as needed. For instance, sensor data from multiple vehicles can be processed simultaneously, with each instance running independently and communicating with the centralized model via websockets. This setup is adaptable and can handle increased data flow without overloading the system, making it suitable for larger-scale simulations with numerous vehicles and pedestrians.\n\nResource Optimization\n\nSeveral strategies can be implemented to improve resource optimization:\n\nMultithreading:\n\nMultithreading enhances processing efficiency by parallelizing tasks, such as sensor input handling, model inference, and visualization updates. The system can fully utilize modern multi-core processors, ensuring that no single process becomes a bottleneck. For example, sensor data processing for cameras and LiDAR can run on separate threads, while PyTorch model inference is processed on another. This improves real-time performance by reducing wait times between stages.\n\nSpecialized Hardware:\n\nGPUs and TPUs (Tensor Processing Units) are used for tasks requiring parallel processing, such as rendering simulated environments in CARLA or running neural network computations in PyTorch. Offloading tasks to dedicated hardware allows the system to process large amounts of data efficiently, enabling higher-fidelity simulations. Additionally, specialized processors like FPGAs (Field-Programmable Gate Arrays) and ASICs (Application-Specific Integrated Circuits) can accelerate specific computations, such as LiDAR point cloud processing or image classification tasks, improving both speed and energy efficiency.\n\nLoad Balancing and Distributed Processing:\n\nAs the simulation grows more complex, tasks like simulation, model inference, and data visualization may need to be distributed across multiple machines. Load balancing ensures tasks are evenly distributed across available hardware, improving system efficiency and preventing any machine from becoming a bottleneck. This approach optimizes resource usage, maintaining high performance even with larger simulations involving multiple vehicles, pedestrians, and complex environments.\n\nBy employing these strategies, the system can scale to handle larger simulations without sacrificing performance, ensuring smooth real-time processing, even as simulation complexity increases.\n\n4.6 Integration with 2D Systems\n\nIntegrating 3D object detection results from LiDAR data with 2D camera views is crucial for creating a unified perception system, often called sensor fusion. This involves projecting the 3D bounding boxes (bbox) predicted by the model from LiDAR point clouds onto the 2D image plane of the vehicle’s cameras using a projection matrix. This enables a more comprehensive view of the environment, where 3D detections from LiDAR can be visualized within the 2D camera feed, similar to how objects are rendered in video games.\n\nProjection Process\n\nThe projection process relies on both intrinsic and extrinsic camera parameters:\n\nIntrinsic parameters: These define the camera’s internal characteristics, such as its focal length, sensor size, and the principal point (center of the image). They are essential for mapping 3D points onto the 2D image plane and controlling how the scene appears in the camera’s view.\n\nExtrinsic parameters: These describe the camera’s position and orientation relative to the vehicle or LiDAR sensor. They define the transformation required to convert 3D points from the LiDAR’s coordinate system into the camera’s coordinate system.\n\nOnce the transformation is completed, the 3D bounding boxes can be represented on the 2D image plane of the cameras by applying the appropriate projection matrix.\n\nPractical Application\n\nThis process works similarly to how objects in 3D games are rendered on a 2D screen. In a 3D scene, objects are represented with depth (X, Y, Z coordinates), but when displayed on a 2D screen, these objects must be projected according to the camera’s viewpoint.\n\nIn autonomous driving, 3D bounding boxes (for detected cars, pedestrians, etc.) are projected into the camera images, allowing both 2D and 3D information to be merged for a better understanding of the vehicle’s surroundings.\n\n5. Conclusions and Further Research\n\nThe research and implementation conducted throughout this project offer valuable insights into the design, testing, and optimization of autonomous driving systems, particularly with regard to integrating 3D and 2D data, real-time processing, and scalability. These systems play a critical role in ensuring that autonomous vehicles can navigate and interact safely with complex environments, utilizing a range of sensors such as LiDAR and cameras to perceive the world around them. This section outlines the conclusions drawn from the research and offers suggestions for further investigation and development in the field.\n\n5.1 Data Analysis\n\nAfter conducting simulations with the YOLO model and VoxelNeXt, a reflective analysis of the results provides critical insights into the performance and limitations of these models in real-world autonomous driving applications.\n\nYOLO Model Performance\n\nThe YOLO model demonstrated a strong ability to detect smaller objects, such as traffic signs, at a distance. Specifically, its capability to detect small traffic signs early, even from afar, was one of the key strengths observed during the simulations. This is crucial for autonomous systems where early detection of traffic signs (like stop signs or speed limits) affects vehicle decision-making. However, an issue arises when the model encounters scenes with more than five traffic signs within a single image. In such cases, the detection accuracy starts to decline, likely due to the inherent complexity of processing multiple objects within a constrained computational framework.\n\nTo address this issue, one possible optimization strategy would be to implement a two-stage detection system. In this approach, a lighter model could first be specialized to detect traffic signs only, cropping their bounding boxes, while a second model could handle other objects in the scene. This modular system could theoretically improve detection accuracy by assigning dedicated resources to specific tasks. However, this approach introduces additional complexity and would increase overall inference time, as it involves running multiple models in sequence.\n\nGiven the constraints of real-time processing, such as minimizing inference time, the decision was made to use a single YOLO model with data augmentation techniques. This approach offers a balance between speed and accuracy, ensuring that the system remains efficient while maintaining reasonable performance in multi-object detection scenarios. Data augmentation helped to improve the model’s ability to generalize across diverse scenarios, reinforcing the choice to prioritize a single model.\n\nVoxelNeXt Performance\n\nThe simulations conducted using VoxelNeXt revealed different strengths and weaknesses compared to the YOLO model. One of the primary challenges observed was class confusion at longer ranges, especially beyond 40 meters. At these distances, the model sometimes struggled to accurately differentiate between objects, leading to classification errors. This is likely due to the nature of LiDAR data at long distances, where the point cloud becomes increasingly sparse. When fewer data points represent an object, the model’s ability to infer precise characteristics, such as shape and class, is diminished. Additionally, missed detections were more common in the 40+ meter range. This issue again ties back to the sparsity of data points in LiDAR detection, which causes the model to lose precision when detecting small or distant objects. Despite these challenges, VoxelNeXt performed well in closer ranges, where point density was higher, enabling accurate and consistent object detection.\n\n5.2 ONNX & TensorRT\n\nONNX (Open Neural Network Exchange) and TensorRT are two advanced tools widely used to optimize and deploy machine learning models, especially in real-time applications that demand low latency and high performance, such as autonomous driving. As models become increasingly complex, especially in fields like 3D object detection or scene understanding, it becomes crucial to ensure that they can be deployed efficiently without sacrificing speed or accuracy. These tools allow developers to streamline the deployment process while maintaining the performance necessary for real-time inference.\n\nONNX\n\nONNX (The Linux Foundation, 2019) is an open-source format for representing machine learning models, developed by Microsoft and Facebook. The core idea behind ONNX is to enable interoperability between different machine learning frameworks. This means that models trained in frameworks like PyTorch or TensorFlow can be converted into ONNX format, allowing them to be easily transferred to other platforms (such as Caffe2, MXNet, or TensorRT) without needing to retrain or rewrite the model. This flexibility facilitates smoother transitions between research and production environments.\n\nAdvantages of ONNX:\n\nInteroperability: By providing a unified format, ONNX enables seamless transitions between different frameworks, reducing dependency on a single tool or environment during development.\nFlexibility: Developers can train models in their preferred framework and then export them to other environments more suited for real-time or production applications.\nOptimization: ONNX includes various optimization techniques that help reduce model size and improve inference speed, making it particularly suitable for deployment on resource-constrained hardware.\nTensorRT\n\nTensorRT (Nvidia Corporation, 2019) is a high-performance deep learning inference engine developed by NVIDIA, specifically designed to optimize machine learning models for deployment on NVIDIA GPUs. TensorRT takes models (often exported in ONNX format from frameworks like PyTorch or TensorFlow) and applies several optimization techniques to accelerate inference. These optimizations are essential in real-time applications, such as autonomous driving, where low-latency detection and decision-making are critical.\n\nAdvantages of TensorRT:\n\nHigh Performance: TensorRT employs techniques such as precision calibration (using FP16 or INT8) to reduce memory usage and improve inference speed, drastically cutting down on latency.\nNVIDIA Hardware Optimization: TensorRT is highly optimized for NVIDIA GPUs, allowing it to fully leverage the parallel computing power of these devices.\nReal-Time Inference: TensorRT is ideal for scenarios where minimal latency is crucial, such as detecting objects in real-time while a vehicle is in motion.\n\nWhile neither ONNX nor TensorRT were implemented within the scope of this research, these tools offer significant advantages that could enhance future deployments of the models. For instance, models like YOLO or VoxelNeXt, which were developed and trained using PyTorch, could be converted into ONNX format. From there, the models could be imported into TensorRT for real-time optimization on NVIDIA GPUs. This would result in reduced inference time, making these models ideal for real-world, real-time applications where rapid decision-making is essential, such as autonomous vehicle navigation.\n\nHowever, implementing ONNX and TensorRT requires additional considerations and infrastructure that go beyond the current focus of this project, which primarily aimed to develop and test models within the simulation environment. Future research could explore how these tools can be integrated to optimize model performance for real-world deployment.\n\n5.3 Practical Applications and Impact\n\nThe outcomes of this research have significant implications for both data-driven systems and the future of autonomous driving. These findings offer practical applications in refining object detection, data labeling, and real-time processing, all of which are critical to ensuring that autonomous vehicles can navigate complex environments safely and efficiently.\n\nBy leveraging advanced models like YOLO and VoxelNeXt, which integrate 2D and 3D data for object detection, this research allows for more precise interaction between autonomous systems and their surroundings. Autonomous driving systems can use this technology to detect objects such as traffic signs, pedestrians, and other vehicles with greater accuracy. Early detection of traffic signs, for instance, enables vehicles to make better-informed decisions, significantly improving response times and reducing the likelihood of accidents. This not only enhances the individual vehicle's safety but also facilitates smoother interaction between multiple autonomous systems, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication networks.\n\nThe enhanced ability to predict data patterns with high accuracy could transform the way autonomous driving systems interact with other systems and with each other. In the future, this research could help establish a framework where autonomous vehicles operate within a connected ecosystem, optimizing traffic flow, reducing congestion, and improving safety by sharing real-time data.\n\nBeyond autonomous driving, the potential impact of this research extends to any domain that relies on accurate, real-time detection and processing of sensor data, such as robotics, industrial automation, and healthcare. As these models improve in both accuracy and efficiency, they can be adapted to various applications that require sophisticated real-time decision-making based on complex sensor inputs.\n\n5.4 Limitations of Current Development\n\nWhile the research has made significant progress in developing reliable 2D and 3D object detection systems, several limitations remain. The YOLO-based 2D system performs well in detecting small and low-resolution objects, even in challenging environments, but struggles with multiple labels or densely packed scenes. Detection accuracy begins to decline when more than five objects are present within the same frame, likely due to computational limitations and the complexity of managing multiple bounding boxes.\n\nThe 3D system, which uses LiDAR and other depth sensors to manage millions of points in real-time, has shown strong performance at close and medium ranges. However, detection accuracy begins to degrade at distances over 40 meters, where the data from LiDAR becomes sparse. This sparseness makes it difficult for the system to accurately classify and detect objects, resulting in missed detections or misclassifications.\n\nAnother critical limitation is the size of the datasets required to train these models. Managing hundreds of gigabytes (and potentially terabytes) of data poses significant challenges, particularly in terms of storage, processing power, and time. The computational cost of parsing and training on these large datasets is immense, often taking several weeks or even months to complete a full training cycle. While smaller subsets of data can be used for testing purposes, training on the full dataset is necessary to achieve the highest level of accuracy. Unfortunately, even small errors during this phase can lead to significant penalties in terms of time, as retraining the entire model can further delay the process.\n\nThese limitations suggest that while the current systems are functional and offer significant promise, there is still much work to be done to improve scalability, computational efficiency, and performance, particularly in handling large datasets and real-world complexities.\n\n5.5 Further Research Directions\n\nWhile the current work presents valuable insights into autonomous driving systems, several areas remain ripe for further research and development. The following sections outline potential directions for future work to build on the foundation laid by this project.\n\nAdvanced Fusion Techniques: The integration of 2D and 3D data has proven to enhance object detection, but there is room for improving sensor fusion methods. Future research could focus on developing more sophisticated algorithms for combining the strengths of LiDAR, camera, radar, and other sensors. By improving the fusion pipeline, autonomous systems could better handle ambiguous or occluded objects, which are often challenging to detect using a single modality.\n\nHandling Sparse Data in Long-Range LiDAR Detection: As mentioned, LiDAR data becomes sparse at greater distances, leading to decreased detection accuracy. Further research could explore techniques for overcoming this limitation, such as using more advanced filtering or interpolation techniques to enhance long-range detection. Alternatively, integrating high-resolution LiDAR systems or combining multiple sensors could address some of these challenges.\n\nReal-Time Optimization with ONNX and TensorRT: Future work could explore the full potential of ONNX and TensorRT for optimizing model deployment. While these tools were not fully integrated into this project, they could significantly improve the real-time inference speed and scalability of complex models. Research into deploying YOLO and VoxelNeXt models using ONNX and TensorRT on various hardware configurations (such as NVIDIA GPUs) would be valuable, especially for large-scale and resource-constrained applications.\n\nMulti-Agent and Vehicle-to-Vehicle Communication: In the realm of autonomous driving, communication between vehicles (V2V) and between vehicles and infrastructure (V2I) is an essential feature for increasing situational awareness and reducing traffic hazards. Future research could explore methods for enhancing V2V communication, using real-time data from the sensor networks of multiple vehicles to improve decision-making algorithms. This would contribute to the development of a more interconnected and collaborative autonomous vehicle network.\n\nDataset Expansion and Management: The computational costs of training models on large datasets remain a challenge. Future research could focus on developing more efficient methods for dataset augmentation, dataset management, and distributed training. Exploring techniques like federated learning, where model training occurs on decentralized devices while maintaining data privacy, could also be an avenue worth investigating.\n\nAutonomous System Testing in Dynamic Environments: While this project focused on simulations, further research should aim to validate the proposed systems in real-world, dynamic environments. Conducting large-scale testing involving various traffic scenarios, different weather conditions, and varying vehicle types would help identify edge cases and ensure that the system can handle the unpredictability of real-world driving.\n\nEnergy Efficiency and Sustainability: Finally, a growing focus on sustainable and energy-efficient systems in autonomous driving is needed. Research could explore how to reduce the power consumption of sensors, onboard processors, and communication systems while maintaining high performance. Given the computational load of running models like YOLO and VoxelNeXt in real-time, optimizing the system's energy consumption would be a key development in advancing autonomous driving technologies.\n\n5.6 Prospective Development\n\nFurther development of these systems will need to focus on deeper integration with the vehicle’s architecture, particularly through optimizing communication pathways such as the Controller Area Network (CAN-bus). The CAN-bus serves as the central communication hub within a vehicle, ensuring that sensors, processors, and actuators can communicate with one another in real-time. To fully realize the potential of systems like YOLO and VoxelNeXt in autonomous driving, it is critical to ensure that the data processed by these models is relayed quickly and accurately to the vehicle’s control systems, such as those responsible for braking, steering, and throttle control.\n\nThe current architecture allows for robust detection of objects, but future improvements should focus on minimizing latency and ensuring seamless communication between the detection models and the car’s distributed systems. For instance, the CAN-bus needs to handle high data rates while maintaining low latency to ensure that the vehicle can react to sudden changes in its environment. This is particularly important in high-speed scenarios where every millisecond counts.\n\nIn addition to enhancing the internal vehicle architecture, future research could explore the use of reinforcement learning to fine-tune the system’s ability to adapt to complex driving scenarios. By continuously learning from real-world sensor data, the models could be trained to anticipate and react to various traffic situations, such as predicting pedestrian movement or identifying anomalies in road conditions. Reinforcement learning could also help optimize resource allocation, allowing the system to dynamically adjust its processing power based on the complexity of the environment.\n\nLastly, ensuring that these systems include built-in redundancy and fail-safes is crucial. In the event of sensor failure or misinterpretation of data, the system must have alternative pathways to ensure safe vehicle operation. These fail-safe mechanisms would ensure that, even in the event of partial system failure, the vehicle can continue to operate safely.\n\n5.7 Reflection on Ethical and Social Implications\n\nThe development of autonomous driving systems presents not only technical challenges but also ethical and social considerations that must be addressed. Autonomous vehicles operate without human drivers, meaning that their decision-making processes must be programmed into their systems in a way that accounts for the potential consequences of those decisions. However, machines lack the ability to reflect on moral dilemmas or consider the broader ethical implications of their actions.\n\nA classic ethical problem, known as the Trolley Problem, highlights the type of decisions autonomous vehicles might face. In this scenario, a person must decide whether to let a trolley continue on its current track, where it will kill five people, or divert it to another track, where it will kill one person. Autonomous vehicles might encounter similar dilemmas in real-world scenarios: for example, choosing between swerving to avoid a pedestrian who has illegally crossed the street and potentially colliding with another vehicle or pedestrian who is following the rules. These split-second decisions have life-or-death consequences, yet autonomous systems cannot consider the moral implications in the same way a human might.\n\nFigure 11. The trolley problem\n\nMoreover, the introduction of autonomous driving into society raises questions about accountability and responsibility. If an autonomous vehicle is involved in an accident, who is responsible? Is it the manufacturer, the designer of the AI, or the owner of the vehicle? These questions highlight the need for new regulations and ethical frameworks that can guide the deployment of these systems in a way that aligns with societal values.\n\nAutonomous driving also has broader social implications, particularly concerning employment. As more advanced autonomous systems are developed, there is the potential for job losses in industries such as transportation and logistics. Balancing technological progress with the social impact of automation will require careful planning, including initiatives that ensure displaced workers have opportunities to transition into new roles.\n\n5.8 Final Conclusions\n\nThe research and development carried out throughout this project offer a promising foundation for the future of autonomous driving systems. By integrating 2D and 3D detection models with real-time processing capabilities, the systems developed here provide the necessary building blocks for a more advanced and reliable autonomous driving framework. However, the research also underscores the significant challenges that remain, particularly regarding data processing, system scalability, and ethical considerations. As further research continues to refine these technologies, there is great potential for these systems to be further developed, ensuring that autonomous vehicles can safely navigate complex environments and adapt to the evolving challenges of the transportation landscape.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAdvancing Autonomous Driving: Deep Learning Innovations for Enhanced Perception and Decision-Making\n\nAbstract\n\nIntroduction\nRelated Works\n\nTraditional Image Processing Techniques\n\nAdvances in Deep Learning for Object Detection\n\nYOLO (You Only Look Once):\n\nVoxel-based 3D Detection:\n\nHybrid Approaches:\n\nEvaluation Metrics and Comparative Analysis\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nFiles\nAdvance Autonomous Driving- Deep Learning Innovations for Enhanced Perception and Decision-Making.pdf\nYou might be interested\nSimple Autonomous Driving Vehicle\nK\nDec 26, 202423 reads\nADASAI+9\nVision-Based Perception Systems for Autonomous Vehicles\nMost Promising Innovation\nF\nA\nA\nI\nF\nDec 30, 202472 reads\nautonomousAutonomous vehicles+16\nAutonomous Driving - Car Detection\nDec 30, 202412 reads\ncar detection systemNon-max Suppression+1\nAdvanced Self Driving Car for Egyptian Streets (V2X)\nW\nDec 24, 202416 reads\nADASBehavior Cloning+12",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "agentconnect-decentralized-collaboration-framework-for-independent-ai-agents-RLFuglEDiwwS",
    "username": "aAkshat Joshi",
    "license": null,
    "title": "AgentConnect: Decentralized Collaboration Framework for Independent AI Agents",
    "publication_description": "Back to publications\nMar 31, 2025\n●\n337 reads\n●\nApache 2.0\nWinner of\nBest Overall Project\nat the\nAgentic AI Innovation Challenge 2025\nAgentConnect: Decentralized Collaboration Framework for Independent AI Agents\nAgent Discovery\nAgent Framework\nAgentConnect\nAI Interoperability\nAutonomous Agents\nCapability-Based Discovery\nDecentralized AI\nInter-Agent Communication\nLLM\nMulti-Agent Systems (MAS)\nOpen Source\nScalable AI Systems\nSecure Communication\nA\nAkshat Joshi\nR\nRavikant Agrawal\nLike\nBookmark\nShare\nTL;DR\n\nAgentConnect provides a novel, decentralized communication and discovery layer enabling independent AI agents—potentially possessing their own complex internal structures—to dynamically find, interact, and collaborate securely. Unlike traditional, centrally controlled multi-agent systems, AgentConnect fosters a peer-to-peer ecosystem where agents built using diverse tools and frameworks can interoperate based on advertised capabilities. It features capability-based discovery via a decentralized registry, secure message routing through a communication hub, robust security protocols, multi-provider LLM support, and integration with LangSmith for monitoring, paving the way for truly scalable and flexible collaborative AI applications.\n\nIntroduction: The Interoperability Challenge for Independent Agents\n\nImagine building a specialized AI agent for financial analysis, while another developer creates a powerful web research agent. How can these two independently developed and operated agents discover each other's capabilities and collaborate on a task requiring both skills? This fundamental challenge of interoperability hinders the development of complex, emergent AI ecosystems. Existing multi-agent frameworks often operate within closed, centrally managed environments, limiting agent autonomy and preventing seamless collaboration between agents from different origins or built with different technologies.\n\nAgentConnect tackles this challenge head-on. It is not another framework for building internal multi-agent workflows within a single system (though agents using AgentConnect can certainly employ such internal structures). Instead, AgentConnect provides a crucial decentralized communication and discovery layer above individual agent implementations. It enables true peer-to-peer collaboration in a scalable, secure, and flexible manner, fostering a \"universe\" of interoperable AI agents where specialized capabilities can be dynamically discovered and leveraged across boundaries.\n\nThis publication introduces the AgentConnect framework, details its innovative architecture, highlights key features, showcases practical examples, and discusses its potential impact on the future of collaborative AI.\n\nThe Problem: Limitations of Centralized Multi-Agent Systems\n\nTraditional approaches to multi-agent AI face significant limitations, particularly when attempting to connect independently developed agents:\n\nCentralized Control & Bottlenecks: Most multi-agent systems rely on a central orchestrator, limiting individual agent autonomy and creating single points of failure that hinder scalability.\nLack of Interoperability: Agents built with different frameworks or by different teams typically cannot communicate or collaborate, creating siloed ecosystems. Integration is often manual, brittle, and non-scalable.\nStatic Connections: Interactions are often pre-defined, preventing dynamic discovery of new capabilities or agents joining the network.\nScalability Challenges: Centralized architectures struggle to manage communication and coordination efficiently as the number of agents grows significantly.\nSecurity Concerns: Establishing trust and secure communication channels between independently managed agents in a decentralized way is complex without a dedicated framework.\n\nThese limitations prevent the realization of a truly dynamic and open ecosystem where diverse AI agents can seamlessly collaborate.\n\nAgentConnect: A Decentralized Solution for Inter-Agent Collaboration\n\nFigure 1: AgentConnect Ecosystem - The four foundational pillars of AgentConnect: Decentralized Discovery for finding collaborators without central control, Secure Communication for verified messaging, Agent Autonomy preserving independent operation, and Tool-Based Interaction that agents dynamically invoke based on reasoning when they cannot complete tasks independently.\n\nAgentConnect provides the necessary infrastructure for a decentralized network of autonomous agents:\n\nDecentralized Network Topology: Eliminates central control, allowing agents to operate as independent peers.\nDynamic Capability-Based Discovery: Agents find collaborators based on what they can do, facilitated by a Decentralized Agent Registry.\nSecure & Standardized Communication: A Communication Hub routes cryptographically signed messages using defined protocols, ensuring trust and interoperability.\nPreservation of Agent Autonomy: AgentConnect provides the connectivity layer; it does not dictate internal agent logic, implementation, or decision-making. Agents can use any internal architecture (e.g., LangGraph, custom code).\nHorizontal Scalability: Designed to support a vast network of interacting agents without performance degradation associated with central coordinators.\nKey Features\n\nAgentConnect offers a rich set of features designed for building robust and flexible decentralized agent networks:\n\nFigure 2: AgentConnect Key Features Flow - This comprehensive diagram illustrates the complete agent collaboration lifecycle. It shows how agents with their own internal multi-agent systems can discover others through the Registry based on capabilities, securely exchange messages through the Communication Hub, and make autonomous decisions about collaboration. The numbered flow demonstrates the end-to-end process from user input to final response.\n\nDynamic Agent Discovery: Benefit: Enables flexible, ad-hoc collaboration based on real-time needs, not static configurations. Supports exact and semantic matching.\nDecentralized Communication: Benefit: Ensures scalability and resilience by avoiding central bottlenecks. Promotes true peer-to-peer interaction logic.\nAgent Autonomy: Benefit: Maximum implementation freedom for developers. Allows agents to encapsulate complex internal processes (including their own MAS) while participating externally.\nSecure Communication: Benefit: Guarantees message authenticity and integrity using cryptographic signing (DID-based) and standardized protocols, crucial for trust in a decentralized setting.\nMulti-Provider LLM Support: Benefit: Flexibility to choose the best AI model (OpenAI, Anthropic, Groq, Google AI) for an agent's specific needs, fostering diverse capabilities within the network.\nIntegrated Monitoring (LangSmith): Benefit: Provides deep observability into complex inter-agent interactions, aiding debugging, performance analysis, and optimization.\nArchitecture Deep Dive\n\nAgentConnect's power stems from its three core pillars: the \nDecentralized Agent Registry\n, the \nCommunication Hub\n, and the principle of Independent Agent Systems. This architecture is specifically designed to facilitate interaction between distinct agent entities, regardless of their internal complexity.\n\nDecentralized Agent Registry\n\nThe \nRegistry\n serves as the network's decentralized directory. It stores AgentRegistration information, including agent IDs, verifiable identities (DIDs), and detailed capabilities. Crucially, it performs \nidentity verification\n upon registration requests (forwarded by the Hub) and facilitates \ncapability-based discovery\n when queried by agent Tools. It does not control agents but acts as a verifiable information source.\n\nCapability Discovery Service\nAgent Registry\nsearch_for_agents Tool\nInternal Workflow\nAgent A\nCapability Discovery Service\nAgent Registry\nsearch_for_agents Tool\nInternal Workflow\nAgent A\nAsync Discovery Process\nAsynchronous tool execution\nNon-blocking operation\nPerforms exact matching or\nsemantic vector search\nAgent can now initiate secure collaboration with discovered agents\nNeeds agent with\nspecific capability\nsearch_for_agents(capability=\"data_analysis\")\nquery_registry(capability=\"data_analysis\")\nfind_by_capability_name() or\nfind_by_capability_semantic()\n[matched_agents_with_scores]\n[agent_metadata, ...]\nReturn matching agents\nanalyze_results()\nSelect appropriate agent\nfor collaboration\n\nFigure 3: Agent Discovery Flow - This diagram illustrates how an agent's internal workflow uses the Registry to find peers with specific capabilities, demonstrating the dynamic discovery mechanism that enables ad-hoc collaboration.\n\nCommunication Hub\n\nThe \nHub\n is the connection and message routing layer. Agents connect directly to the Hub to join the network.\n\nRecipient Agent\nCommunication Hub\nsend_collaboration_request Tool\nSender Agent\nRecipient Agent\nCommunication Hub\nsend_collaboration_request Tool\nSender Agent\nSecure Asynchronous Communication Process\nCryptographic Security Verification\nProtocol Validation\nAsynchronous Message Routing\nNon-blocking delivery\nusing asyncio.create_task()\nAsynchronous Internal Processing\nResponse Correlation\nComplete request-response cycle with protocol-verified secure communication\nsend_collaboration_request(recipient_id, task)\nroute_message(message)\n1. Verify sender identity (via DID)\n2. Verify receiver identity (via DID)\n3. Verify message signature (cryptographic)\n4. Validate protocol compliance\n1. Check protocol version compatibility\n2. Validate message type against protocol\n3. Apply protocol-specific rules\ncreate_task(deliver_message())\nreceive_message(message)\n1. Queue message (asyncio.Queue)\n2. Process in worker task\n3. Invoke internal workflow\nresponse_message\nMatch response to original request ID\nresponse\ncollaboration_result\n\nFigure 4: Agent Communication Flow - Illustrating AgentConnect's communication feature, this diagram shows how agents securely exchange messages through the Hub, which handles verification, routing, and response correlation.\n\nRegistration: When an agent connects, the Hub coordinates with the Registry to verify the agent's identity and record its registration details.\n\n# Example: Agent Registration (Developer facing code)\n# (Inside an async function)\nfrom agentconnect.agents import AIAgent\nfrom agentconnect.communication import CommunicationHub\nfrom agentconnect.core.registry import AgentRegistry\n# ... other imports: AgentIdentity, Capability, etc.\n\n# 1. Initialize Hub and Registry\nregistry = AgentRegistry()\nhub = CommunicationHub(registry)\n\n# 2. Create Agent with Identity and Capabilities\nai_identity = AgentIdentity.create_key_based()\nai_capabilities = [Capability(name=\"summarize\", description=\"Summarizes text\", ...)]\nai_agent = AIAgent(\n    agent_id=\"ai_summarizer\",\n    identity=ai_identity,\n    capabilities=ai_capabilities,\n    # ... other parameters: provider, model, api_key ...\n)\n\n# 3. Register the agent with the Hub\nif await hub.register_agent(ai_agent):\n    print(f\"Agent {ai_agent.agent_id} registered successfully.\")\n    # Agent is now discoverable and can communicate\nelse:\n    print(f\"Failed to register agent {ai_agent.agent_id}.\")\n\nSecure Routing: The Hub ensures secure message delivery between verified agents. When a message is sent (typically via a Tool calling Hub methods like send_collaboration_request), the Hub verifies the sender's signature against their registered identity before forwarding the message to the recipient agent's receive_message method. It also manages the asynchronous request-response lifecycle. The Hub guarantees transport security but does not dictate agent behavior or interpret message content beyond verification needs.\n\nIndependent Agent Systems and the Role of Tools\n\nAgents\n are autonomous entities. The \nBaseAgent\n class defines the core structure and the interface required to interact with the Hub.\n\n# Example BaseAgent interface methods developers implement/use\nclass BaseAgent(ABC):\n    # ... __init__ ...\n\n    # Called BY THE HUB to deliver a message\n    async def receive_message(self, message: Message):\n        \"\"\"Puts an incoming message onto the agent's internal queue.\"\"\"\n        await self.message_queue.put(message)\n        # ...\n\n    # Implemented BY THE DEVELOPER (subclass like AIAgent)\n    @abstractmethod\n    async def process_message(self, message: Message) -> Optional[Message]:\n        \"\"\"Contains the agent's core logic for handling a message from its queue.\"\"\"\n        pass\n\n    # Optionally Implemented BY THE DEVELOPER (subclass like AIAgent)\n    async def run(self):\n        \"\"\"Agent's main loop: gets messages from queue, processes them via process_message.\"\"\"\n        # ... loop structure ...\n        pass\n\n    # Agent's Identity property (used BY THE HUB for verification)\n    @property\n    def identity(self) -> AgentIdentity:\n        # ... returns the agent's identity object ...\n        pass\n    # ... other base methods ...\n\nCrucially, complex agents like \nAIAgent\n typically implement their process_message logic by invoking an internal reasoning workflow (e.g., a ReAct agent built with LangGraph, using workflows defined in the \nPrompts module\n). This workflow makes decisions and interacts with the outside world using Tools.\n\nThe Role of \nTools\n: Tools bridge the agent's internal reasoning loop with the AgentConnect framework's capabilities:\n\nDiscovery: The workflow might decide it needs an agent with a specific capability. It calls the search_for_agents Tool. This tool interacts with the AgentRegistry to find suitable peers and returns the information to the workflow.\nCollaboration: The workflow formulates a request for another agent. It calls the send_collaboration_request Tool. This tool interacts with the CommunicationHub to securely send the request message and manage the response.\nInternal Tasks: Tools like decompose_task help the agent manage its own process.\n\nThis Tool-based approach keeps the core Agent classes focused on state management and the basic communication interface, while the dynamic, intelligent interaction logic resides within the agent's configurable workflow and the specialized Tools it uses.\n\nUse Cases & Examples\n\nAgentConnect enables a wide range of collaborative agent applications across different domains. Our examples demonstrate the framework's versatility:\n\nFigure 5: AgentConnect Use Case Implementation - This diagram illustrates how the framework enables specialized independent agents to collaborate. Each agent offers distinct capabilities (web searching, data analysis, content processing, and user interface) while maintaining full autonomy. The \nexamples\n directory provides working implementations of these agents, demonstrating dynamic discovery and secure communication through the AgentConnect hub.\n\nResearch & Knowledge Tasks\n\nBuild assistants that can dynamically discover and collaborate with specialized knowledge agents. These systems can retrieve information from multiple sources, analyze findings, and synthesize comprehensive responses to complex research queries by leveraging the unique capabilities of different agents in the network.\n\nData Analysis & Insights\n\nCreate data-focused applications where agents specializing in different analytical techniques collaborate to process, visualize, and extract insights from datasets. One agent might handle data preparation, another specialized in statistical analysis, and a third focused on generating visualizations or natural language explanations.\n\nMulti-Stage Complex Problem Solving\n\nImplement sophisticated workflows where a coordinating agent breaks down complex tasks, discovers specialized agents with relevant capabilities, and orchestrates their collaboration to solve multi-stage problems that would be difficult for a single agent to handle.\n\nIntegration with External Platforms & Services\n\nConnect agent networks to external platforms like messaging services, allowing users to interact with a front-end agent that can seamlessly leverage capabilities from other agents in the network. This enables building sophisticated user-facing applications with rich, distributed functionality behind the scenes.\n\nThe framework's flexible architecture supports many more use cases, from creative content generation to complex decision support systems. For detailed examples and code, see the \nExamples Documentation\n and \nExamples on GitHub\n.\n\nImplementation Highlights\n\nAgentConnect is built using robust and modern technologies:\n\nCore Logic: Python 3.11+, leveraging asyncio for non-blocking agent operations and concurrent message processing\nAI Integration: Built with LangChain for multi-provider LLM support and LangSmith integration for tracing agent interactions\nSecurity: Implements message signing and verification using Python's cryptography library and DID-based identity verification\nModularity: Well-structured codebase with clean separation between core components for maintainability\nTool-Based Interaction: Flexible agent capabilities implemented through standardized tool interfaces\nProtocol Support: Extensible communication protocols enabling secure, standardized agent interactions\nProvider Flexibility: Support for multiple LLM providers (OpenAI, Anthropic, Groq, Google AI)\n\nNote: For complete implementation details and API references, please refer to the \nAgentConnect GitHub repository\n and the \nOfficial Documentation\n.\n\nInnovation & Impact\n\nInnovation:\n\nAgentConnect's primary innovation lies in its decentralized approach to inter-agent discovery and communication. It moves beyond monolithic, centrally controlled MAS to enable an open ecosystem where independently developed agents can collaborate as peers. This capability-based dynamic discovery and secure, framework-agnostic communication layer is a novel contribution to the field of agentic AI.\n\nPotential Impact:\n\nAgent Marketplaces: Facilitates the creation of marketplaces where specialized agents can offer their capabilities as services.\nComplex Problem Solving: Enables tackling problems requiring diverse expertise by composing workflows across independent, specialized agents.\nIncreased AI Automation: Lowers the barrier for integrating different AI tools and services developed by different teams or vendors.\nFoundation for Emergent Systems: Creates an environment where complex collaborative behaviors can emerge from the interactions of autonomous agents.\nRoadmap\n\nOur vision for AgentConnect's future development:\n\n🔐 Agent ID and Reputation System\nUnique, verifiable identity for each agent\nDecentralized reputation tracking via performance metrics\nHistorical interaction records for trust and accountability\n🔍 Enhanced Agent Discovery Marketplace\nMarketplace-style discovery with detailed filtering\nAgent capability and performance metrics as search parameters\nIntelligent matching for optimal agent collaboration\n⚡ Anthropic MCP Server Integration\nSeamless integration with Anthropic's MCP server\nAdvanced reasoning and memory management\nEnhanced coordination for complex multi-agent tasks\n🧩 Structured Agent SDK\nStandardized schemas for agent I/O and capabilities\nConsistent definition of parameters and SLAs\nImproved interoperability and discovery efficiency\n💰 Decentralized Payment Infrastructure\nSecure on-chain micropayments via Coinbase's AgentKit\nSupport for pay-per-use and subscription models\nAutonomous economic operation with crypto wallets\n\nThese developments will transform AgentConnect from a powerful communication framework into a complete ecosystem for autonomous agent collaboration.\n\nResources\nGitHub Repository\n - Source code and development\nDocumentation\n - Complete API references and guides\nExamples\n - Working implementations and use cases\nContribution Guide\n - How to get involved\nQuickStart Guide\n - Get up and running quickly\nConclusion\nOvercoming Traditional Limitations\n\nBy eliminating central control points and enabling direct peer-to-peer communication, AgentConnect resolves the fundamental limitations of conventional multi-agent systems. Its decentralized registry and secure communication hub create a resilient network that scales horizontally without traditional bottlenecks.\n\nEmpowering Developer Innovation\n\nThe framework provides developers with unprecedented freedom to build specialized agents that can discover and collaborate with other agents dynamically. This capability-based approach allows developers to focus on their agents' core competencies while leveraging complementary capabilities from the network.\n\nFostering an Open Ecosystem\n\nAgentConnect lays the foundation for a truly open AI ecosystem where independently developed agents can seamlessly interoperate. This interoperability will drive innovation, specialization, and the emergence of new collaborative patterns that would be impossible in siloed frameworks.\n\nBuilding the Future of Collaborative AI\n\nAs AI systems become more sophisticated, the ability for independent agents to collaborate effectively becomes increasingly crucial. AgentConnect provides the vital infrastructure layer for this next generation of AI applications, enabling complex workflows across organizational and technical boundaries.\n\nWe invite the community to join us in exploring and expanding the possibilities of AgentConnect. By contributing to its development, building new agents, and sharing insights, we can collectively shape the future of decentralized, collaborative artificial intelligence.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nTL;DR\n\nIntroduction: The Interoperability Challenge for Independent Agents\n\nThe Problem: Limitations of Centralized Multi-Agent Systems\n\nAgentConnect: A Decentralized Solution for Inter-Agent Collaboration\n\nKey Features\n\nArchitecture Deep Dive\n\nDecentralized Agent Registry\n\nCommunication Hub\n\nIndependent Agent Systems and the Role of Tools\n\nUse Cases & Examples\n\nView all\nComments\n(2)\nMost recent\n\n✨ Want to join the conversation?\n\nSign in or create an account to comment.\nB\nBHABANI SANKAR BREDEKA\nLast month\n\nSome one is required to explain this huge description\n\nLike\nReply\nA\n@anasboss270\n2 months ago\n\nCertificate\n\nLike\nReply\nYou might be interested\nWhat is Agentic AI (AAIDC-Week1-Lesson-1)\nMay 15, 20255357 reads\nAAIDCAgentic AI+5\nWeek 6 Preview: Architecting Multi-Agent Systems (AAIDC-Week6-Preview)\nJun 22, 2025144 reads\nAAIDCAgent Collaboration+9\nArchitecting Intelligence: Design Patterns for Multi-Agent AI Systems (AAIDC-Week6-Lesson-1)\nJun 22, 2025205 reads\nAAIDCAgent Collaboration+9\nAI Agent Penetration Testing Workflow\nC\nMar 31, 202517 reads",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "agentic-ai-a-step-towards-artificial-general-intelligence-7kM1peUvZW9y",
    "username": "aAmina Javaid",
    "license": "MIT License",
    "title": "Agentic AI - A Step towards Artificial General Intelligence",
    "publication_description": "Back to publications\nMar 26, 2025\n●\n95 reads\n●\nMIT License\nWinner of\nDistinguished Technical Deep-Dive\nat the\nAgentic AI Innovation Challenge 2025\nAgentic AI - A Step towards Artificial General Intelligence\nAgentic AI\nAgents\nagno\nArtificial General Intelligence\nFastAPI\nGenerative AI\nLLMs\nMulti-agent Systems\nReAct\nResearch Assistant\nResearch Copilot\nStreamlit\nTools\nA\nAmina Javaid\nLike\nBookmark\nShare\n\nIntroduction\n\nArtificial Intelligence is the ability of a machine to think and behave as close as possible to humans. It enables a system to learn, reason, and solve problems just like humans do. AI agents are at the next level of artificial intelligence. There has been a lot of discussion about AI agents since the beginning of this year as we have been hearing statements from the CEOs of Meta, Microsoft, and Nvidia about the rise of AI agents and the dominance of autonomous AI systems all over the world, potentially reshaping the technological landscape. These big tech giants are cutting down their work force by embedding agentic behavior into their routine tasks. Many companies are rapidly integrating AI agents into their workflows, transforming their processes through intelligent, autonomous systems in order to increase their productivity.\n\nAI agents are considered as the biggest transformation in the history of Artificial Intelligence. They are set to revolutionize the way work gets done in any specific domain.\n\nAI agents have the potential to redefine how work is done across industries. The first question that comes to mind is what exactly are AI agents and how is agentic AI different from traditional AI, specifically generative AI? In this article, I'll explore AI agents in detail, defining them in the simplest possible way and taking them to the next level, exploring agentic patterns, architecture and workflows along with some real-world examples of AI agents and their implementation.\n\nWhat are AI agents?\n\nAn AI agent is an intelligent system that solves a real-world problem by automating tasks, eventually making one's life easier and improving productivity.\n\nThink of AI as raw intelligence. It has got the potential to behave intelligently but it's not really useful until you give it a job. It needs a clear direction in order to utilize its full potential, but where does all this intelligence reside? Well, large language models (LLMs) possess this vast amount of intelligence. They have been trained on huge corpuses of text and are becoming more and more intelligent with every day passing by. Despite all this intelligence, LLMs don't take action on their own. This is where an AI agent steps in. An AI agent is a system which helps achieve a useful goal by using all the intelligence of the LLMs.\nAI agents expand beyond the capabilities of a generative AI model alone. Unlike traditional AI models that generate responses based on prompts, AI agents go a step further. They are provided with clear goals and objectives, and they can autonomously make decisions, interact with external systems, and execute tasks whether its answering questions, managing workflows, or automating daily tasks. Its the difference between knowing how to do something and actually doing it.\n\nHere's a simple way to think about it:\n\nAI = Brain (Knowledge and Intelligence)\nAI Agent = The Executor (The one who makes things happen)\n\nAn AI agent follows a structured process:\n\nUnderstand - Recognize the task at hand.\nThink - Process information and determine the best course of action.\nAct - Execute the task or deliver what is required whether it's retrieving data, solving a problem, or interacting with an external system.\n\nLet's think of an analogy to understand an AI agent:\n\nImagine you return from a long vacation and find your inbox with hundreds of emails waiting for you to go through. You get overwhelmed. You obviously don't have time to read everything.\nNow, think of an AI agent as your smart assistant. It\n\nScans all emails and filters out the most important ones.\nSummarizes key messages so you get a quick overview.\nFlags urgent emails that require immediate action before your meeting.\n\nYou give instructions to your agent according to your preferences and it performs tasks accordingly, saving your time and effort. That's exactly how an AI agent works. Developers provide these agents with detailed instructions so they know exactly how to respond in different situations. Some agents come programmed with basic instructions like an experienced assistant capable of performing routine tasks. Others can be customized to fit your needs like giving the assistant some special rules according to your needs and preferences.\n\nA more formal definition of an AI agent could be:\n\nAn AI agent is a software system which is capable of performing tasks autonomously in order to achieve a specific goal by leveraging the capabilities of various tools and external systems such as APIs and databases, and by interacting with other agents.\n\nAn AI agent takes your goal, processes information, makes decisions, and executes actions - automating tedious tasks so you don't have to.\nAgents are not as simple as we think they are. They are complex systems involving\n\nadvanced problem-solving,\nfull-stack development, and\nintelligent decision-making.\n\nNot everything is an AI agent. Nowadays, every developer wants to develop an AI agent whether it's useful or not.\n\nBe cautious! Every single thing IS NOT and SHOULD NOT be an AI agent.\n\nWe have to be very thoughtful while putting in efforts developing an AI agent. It should solve a real-world problem and be intelligent enough to help humans in their routine tasks enhancing their productivity. It can also be helpful in specialized tasks using its intelligence and decision-making capabilities. Otherwise, it's just a piece of complicated software.\n\nImportance of AI Agents\n\nAgentic AI is the future!\n\nHowever, the concept of agents is not new - it has already been there since decades.\n\nIn their book \"Artificial Intelligence - A Modern Approach\", Stuart J. Russel & Peter Norvig define an agent as:\n\nAn agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\n\nToday, when we talk about AI agents, we are usually referring to LLM-powered AI agents.\n\nLLMs on their own have certain limitations like they are constrained by the data they have been trained on. They are capable of performing a variety of tasks like summarizing documents, creating drafts for emails, and generating reports to name a few. Fine-tuning LLMs requires a substantial amount of investment in data and resources. This is where AI agents take the charge. The real magic happens when we start building systems around LLMs, essentially taking the LLM and integrating it into our processes and workflows.\n\nFor example, imagine an autonomous AI agent that scans your inbox to summarize important emails, highlighting the ones requiring urgent attention, saving a lot of time that can be used to focus on more productive tasks.\n \nTaking this a step further, think of a collaborative research copilot, a team of AI agents working together to achieve the research goals. \n\nA paper extraction agent fetches relevant research papers.\nA summarization agent generates concise summaries of the papers.\nAn interactive chat agent allows meaningful interaction with those research papers.\n \nThese agents, working autonomously and in coordination could help generate meaningful literature reviews, enabling the researchers focus on experimentation and core research objectives.\nSo, what's the role of LLMs in AI agents? They serve as the main intelligence layer, helping the agents plan, organize, and make decisions about task execution. The role of LLMs is to help the agents achieve the level of autonomy they require, adapting them to tasks dynamically rather than following rule-based workflows.\n\nThe future of AI isn't just about more powerful models - it's about intelligent, autonomous systems that enhance productivity and decision-making. Systems that automate repetitive tasks, taking your business to the next level. That's the power of Agentic AI.\n\nAgentic AI is potentially being considered as the biggest transformation we'll see in our lifetime. Those who embrace AI agents now will have a major advantage over those who don't. The world of AI is changing at a fast pace and we must understand the importance of AI agents and their use in our particular scenario, otherwise we might be left behind in this rapidly changing world. The key is identifying the right agent type based on our particular requirements.\n\nAgentic AI vs Generative AI\n\nHow is an AI agent different from a traditional AI system?\n\nThe key difference lies in its autonomy- the ability to think independently (to a certain extent) in order to make decisions and give intelligent suggestions.\n\nAccording to the Agents whitepaper by Google,\n\nThe combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent.\n\nAn AI agent has a goal to achieve.\nIt utilizes a generative AI model to perform some of its basic tasks.\nIt learns from historical data, improving its capabilities over time.\nIt is aware of its environment and can access various tools to gather and process real-time information.\n\nWhile a traditional AI system completes its task as directed, an AI agent adds more intelligence to traditional AI capabilities, behaving more like a human.\n\nLet's say we want to automate the process of ordering food at a restaurant. A traditional AI chatbot would interact with the user, take the order, extract important information and place that order according to the user's requirement. However, an AI agent would remember the returning users, recall what they ordered from their past experiences, and give them valuable suggestions or recommendations to make their experience more pleasant. That sounds more like an intelligent human who is conscious and aware of his returning customers.\n\nGenerative AI is all about LLM models. Big tech giants have been competing to build the best LLM models for the past few years. 2024 has been the year to build custom chatbots using LLMs like OpenAI, Llama3, and others. These LLMs help generate content. A query by the user is sent to the LLM in the form of a prompt which generates a response as an output. These LLMs can also be further fine-tuned for specific tasks.\n\nAgentic AI, on the other hand, are autonomous AI systems which are able to perform tasks on their own. 2025 is the year to build autonomous AI agents. They have a specific goal to achieve. These AI systems work independently without any human intervention to achieve their goal. You can integrate as many tools into these systems as you want. The goal is mostly based on a business outcome. They have complex workflows and can fine-tune themselves to improve their performance.\n\nWhere do LLMs stand in the scope of AI Agents?\n\nLLMs are the brain of AI agents. They provide the agents with the autonomy they require to perform specific tasks. They are smart enough to break the bigger task into smaller sub-tasks and decide a plan of action to execute dynamically. They enable the agent to perform in a specific manner using memory, knowledge, and tools.\n\nAgentic workflows have LLM calls chained together where an agent lets the LLM decide how many times to run. It may continue the loop until it finds a resolution. For example, talking to a customer for customer support or iterating on code changes. When you don't know the number of steps to complete a task, it is an autonomous workflow where we don't predefine a path. All this has been possible with the evolution of large language models (LLMs). LLMs and tools are getting better and smarter every day making the agents more capable and prevalent.\n\nImportance of Prompt for an AI Agent\n\nPrompt is the most important part in giving instructions to an AI agent. It should be clear and thoughtful. It must define the goal that wants to be achieved and specify the methodology to achieve that goal. The tools that are given to the model must be properly documented. This helps the LLM in deciding which tool to use for a particular task. The prompt should be detailed enough but we cannot deny the importance of properly documented tools. How can you use tools and function calling if you don't know what the parameters mean and what that tool actually does? Tools should be described properly to understand their purpose. Writing a good tool description influences other parts of the agent prompt.\n\nControl logic of a compound AI system is the path to answer a query. Previously, this control logic had been defined by hard coded rules. Another way to control the logic of a compound AI system is to put the LLM in charge. This has only become possible because we are seeing a lot of improvement in the reasoning capabilities of large language models. You can feed complex problems to LLMs and then prompt them to break the problem into smaller problems and come up with a plan on how to tackle them.\n\nIf we control the logic programmatically, we are designing our system to stick with the instructions being given without deviating from a set path. On the other hand, if we control the logic through LLM, then we are designing our system to break the problem down into sub-problems, make a plan to solve them, try to come up with a solution, and see if we need to readjust the plan accordingly.\n\nAgentic AI approach is putting LLMs in charge of the whole system.\n\nEvolution of AI Agents in Recent Years\n\nSince the release of ChatGPT in November, 2022, there have been various shifts in the field of Generative AI.\n\nModels\n\n2022 was the year of LLMs. Models on their own are limited by the data they have been trained on. This impacts what they know about the world and what tasks they may solve. So they have limited knowledge and are hard to adapt to new changes in the real-world. Information is being generated at the speed of light. Data is all around and new data keeps on being added to the existing data. One option is to fine tune the models but that requires an investment in data, resources, and time. Models on their own could be useful for a variety of tasks like summarizing documents, creating first drafts of emails, or creating reports.\n\nCompound AI Systems\n\nIn 2023, compound AI systems started to emerge. Certain problems are better solved when you apply the principles of system design. Building an AI system around the model makes it possible to solve many complex real-world problems where context is important.\n\nAccomplishing a task through system design is inherently easier to solve than tuning a model. Systems are faster, quicker, and easier to adapt. Systems have programmatic components such as output verifiers, query breakers, database search engines, and customized tools. We can pick the right components to solve our problem.\n\nRetrieval Augmented Generation (RAG) is one of the most popular and commonly used compound AI systems. It utilizes the capabilities of LLMs and adds context to the information, also trying to resolve the hallucination of LLMs.\n\nAI Agents\n\nIn 2024, all focus shifted to building AI agents, LLM agents to be specific. AI Agents are systems built around the large language models (LLMs), actually taking the model and integrating into the processes that we have.\n\nComponents of AI Agents:\n\nFollowing are the core components of an AI agent:\n\nModel - Large Language Model (LLM)\nTools - An agent uses tools to fetch the latest information. They can be databases, APIs, or external systems.\nMemory - Short term and long term memory\nKnowledge - Databases\nReasoning - The ability to think, divide a task into sub-tasks, and plan to execute them autonomously.\n\nEvery agent defines its role, description, and specific instructions to carry out tasks. It specifies the model (LLM) and available tools to be used at various stages during its course of action. Similarly, it also specifies the memory and knowledge base to be used for storing conversations, interactions, and other information.\n\nRole\n\nThe role of an agent defines its personality. It allows the agent to behave in a specific manner appropriate to its assigned task.\n\nModel\n\nLarge language models (LLMs) are the basic building blocks of an AI agent. They are the brain of the agent allowing it to think, understand, reason, plan, and act accordingly. In addition to that, they serve as the generative component of the agent as well, helping it to generate meaningful responses.\n\nTools\n\nTools are external resources that an agent can use to achieve its goal. They can be as simple as Python functions, or APIs, databases, or other external systems. Tools enhance the capabilities of an agent by helping it access information and interact with its environment. Agents are smart enough to understand the functionalities of tools and utilize them as required during their workflow. Examples of tools include:\n\nSearch (web or database),\nCalculator (mathematical calculations),\nCoding agent (to manipulate a database),\nLLM (summarization, translation), and\nAPIs (weather, google maps).\nMemory\n\nAgents possess short term and long term memory for specific purposes. Short term memory is used for immediate interactions with the environment whereas long term memory is used for storing information and conversations. The agents can maintain context, learn from historical data and past experiences, and improve their performance. They are adaptive and learn from past interactions.\n\nKnowledge\n\nKnowledge base helps agents store the information and context which is required to perform their tasks.\n\nReasoning\n\nAgents possess the ability to think and reason utilizing the large language models (LLMs) at their disposal. This helps them achieve the level of autonomy required to complete various tasks moving towards their goal.\n\nReal-World Agentic Use Cases\nAI Trip Planning Assistant\n\nImagine you are going on a 5-day business trip to Netherlands next month. You want to pack efficiently, considering the weather, your meetings, and any formal events.\n\nYou ask an AI agent:\n\nI have a 5-day business trip to Netherlands next month. I need to know what clothes to pack based on the weather, my meetings, and evening events. Can you please help?\n\nAn LLM alone won't be able to solve this problem for you because it needs your specific travel information along with some real-time weather information which it might acquire through APIs.\n\nBreaking Down the Problem\nTrip Duration & Schedule - The agent retrieves your travel dates from your calendar.\nWeather Forecast- It checks the expected temperature, chances of rain, and humidity in Netherlands for that week.\nEvent Types - It scans your calendar for meetings, formal dinners, and casual outing plans, and asks if you would need business attire and whether you would be attending any special events requiring formal wear.\nPacking Strategy - The agent suggests outfit combinations to reduce unnecessary packing. It also considers the number of shoes & accessories needed.\nAdditional Essentials - It reminds you to bring: travel-size toiletries (if your airline has liquid restrictions), chargers, adapters, and documents.\nFinal Packing List - The agent compiles a checklist tailored to your needs. It even reminds you when to pack based on your departure time.\n\nThis is a modular, multi-step planning problem, where the agent retrieves external data, personalizes recommendations, and optimizes the process, making it a great AI agent use case!\n\nHere are a few other agentic AI use-cases which help solve real-world problems.\n\n1. AI Coding Assistant\n\nThink of an agent which writes a code and then tests it to pass all unit tests, corrects the code and continues this loop making the program perfect in terms of quality. When the coding agent sees the error during each step of the loop, it helps the model converge towards the right solution sooner after getting this feedback.\n\n2. Customer Service Chatbot\n\nThink of a chatbot which remembers the customer, knows all the information about particular customers and serves them according to their preferences.\n\n3. AI Healthcare Assistant\n\nA personal healthcare assistant which remembers everything about a person's health records, nutrition requirements, fitness and well-being. It has access to a person's vitals through fitness trackers. It advises a person about regular physical and mental examination based on his/her current condition.\n\n4. Emergency Response System\n\nThink of an automated system which triggers important alarms automatically assessing an emergency situation through its sensors. It might call the appropriate helplines and rescue operations when in need.\n\n5. Email Manager\n\nAn agent to review emails and prepare tasks on a digital to-do list based on the highest priority emails.\n\n6. Environmental Monitoring System\n\nAn environmental sustainability agent that uses weather data, satellite images, and other information to decide how to take care of plants and their environment.\n\n7. AI Travel Planner\n\nA travel planning agent that takes a user's preferences and needs into account, researches things like destinations, hotels, and activities, and then suggests an itinerary for the user. It might use a collaborative agent to book everything taking the user into confidence.\n\nHow do AI Agents Work?\n\nAgents have a clear and specific goal to achieve. Every step they take moves them closer to their end goal. They plan and divide the goal into sub-tasks. They process information iteratively, making decisions at every step about the next action to take based on the output from the previous step. The orchestration layer is responsible for the overall execution of an agent delegating responsibilities to various components of the agentic architecture. Prompts define the role and task of an agent. The better the prompt, the easy it is for the agent to achieve the required objective. The evolution of prompt engineering and task planning for language models helps make the agents much more better with the passage of time.\n\nHere are a few prompt engineering frameworks considered as best for the agents:\n\nReAct - Reason & Act\nCoT - Chain of Thoughts\nToT - Tree of Thoughts\nReAct - Reason & Act\n\nThe ReAct framework is a technique for building AI agents that combine the reasoning capabilities of LLMs with the ability to take actions, allowing them to solve complex problems.\n\nCoT - Chain of Thoughts\n\nChain-of-thought prompting is a technique that enhances the reasoning capabilities of LLMs by guiding them to generate step-by-step explanations before arriving at a final answer.\n\nToT - Tree of Thoughts\n\nTree-of-thoughts framework is a problem-solving technique that allows LLMs to explore multiple reasoning paths simultaneously, and evaluating different solution paths.\n\nAgents can utilize these frameworks or other techniques to choose the next best action based on a user query.\n\nThe quality of an agent's response highly depends on the quality of the prompt it receives.\n\nThese prompts enable the agent to think and plan their tasks in an effective manner. They help them decide which tools or external resources to use in order to complete their assigned tasks properly.\nAn AI agent is as good as the model you are using. You can totally customize your agents with agentic frameworks by providing various tools. Docstring of a function or tool is extremely important. When you register a tool, the LLM understands the purpose of that tool by going over the docstring. The agent gives the docstring to the LLM which can understand it and map it to the intent of the user query. The agent basically uses docstrings to enhance its understanding of its tools. Based on the docstring, the LLM is smart enough to map the function or tool to the user query and also give the required arguments to the function or tool.\n\nTypes of AI Agents\n\nThere are many types of AI agents based on the level of autonomy, task execution, and integration with tools and memory. Here are a few common types of agents:\n\n1. Simple Reflex Agents \n\nThey are the simplest form of automation. They execute preprogrammed instructions without thinking and adaptation. They are mostly efficient but not much flexible. These agents are best suited for repetitive tasks such as email autoresponders and invoice processing agents.\n\n2. Model-based Reflex Agents\n\nThese are LLM-enhanced agents for contextual understanding. They are simple but intelligent and mostly suitable for low-complexity, high-volume tasks such as filtering emails and AI-enhanced content generation.\n\n3. Goal-based Agents\n\nThey have a clear, specific goal to achieve. They instruct the LLM to achieve that goal which thinks and makes a plan of action. They utilize tools, knowledge, and memory to achieve intermediate tasks and iterate until the goal is complete. Examples include coding copilots and automated data analysis systems.\n\n4. Learning Agents\n\nThese agents learn, adapt, evolve, and improve themselves over time. They don't need constant human intervention. They use reasoning, memory, and autonomous learning capabilities such as complex research and simulation agents.\n\n5. ReAct Agents\n\nThese agents are a combination of reasoning and action. They involve strategic thinking for task decomposition and multi-step decision-making. They are the closest to human problem-solving behavior dynamically adjusting their approach. Their examples include project planning tools and content generation agents which produce and critique their content in an iterative manner in order to improve the response in every step of the way. Real-time external knowledge combined with a ReAct agent makes it even more powerful for precision critical tasks.\n\n6. Memory-Enhanced Agents\n\nThese agents use historical context, task history, and previous interactions to remember user preferences. Examples include customer service chatbots and personalized recommendation systems for shopping, travel, and other things.\n\nBasic Agentic Workflow\n\nAn AI agent is a combination of two things:\n\nAn interface\nA workflow\n\nThe interface is how you interact with an AI agent. It may be through a chat window, a voice assistant or a button on a website.\n\nBehind that interface is the workflow. You ask the AI agent a question or give it a command through the interface. From there, an AI agent follows a series of steps in a workflow. These steps could involve accessing information from a database, making a decision based on what it finds or even performing a task like sending an email. The steps in this workflow may change depending on what you ask or what the agent needs to do next. The AI agent works through a whole process to get you the best result.\n\nAt a higher level, an agentic workflow is a 4-step process:\n\nInput - You ask a question or give a command to the agent.\nProcessing - The AI agent thinks about what you have asked and searches for the best way to respond.\nAction - The AI agent gives you an answer, finds information or performs a task.\nLearning - The more you use it, the smarter it gets, learning from past interactions to improve.\n\nAt a lower level, an AI agent works in the following way:\n\nThe user gives instructions to an AI agent about what he wants to achieve.\nThe agent is provided with a detailed prompt based on the user's query.\nThe agent breaks down the task into smaller subtasks and tries to accomplish them by interacting with different tools or other agents.\n\nBefore LLMs, these agents had defined rules which helped them make decisions on the go. LLMs help the agents achieve that level of autonomy which is required to make decisions in real-time based on the current situation and responses from the agent itself.\n\nAutonomous AI agents work by simplifying and automating complex tasks.\n\nBasic agentic workflow includes:\n\n1. Planning\n\nThe agent receives a specific instruction or goal from the user. Planning involves:\n\nDetermining the goal.\nPlanning tasks.\nBreaking down tasks into smaller actionable tasks.\nPerforming the tasks in a specific order to achieve the goal.\n2. Information Acquisition\n\nIn order to act on the planned tasks, agents may acquire information in the following ways:\n\nExtracting conversation logs,\nSearching the internet to retrieve information, and\nInteracting with other agents or ML models to access or exchange information.\n3. Task Implementation\n\nTask implementation involves accomplishing a task and moving to the next one in plan. The agent needs to evaluate if the goal has been achieved before moving on to the next task using external feedback or inspecting its own logs. This might require creating and acting on more tasks as the agent moves towards its goal.\n\nWorkflow Prompt vs Agent Prompt\n\nWorkflow prompt is a prompt which you feed to an LLM, then take its response and feed it to the same or another LLM, and continue this pattern until you are done. You might have a fixed number of steps or you may check the intermediate results to decide of they are good enough to stop. Each of these prompts is very specific in terms of taking one input and transforming it into an output.\n\nIn contrast, an agent prompt is much more open-ended and usually gives the model tools and multiple things to check. Feedback is very important in order for the agent to converge to the right answer.\n\nIn multiagent collaborative systems, we may use a combination of workflow prompt and agent prompts. Every single agent which is a part of the system is given an open-ended prompt to accomplish its task. These single agents can then be put into a workflow in which the input of the next agent depends on the output of the previous agent and so on. This helps agents achieve a bigger goal in a collaborative manner while achieving their defined tasks autonomously.\n\nAgentic Patterns\n\nAI agents can be configured in many ways. The configuration of an AI agent in a specific manner is called an agentic pattern.\n\nMost popular agentic patterns include:\n\nReflection Pattern\nTool Use Pattern\nPlanning Pattern\nMulti-agent Pattern\nReflection Pattern\n\nThe reflection pattern involves a Generate agent and a Reflect agent both working in a loop. One agent generates the content and the other agent critiques or reflects upon its response including its own feedback and sending the response back to the generate agent so that it can improve or make it better. When an agent iteratively reflects on its response to improve it, we eventually get far better results.\n\nLarge language models can become effective AI agents by reflecting on their own behavior.\n\nReflection Workflow:\nUser provides a prompt such as \"Generate an article about something…\" using the Generate agent.\nThe article is generated as a text.\nThe text is sent to the Reflect agent which provides some critique to the Generate agent. It also provides some suggestions and modifications to the original content.\nThis critique is again sent to the Generate agent which generates another version of the article by taking into consideration all the critique and feedback that has been sent to it.\nThe loop continues for 'n' number of steps or until the Reflect agent is satisfied by the response. This pattern improves the results of your LLM calls.\nTool Use Pattern\n\nA tool is a way for an LLM to access the outside world. It is like a Python function that the LLM can access and run to fetch some relevant result. e.g.\n\nusing an API, or\nparsing web content.\n\nHow can we make this function (tool) available to an LLM? We use system prompt in which we tell the LLM to behave as a function calling AI model.\nTool use agent has the capability of using tools. It has a list of tools and it selects the right tool for the question that we have asked. Then it runs the tool to fetch the relevant information from the outside world. It ultimately returns this information in natural language as a response.\n\nPlanning Pattern\n\nThis pattern is also called ReAct - reason and act. It tries to improve the learning and reasoning capabilities of LLMs.\n\nPlanning Workflow:\nAction step is basically running a tool when the agent decides to run a tool based on its task decomposition.\nObservation step is where the LLM makes an observation about the output of the tool.\nThought step is where the agent reflects upon the observation and decides to take another action or return the response if the goal has been achieved.\n\nCore agents implement the ReAct technique because it is one of the most known techniques for improving the capabilities of LLMs.\n\nMulti-agent Pattern\n\nThe main idea of this pattern is to divide a task into smaller, simpler subtasks that are executed by different agents. Each agent adopts a role and solves a smaller task. When all small tasks have been completed, we may say that the bigger task has been achieved by the crew of agents or multi-agents. There is a dependency between the agents in a multi-agent system.\n\nAgentic Architecture\n\nAccording to the Agents whitepaper by Google, the three essential components in an agent runtime from user query to agent response include:\n\nModel\nTools\nOrchestration\nModel\n\nLarge language model (LLM) is the main decision-maker or task-planner in an agent. \n\nThere can be one or multiple models in an agentic workflow. \nModels may be small or large depending on the required task at hand.\nLLMs are capable of following reasoning frameworks such as ReAct, Chain-of-Thoughts, and Tree-of-Thoughts. \nThey can be general-purpose, multimodal, or fine-tuned.\nThe chosen LLM should best fit your required application goals.\nTools\n\nFoundation models are constrained by their inability to interact with the outside world. This brings the importance of using tools in agentic architecture.\n\nTools enable the agent to interact with external data and services, enhancing its capabilities.\nExamples of tools include weather API, database, or an external system.\nTools help access and process real-world information.\nThey bridge the gap between the agent's internal capabilities and external world unlocking a broader range of possibilities.\nThey can be functions, extensions, data stores, or plugins.\nOrchestration\n\nOrchestration is an iterative process where an agent takes information, does internal reasoning, and makes the next decision. The loop continues until the agent has reached its goal or a stopping point. It can be simple (calculations with decision rules) or complex (chained logic, additional machine learning algorithms, probabilistic reasoning techniques) depending on the goal and tasks.\n\nModel vs Agent\n\nLet's differentiate between a model and an agent.\n\nA model has limited knowledge - the one it has been trained on whereas an agent can access and process extended knowledge via tools interaction.\nA model generates a single inference based on user query while an agent maintains session history and context to support long-term interaction.\nA model does not have any tools while and agent implements tools or accesses information through already built tools like APIs, databases, etc.\nA model has no logical layer. It is capable of handling simple to complex prompts. On the other hand, an agent has reasoning frameworks in the architecture such as ReAct, CoT, etc.\nAgent vs Workflow\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\nPopular Agentic Frameworks\n\nFollowing are a few agentic frameworks used to build cutting-edge AI agents for real-world use-cases.\n\nCrewAI\nLangChain\nMicrosoft AutoGen\nAgno\nLangFlow\nLangGraph\nBenefits of AI Agents\nAgents save a lot of time and effort enabling us to focus on more important tasks.\nThey help automate routine tasks and repetitive workflows. \nAgents help reduce the cost of workforce by taking charge of business processes.\nQuality of responses improves by using agentic patterns like ReAct.\nThey help improve customer experience through customer service chatbots providing accurate and efficient responses. \nThey are more flexible and versatile than traditional computer programs because of LLMs.\nAgents are great for complex and unpredictable tasks.\nThey improve the overall productivity and efficiency of a business process.\nChallenges\n\nDespite having a lot of benefits, agents pose some significant challenges which include:\n\nData privacy concerns\nEthical challenges\nTechnical complexities\nLimited compute resources\nMultiagent dependencies\nInfinite feedback loops\nComputational complexity\nWhen to build AI agents?\n\nConfused about whether you should build an AI agent or not for your use case? Consider the following aspects.\n\nWhat level of autonomy you require in your system? If it's a simple task, then going through the programmatic approach might be a better option instead of overcomplicating the system with agentic behavior.\nFor complex tasks, like trying to solve GitHub issues independently and to handle a variety of queries, an agentic route could be helpful because it would take you too much effort to configure every single path in the system.\nAgentic behavior also requires human in the loop to some extent depending on the task in hand. So you need to consider this aspect as well while designing an AI agent.\n\nAgents are highly beneficial when tasks require complex decision-making, autonomy, and adaptability. For example:\n\nEmail autoresponders\nCustomer inquiry chatbots\nComplex data analysis & report generation\n\nAgents are not useful for straight-forward, infrequent, and minimal automation tasks or tasks requiring deep domain-specific knowledge. They are also not suitable for high-stake decision-making tasks involving financial transactions or data security and privacy concerns.\n\nCase Study - Building AI Agents\n\nLet's build two AI agents - a simple AI research agent and a multi-agent collaborative system - in order to understand the agentic workflow process. We'll use agno to build our AI agents. I'll take you step by step explaining each step in detail so that you get an in-depth understanding of the agentic workflow.\n\nPrerequisites\nYou must have a basic understanding of programming and some experience with coding in Python.\nIts good to have some knowledge of creating and working in virtual environments.\nagno - Framework to build AI Agents\n\nAgno is an open-source Python framework for building multi-modal AI agents. It claims to be the fastest framework for building AI agents. It is simple, intuitive, efficient, and easy to learn. It enhances LLMs with memory, knowledge, tools, and reasoning as required.\n\nProblem Statement\n\nFinding and reading research papers is a tedious task which requires days and months of effort. Understanding the papers and writing literature reviews is a very time-consuming task. Automating this task can make the lives of students, researchers, and AI professionals easier in terms of understanding and reviewing research papers.\n\nWe'll build the following two AI agents to understand the agentic process. \n\nSimple Research Agent - A single agent research assistant\nResearch Copilot - A multi-agent collaborative system\nBuilding a Simple AI Agent\nSimple Research Agent\n\nLet's build a simple research agent which finds and extracts research papers on a given topic, extracts their metadata, summarizes them, and generates a comprehensive literature review using a single research agent.\n\nImplementation\nProject Structure\nsimple-research-agent/           # Main project folder\n│── frontend/                    # Frontend directory\n│   ├── app.py                   # Streamlit application\n│\n│── backend/                     # Backend directory\n│   ├── main.py                   # FastAPI backend\n│   ├── pdf.py                    # Converts output to PDF\n│   ├── utils.py                  # Utility functions\n│   ├── research_agent.py         # Simple Research Agent\n│\n│── requirements.txt              # Dependencies\n│── .env                          # Environment variables\n│── LICENSE                       # License file\n│── README.md                     # Documentation\nSetting up the environment\n\nCreate a new virtual environment and install the dependencies using requirements.txt file. \n\nrequirements.txt\n\nstreamlit\nuvicorn\nfastapi\nagno\ngroq\nopenai\nrequests\narxiv\npypdf\nreportlab\nhtml2text\n\n.env\nCreate a .env file and add your API key.\n\nTOGETHER_API_KEY=\"your_api_key_here\"\nFRONTEND\n\nThe frontend of the simple research agent looks like this:\n\nUser Input: User enters a research topic and the number of papers to fetch from arXiv.\n\nOutput: Literature review of the fetched papers.\n\napp.py\nThis is the main entry point of the application built in Streamlit. It uses requests to interact with the FastAPI endpoints at the backend.\n\nimport streamlit as st\nimport requests\nimport json\n\n# Set Streamlit page configuration\nst.set_page_config(page_title=\"AI Research Agent\", layout=\"wide\")\n\n# Initialize session state variables\nif \"result_json\" not in st.session_state:\n    st.session_state.result_json = None  # Stores fetched research paper results\n\nif \"research_topic\" not in st.session_state:\n    st.session_state.research_topic = \"\"  # Stores user-entered research topic\n\nif \"max_papers\" not in st.session_state:\n    st.session_state.max_papers = 5  # Stores user-defined number of papers to fetch\n\n# Sidebar UI elements\nwith st.sidebar:\n    st.image(\"../images/research_logo.png\", width=150)  # Display research agent logo\n    st.subheader(\"Search and Generate Literature Reviews\")\n    st.header(\"🔍 Search Papers\")\n    \n    # User input fields for topic and number of papers\n    topic = st.text_input(\"Enter Research Topic:\", key=\"topic_input\", value=st.session_state.research_topic)\n    max_papers = st.number_input(\"Number of Papers to Fetch:\", min_value=1, max_value=10, \n                                 value=st.session_state.max_papers, key=\"papers_input\")\n    \n    # Button columns for fetching papers and refreshing\n    col_fetch, col_refresh = st.columns([2, 1])\n\n    with col_fetch:\n        if st.button(\"Fetch Papers & Generate Review\"):\n            if topic:\n                st.session_state.research_topic = topic  # Save topic in session state\n                try:\n                    with st.spinner(\"Fetching papers...\"):\n                        API_URL = \"http://127.0.0.1:8000/fetch_papers/\"  # Backend API endpoint\n                        response = requests.post(API_URL, json={\"topic\": topic, \"max_papers\": max_papers})\n\n                    if response.status_code != 200:\n                        st.error(\"Error: Invalid response from server.\")\n                        st.stop()\n\n                    result_decoded = response.content.decode(\"utf-8\")\n                    st.session_state.result_json = json.loads(result_decoded)  # Store API response in session state\n\n                except json.JSONDecodeError:\n                    st.error(\"Error: Received an unreadable response from the server.\")\n                    st.stop()\n                except requests.exceptions.RequestException:\n                    st.error(\"Error: Could not connect to the server.\")\n                    st.stop()\n\n    with col_refresh:\n        if st.button(\"🔄 Refresh\"):\n            st.session_state.result_json = None  # Clear previous results\n            st.session_state.research_topic = \"\"  # Reset topic field\n            st.session_state.max_papers = 5  # Reset number of papers\n            st.rerun()  # Force a page refresh\n\n# Main content layout\ncol1, col2 = st.columns([1, 11])\n\n# Page title\nst.title(\"AI Research Agent\")\n\n# Initial message when no results are available\nif st.session_state.result_json is None:\n    st.info(\"Enter a research topic and click 'Fetch Papers & Generate Review' to get started.\")\n\n# Display results if available\nif st.session_state.result_json:\n    research_topic = st.session_state.research_topic\n    research_topic = ' '.join(word.capitalize() for word in research_topic.split())  # Capitalize topic words\n    \n    st.header(f\"📖 Literature Review: {research_topic}\")\n\n    # If a PDF path is provided in the response, display download option\n    if \"pdf_path\" in st.session_state.result_json:\n        pdf_path = st.session_state.result_json[\"pdf_path\"]\n        filename = f\"{topic.replace(' ', '_')}_literature_review.pdf\"\n\n        # Create a row layout for success message + download button\n        col_success, col_download = st.columns([7, 1])\n\n        with col_success:\n            st.success(\"Literature Review Generated!\")\n\n        with col_download:\n            st.download_button(label=\"📥 Download\", \n                               data=open(pdf_path, \"rb\"), \n                               file_name=filename, \n                               mime=\"application/pdf\")\n        \n        # Display the generated literature review text\n        st.write(st.session_state.result_json.get(\"response\", \"No review available.\"))  \n    else:\n        st.error(\"No papers found!\")\nBACKEND\nmain.py\nThis is the main backend file with the following FastAPI endpoint.\nfetch_papers - This endpoint fetches the research papers on a given topic from arXiv, extracts and saves their metadata and generates a comprehensive literature review of the papers through the research agent. It saves the generated literature review in PDF format.\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom agno.agent import RunResponse\nfrom research_agent import research_agent\nfrom utils import extract_metadata, save_paper_metadata, generate_pdf\nimport os\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Define request model for research paper fetching\nclass ResearchRequest(BaseModel):\n    topic: str  # Research topic provided by the user\n    max_papers: int = 5  # Default number of papers to fetch\n\n# Define API endpoint to fetch research papers\n@app.post(\"/fetch_papers/\")\nasync def fetch_papers(request: ResearchRequest):\n    # Call the research agent to search for relevant papers and generate a literature review\n    response: RunResponse = research_agent.run(\n        f\"Search for {request.max_papers} most relevant papers on {request.topic} and generate a literature review.\"\n    )\n    \n    if response:\n        # Extract metadata from the response (e.g., title, authors, publication year, etc.)\n        metadata = extract_metadata(response.content)\n        \n        # Save extracted metadata to a file for future reference\n        metadata_file = save_paper_metadata(request.topic, metadata)\n        \n        # Generate a PDF containing the literature review\n        generate_pdf(request.topic, response.content)\n        \n        # Define the PDF file path based on the topic name\n        pdf_path = os.path.join(\"../literature_reviews\", f\"{request.topic.replace(' ', '_')}_literature_review.pdf\")\n        \n        # Return the generated literature review details\n        return {\n            \"message\": \"Literature review generated!\", \n            \"pdf_path\": pdf_path, \n            \"response\": response.content\n        }\n    else:\n        # Return an error message if no papers were found\n        return {\"error\": \"No papers found\"}\npdf.py\nThe PDFDocument class is used to convert the literature review generated by the research agent into a PDF format.\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib.units import inch\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.colors import blue, black, gray\nimport re\n\nclass PDFDocument:\n    def __init__(self, filename, title):\n        \"\"\"\n        Initializes the PDF document with the given filename and title.\n        \"\"\"\n        self.canvas = canvas.Canvas(filename, pagesize=letter)\n        self.canvas.setTitle(title)\n        self.width, self.height = letter\n\n    def add_page_number(self, page_num):\n        \"\"\"\n        Adds a page number at the bottom of each page.\n        \"\"\"\n        self.canvas.setFont('Helvetica', 10)\n        self.canvas.drawRightString(self.width - inch, 0.5 * inch, f\"Page {page_num}\")\n        self.canvas.line(inch, 0.6 * inch, self.width - inch, 0.6 * inch)\n\n    def draw_text(self, text, x, y, size=12, line_height=14, indent=0):\n        \"\"\"\n        Draws formatted text onto the PDF, handling bold, italic, and links.\n        \"\"\"\n        def get_chunks(text):\n            \"\"\"\n            Splits the text into chunks based on formatting (bold, italic, links).\n            \"\"\"\n            patterns = [r'\\*\\*(.*?)\\*\\*', r'\\*(.*?)\\*', r'(https?://\\S+)']\n            cursor = 0\n            for match in re.finditer('|'.join(patterns), text):\n                if cursor < match.start():\n                    yield text[cursor:match.start()], 'Helvetica', black\n                if match.group().startswith('**'):\n                    yield match.group()[2:-2], 'Helvetica-Bold', black\n                elif match.group().startswith('*') and not match.group().startswith('**'):\n                    yield match.group()[1:-1], 'Helvetica', black\n                elif re.match(r'https?://', match.group()):\n                    yield match.group(), 'Helvetica-Oblique', blue\n                cursor = match.end()\n            if cursor < len(text):\n                yield text[cursor:], 'Helvetica', black\n\n        cursor_x = x + indent\n        cursor_y = y\n        line_width = self.width - 2 * inch - indent\n\n        for chunk, style, color in get_chunks(text):\n            self.canvas.setFont(style, size)\n            self.canvas.setFillColor(color)\n\n            words = chunk.split()\n            for word in words:\n                word_width = self.canvas.stringWidth(word, style, size)\n                if cursor_x + word_width > x + line_width:\n                    cursor_y -= line_height\n                    cursor_x = x + indent\n                self.canvas.drawString(cursor_x, cursor_y, word)\n                cursor_x += word_width + self.canvas.stringWidth(' ', style, size)\n\n            self.canvas.setFillColor(black)\n\n        return x, cursor_y - line_height\n\n    def create_pdf(self, topic, content):\n        \"\"\"\n        Generates a literature review PDF from the provided topic and content.\n        \"\"\"\n        page_num = 1\n        y = self.height - 1.5 * inch\n        sections = content.split(\"\\n\")\n        bullet = u\"\\u2022\"  # Unicode for bullet point\n        list_counter = 1\n\n        # Add Title at the top\n        self.canvas.setFont('Helvetica-Bold', 20)\n        self.canvas.drawString(inch, y, f\"Literature Review: {topic}\")\n        y -= 0.5 * inch  # Space below the title\n\n        for section in sections:\n            # Add new page if the section is a new heading and space is low\n            if section.startswith(\"### \") and y < self.height - 2 * inch:\n                self.add_page_number(page_num)\n                self.canvas.showPage()\n                page_num += 1\n                y = self.height - inch\n\n            # Add new page if reaching bottom of the page\n            if y < 2 * inch:\n                self.add_page_number(page_num)\n                self.canvas.showPage()\n                page_num += 1\n                y = self.height - inch\n\n            # Handle headings\n            if section.startswith(\"### \"):\n                y -= 0.5 * inch\n                self.canvas.setFont('Helvetica-Bold', 18)\n                self.canvas.drawString(inch, y, section[4:].strip())\n                y -= 0.3 * inch\n                self.canvas.line(inch, y, self.width - inch, y)\n                y -= 0.4 * inch\n            # Handle bold text\n            elif section.startswith(\"**\"):\n                y -= 0.3 * inch\n                _, y = self.draw_text(section, inch, y, 14)\n            # Handle numbered lists\n            elif section.startswith(\"* \"):\n                section = f\"**{list_counter}.** {section[2:]}\"\n                list_counter += 1\n                _, y = self.draw_text(section, inch, y, 12, 14, 10)\n            # Handle bullet points\n            elif section.startswith(\"  + \") or section.startswith(\"  - \"):\n                section = f\"{bullet} {section[4:]}\"\n                _, y = self.draw_text(section, inch, y, 12, 14, 20)\n            # Handle normal text\n            else:\n                _, y = self.draw_text(section, inch, y)\n\n            y -= 0.1 * inch  # Add slight space between sections\n\n        # Add final page number and save PDF\n        self.add_page_number(page_num)\n        self.canvas.save()\nutils.py\nThese are helper functions to extract and save metadata from the research papers and generate PDF.\nimport os\nimport json\nimport re\nfrom pdf import PDFDocument\n\ndef extract_metadata(response_content: str):\n    metadata = []\n    paper_sections = response_content.split(\"## \")[1:]  # Splitting by section titles\n    \n    for section in paper_sections:\n        lines = section.strip().split(\"\\n\")\n        title = lines[0].strip()\n        \n        # Skip non-research sections\n        if title.lower() in [\"conclusion\", \"references\"]:\n            continue\n\n        authors = re.search(r\"\\*\\*Authors\\*\\*: (.+)\", section)\n        publication_date = re.search(r\"\\*\\*Publication Date\\*\\*: (.+)\", section)\n        keywords = re.search(r\"\\*\\*Keywords\\*\\*: (.+)\", section)\n        source_link = re.search(r\"(http[s]?://[^\\s]+)\", section)\n\n        metadata.append({\n            \"title\": title,\n            \"authors\": authors.group(1) if authors else \"\",\n            \"publication_date\": publication_date.group(1) if publication_date else \"\",\n            \"keywords\": [kw.strip() for kw in keywords.group(1).split(\",\")] if keywords else [],\n            \"source_link\": source_link.group(1) if source_link else \"\"\n        })\n\n    return metadata\n\n# Save paper metadata as JSON\ndef save_paper_metadata(topic, papers):\n    PAPERS_DIR = \"../papers_metadata\"\n    os.makedirs(PAPERS_DIR, exist_ok=True)\n\n    file_path = os.path.join(PAPERS_DIR, f\"{topic}_papers.json\")\n    with open(file_path, \"w\") as f:\n        json.dump(papers, f, indent=4)\n    return file_path\n\ndef generate_pdf(topic, response):\n    PDF_DIR = \"../literature_reviews\"\n    os.makedirs(PDF_DIR, exist_ok=True)\n\n    pdf_path = os.path.join(PDF_DIR, f\"{topic.replace(' ', '_')}_literature_review.pdf\")\n\n    print(\"Generating PDF...\")\n\n    # Initialize PDFDocument\n    pdf = PDFDocument(pdf_path, f\"Literature Review: {topic}\")\n\n    # Generate PDF content using the class method\n    pdf.create_pdf(topic, response)\n\n    print(f\"PDF successfully generated: {pdf_path}\")\n    return pdf_path\nresearch_agent.py\nThis is the AI agent powered by LLM which does all the autonomous work of finding, summarizing, and review generation of the research papers from arXiv. \n\nWe are using the Agent class from the agno framework. It uses a model (LLM) and a tool (ArXivTools) to accomplish its task. The LLM is used through the Together API provided in the framework. Similarly, ArxivTools are being used from built in toolkits in the framework to search and find research papers from arXiv. The description and instructions provided to the agent are very important here. We give a comprehensive set of instructions which help the agent autonomously decide which course of actions to take in order to achieve its goal. First, it uses ArxivTools to search the papers and bring their content including metadata and summaries. Then it utilizes those summaries to generate a comprehensive literature review.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.tools.arxiv import ArxivTools\nfrom agno.utils.pprint import pprint_run_response\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the research agent\nresearch_agent = Agent(\n    name=\"research-agent\",\n    model=Together(\n        id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", \n        api_key=os.getenv(\"TOGETHER_API_KEY\")),\n    tools=[ArxivTools()],\n    description=\n    '''\n        You are a research agent that will fetch papers from arXiv based on a given topic and maximum number of papers.\n        You will generate a comprehensive literature review of these papers.\n    ''',\n    instructions=[\n        \"Search for research papers on the given topic from arXiv.\",\n        \"If the number of papers is not specified, fetch 5 by default.\",\n        \"Process papers one at a time to reduce token usage.\",\n        \"For each paper, extract the following metadata: title, abstract, authors, publication date, keywords, and source link.\",\n        \"Limit extracted text to at most 2000 tokens per paper.\",\n        \"Generate a concise summary (max 300 tokens) for each paper.\",\n        \"After all papers are processed, generate a literature review using only stored summaries.\",\n        \"The final response should include the literature review of each research paper in a separate paragraph.\"\n        \"Each paper review should start with the number and title of the paper as a subheading.\"\n        \"The metadata of each paper should be displayed after the title.\",\n        \"Follow the following format for the metadata of each paper in normal text displaying each metadata item on a new line:\",\n            \"**Authors**: [Authors of the Paper]\",\n            \"**Publication Date**: [Date of Publication]\",\n            \"**Keywords**: [Keywords of the Paper]\",\n        \"Follow the following format for the review of each paper in italic text:\",\n            \"**Review**: *[Literature review of the Paper]*\",\n        \"Do not include a Literature Review heading in the response.\",\n        \"Include a conclusion section providing a combined summary of all papers.\",\n        \"Include a references section with citations and source links at the end of the literature review.\",\n        \"Include numbers for each paper in front of their titles.\",\n        \"Generate the literature review by processing the summaries of the papers in smaller chunks instead of one long response.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\nDEMO - Simple Research Agent\n\nHere is a demonstration of using the simple research agent.\n\nRunning the Simple Research Agent\nClone the repository from GitHub. The link has been provided in the References section.\nNavigate to the backend folder and start the FastAPI server:\ncd backend\nuvicorn main:app --reload\nNavigate to the frontend folder and start the Streamlit application:\ncd frontend\nstreamlit run app.py\n\nAccess the application through the following link.\n\nhttp://localhost:8501\nBuilding a Multi-Agent Collaborative System\nResearch Copilot\n\nLet's build a multi-agent collaborative system by enhancing the simple research agent built previously. This system finds and extracts research papers on a given topic, extracts their metadata, and summarizes them using a summarization agent, and generates a comprehensive literature review using a review generation agent. Both these agents work in an agentic workflow as the review generation agent is dependent on the response from the summarization agent.\n\nImplementation\nProject Structure\nresearch-copilot/               # Main project folder\n│── frontend/                   # Frontend directory\n│   ├── app.py                   # Streamlit application\n│\n│── backend/                     # Backend directory\n│   ├── main.py                   # FastAPI backend\n│   ├── pdf_from_json.py          # Converts JSON output to PDF\n│   ├── utils.py                  # Utility functions\n│   │\n│   ├── agents/                   # Agents folder\n│   │   ├── summarization_agent.py\n│   │   ├── review_generation_agent.py\n│   │\n│   ├── tools/                    # Tools folder\n│   │   ├── paper_download_tool.py\n│   │\n│   ├── workflows/                # Workflows folder\n│   │   ├── research_workflow.py\n│\n│── requirements.txt              # Dependencies\n│── .env                          # Environment variables\n│── LICENSE                       # License file\n│── README.md                     # Documentation\nSetting up the environment\n\nCreate a new virtual environment and install the dependencies using requirements.txt file.\n\nrequirements.txt\nstreamlit\nuvicorn\nfastapi\nagno\nrequests\narxiv\npypdf\nsqlalchemy\nreportlab\nhtml2text\npymupdf\n.env\nCreate a .env file and add your API key.\nTOGETHER_API_KEY=\"your_api_key_here\"\nFRONTEND\n\nThe frontend of the research copilot looks like this:\n\nUser Input: User enters a research topic and the number of papers to fetch from arXiv.\n\nOutput: Literature review of the fetched papers in JSON and PDF format.\n\napp.py\n\nThis is the main entry point of the application built in Streamlit. It uses requests to interact with the FastAPI endpoints at the backend.\n\nimport streamlit as st\nimport requests\nimport json\n\n# Set Streamlit page configurations\nst.set_page_config(page_title=\"AI Research Copilot\", layout=\"wide\")\n\n# Initialize session state variables to maintain user input and results\nif \"result_json\" not in st.session_state:\n    st.session_state.result_json = None\nif \"research_topic\" not in st.session_state:\n    st.session_state.research_topic = \"\"\nif \"max_papers\" not in st.session_state:\n    st.session_state.max_papers = 5\nif \"selected_paper\" not in st.session_state:\n    st.session_state.selected_paper = None\nif \"chat_history\" not in st.session_state:\n    st.session_state.chat_history = []\n\n# Define API URL for backend communication\nAPI_URL = \"http://127.0.0.1:8000\"\n\n# Sidebar UI for user inputs and controls\nwith st.sidebar:\n    st.image(\"../images/research_copilot_logo.png\", width=150)  # Display logo\n    st.subheader(\"Search and Generate Literature Reviews\")\n    st.header(\"🔍 Search Papers\")\n    \n    # Input fields for research topic and number of papers\n    topic = st.text_input(\"Enter Research Topic:\", key=\"topic_input\", value=st.session_state.research_topic)\n    max_papers = st.number_input(\"Number of Papers to Fetch:\", min_value=1, max_value=10, value=st.session_state.max_papers, key=\"papers_input\")\n    \n    col_btn1, col_btn2 = st.columns([2, 1])  # Layout for buttons\n    \n    with col_btn1:\n        # Fetch papers button\n        if st.button(\"Fetch Papers & Generate Review\"):\n            if topic:\n                st.session_state.research_topic = topic\n                try:\n                    with st.spinner(\"Fetching papers from arXiv...\"):\n                        response = requests.post(API_URL+\"/fetch_papers/\", json={\"topic\": topic, \"max_papers\": max_papers})\n                    \n                    if response.status_code != 200:\n                        st.error(\"Error: Invalid response from server.\")\n                        st.stop()\n                    \n                    # Parse response JSON\n                    result_decoded = response.content.decode(\"utf-8\")\n                    st.session_state.result_json = json.loads(result_decoded)\n                \n                except json.JSONDecodeError:\n                    st.error(\"Error: Received an unreadable response from the server.\")\n                    st.stop()\n                except requests.exceptions.RequestException:\n                    st.error(\"Error: Could not connect to the server.\")\n                    st.stop()\n    \n    with col_btn2:\n        # Refresh button to clear results and reset inputs\n        if st.button(\"🔄 Refresh\"):\n            st.session_state.result_json = None\n            st.session_state.research_topic = \"\"\n            st.session_state.max_papers = 5\n            st.rerun()\n\n# Display main application title\nst.title(\"AI Research Copilot\")\n\n# Show instructions if no results are available\nif st.session_state.result_json is None:\n    st.info(\"Enter a research topic and click 'Fetch Papers & Generate Review' to get started.\")\n\n# Display results if available\nif st.session_state.result_json:\n    research_topic = st.session_state.research_topic.title()  # Capitalize topic words\n    st.header(f\"📖 Literature Review: {research_topic}\")\n    \n    if \"pdf_path\" in st.session_state.result_json:\n        pdf_path = st.session_state.result_json[\"pdf_path\"]\n        filename = f\"{topic.replace(' ', '_')}_literature_review.pdf\"\n        \n        # Layout for success message and download button\n        col_success, col_download = st.columns([5, 1])\n        \n        with col_success:\n            st.success(\"Literature Review Generated!\")\n        \n        with col_download:\n            st.download_button(label=\"📥 Download\", \n                               data=open(pdf_path, \"rb\"), \n                               file_name=filename, \n                               mime=\"application/pdf\")\n    \n    # Extract and display literature review details\n    response = st.session_state.result_json.get(\"response\")\n    data = json.loads(response)\n    papers = data.get(\"papers\", [])\n    conclusion = data.get(\"conclusion\", \"\")\n    references = data.get(\"references\", [])\n    \n    if papers:\n        # Iterate through each retrieved paper and display details\n        for i, paper in enumerate(papers):\n            with st.container():\n                st.subheader(f\"📄 {i+1}. {paper['title']}\")\n                st.markdown(f\"**🖊️ Authors:** {paper['authors']}\")\n                st.markdown(f\"**📅 Publication Date:** {paper['publication_date'][:10]}\")\n                st.markdown(f\"**🔑 Keywords:** {paper['keywords']}\")\n                st.markdown(\"**📌 Summary:**\")\n                st.write(paper[\"summary\"])\n                st.markdown(\"**📝 Review:**\")\n                st.write(paper[\"review\"])\n                st.divider()\n        \n        # Display conclusion section\n        st.subheader(\"🧐 Conclusion\")\n        st.write(conclusion)\n        \n        # Display references section\n        st.subheader(\"📚 References\")\n        for ref in references:\n            st.markdown(f\"- {ref}\")\n    else:\n        st.error(\"No papers found!\")\nBACKEND\nmain.py\n\nThis is the FastAPI backend of the application. It has the following endpoint:\nfetch_papers - This endpoint fetches the research papers on a given topic from arXiv, extracts and saves their metadata and generates a comprehensive literature review of the papers using the research workflow. It saves the generated literature review in PDF format and returns the response in JSON format to be displayed at the frontend.\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom agents.chat_agent import chat_agent\nfrom workflows.research_workflow import research_workflow\nfrom agno.agent import RunResponse\nfrom knowledge.knowledge_base import pdf_knowledge_base\nfrom utils import extract_metadata, save_paper_metadata, generate_pdf\nimport os\n\n# Initialize FastAPI application\napp = FastAPI()\n\n# Define request model for research paper fetching\nclass ResearchRequest(BaseModel):\n    topic: str  # Research topic to fetch papers for\n    max_papers: int = 5  # Default number of papers to fetch\n\n@app.post(\"/fetch_papers/\")\nasync def fetch_papers(request: ResearchRequest):\n    \"\"\"\n    Endpoint to fetch research papers and generate a literature review.\n    Executes the research workflow and processes the retrieved papers.\n    \"\"\"\n    print(\"Executing workflow...\")\n    \n    # Run the research workflow to fetch relevant papers\n    response: RunResponse = research_workflow.run(topic=request.topic, max_papers=request.max_papers)\n\n    if response:\n        # Extract metadata from the response\n        metadata = extract_metadata(response.content)\n        metadata_file = save_paper_metadata(request.topic, metadata)\n        \n        # Generate and save the literature review PDF\n        generate_pdf(request.topic, response.content)\n        pdf_path = os.path.join(\"../literature_reviews\", f\"{request.topic.replace(' ', '_')}_literature_review.pdf\")\n\n        return {\n            \"message\": \"Literature review generated!\",\n            \"pdf_path\": pdf_path,\n            \"response\": response.content\n        }\n    else:\n        return {\"error\": \"No papers found\"}\npdf_from_json.py\nThis script converts the JSON response from the literature review agent into PDF format.\nfrom reportlab.lib.pagesizes import A4\nfrom reportlab.lib import colors\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n\ndef generate_pdf_from_json(topic, json_data, output_filename=\"research_report.pdf\"):\n    \"\"\"\n    Generate a PDF from a JSON response containing research papers, a conclusion, and references.\n\n    Args:\n        json_data (dict): JSON response with papers, conclusion, and references.\n        output_filename (str): Name of the output PDF file.\n    \"\"\"\n    doc = SimpleDocTemplate(output_filename, pagesize=A4)\n    elements = []\n\n    # Define styles with Unicode font\n    styles = getSampleStyleSheet()\n    title_style = ParagraphStyle(name=\"Title\", fontName=\"Helvetica-Bold\", fontSize=16, spaceAfter=10)\n    heading_style = ParagraphStyle(name=\"Heading2\", fontName=\"Helvetica\", fontSize=14, spaceAfter=8, textColor=colors.darkblue)\n    body_style = ParagraphStyle(name=\"BodyText\", fontName=\"Helvetica\", fontSize=12, spaceAfter=6)\n    bold_style = ParagraphStyle(name=\"Bold\", parent=body_style, fontName=\"Helvetica-Bold\")\n\n    # Add Title\n    elements.append(Paragraph(f\"Literature Review: {topic}\", title_style))\n    elements.append(Spacer(1, 12))\n\n    # Process Papers Section\n    if \"papers\" in json_data:\n        elements.append(Paragraph(\"Papers\", heading_style))\n        elements.append(Spacer(1, 6))\n\n        for idx, paper in enumerate(json_data[\"papers\"], 1):\n            elements.append(Paragraph(f\"{idx}. {paper['title']}\", bold_style))\n            elements.append(Paragraph(f\"Authors: {paper['authors']}\", body_style))\n            elements.append(Paragraph(f\"Publication Date: {paper['publication_date'][:10]}\", body_style))\n            elements.append(Paragraph(f\"Keywords: {', '.join(paper['keywords'])}\", body_style))\n            elements.append(Paragraph(f\"Source: <a href='{paper['source_link']}'>{paper['source_link']}</a>\", body_style))\n            elements.append(Spacer(1, 6))\n\n            # Add Abstract\n            # elements.append(Paragraph(\"Abstract:\", bold_style))\n            # elements.append(Paragraph(paper[\"abstract\"], body_style))\n            # elements.append(Spacer(1, 6))\n\n            # Add Summary\n            elements.append(Paragraph(\"Summary:\", bold_style))\n            elements.append(Paragraph(paper[\"summary\"], body_style))\n            elements.append(Spacer(1, 12))\n\n            # Add Review (if available)\n            if \"review\" in paper and paper[\"review\"]:\n                elements.append(Paragraph(\"Review:\", bold_style))\n                elements.append(Paragraph(paper[\"review\"], body_style))\n                elements.append(Spacer(1, 12))\n\n    # Conclusion Section\n    if \"conclusion\" in json_data:\n        elements.append(Paragraph(\"Conclusion\", heading_style))\n        elements.append(Spacer(1, 6))\n        elements.append(Paragraph(json_data[\"conclusion\"], body_style))\n        elements.append(Spacer(1, 12))\n\n    # References Section\n    if \"references\" in json_data:\n        elements.append(Paragraph(\"References\", heading_style))\n        elements.append(Spacer(1, 6))\n\n        # Format references as a table\n        ref_data = [[f\"{idx}. {ref}\"] for idx, ref in enumerate(json_data[\"references\"], 1)]\n        ref_table = Table(ref_data, colWidths=[500])\n\n        # Style the table\n        ref_table.setStyle(TableStyle([\n            (\"TEXTCOLOR\", (0, 0), (-1, -1), colors.black),\n            (\"ALIGN\", (0, 0), (-1, -1), \"LEFT\"),\n            (\"FONTNAME\", (0, 0), (-1, -1), \"Helvetica\"),  # ✅ Change to built-in font\n            (\"FONTSIZE\", (0, 0), (-1, -1), 10),\n            (\"BOTTOMPADDING\", (0, 0), (-1, -1), 6),\n            (\"GRID\", (0, 0), (-1, -1), 1, colors.black),\n        ]))\n\n        elements.append(ref_table)\n\n    # Build PDF\n    doc.build(elements)\n    print(f\"✅ PDF generated successfully: {output_filename}\")\nutils.py\n\nThese are utility functions to extract and save metadata and generate PDF.\n\nimport os\nimport json\nimport re\nfrom pdf_from_json import generate_pdf_from_json\n\ndef extract_metadata(response_content: str):\n    metadata = []\n    paper_sections = response_content.split(\"## \")[1:]  # Splitting by section titles\n    \n    for section in paper_sections:\n        lines = section.strip().split(\"\\n\")\n        title = lines[0].strip()\n        \n        # Skip non-research sections\n        if title.lower() in [\"conclusion\", \"references\"]:\n            continue\n\n        authors = re.search(r\"\\*\\*Authors\\*\\*: (.+)\", section)\n        publication_date = re.search(r\"\\*\\*Publication Date\\*\\*: (.+)\", section)\n        keywords = re.search(r\"\\*\\*Keywords\\*\\*: (.+)\", section)\n        journal = re.search(r\"\\*\\*Journal\\*\\*: (.+)\", section)\n        review_match = re.search(r\"\\*\\*Review\\*\\*: \\*(.+?)\\*\", section, re.DOTALL)\n        source_link = re.search(r\"(http[s]?://[^\\s]+)\", section)\n\n        metadata.append({\n            \"title\": title,\n            \"authors\": authors.group(1) if authors else \"\",\n            \"publication_date\": publication_date.group(1) if publication_date else \"\",\n            \"keywords\": [kw.strip() for kw in keywords.group(1).split(\",\")] if keywords else [],\n            \"journal\": journal.group(1) if journal else \"\",\n            \"source_link\": source_link.group(1) if source_link else \"\"\n        })\n\n    return metadata\n\n# Save paper metadata as JSON\ndef save_paper_metadata(topic, papers):\n    PAPERS_DIR = \"../papers_metadata\"\n    os.makedirs(PAPERS_DIR, exist_ok=True)\n\n    file_path = os.path.join(PAPERS_DIR, f\"{topic}_papers.json\")\n    with open(file_path, \"w\") as f:\n        json.dump(papers, f, indent=4)\n    return file_path\n\ndef generate_pdf(topic, response):\n    PDF_DIR = \"../literature_reviews\"\n    os.makedirs(PDF_DIR, exist_ok=True)\n\n    pdf_path = os.path.join(PDF_DIR, f\"{topic.replace(' ', '_')}_literature_review.pdf\")\n\n    print(\"Generating PDF...\")\n\n    # Generate PDF from JSON\n    json_response = json.loads(response)\n    generate_pdf_from_json(topic, json_response, pdf_path)\n\n    print(f\"PDF successfully generated: {pdf_path}\")\n    return pdf_path\npaper_download_tool.py\n\nThis tool helps download the research papers from arXiv into a local directory for further reference and processing.\n\nimport requests\nimport os\n\ndef download_arxiv_papers(topic, links):\n    \"\"\"\n    Downloads PDFs from given arXiv source links and saves them locally.\n\n    Args:\n        topic (str): Topic of the papers (used as the directory name).\n        links (list): List of arXiv paper URLs (e.g., \"https://arxiv.org/abs/2403.12345\").\n        \n    Returns:\n        list: A list of dictionaries containing 'link', 'file_path' (or 'error' if any).\n    \"\"\"\n    save_dir = f\"downloaded_papers/{topic}\"\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)  # Create directory if it doesn't exist\n\n    results = []\n\n    for link in links:\n        result_entry = {\"link\": link}  # Initialize entry with the link\n\n        try:\n            # Convert abstract page URL to PDF URL\n            if \"arxiv.org/abs/\" in link:\n                pdf_url = link.replace(\"arxiv.org/abs/\", \"arxiv.org/pdf/\") + \".pdf\"\n            elif \"arxiv.org/pdf/\" in link and not link.endswith(\".pdf\"):\n                pdf_url = link + \".pdf\"\n            else:\n                pdf_url = link  # Assume it's already a direct PDF link\n\n            paper_id = pdf_url.split(\"/\")[-1]  # Extract paper ID\n            file_path = os.path.join(save_dir, f\"{paper_id}\")\n\n            # Download the PDF\n            response = requests.get(pdf_url, stream=True)\n            response.raise_for_status()  # Raise error for failed requests\n\n            with open(file_path, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=1024):\n                    file.write(chunk)\n\n            result_entry[\"file_path\"] = file_path  # Store success result\n\n        except requests.exceptions.RequestException as e:\n            result_entry[\"error\"] = str(e)  # Store error message\n\n        results.append(result_entry)  # Append the result entry to the list\n\n    return results\nsummarization_agent.py\n\nThis is the summarization agent which fetches papers from arXiv based on a given topic and maximum number of papers. It extracts the metadata, summary, and a list of source links of the papers and downloads them in PDF format into a local directory.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.tools.arxiv import ArxivTools\nfrom tools.paper_download_tool import download_arxiv_papers\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the summarization agent\nsummarization_agent = Agent(\n    name=\"summarization-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\")),\n    tools=[ArxivTools(), download_arxiv_papers],\n    role=\"Download papers from arXiv and save them for further processing.\",\n    description=\n    '''\n        You are a summarization agent that will fetch papers from arXiv based on a given topic and maximum number of papers.\n        You will extract the metadata, summary, and a list of source links of the papers and download them in pdf format.\n    ''',\n    instructions=[\n        \"Search for the latest and most relevant research papers on the given topic from arXiv.\",\n        \"If the number of papers is not specified, fetch 5 by default.\",\n        \"Extract the metadata, summary, and a list of source links of the papers for downloading.\",\n        \"For each paper, extract the following metadata: title, abstract, authors, publication date, keywords, and source link.\",\n        \"Download the papers in pdf format for processing by other agents.\",\n        \"Your response should include a list of the extracted papers including the following information for each paper:\"\n        \"1. Metadata: Title, Authors, Abstract,Publication Date, Keywords, Source Link\",\n        \"2. Summary: Abstract or a concise summary of the paper\",\n        \"3. Paths to the downloaded papers for further processing\",\n        \"The information for each paper must be included in the following format.\",\n            \"Title: [Title of the Paper]\",\n            \"Authors: [Authors of the Paper]\",\n            \"Abstract: [Abstract of the Paper]\",\n            \"Publication Date: [Date of Publication]\",\n            \"Keywords: [Keywords of the Paper]\",\n            \"Source Link: [Source Link of the Paper]\",\n            \"Summary: [Summary of the Paper]\",\n            \"PDF Path: [Path to the Downloaded Paper]\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\nreview_generation_agent.py\n\nThis is the review generation agent which generates a comprehensive literature review of the research papers using the provided metadata and summaries of papers generated by the summarization agent. It saves the literature review in PDF format for downloading.\n\nfrom agno.agent import Agent\nfrom agno.models.together import Together\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the review generation agent\nreview_generation_agent = Agent(\n    name=\"review-generation-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\")),\n    description=\n    '''\n        You are a review generation agent that will generate a comprehensive literature review of research papers using \n        the provided metadata and summaries of papers generated by the Summarization Agent.\n    ''',\n    instructions=[\n        \"Generate a literature review using the metadata, abstracts, and summaries of the papers from the Summarization Agent.\",\n        \"The final response should include the literature review of each research paper in a separate paragraph.\",\n        \"Each paper review should start with the title of the paper as a subheading, along with metadata of that paper at the beginning.\"\n        \"Follow the following format for the metadata of each paper in normal text displaying each metadata item on a new line:\",\n            \"**Authors**: [Authors of the Paper]\",\n            \"**Publication Date**: [Date of Publication]\",\n            \"**Keywords**: [Keywords of the Paper]\",\n            \"**PDF Path**: [Path to the Downloaded Paper]\",\n        \"Display each item of the metadata on a new line.\",\n        \"Follow the following format for the review of each paper in italic text:\",\n            \"**Review**: *[Literature review of the Paper]*\",\n        \"Do not include a Literature Review heading in the response.\",\n        \"Include a conclusion section providing a combined summary of all papers.\",\n        \"Include a references section at the end of the literature review which includes a list of citations with source links.\",\n        \"Generate the literature review by processing the summaries of the papers in smaller chunks instead of one long response.\"\n        \"Important: You final response must be in JSON format with the following structure:\",\n            \"{\"\n            \"    'papers': [\"\n            \"        {\"\n            \"            'title': 'Title of the Paper',\"\n            \"            'authors': 'Authors of the Paper',\"            \n            \"            'abstract': 'Abstract of the Paper',\"\n            \"            'publication_date': 'Date of Publication',\"\n            \"            'keywords': 'Keywords of the Paper',\"\n            \"            'source_link': 'Source Link of the Paper',\"\n            \"            'summary': 'Summary of the Paper',\"\n            \"            'review': 'Literature Review of the Paper'\",\n            \"            'pdf_path': 'Path to the Downloaded Paper'\"\n            \"        },\"\n            \"        {...}\"\n            \"    ]\",\n            \"    'conclusion': 'Combined summary of all papers',\"\n            \"    'references': 'List of citations with source links'\"\n            \"}\"\n        \"The above JSON response must include all papers with required information, conclusion, and references.\",\n        \"The response MUST be in proper JSON format with keys and values in double quotes.\",\n        \"The final response MUST not include anything else other than the JSON response.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\nresearch_workflow.py\n\nThis is the agentic workflow of this multi-agent collaborative system. It runs the summarization agent and the review generation agent step by step as the input of one agent depends on the output of the other agent. It is an end to end workflow that helps the agents collaborate and work together effectively to achieve their goal.\n\nfrom agno.agent import Agent\nfrom agno.workflow import Workflow, RunResponse, RunEvent\nfrom agno.storage.workflow.sqlite import SqliteWorkflowStorage\nfrom agno.utils.pprint import pprint_run_response\nfrom agno.utils.log import logger\n\nfrom agents.summarization_agent import summarization_agent\nfrom agents.review_generation_agent import review_generation_agent\n\n# Define the ResearchCopilot workflow \nclass ResearchCopilot(Workflow):\n    summarization_agent: Agent = summarization_agent\n    review_generation_agent: Agent = review_generation_agent\n\n    def run(self, topic: str, max_papers: int = 5) -> RunResponse:\n        \"\"\"\n            Executes the research workflow:\n            1. Searches for research papers related to the topic.\n            2. Generates a literature review based on the extracted papers.\n        \"\"\"\n        logger.info(f\"Generating a literature review of {max_papers} research papers from arXiv on: {topic}\")\n\n        # Step 1: Search arXiv for research papers on the topic and summarize them\n        extracted_papers = self.get_extracted_papers(topic, max_papers)\n        \n        # If no extracted papers are found for the topic, end the workflow\n        if extracted_papers is None:\n            return RunResponse(\n                event=RunEvent.workflow_completed,\n                content=f\"Sorry, could not find any research papers on the topic: {topic}\",\n            )           \n        \n        print(\"Extracted papers:\", extracted_papers)\n\n        # Step 2: Generate a literature review of the extracted papers\n        literature_review: RunResponse = self.review_generation_agent.run(extracted_papers)\n        if literature_review is None:\n            return RunResponse(\n                event=RunEvent.workflow_completed,\n                content=\"Sorry, could not generate a literature review of the research papers.\",\n            )\n        \n        print(\"Literature review:\", literature_review.content)\n        return RunResponse(\n            event=RunEvent.workflow_completed,\n            content=literature_review.content,\n        )\n\n    def get_extracted_papers(self, topic: str, max_papers: int):\n        \"\"\"Get the search results for a topic.\"\"\"\n\n        MAX_ATTEMPTS = 3\n\n        for attempt in range(MAX_ATTEMPTS):\n            try:\n                prompt = f\"Search for {max_papers} most relevant papers on {topic}.\"\n                summarizer_response: RunResponse = self.summarization_agent.run(prompt)\n\n                # Validate response content\n                if not summarizer_response or not summarizer_response.content:\n                    logger.warning(f\"Attempt {attempt + 1}/{MAX_ATTEMPTS}: Empty searcher response\")\n                    continue\n\n                logger.info(f\"Found papers on the topic {topic} in attempt {attempt + 1}\")\n\n                print(\"Extracted papers:\", summarizer_response.content)\n                return summarizer_response.content\n            \n            except Exception as e:\n                logger.warning(f\"Attempt {attempt + 1}/{MAX_ATTEMPTS} failed: {str(e)}\")\n\n        logger.error(f\"Failed to get extracted papers after {MAX_ATTEMPTS} attempts\")\n        return None\n    \n\n# Initialize the Research Copilot workflow with SQLite storage\nresearch_workflow = ResearchCopilot(\n    session_id=f\"generate-literature-review\",\n    storage=SqliteWorkflowStorage(\n        table_name=\"generate_literature_review_workflows\",\n        db_file=\"workflows/db/workflows.db\",\n    ),\n)\nDEMO - Research Copilot\n\nThis is a demonstration of using the research copilot.\n\nRunning the Research Copilot\nClone the repository from GitHub. The link has been provided in the References section.\nNavigate to the backend folder and start the FastAPI server:\ncd backend\nuvicorn main:app --reload\nNavigate to the frontend folder and start the Streamlit application:\ncd frontend\nstreamlit run app.py\n\nAccess the application through the following link.\n\nhttp://localhost:8501\nLimitations & Considerations\nBoth the Simple Research Agent and the Research Copilot use Llama 3.3 70B Instruct turbo Free as an LLM from the Together API. This model has certain limitations in terms of token usage.\nThe prompts given to the agents have been specifically designed to generate short summaries and reviews in order to limit the token usage as the basic purpose of building these agents is to understand the whole agentic AI workflow.\nThe Simple Research Agent returns the review generated by the LLM without any formatting, therefore, the layout of its response would be different every time.\nThe Research Copilot has been explicitly instructed to generate the response in JSON format so that it can be displayed at the frontend in a consistent manner.\nPotential future enhancements could be adding an interactive chat agent to chat with individual research papers, integrating other research paper repositories like Semantic Scholar and IEEE Xplore, and introducing enhanced citation formatting.\nA paper assessment agent could be added to facilitate effective research paper writing.\nA paper analysis agent could be integrated to critically analyze different sections of a research paper, finding potential discrepancies and suggesting new research directions.\nPersonal Tips & Tricks\n\nAgents are no magic. They are complex software systems powered by LLMs. Here are a few tips from my personal experience for building effective agents:\n\nYou must have a clear idea about what you are going to build. If you start with the right problem formulation, you'll end up building the right agentic system.\nAn agent must have a clear, specific goal to achieve.\nGive every agent a small specific task so that it doesn't get confused or it may start hallucinating as a result. As the agent builds upon an LLM, there is a fair chance that it might get off track and does not produce the right output. The more focused a single agent is, the better it is.\nPrompt is the most important thing in the agentic workflow. If it is not correct or thoughtfully prepared, the agent won't work as required. \nTest different models (LLMs) and tool combinations to find the right fit for your specific use-case.\nFuture of AI Agents\n\nAI agents are going to totally transform how we live and work especially for local businesses. Imagine AI handling routine tasks, freeing business owners to focus on growth and innovation.\n\nLocal businesses would be able to provide smarter customer service and more efficient operations with the integration of AI agents.\nService providers would be able to deliver more value with less effort.\n\nAI agents are set to revolutionize how work gets done with a huge impact in every domain. They can easily take up routine tasks freeing you to focus on more important things.\n\nPowerful agentic AI systems have been possible because of the recent advancements in LLMs and their reasoning capabilities. The future of AI agents seems to be very bright incorporating multi-agent collaborative systems into business processes.\n\nThere will be an increase in businesses adopting AI agents to automate their routine tasks.\nMulti-agent collaborative systems would be working in production environments.\nMultiple LLMs would be playing their roles in agentic workflows, each targeting its own domain-specific task.\nThere is going to be a rise in business development agents improving productivity like never before.\nNext Steps…\n\nThis article serves as a starting point for anyone who wants to understand the concept of AI agents from a very basic level along with real-world use cases and implementation. To further enhance your understanding and taking it to the next level, consider:\n\nFocusing on different agentic patterns and understanding their strengths and weaknesses in detail. \nExploring different agentic AI frameworks and understanding their key features and limitations.\nDoing hands-on projects solving real-world problems through AI agents.\nConclusion\n\nAI agents are the most powerful innovation in the history of artificial intelligence. They are software applications that use large language models (LLMs) to autonomously perform specific tasks, ranging from answering research questions to handling backend services. They work towards achieving a specific goal in an optimized way marking their true impact while making our lives easier. AI agents are incredibly useful for tasks that demand complex decision making, autonomy, and adaptability. LLMs, tools, and orchestration are the core components of an agentic architecture. AI agents leverage AI models (LLMs) to understand goals, generate tasks, and go about completing them in an efficient manner. Examples of AI agents include coding agents, customer service chatbots, and personalized learning management systems.\n\nReferences\nAgents white paper - Google\nMastering AI Agents e-book - Galileo\nChapter 2: Intelligent Agents from Artificial Intelligence - A Modern Approach by Stuart J. Russel & Peter Norvig\nIntro to AI Agents - YouTube: Google Cloud Tech\nDocumentation: agno\nWhat are AI Agents? - aws\nWhat are AI Agents ? - IBM\nWhat is an AI Agent? - Google Cloud\nBuilding Effective Agents - Anthropic\nAgentic Patterns from Scratch - The Neural Maze\nSimple Research Agent - A Single Agent\nResearch Copilot - A Multi-Agent Collaborative System\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nIntroduction\n\nWhat are AI agents?\n\nImportance of AI Agents\n\nAgentic AI vs Generative AI\n\nWhere do LLMs stand in the scope of AI Agents?\n\nImportance of Prompt for an AI Agent\n\nEvolution of AI Agents in Recent Years\n\nModels\n\nCompound AI Systems\n\nAI Agents\n\nView all\nComments\nCode\nYou might be interested\nAgentic AI Applications\nDistinguished Technical Deep-Dive\nY\nMar 24, 202583 reads\nAgentic AIArtificial intelligence+3\nAn Intelligent Research Paper Search Agent\nA\nMar 19, 202525 reads\nAcademic Research Supportagentic-ai+7\nBuilding an Agentic AI: A Practical Guide with a Task Management Example\nDec 30, 202416 reads\nMulti-Agent Research Assistant\nJul 31, 202511 reads\nLangchainMULTI-AGENT+2",
    "awards": [
      "distinguished technical deep-dive"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "agentic-ai-applications-EWPgeS4fdJlB",
    "username": "yYash Nikumbh",
    "license": "There are some Access and Availability Status that should be considered before using any frameworks or tools -\nLicense Agreements: Some frameworks or tools (like Azure OpenAI, or certain paid APIs) may require users to accept specific license agreements before use.\nAPI Keys & Authentication: Many tools, especially cloud-based ones like Azure OpenAI, will require users to create accounts and obtain API keys. Make sure to follow the authentication steps outlined in their respective documentation.\nBeta Versions: Be aware that some frameworks may be in beta or experimental stages. These might offer advanced features but could come with stability issues.",
    "title": "Agentic AI Applications",
    "publication_description": "Back to publications\nMar 24, 2025\n●\n83 reads\n●\nApache 2.0\nWinner of\nDistinguished Technical Deep-Dive\nat the\nAgentic AI Innovation Challenge 2025\nAgentic AI Applications\nAgentic AI\nArtificial intelligence\nAzureOpenAI\nLLM\nOrchestration\nY\nYash Nikumbh\nLike\nBookmark\nShare\nIntroduction\n\nHey folks, a quick introduction about the content that I'll be sharing here today. This publication will help you deeply understand the application of Agentic AI programmatically. I will be covering a few popular frameworks which will help you get started with Agentic AI Frameworks.\n\n\nAbstract\n\nThis paper looks at agentic artificial intelligence (AI) frameworks, which allow AI systems to make decisions, pursue goals, and learn on their own. Agentic AI is a big step forward in how intelligent systems work, enabling them to handle complex tasks with little human input. In this paper, we explore the basics of agentic AI and how it can be used in areas like robotics, virtual assistants, and decision-making tools. We also dive into how developers can code these frameworks, offering practical examples and code snippets to help bring agentic AI to life. By exploring various case studies and providing practical coding examples, this research aims to bridge the gap between conceptual understanding and practical implementation, equipping developers and researchers with the tools needed to harness the power of agentic AI. Even simple applications of agentic AI can add significant value by automating everyday tasks and improving efficiency. Application of these frameworks will show how automation can be achieved for small intermediate steps in business.\n\nMethodology\n\nAs all industries are moving towards AI-powered solutions, it is important for developers to approach the integration of agentic AI with responsibility and foresight. As creators of these systems, developers must ensure that the AI's autonomy is balanced with ethical considerations, safety protocols, and transparency. By doing so, we can create AI that not only enhances productivity but also aligns with societal values and benefits. The few frameworks that this paper would be covering is Autogen, LangGraph, Crewai, SmolAgents and LlamaIndex . By first understanding these key terms, we lay the groundwork for better understanding how agentic AI frameworks function in practice;\n[Ref - \nPrepare for agentic AI\n]\n\nTo keep the content concise, references are included throughout the paper. Please feel free to navigate to them for a deeper understanding of each concept.\n\nAgentic AI - Agentic AI uses sophisticated reasoning and iterative planning to solve complex, multi-step problems.\n[Ref - \nWhat is Agentic AI?\n]\n\nAgentic AI frameworks - Agentic frameworks are the building blocks for developing, deploying and managing AI agents.\n[Ref - \nWhat is Agentic AI?\n]\n\nAgent - An artificial intelligence (AI) agent is a software program that can interact with its environment, collect data, and use the data to perform self-determined tasks to meet predetermined goals. Humans set goals, but an AI agent independently chooses the best actions it needs to perform to achieve those goals.\n[Ref - \nWhat are AI agents?\n]\n\nTool Use - Interaction with external systems like APIs, databases, calculators, and even web searches.\n[Ref - \nTool calling Agents\n]\n\nKnowledge Graph - A collection of Entities, Entity Types, and Entity Relationship Types that enable reasoning and inference.\n[Ref - \nTool calling Agents\n]\n\nMulti-Agent systems - A multi-agent system (MAS) is composed of multiple interacting intelligent agents - autonomous entities that can sense, learn models of their environment, make decisions and act upon them.\n[Ref - \nConcept of Multi-agentic system.\n]\n\nOrchestration - Artificial intelligence (AI) agent orchestration is the process of coordinating multiple specialized AI agents within a unified system to efficiently achieve shared objectives.\n[Ref - \nAI agent Orchestration\n]\n\nHuman-in-loop - A human acting as a mediator/orchestrator helps agent gather feedback. This integration allows human to provide guidance and oversight before tasks are executed, ensuring that the agent stays on the right track and aligns with the intended goals.\n[Ref - \nHuman-in-loop\n]\n\nPrompt - In the context of AI, a prompt is the input you provide to the model to elicit a specific response. This can take various forms, ranging from simple questions or keywords to complex instructions, code snippets, or even creative writing samples. The effectiveness of your prompt directly influences the quality and relevance of the AI's output.\n[Ref - \nAI agent Orchestration\n]\n\nTokens - Tokens can be thought of as pieces of words. Before the API processes the request, the input is broken down into tokens. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words.\n[Ref - \nWhat are tokens?\n]\n\nNow that we have a solid understanding of the key concepts in agentic AI, let's dive into the frameworks that enable the development and deployment of such systems. These frameworks provide the structure and tools necessary for building autonomous AI systems that can make decisions, learn from experience, and interact with their environments effectively.\n\nThere are some Access and Availability Status that should be considered before using any frameworks or tools -\nLicense Agreements: Some frameworks or tools (like Azure OpenAI, or certain paid APIs) may require users to accept specific license agreements before use.\nAPI Keys & Authentication: Many tools, especially cloud-based ones like Azure OpenAI, will require users to create accounts and obtain API keys. Make sure to follow the authentication steps outlined in their respective documentation.\nBeta Versions: Be aware that some frameworks may be in beta or experimental stages. These might offer advanced features but could come with stability issues.\n\nBefore we jump into the use cases and how these frameworks work in practice, let’s quickly go over what you’ll need to get started. Here's a list of the essentials to set everything up and make the most of these tools:\n\nAny local IDE - VS code, Pycharm, etc\nPython (Version 3.10 or higher)\nVirtual Environment Setup (Optional but Recommended) -\npython -m venv myenv\nsource myenv/bin/activate\nEnvironment Configuration and Setup(using .env)\n\nHere is the summary of access and availability status for each framework:\n\nAutoGen\nAccess: AutoGen is available through the official GitHub repository. It is an open-source framework and can be freely accessed and integrated into your projects.\nAvailability: As of now, AutoGen is actively maintained and frequently updated. Make sure to check the repository for the latest release.\nLimitations: Users need an Azure account to use certain services within AutoGen (e.g., language models). Some features may require specific Azure permissions.\n\nLangChain\nAccess: LangChain is also open-source and can be accessed via the LangChain GitHub repository. It's widely used for chaining together different components, such as LLMs and other tools.\nAvailability: LangChain is actively maintained, with regular updates and a large community of contributors. The framework is publicly available and free to use.\nLimitations: While LangChain itself is free to use, certain tools and services (such as APIs or cloud-based integrations) may require separate licensing or API keys.\n\nCrewai\nAccess: Crewai is available as part of the CrewAI package. The library is open-source and free to use.\nAvailability: Crewai is currently stable but has periodic updates. It's available for integration into custom workflows and projects.\nLimitations: Crewai’s functionality can be limited by the complexity of the tasks it is set to perform. Users need to ensure they are using compatible tools and agents for optimal performance.\n\nSmolAgents\nAccess: SmolAgents is available via GitHub. It’s an experimental framework with some parts still under active development.\nAvailability: SmolAgents is publicly available, though some features may be subject to changes as it evolves. Be sure to check the repository for any breaking changes or updates.\nLimitations: SmolAgents is not fully stable in every scenario, especially for larger projects. Users may experience issues related to scalability and integration with certain APIs.\n\nLlamaIndex\nAccess: LlamaIndex is an open-source project and can be found on GitHub. It is widely used in LLM-powered applications.\nAvailability: The framework is stable and maintained, and it is supported by a large user community.\nLimitations: While LlamaIndex is freely available, users may face limitations in terms of scalability when handling large data sets. Performance might be affected based on the deployment scale.\n\nIn this section, we’ll explore several widely-used agentic AI frameworks, examining their core features, practical applications, and how developers can implement them to create intelligent, goal-directed systems. Each framework has its unique strengths and use cases, and we’ll discuss how they can be leveraged across various industries, from robotics and virtual assistants to complex decision-making systems.\n\nSmart Orchestration for Local Code Generation and Execution using AutoGen -\n\nAutogen\n\n\n\nVersion Used - 0.047\nFramework description -\nAutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. AutoGen aims to provide an easy-to-use and flexible framework for accelerating development and research on agentic AI.\n\n\nKey Features :\n\nAsynchronous messaging\nModular and extensible\nObservability and debugging\nScalable and distributed\nBuilt-in and community extensions\nCross-language support\nFull type support\n\nArchitecture of class demonstrated(MagenticOneGroupChat)\n\n\nMagentic-One is a versatile and generalist multi-agent system designed to solve open-ended tasks, both web-based and file-based, across a variety of domains. It was built using the autogen-core library but has since been upgraded to work with autogen-agentchat, offering a more modular and user-friendly interface. The system is powered by an orchestrator called MagenticOneGroupChat, which now functions as an AgentChat team, supporting various agents.\n\nInstallation and Usage Instructions :\n\nIt is recommended to create a virtual environment on your local machine before installing any packages.\n\nPlease refer to \ninstallation guide\n for getting started.\n\nLocalCommandLineCodeExecutor functionality is not recommended by Autogen. Due to resource limitation I have used local executor. Consider using docker!\n\nProblem Statement Addressed - The need for a self-healing, autonomous orchestration system for local code generation and execution, capable of operating without human intervention.\n\n## Note - AutoGen internally uses a lot of packages which might need to download externally\n# importing required libraries\nimport asyncio\n\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\nfrom autogen_ext.agents.magentic_one import MagenticOneCoderAgent\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\nfrom autogen_ext.agents.file_surfer import FileSurfer\nfrom autogen_ext.tools.code_execution import PythonCodeExecutionTool\n\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent\n\nfrom autogen_core import SingleThreadedAgentRuntime\n\n# Using AutoGen with Async as it is supported in >=v0.4\nasync def main() -> None:\n    # Setting runtime for the team which we are using - MagenticOneGroupChat\n    runtime = SingleThreadedAgentRuntime()\n    runtime.start()\n\n    # Enabling Local code Execution supported in AutoGen\n    ## WARNING - Functionality is not recommended by Autogen. Due to resource limitation I have used local executor. Consider using docker!\n    executor = LocalCommandLineCodeExecutor(work_dir=\"output\")\n    python_tool = PythonCodeExecutionTool(executor=executor)\n\n    # Start off by setting up model client which is essential for defining an agent \n    model_client = AzureOpenAIChatCompletionClient(\n    model='gpt-4o',\n    azure_endpoint='https://example.deployment.azure.com/',\n    azure_deployment='deployment_name',\n    api_version='2024-08-01-preview',\n    api_key='API_key',\n    )\n\n    # Define agents \n    web_surfer = MultimodalWebSurfer(\n    \"webSurfer\",\n    model_client=model_client,\n    headless=True,\n    to_save_screenshots=True,\n    use_ocr=True,\n    debug_dir=\"./output\"\n    )\n\n    coder = MagenticOneCoderAgent(\n    \"coder\",\n    model_client=model_client,\n    )\n\n\n    unit_tester = AssistantAgent(\n    \"tester\",\n    model_client=model_client,\n    description=\"An AI assistant that helps in creating unit test cases and conduct a security scan for the code.\",\n    system_message=\"\"\"create and execute 3 unit test cases using the following format:\n    Unit test case - 1: Test case description\n    Input: Input values\n    Expected Output: Expected output.\n    Code: Unit test code\n    Repeat the same for all test cases.\n    \"\"\",\n    tools=[python_tool],\n    )\n\n    local_file_surfer = FileSurfer(\n    \"fileSurfer\",\n    model_client=model_client,\n    description=\"A local file surfer that can access files on the local system.\",\n    )\n\n    code_executor = CodeExecutorAgent(\n    \"CoderExecutor\",\n    code_executor=executor,\n    )\n\n    critic_agent = AssistantAgent(\n    \"critic\",\n    model_client=model_client,\n    description=\"An AI assistant that helps in reviewing the code and ensure it is executing properly.\",\n    )\n\n    # Use agents as participants in group chats\n    participants = [unit_tester, local_file_surfer, code_executor, web_surfer, coder, critic_agent]\n\n    # Use agents as participants in group chats\n    team = MagenticOneGroupChat(\n    participants=participants,\n    model_client=model_client,\n    max_turns=50,\n    runtime=runtime,\n    )\n\n    # try running user query\n    await Console(team.run_stream(task=\"write a code to scrape web pages using python\"), output_stats=True)\n    await runtime.stop()\n\nasyncio.run(main())\n\nCode Explanation -\nThe provided code sets up an asynchronous environment to manage a team of AI agents, each with a specific role, using the SingleThreadedAgentRuntime. This setup allows each agent to perform distinct tasks, such as web surfing, code execution, file browsing, and unit testing, working together to complete a larger goal. The main function initializes these agents, starts the runtime, and triggers a task using the console interface.\n\nImports:\nThe code begins by importing various necessary modules and classes that help set up the agents and their functionalities. These imports include tools for code execution, interacting with models, and managing agent behaviors.\n\nMain Function:\nAn asynchronous main function is defined to initialize and execute the agents. It sets up the overall workflow of the agents in the environment, ensuring that tasks are carried out as expected.\n\nRuntime Initialization:\nThe SingleThreadedAgentRuntime is initialized and started within the main function. This runtime serves as the backbone for managing agent operations, ensuring that each task is processed sequentially and efficiently in a single-threaded environment.\n\nCode Executor:\nThe LocalCommandLineCodeExecutor is set up to execute code locally, specifically within the \"output\" directory. This executor facilitates running code on the local machine rather than relying on remote servers. It ensures that any code generated by the agents is executed in a controlled, local environment. The PythonCodeExecutionTool is created using this executor, enabling the execution of Python code tasks.\n\nModel Client:\nThe AzureOpenAIChatCompletionClient is initialized with necessary parameters to interact with the Azure OpenAI service. This client allows the agents to communicate with a language model (such as GPT) to generate code, solve problems, or perform various tasks that require natural language processing.\n\nAgents Initialization:\nSeveral agents are initialized, each serving a unique role;\n\nWeb Surfer(The MultimodalWebSurfer is configured to carry out web surfing tasks, allowing it to browse the web and gather information.)\n\nCoder(The MagenticOneCoderAgent is initialized to handle coding tasks, such as writing scripts or solving coding challenges.)\n\nUnit Tester(The AssistantAgent is configured to create and run unit tests, ensuring that the code functions correctly.)\n\nFile Surfer(The FileSurfer agent is set up to navigate local files, accessing documents, configurations, or code that may be needed.)\n\nCode Executor(The CodeExecutorAgent is set up to handle the execution of the code generated by the other agents.)\n\nCritic Agent(Another AssistantAgent is used as a critic to review and verify the correctness of the code execution.)\n\nTeam Setup:\nA list of agents (referred to as participants) is created to collaborate on the task. The MagenticOneGroupChat orchestrates the interactions between these agents, enabling them to work together seamlessly. It is initialized with the participants, model client, and the runtime environment, ensuring that all agents are ready to cooperate.\n\nTask Execution:\nUsing the console interface, a task is run where the agents work together to write Python code for scraping web pages. The task involves multiple agents interacting with each other—web surfing agents collect information, coding agents write the code, and unit testing agents ensure the code works as intended. Once the task is completed, the runtime is stopped.\n\nRun Main Function:\nFinally, the asyncio.run(main()) method is used to run the main function asynchronously, initiating the entire process and allowing the agents to execute their respective roles in the workflow.\n\nSample Output -\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an example of an implementation where the system can autonomously generate and execute code, create and run test cases, and even heal itself when issues arise. You can add or remove agents as per your requirements. This system can implement any library/framework as it has access to web it can gather relevant information and try out a sample implementation.\n\nLimitations/Drawbacks : The orchestration class can be token-hungry, especially when the Group Chat exceeds 8-10 messages, often leading to token shortages. This becomes a significant challenge as the system scales with more complex interactions.\nGaps in existing research : While the central orchestrator approach works well, it may not suit all use cases. For more process-oriented approaches, you might want to explore other orchestration models, such as Swarm (for structured hand-offs), SelectorGroupChat (for LLM-based speaker selection), SocietyOfMind (where agents collaborate to produce a single output), and RoundRobinGroupChat (based on a Round Robin approach for task allocation). These alternatives offer unique strengths depending on your project's needs. Human-in-loop approches are also supported in AutoGen which can be helpful if human intervention is needed at intermediate steps.\n\nAutomating Data Visualization using LangGraph -\n\nLangGraph\n\n\n\nVersion Used - 0.3.16\nFramework description -\nLangGraph simplifies the creation of stateful, multi-actor applications using LLMs. Building on LangChain, it adds the ability to create and manage cyclical graphs, which are crucial for developing advanced agent runtimes. The core features of LangGraph include its graph structure, state management, and coordination capabilities.\n\n\nKey Features :\n\nModular Architecture\nFramework customizability\nUnified Interface\nExtensive Documentation and Examples\n\nDescription of class implementated(create_pandas_dataframe_agent)\nLangChain's Pandas DataFrame Agent allows language models (LLMs) to interact with and analyze data in Pandas DataFrames using natural language queries, making complex data tasks like filtering, aggregation, and visualization more accessible for data scientists and analysts. It streamlines workflows by combining the power of LLMs with the flexibility of Pandas. However, since the agent uses a Python REPL to execute code, it introduces potential security risks, such as executing malicious code, if not carefully managed. To mitigate these risks, users must opt-in to this feature by setting a specific parameter to True, ensuring they understand the security implications and have safeguards in place.\n\nInstallation and Usage Instructions :\n\nIt is recommended to create a virtual environment on your local machine before installing any packages.\n\nPlease refer to \ninstallation guide\n for getting started.\n\ncreate_pandas_dataframe_agent is an experimental function. Use it at your own risk. Enabling parameters like allow_dangerous_code can pose security risks, so proceed with caution and use them at your discretion.\n\nProblem Statement Addressed - The need for automating data visualization processes, enabling efficient and dynamic creation of visual representations from data.\n\nimport httpx\nimport pandas as pd\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain.agents import StructuredChatAgent, AgentExecutor, Tool\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n\nfrom langgraph.graph import StateGraph\n\n# Create an HTTP client with SSL verification disabled\nclient = httpx.Client(verify=False)\n\n# Initialize the AzureChatOpenAI language model\nllm = AzureChatOpenAI(\n    model='gpt-4o',\n    azure_endpoint='https://your.endpoint.azure.com/',\n    azure_deployment='your_deployment',\n    api_version='2024-08-01-preview',\n    api_key='YOUR_API_KEY' \n)\n\n# Function to create an agent with specified tools\ndef create_agent(csv_path, llm):\n    # Initialize the DuckDuckGo search tool\n    search = DuckDuckGoSearchRun(max_results=3, verify=False)\n    \n    # Create a pandas dataframe agent\n    pd_agent = create_pandas_dataframe_agent(\n        llm,\n        pd.read_csv(csv_path),\n        verbose=True,\n        allow_dangerous_code=True,\n        agent_executor_kwargs={\"handle_parsing_errors\": True},\n    )\n    \n    # Define the tools available to the agent\n    tools = [\n        Tool(\n            name=\"Search\",\n            func=search.run,\n            description=\"\"\"\n    Useful for when you need to find information related to Exploratory Data Analysis(EDA) on web.\n    Input should be a search query.\n    \"\"\"\n        ),\n        Tool(\n            name=\"CSV_Agent\",\n            func=pd_agent.run,\n            description=\"\"\"\n    Useful when you need to carry out EDA with insights from the dataset provided from.\n    Input should be a search query.\n    \"\"\"\n        ),\n    ]\n\n    # Define the prefix, format instructions, and suffix for the agent's responses\n    PREFIX = \"\"\"[INST]Respond to the human as helpfully and accurately as possible. You have access to the following tools:\"\"\"\n    FORMAT_INSTRUCTIONS = \"\"\"Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n\n    Valid \"action\" values: \"Final Answer\" or {tool_names}\n    while passing query to the tool make sure not to pass multiple queries.\n    Provide only ONE action per $JSON_BLOB, as shown:\n\n    ```\n    {{{{\n    \"action\": $TOOL_NAME,\n    \"action_input\": $INPUT\n    }}}}\n    ```\n\n    Follow this format:\n\n    Question: input question to answer\n    Thought: consider previous and subsequent steps\n    Action:\n    ```\n    $JSON_BLOB\n    ```\n    Observation: action result\n    ... (repeat Thought/Action/Observation N times)\n    Thought: I know what to respond\n    Action:\n    ```\n    {{{{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"Final response to human\"\n    }}}}\n    ```[/INST]\"\"\"\n    SUFFIX = \"\"\"Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\n    Thought:[INST]\"\"\"\n\n    HUMAN_MESSAGE_TEMPLATE = \"{input}\\n\\n{agent_scratchpad}\"\n\n    # Create the structured chat agent\n    agent = StructuredChatAgent.from_llm_and_tools(\n        llm,\n        tools,\n        prefix=PREFIX,\n        suffix=SUFFIX,\n        human_message_template=HUMAN_MESSAGE_TEMPLATE,\n        format_instructions=FORMAT_INSTRUCTIONS,\n    )\n    \n    # Create the agent executor\n    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n    return agent_executor\n\n# Path to the CSV file\ncsv_path = 'path_to_your_CSV_dataset'\n\n# Define the state structure\nclass State(TypedDict):\n    messages: List\n\n# Initialize the state graph\ngraph_builder = StateGraph(State)\n\n# Define the chatbot function\ndef chatbot(state: State):\n    chatbot = create_agent(csv_path, llm)\n    return {\"messages\": [chatbot.invoke(state[\"messages\"])]}\n\n# Add the chatbot node to the graph\ngraph_builder.add_node(\"chatbot\", chatbot)\n\n# Set the entry and finish points for the graph\ngraph_builder.set_entry_point(\"chatbot\")\ngraph_builder.set_finish_point(\"chatbot\")\n\n# Compile the graph\ngraph = graph_builder.compile()\n\n# Main loop to interact with the user\nwhile True:\n    user_input = input(\"User: \")\n    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n        print(\"Goodbye!\")\n        break\n    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n        for value in event.values():\n            print(\"Message structure:\", value[\"messages\"][-1])\n            print(\"Assistant:\", value[\"messages\"][-1])\n        print('-' * 50)\n\nCode Explanation -\nThe provided code sets up a chatbot system that utilizes the langchain library to interact with users in an intelligent, dynamic way. The chatbot uses multiple tools, such as a web search tool (DuckDuckGo) and a pandas dataframe agent, to respond accurately to user queries. The system operates based on a state graph that manages the flow of conversation, helping to keep track of interactions and guide the user through a series of actions.\n\nLanguage Model Initialization:\nThe AzureChatOpenAI language model is initialized with specific parameters. This model is responsible for processing the natural language inputs from the user and generating appropriate responses. By using this model, the chatbot can handle complex language tasks and provide insightful answers.\n\nAgent Creation Function:\nThe code defines a function to create a chatbot agent that is equipped with various tools for enhanced functionality;\n\nDuckDuckGoSearchRun(This tool allows the agent to perform web searches using DuckDuckGo, which is useful for retrieving up-to-date information or answering questions that require browsing the web.)\n\ncreate_pandas_dataframe_agent(This tool enables the agent to interact with data stored in pandas dataframes, making it capable of analyzing and manipulating structured data.)\n\nTool(This is a general definition for the tools that are available to the agent, providing flexibility in adding different types of functionalities.)\n\nStructuredChatAgent(This creates an agent with the specified tools and sets response format instructions, ensuring that the agent responds in a structured and predictable manner.)\n\nAgentExecutor(This is responsible for executing the agent's tasks using the defined tools and ensuring the proper sequence of operations.)\n\nCSV Path:\nThe path to the CSV file used by the pandas dataframe agent is specified. This file provides structured data that the agent can reference, making it capable of performing tasks like data analysis or answering questions based on the data.\n\nState Definition:\nThe state structure is defined, outlining the different states or stages in the conversation. This structure helps track where the conversation is and guides the flow of interaction based on user input.\n\nState Graph Initialization:\nThe state graph is initialized with the defined state structure. The state graph is crucial because it manages the transitions between different states, ensuring that the chatbot can handle multi-turn conversations effectively by keeping track of the context.\n\nChatbot Function:\nA function is defined to create and invoke the chatbot with the current state. This function integrates all the components—language model, tools, and state graph—into a cohesive system that can respond to user inputs.\n\nGraph Configuration:\nThe state graph is further configured by adding a chatbot node, setting the entry point (where the conversation starts), and specifying the finish point (where the conversation ends). This configuration ensures that the chatbot knows how to begin and end interactions with the user.\n\nMain Loop:\nThe main loop continuously interacts with the user. It accepts user input, processes it through the state graph to determine the appropriate response, and then outputs the assistant’s reply. The loop keeps running until the user types a command like \"quit\", \"exit\", or \"q\", at which point the chatbot ends the session.\n\nSample Output -\n\n\nThis simple agentic implementation helps visualizing any dataset you have. It also helps find outliers and assists in selecting the most relevant features for your machine learning model. Alternatively, you can create a \ngraph for tools\n and enable the agent to interact with the tools as needed based on the specific requirements of the task.\n\nLimitations/Drawbacks: Managing the state of the graph is challenging, especially since the state function changes based on the use case. If proper state management is not implemented, errors may arise later in the process, leading to instability.\nGaps in Existing Research: This approach is highly use case-specific, which limits its broader applicability. To improve flexibility, some approaches incorporate a tool node, allowing the agent to select tools based on specific requirements. Additionally, adding graphs under nodes for a more use case-centric design could provide greater adaptability and scalability.\n\nCode Review and Technology stack analysis using CrewAI -\n\nCrewAI\n\n\n\nVersion Used - 0.102.0\nFramework description -\nCrewAI is a fast, lightweight Python framework built from the ground up, independent of LangChain or other agent frameworks, offering developers both simplicity and fine-grained control. It is designed for creating autonomous AI agents that can be customized for any use case. CrewAI provides two key features: Crews, which allow developers to create AI teams where each agent has specific roles, tools, and goals to collaborate effectively, and Flows, which offer event-driven control, enabling precise task orchestration with single LLM calls, while also supporting Crews natively. This combination makes CrewAI a flexible solution for building complex, collaborative AI systems.\n\n\nKey Features :\n\nWorkflow management\nFlexible tools\nTask Management\nIntelligent collaboration\n\nDescription of class implemented(sequential process)\nThe sequential process ensures tasks are completed in a specific, linear order, making it ideal for projects that require clear, step-by-step execution. Key features include a linear task flow that ensures tasks follow a predetermined sequence, simplicity for projects with straightforward tasks, and easy monitoring for tracking task completion. To implement the sequential process, users can assemble their AI crew and define the tasks in the order they need to be executed, ensuring an efficient and organized approach to project completion.\n\nInstallation and Usage Instructions :\n\nIt is recommended to create a virtual environment on your local machine before installing any packages.\n\nPlease refer to \ninstallation guide\n for getting started. Things get a bit complicated when it comes to CrewAI. This is due to the folder structure that the library follows. Strictly refer to the installation guide provided above. Before making any further changes try executing \nsample code\n and make sure the flow works end-too-end. In case you get any errors while running the crew please refer to the \ndiscussions\n. Make sure you have the following directory structure :\n\nProblem Statement Addressed - The need for automating code review and technology stack analysis, enabling efficient evaluation and assessment of code quality and technology choices.\n\nCodeReviewCrew/\n├── .env\n├── .gitignore\n├── pyproject.toml\n├── README.md\n├── uv.lock\n├── knowledge/\n│ └── user_preference.txt\n├── output/\n├── src/\n│ └── CodeReviewCrew/\n│ ├── pycache/\n│ ├── config/\n│ │ ├── agents.yaml\n│ │ └── agents.yaml\n│ ├── tools/\n│ │ ├── init.py\n│ │ └── custom_tool.py\n│ ├── main.py\n│ ├── init.py\n│ ├── crew.py\n├── tests/\n└── ...\n\nOnce you are have the crew running and directory in place make following changes:\n\nChange your .env file to -\n\nMODEL=gpt-4o\nAZURE_API_KEY=your_api_key\nAZURE_API_BASE=\nhttps://example.baseURL.azure.com/\n\nAZURE_API_VERSION=2024-05-01-preview\nAZURE_SDK_TRACING_IMPLEMENTATION=opentelemetry\n\nEnsure that you include the exact same parameters in your environment file (do not modify the names or remove the uppercase letters). For tracing, in some versions, if \"opentelemetry\" doesn't work, try replacing it with \"opentelemetry\".\n\nAdd some context to the user preference file -\nUser name is XXXX.\nUser is an AI Engineer.\nUser is interested in AI Agents.\nUser creates documents based on the code provided for more code readability.\nSetting up the agent configs -\n\nAdd this to agents.yaml\n\ncode_interpretor:\n  role: >\n    {topic} Senior Python coder\n  goal: >\n    Create high level description of code based on {topic} provided.\n  backstory: >\n    You are an experienced code reviewer with strong code readability and analyzing skills.\n  llm: azure/gpt-4o\n \ndocumentation_expert:\n  role: >\n    {topic} Documentation Expert\n  goal: >\n    Create document based on information provided on {topic} to create a detailed report. \n  backstory: >\n    You are an experienced documentation expert with strong writing and analytical skills. You are capable of creating detailed reports based on the information provided.\n  llm: azure/gpt-4o\n \ntechnology_expert:\n  role: >\n    {topic} Technology Expert\n  goal: >\n    Understand the technology stack used in {topic}.\n    Gather information from web and provide detailed report on the technology stack.\n  backstory: >\n    You are an experienced technology expert with strong knowledge of the technology stack used in {topic}. You are capable of providing detailed reports on the technology stack.\n  llm: azure/gpt-4o\n\n\nAdd this to tasks.yaml\n\ncode_interpretor_task:\n  description: >\n    Review the code you got and create a high-level description of the code, integrating it into a full section for a report.\n    Ensure the report is detailed, includes all relevant code snippets, and provides insights into the code structure and functionality.\n  expected_output: >\n    A fully fledged report on explainability of the code, including code snippets, code structure, and functionality.\n  agent: code_interpretor\n \ndocumentation_expert_task:\n  description: >\n    Review the context you got and document the information provided to create a detailed report.\n    Ensure the report is detailed, includes all relevant information, and provides insights into the {topic}.\n  expected_output: >\n    A fully fledged report with detailed information, analysis, and insights into the {topic}.\n  agent: documentation_expert\n \n \ntechnology_expert_task:\n  description: >\n    Review the context you got and provide a detailed report on the technology stack used in {topic}.\n    Do thorough research on the web to gather information on the technology stack.\n    Ensure the report is detailed, includes all relevant information, and provides insights into the technology stack.\n  expected_output: >\n    A fully fledged report with detailed information, analysis, and insights into the technology stack used in {topic}.\n  agent: technology_expert\n\nAdding a custom tool -\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom crewai.tools import BaseTool\n\n\nclass MyCustomDuckDuckGoTool(BaseTool):\n    name: str = \"DuckDuckGo Search Tool\"\n    description: str = \"Search the web for a given query.\"\n\n    def _run(self, query: str) -> str:\n        # Ensure the DuckDuckGoSearchRun is invoked properly.\n        duckduckgo_tool = DuckDuckGoSearchRun()\n        response = duckduckgo_tool.invoke(query)\n        return response\n\n    def _get_tool():\n        # Create an instance of the tool when needed\n        return MyCustomDuckDuckGoTool()\n\nChange the main.py and crew.py\n# Replace main.py with the following code\n\nimport sys\nimport warnings\n\nfrom datetime import datetime\n\nfrom CodeReviewCrew.crew import CodeReviewCrew\n\nwarnings.filterwarnings(\"ignore\", category=SyntaxWarning, module=\"pysbd\")\n\n# This main file is intended to be a way for you to run your\n# crew locally, so refrain from adding unnecessary logic into this file.\n# Replace with inputs you want to test with, it will automatically\n# interpolate any tasks and agents information\n\ndef run():\n    \"\"\"\n    Run the crew.\n    \"\"\"\n    inputs = {\n        'topic': '''\n#python code\nfrom swarm import Agent\n\n\ndef process_refund(item_id, reason=\"NOT SPECIFIED\"):\n    \"\"\"Refund an item. Refund an item. Make sure you have the item_id of the form item_... Ask for user confirmation before processing the refund.\"\"\"\n    print(f\"[mock] Refunding item {item_id} because {reason}...\")\n    return \"Success!\"\n\n\ndef apply_discount():\n    \"\"\"Apply a discount to the user's cart.\"\"\"\n    print(\"[mock] Applying discount...\")\n    return \"Applied discount of 11%\"\n\n\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"Determine which agent is best suited to handle the user's request, and transfer the conversation to that agent.\",\n)\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"Be super enthusiastic about selling bees.\",\n)\nrefunds_agent = Agent(\n    name=\"Refunds Agent\",\n    instructions=\"Help the user with a refund. If the reason is that it was too expensive, offer the user a refund code. If they insist, then process the refund.\",\n    functions=[process_refund, apply_discount],\n)\n\n\ndef transfer_back_to_triage():\n    \"\"\"Call this function if a user is asking about a topic that is not handled by the current agent.\"\"\"\n    return triage_agent\n\n\ndef transfer_to_sales():\n    return sales_agent\n\n\ndef transfer_to_refunds():\n    return refunds_agent\n\n\ntriage_agent.functions = [transfer_to_sales, transfer_to_refunds]\nsales_agent.functions.append(transfer_back_to_triage)\nrefunds_agent.functions.append(transfer_back_to_triage)\n''',\n        'current_year': str(datetime.now().year)\n    }\n    \n    try:\n        CodeReviewCrew().crew().kickoff(inputs=inputs)\n    except Exception as e:\n        raise Exception(f\"An error occurred while running the crew: {e}\")\n\n\ndef train():\n    \"\"\"\n    Train the crew for a given number of iterations.\n    \"\"\"\n    inputs = {\n        \"topic\": \"AI LLMs\"\n    }\n    try:\n        CodeReviewCrew().crew().train(n_iterations=int(sys.argv[1]), filename=sys.argv[2], inputs=inputs)\n\n    except Exception as e:\n        raise Exception(f\"An error occurred while training the crew: {e}\")\n\ndef replay():\n    \"\"\"\n    Replay the crew execution from a specific task.\n    \"\"\"\n    try:\n        CodeReviewCrew().crew().replay(task_id=sys.argv[1])\n\n    except Exception as e:\n        raise Exception(f\"An error occurred while replaying the crew: {e}\")\n\ndef test():\n    \"\"\"\n    Test the crew execution and returns the results.\n    \"\"\"\n    inputs = {\n        \"topic\": \"AI LLMs\"\n    }\n    try:\n        CodeReviewCrew().crew().test(n_iterations=int(sys.argv[1]), openai_model_name=sys.argv[2], inputs=inputs)\n\n    except Exception as e:\n        raise Exception(f\"An error occurred while testing the crew: {e}\")\n\n# Replace crew.py with the following code\nfrom crewai import Agent, Crew, Process, Task\nfrom crewai.project import CrewBase, agent, crew, task\n# from crewai_tools import (\n#     DirectoryReadTool,\n#     FileReadTool,\n#     SerperDevTool,\n#     WebsiteSearchTool\n# )\nfrom CodeReviewCrew.tools.custom_tool import MyCustomDuckDuckGoTool\n\n\nDuck_search = MyCustomDuckDuckGoTool()\n# If you want to run a snippet of code before or after the crew starts, \n# you can use the @before_kickoff and @after_kickoff decorators\n# https://docs.crewai.com/concepts/crews#example-crew-class-with-decorators\n\n@CrewBase\nclass CodeReviewCrew():\n\t\"\"\"CodeReviewCrew crew\"\"\"\n\n\t# Learn more about YAML configuration files here:\n\t# Agents: https://docs.crewai.com/concepts/agents#yaml-configuration-recommended\n\t# Tasks: https://docs.crewai.com/concepts/tasks#yaml-configuration-recommended\n\tagents_config = 'config/agents.yaml'\n\ttasks_config = 'config/tasks.yaml'\n\t\n\n\t# If you would like to add tools to your agents, you can learn more about it here:\n\t# https://docs.crewai.com/concepts/agents#agent-tools\n\t@agent\n\tdef code_interpretor(self) -> Agent:\n\t\treturn Agent(\n\t\t\tconfig=self.agents_config['code_interpretor'],\n\t\t\tverbose=True\n\t\t)\n\n\t@agent\n\tdef documentation_expert(self) -> Agent:\n\t\treturn Agent(\n\t\t\tconfig=self.agents_config['documentation_expert'],\n\t\t\tverbose=True\n\t\t)\n\t\n\t@agent\n\tdef technology_expert(self) -> Agent:\n\t\treturn Agent(\n\t\t\tconfig=self.agents_config['technology_expert'],\n\t\t\tverbose=True,\n\t\t\ttools=[Duck_search]\n\t\t)\n\n\t# To learn more about structured task outputs, \n\t# task dependencies, and task callbacks, check out the documentation:\n\t# https://docs.crewai.com/concepts/tasks#overview-of-a-task\n\t@task\n\tdef code_interpretor_task(self) -> Task:\n\t\treturn Task(\n\t\t\tconfig=self.tasks_config['code_interpretor_task'],\n\t\t\toutput_file='output/Code_report.md'\n\t\t)\n\n\t@task\n\tdef documentation_expert_task(self) -> Task:\n\t\treturn Task(\n\t\t\tconfig=self.tasks_config['documentation_expert_task'],\n\t\t\toutput_file='output/code_document_report.md'\n\t\t)\n\t\n\t@task\n\tdef technology_expert_task(self) -> Task:\n\t\treturn Task(\n\t\t\tconfig=self.tasks_config['technology_expert_task'],\n\t\t\toutput_file='output/technology_stack_report.md'\n\t\t)\n\n\t@crew\n\tdef crew(self) -> Crew:\n\t\t\"\"\"Creates the CodeReviewCrew crew\"\"\"\n\t\t# To learn how to add knowledge sources to your crew, check out the documentation:\n\t\t# https://docs.crewai.com/concepts/knowledge#what-is-knowledge\n\n\t\treturn Crew(\n\t\t\tagents=self.agents, # Automatically created by the @agent decorator\n\t\t\ttasks=self.tasks, # Automatically created by the @task decorator\n\t\t\tprocess=Process.sequential,\n\t\t\tverbose=True,\n\t\t\t# process=Process.hierarchical, # In case you wanna use that instead https://docs.crewai.com/how-to/Hierarchical/\n\t\t)\n\nRunning the crew -\n\nTo run the completed crew navigate to the root of your project\n\ncd .\\CodeReviewCrew\\\n\nOnce you navigate to the root of your project run the crew\n\ncrewai run\nResults -\nCheck the output folder for results. LLM powered reports will be generated by each agent in .md format.\n\nSample Output -\n\n\n\n\n\n\nThis implementation automates code reviews and technology stack analysis by taking advantage of CrewAI’s advanced features, including built-in document-saving capabilities. This implementation helps in the process of identifying code quality issues, optimizing performance, and assessing compatibility across various technologies within a project.\n\nLimitations/Drawbacks: There are recurring telemetry issues, despite attempts to resolve them using OpenTelemetry or other fixes. While these issues don’t impact the output of the crew, they negatively affect the clarity of the terminal output, which can be problematic for troubleshooting and monitoring.\nGaps in Existing Research: This implementation includes the sequential process approach to address the problem statement; however, a hierarchical process could also be a valuable alternative, depending on the complexity of the tasks. Additionally, there is potential to configure custom flows tailored to specific use cases, and agents could be trained over time to improve their performance and adaptability.\n\nAgentic RAG using HuggingFace SmolAgents -\n\nSmolAgents\n\n\n\nVersion Used - 1.11.0\nFramework description -\nFor certain low-level agentic tasks, such as chains or routers, writing all the code yourself can give you full control and a better understanding of your system. However, as the complexity increases—like when you want an LLM to call a function (tool calling) or run a while loop (multi-step agent)—you’ll need some abstractions. SmolAgents is a lightweight library developed by Hugging Face that makes it easier to create and run powerful agents. Its minimalist design sets it apart, with the entire agent logic contained in just about 1,000 lines of code. This simple yet efficient approach ensures that users can quickly get started while still benefiting from the full functionality needed for building robust agents.\n\n\nKey Features :\n\nSimplicity\nHub Integrations\nCode Agent Support\nMulti LLM support\n\nDescription of class implemented(ToolCallingAgent)\nThe ToolCallingAgent is a special agent in the smolagents library that helps solve tasks by using predefined tools or managed agents. It works by creating and using structured JSON-like tool calls, which help it communicate clearly with external systems like APIs or databases. Key features of the ToolCallingAgent include dynamically executing tools, managing intermediate results, keeping track of its actions through logging, and offering transparency in decision-making. It also uses a fallback mechanism to provide a final answer if it can't solve the task within a certain number of steps. Essentially, it makes interacting with tools more organized and efficient.\n\nInstallation and Usage Instructions :\n\nIt is recommended to create a virtual environment on your local machine before installing any packages.\n\nPlease refer to \ninstallation guide\n for getting started.\n\nProblem Statement Addressed - The need for automating Retrieval-Augmented Generation (RAG) processes, enabling intelligent and context-aware generation of responses.\n\nfrom smolagents import AzureOpenAIServerModel\nfrom smolagents import ToolCallingAgent\nfrom smolagents import Tool\n\nfrom langchain_core.vectorstores import VectorStore\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n\nfrom transformers import AutoTokenizer\n\nfrom dotenv import load_dotenv\nimport fitz\nfrom tqdm import tqdm\n\n\n# Load environment variables\nload_dotenv()\n\n\n# Initialize the Azure OpenAI model\nmodel = AzureOpenAIServerModel(\n    model_id=\"gpt-4o\",\n    azure_endpoint=\"https://example.endpoint.azure.com/\",\n    api_key=\"your_api_key\",\n    api_version=\"2024-08-01\"\n)\n\n# Function to extract text from a PDF file\ndef extract_text_from_pdf(pdf_path):\n    doc = fitz.open(pdf_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    return text\n\n# Path to your PDF file\npdf_path = \"path_to_your_pdf\"\n\n# Extract text from the PDF\npdf_text = extract_text_from_pdf(pdf_path)\n\n# Create a Document object from the extracted text\npdf_document = Document(page_content=pdf_text, metadata={\"source\": \"pdf_file\"})\n\n# Use the extracted PDF document as the source document\nsource_docs = [pdf_document]\n\n# Initialize the text splitter\ntext_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n    chunk_size=200,\n    chunk_overlap=20,\n    add_start_index=True,\n    strip_whitespace=True,\n    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n)\n\n# Split documents and keep only unique ones\nprint(\"Splitting documents...\")\ndocs_processed = []\nunique_texts = {}\nfor doc in tqdm(source_docs):\n    new_docs = text_splitter.split_documents([doc])\n    for new_doc in new_docs:\n        if new_doc.page_content not in unique_texts:\n            unique_texts[new_doc.page_content] = True\n            docs_processed.append(new_doc)\n\n# Embed the processed documents\nprint(\"Embedding documents...\")\nembedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\nhuggingface_doc_vector_db = FAISS.from_documents(\n    documents=docs_processed,\n    embedding=embedding_model,\n    distance_strategy=DistanceStrategy.COSINE,\n)\n\n# Define the RetrieverTool class\nclass RetrieverTool(Tool):\n    name = \"retriever\"\n    description = \"Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n    inputs = {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n        }\n    }\n    output_type = \"string\"\n\n    def __init__(self, vectordb: VectorStore, **kwargs):\n        super().__init__(**kwargs)\n        self.vectordb = vectordb\n\n    def forward(self, query: str) -> str:\n        assert isinstance(query, str), \"Your search query must be a string\"\n\n        docs = self.vectordb.similarity_search(\n            query,\n            k=7,\n        )\n\n        return \"\\nRetrieved documents:\\n\" + \"\".join(\n            [f\"===== Document {str(i)} =====\\n\" + doc.page_content for i, doc in enumerate(docs)]\n        )\n\n# Initialize the retriever tool and agent\nhuggingface_doc_retriever_tool = RetrieverTool(huggingface_doc_vector_db)\nretriever_agent = ToolCallingAgent(\n    tools=[huggingface_doc_retriever_tool],\n    model=model,\n)\n\n# Function to run the agentic RAG process\ndef run_agentic_rag(question: str) -> str:\n    question_prompt = f\"\"\"Using the information contained in your knowledge base, which you can access with the 'retriever' tool,\n    give a comprehensive answer to the question below.\n    Respond only to the question asked, response should be concise and relevant to the question.\n    If you cannot find information, do not give up and try calling your retriever again with different arguments!\n    Make sure to have covered the question completely by calling the retriever tool several times with semantically different queries.\n\n    Question:\n    {question}\"\"\"\n\n    return retriever_agent.run(question_prompt)\n\n\nquestion = \"Explain core capabilities of autogen\"\n\nanswer = run_agentic_rag(question)\nprint(f\"Question: {question}\")\nprint(f\"Answer: {answer}\")\n\nCode Explanation -\nThis script is designed to extract text from a PDF, break it down into smaller pieces, embed these pieces into a vector database, and then use a retriever tool to find the most relevant documents based on a query. The code relies on several libraries and tools such as smolagents, langchain, transformers, and fitz to process the PDF and perform the retrieval tasks.\n\nImports and Environment Setup:\nThe script begins by importing the necessary libraries and modules required for the entire process. It also loads environment variables using the dotenv library, ensuring that any sensitive data, like API keys or configuration details, are securely managed and accessed.\n\nModel Initialization:\nThe script initializes the Azure OpenAI model with specific parameters. This model will be responsible for processing the query and interacting with the data, allowing the agent to generate answers based on the retrieved documents.\n\nPDF Text Extraction:\nA function is defined to extract text from a given PDF file using the fitz library, which is a part of the PyMuPDF package. The function takes in a PDF file, processes it, and converts the extracted text into a Document object, making it easier to handle and manipulate.\n\nText Splitting:\nOnce the text is extracted, a text splitter is initialized using a tokenizer from HuggingFace. This step breaks the long text into smaller, manageable chunks, ensuring that each chunk is unique. This is crucial because smaller chunks make it easier to work with the data and improve the quality of retrieval results.\n\nDocument Embedding:\nThe text chunks are then embedded into a vector database using the FAISS library, which is used for efficient similarity search. A HuggingFace embedding model is applied to transform the text chunks into vector representations. These embeddings allow the system to understand the semantic meaning of the text and perform more accurate retrieval when a query is made.\n\nRetriever Tool Definition:\nThe code defines a RetrieverTool class that is responsible for retrieving documents from the vector database. It does this by comparing the semantic similarity between the query and the embedded text chunks. This ensures that the documents retrieved are the most relevant to the given query.\n\nAgent Initialization:\nA ToolCallingAgent is initialized with the retriever tool and the Azure OpenAI model. This agent will manage the interaction between the user’s query and the retrieval system, ensuring that the query is processed and the correct documents are fetched from the database.\n\nQuestion Answering Function:\nThe function run_agentic_rag is defined to handle the process of enhancing the query and using the retriever agent to find the most relevant documents. The function takes the question, processes it, and retrieves the documents that are most related to it, ultimately returning the best possible answer based on the information stored in the vector database.\n\nExecution:\nFinally, a sample question is defined and passed into the run_agentic_rag function to retrieve an answer. The script prints out the question along with the answer retrieved from the vector database, providing an example of how the system works in practice.\n\nSample Output -\n\n\n\n\n\n\n\n\n\n\nThe script automates the process of querying a PDF document for information using AI. It extracts text, processes it into manageable chunks, converts the text into vector embeddings, and then uses a semantic search mechanism to find relevant answers to user queries. Agentic approach helps gather information multiple times for different relevant input prompt allowing more accurate answer.\n\nLimitations/Drawbacks: The MultiStepAgent class encounters frequent issues related to Git, as the framework is still experimental and subject to frequent changes in functions, methods, and classes. This instability can create challenges when trying to maintain consistency.\nGaps in Existing Research: This approach lacks central orchestration or process integrations, primarily due to limited community support and insufficient examples in the documentation. While there are some multi-agent handling capabilities, they remain underexplored and are still in development. The CodeAgent class, however, is the most stable and reliable agent for code-related use cases.\n\nAI powered Research and report generation using LlamaIndex -\n\nLlamaIndex\n\n\n\nVersion Used - 0.12.22\nFramework description -\nLlamaIndex is a framework for creating LLM-powered agents and applications that enhance the ability of language models (LLMs) to interact with various types of data. Agents are assistants powered by LLMs that use tools to perform tasks such as research or data extraction. They can range from simple question-answering to more complex tasks involving decision-making and actions. Context augmentation is the process of making private or specific data accessible to LLMs so they can use it to solve problems. LlamaIndex helps with context-augmentation by allowing users to ingest, index, and process data from various sources like APIs, databases, PDFs, and more. LlamaIndex provides a comprehensive framework for building advanced LLM applications, combining agents, workflows, and context-augmentation to interact with diverse data sources. It supports a range of use cases, from simple question-answering to complex multi-modal applications and autonomous agents.\n\n\nKey Features :\n\nDistributed Service Oriented Architecture\nEase of deployment\nScalability and resource management\nCustomized orchestration flows\n\nDescription of class implemented(AgentWorkflow)\nThe AgentWorkflow is a tool that helps manage and run a system with one or more agents. It acts as an orchestrator, coordinating how agents interact and complete tasks. It ensures that tasks are completed in an orderly and efficient manner. It is especially useful for handling workflows that involve multiple steps or agents working together.\n\nInstallation and Usage Instructions :\n\nIt is recommended to create a virtual environment on your local machine before installing any packages.\n\nPlease refer to \ninstallation guide\n for getting started.\n\nProblem Statement Addressed: The need for automating AI-powered research and report generation, enabling efficient gathering, processing, and summarization of information to produce detailed reports.\n\nimport asyncio\nfrom llama_index.llms.azure_openai import AzureOpenAI\nfrom llama_index.core.agent.workflow import (\n    AgentOutput,\n    ToolCall,\n    ToolCallResult,\n)\nfrom llama_index.core.workflow import Context\nfrom llama_index.core.agent.workflow import FunctionAgent\nfrom llama_index.core.agent.workflow import AgentWorkflow\n\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\n\n# Azure OpenAI API credentials\naoai_api_key = \"ede5ff1d10f04521b9db46a6e15c19ff\"\naoai_endpoint = \"https://azureaieu6682939841.cognitiveservices.azure.com/\"\naoai_api_version = \"2024-08-01-preview\"\n\n# Initialize the Azure OpenAI LLM\nllm = AzureOpenAI(\n    engine=\"gpt-4o-Auto-Agent\",\n    model=\"gpt-4o\",\n    api_key=aoai_api_key,\n    azure_endpoint=aoai_endpoint,\n    api_version=aoai_api_version,\n)\n\n# Define the search_web tool\nasync def search_web(query: str) -> str:\n    \"\"\"Useful for using the web to answer questions.\"\"\"\n    web_search = DuckDuckGoSearchRun()\n    return str(web_search.run(query))\n\n# Define the record_notes tool\nasync def record_notes(ctx: Context, notes: str, notes_title: str) -> str:\n    \"\"\"Useful for recording notes on a given topic. Your input should be notes with a title to save the notes under.\"\"\"\n    current_state = await ctx.get(\"state\")\n    if \"research_notes\" not in current_state:\n        current_state[\"research_notes\"] = {}\n    current_state[\"research_notes\"][notes_title] = notes\n    await ctx.set(\"state\", current_state)\n    return \"Notes recorded.\"\n\n# Define the write_report tool\nasync def write_report(ctx: Context, report_content: str) -> str:\n    \"\"\"Useful for writing a report on a given topic. Your input should be a markdown formatted report.\"\"\"\n    current_state = await ctx.get(\"state\")\n    current_state[\"report_content\"] = report_content\n    await ctx.set(\"state\", current_state)\n    return \"Report written.\"\n\n# Define the review_report tool\nasync def review_report(ctx: Context, review: str) -> str:\n    \"\"\"Useful for reviewing a report and providing feedback. Your input should be a review of the report.\"\"\"\n    current_state = await ctx.get(\"state\")\n    current_state[\"review\"] = review\n    await ctx.set(\"state\", current_state)\n    return \"Report reviewed.\"\n\n# Define the ResearchAgent\nresearch_agent = FunctionAgent(\n    name=\"ResearchAgent\",\n    description=\"Useful for searching the web for information on a given topic and recording notes on the topic.\",\n    system_prompt=(\n        \"You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic. \"\n        \"Once notes are recorded and you are satisfied, you should hand off control to the WriteAgent to write a report on the topic. \"\n        \"You should have at least some notes on a topic before handing off control to the WriteAgent.\"\n    ),\n    llm=llm,\n    tools=[search_web, record_notes],\n    can_handoff_to=[\"WriteAgent\"],\n)\n\n# Define the WriteAgent\nwrite_agent = FunctionAgent(\n    name=\"WriteAgent\",\n    description=\"Useful for writing a report on a given topic.\",\n    system_prompt=(\n        \"You are the WriteAgent that can write a report on a given topic. \"\n        \"Your report should be in a markdown format. The content should be grounded in the research notes. \"\n        \"Once the report is written, you should get feedback at least once from the ReviewAgent.\"\n    ),\n    llm=llm,\n    tools=[write_report],\n    can_handoff_to=[\"ReviewAgent\", \"ResearchAgent\"],\n    handoff_logic=lambda state: \"ReviewAgent\" if state.get(\"report_content\") != \"Not written yet.\" else \"ResearchAgent\"\n)\n\n# Define the ReviewAgent\nreview_agent = FunctionAgent(\n    name=\"ReviewAgent\",\n    description=\"Useful for reviewing a report and providing feedback.\",\n    system_prompt=(\n        \"You are the ReviewAgent that can review the write report and provide feedback. \"\n        \"Your review should either approve the current report or request changes for the WriteAgent to implement. \"\n        \"If you have feedback that requires changes, you should hand off control to the WriteAgent to implement the changes after submitting the review.\"\n    ),\n    llm=llm,\n    tools=[review_report],\n    can_handoff_to=[\"WriteAgent\"],\n)\n\n# Define the agent workflow\nagent_workflow = AgentWorkflow(\n    agents=[research_agent, write_agent, review_agent],\n    root_agent=research_agent.name,\n    initial_state={\n        \"research_notes\": {},\n        \"report_content\": \"Not written yet.\",\n        \"review\": \"Review required.\",\n    },\n)\n\n# Main function to run the agent workflow\nasync def main():\n    handler = agent_workflow.run(\n        user_msg=(\n            \"Write me a report on the Top 5 Agentic Frameworks. \"\n            \"Briefly describe the all the frameworks in detail and give sample code implementations in python \"\n            \"Also give technology stack used in each framework.\"\n        )\n    )\n\n    current_agent = None\n    current_tool_calls = \"\"\n\n    # Process events from the agent workflow\n    async for event in handler.stream_events():\n        if (\n            hasattr(event, \"current_agent_name\")\n            and event.current_agent_name != current_agent\n        ):\n            current_agent = event.current_agent_name\n            print(f\"\\n{'='*50}\")\n            print(f\"🤖 Agent: {current_agent}\")\n            print(f\"{'='*50}\\n\")\n        # if isinstance(event, AgentStream):\n        #     if event.delta:\n        #         print(event.delta, end=\"\", flush=True)\n        # elif isinstance(event, AgentInput):\n        #     print(\"📥 Input:\", event.input)\n        elif isinstance(event, AgentOutput):\n            if event.response.content:\n                print(\"📤 Output:\", event.response.content)\n            if event.tool_calls:\n                tool_names = [call.tool_name for call in event.tool_calls]\n                current_tool_calls += f\"🛠️  Planning to use tools: {tool_names}\\n\"\n                print(\"🛠️  Planning to use tools:\", tool_names)\n        elif isinstance(event, ToolCallResult):\n            tool_result = (\n                f\"🔧 Tool Result ({event.tool_name}):\\n\"\n                f\"  Arguments: {event.tool_kwargs}\\n\"\n                f\"  Output: {event.tool_output}\\n\"\n            )\n            current_tool_calls += tool_result\n            print(tool_result)\n        elif isinstance(event, ToolCall):\n            tool_call = (\n                f\"🔨 Calling Tool: {event.tool_name}\\n\"\n                f\"  With arguments: {event.tool_kwargs}\\n\"\n            )\n            current_tool_calls += tool_call\n            print(tool_call)\n\n    # Print a summary of tool calls\n    print(\"\\nSummary of tool calls:\")\n    print(current_tool_calls)\n\n# Run the main function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nCode Explanation -\nThis code defines an agent-based workflow utilizing the llama_index and langchain_community libraries to create a collaborative environment where three agents work together to generate a report on a given topic. The workflow is designed to have each agent perform specific tasks in a sequential manner, with each agent's output serving as the input for the next one.\n\nAzure OpenAI Initialization:\nThe script begins by initializing the Azure OpenAI LLM (Language Model) using the provided API credentials. This sets up the model that will power the agents, allowing them to generate natural language responses and process queries.\n\nTool Definitions:\nSeveral tools are defined to assist the agents in carrying out their tasks;\nsearch_web(Uses DuckDuckGo to search the web for information related to the topic at hand.)\nrecord_notes(A tool that helps record notes about the topic, which will later be used by the writing agent to create the report.)\nwrite_report(A tool designed to write a report on the given topic using the recorded notes.)\nreview_report(A tool that reviews the generated report and provides feedback for improvement.)\n\nAgent Definitions:\nThe script defines three agents, each with a specific role;\nResearchAgent(This agent is responsible for searching the web for relevant information and recording notes.)\nWriteAgent(The WriteAgent uses the research notes provided by the ResearchAgent to write a detailed report on the topic.)\nReviewAgent(After the report is written, this agent reviews the content and provides feedback or suggests improvements to ensure the quality of the report.)\n\nAgent Workflow:\nThe agent workflow is structured to start with the ResearchAgent, which performs the web search and note-taking. After the research phase, the WriteAgent takes over, utilizing the notes to write the report. Finally, the ReviewAgent reviews the report, ensuring that it meets the desired quality before completion. This collaborative process allows the agents to hand off tasks and work together to create a polished, comprehensive report.\n\nMain Function:\nThe main function runs the agent workflow and processes events as the agents perform their tasks. It prints the outputs and tool calls, allowing you to track the workflow and understand how each agent interacts with the tools and contributes to the final result. The agents' outputs are passed from one to the next in a smooth sequence, making the process of generating the report automated and efficient.\n\nSample Output -\n\n\n\n\n\n\nA simple multi-agentic approach to perform research, write reports, and review them, utilizing a combination of tools such as web search and note recording. The code demonstrates a multi-agent workflow to research a topic, generate a report, and refine it through feedback. The agents work together to handle the process from gathering information to producing a final reviewed report.\n\nLimitations/Drawbacks: The system is heavily dependent on OpenAI, as it internally utilizes the entire OpenAI ecosystem. Additionally, there is a lack of conceptual documentation—while the documentation offers numerous code examples, it doesn't provide a clear understanding of the architectural framework.\nGaps in Existing Research: For this problem statement, I used the workflow concept from LlamaIndex, but there are many other orchestration deployment patterns available. The framework also supports older agent classes (such as OldOpenAIAgent and OldReActAgent), making it easier to migrate between different versions of the framework.\n\nDeployment Considerations -\nWhen implementing these agentic AI frameworks in a production environment, security considerations are crucial to ensure safe and reliable operation. For instance, frameworks like LangGraph that execute Python code via a REPL environment need strict safeguards to prevent the execution of malicious code. It's important to opt-in to certain features and use access controls to limit exposure to security risks. Additionally, frameworks such as SmolAgents and Autogen, which interact with external tools and APIs, must ensure secure communication and handle potential vulnerabilities in data exchanges. Crewai's sequential process must be closely monitored to avoid unauthorized task execution, while LlamaIndex workflows must maintain integrity to prevent malicious agents from hijacking task execution. In production, these frameworks should be deployed with comprehensive logging, access control mechanisms, and continuous security auditing to minimize risks and ensure safe deployment.\n\nReader next steps -\n\nNow that you have an understanding of the prerequisites and requirements, the next step is to get hands-on with the frameworks. Start by setting up your development environment, installing the necessary libraries, and obtaining any required API keys. Once everything is set up, try experimenting with some basic examples to get familiar with how each framework works. Here are some more frameworks that you can consider exploring :\n\nMicrosoft - \nSemantic Kernel Agents\nMicrosoft - \nPrompt Flow\nAzureAI - \nAgent Service\n\nMicrosoft provide industry scalable solutions that you can directly integrate with your existing system or flow. You can also try out enterprise versions for including agentic frameworks into your organization.\n\nResults\n\nAs we reach the end of this research and its findings, I’d like to note that while I initially intended to explore more frameworks, I focused on these key ones—Autogen, LangGraph, Crewai, SmolAgents, and LlamaIndex—to showcase their unique functionalities and applications in agentic AI.\n\nFor Autogen, I implemented the MagenticOneGroupChat system, which is a versatile, multi-agent architecture capable of tackling open-ended tasks across various domains. This system, built on the autogen-core library and upgraded with autogen-agentchat, is orchestrated through the MagenticOneGroupChat, which supports collaborative agent teams for task execution. This provided a highly modular and user-friendly interface.\n\nIn LangGraph, I worked with the create_pandas_dataframe_agent class, which allows language models to seamlessly interact with data in Pandas DataFrames using natural language queries. This feature greatly simplifies complex tasks like filtering, aggregation, and visualization. However, a potential security concern arises from executing Python code in a REPL environment, requiring users to opt-in and manage potential risks carefully.\n\nFor Crewai, I focused on implementing the sequential process class. This class ensures tasks are executed in a specific, linear order, which is perfect for projects needing clear, step-by-step execution. It’s simple yet effective for straightforward tasks, with easy monitoring for tracking progress.\n\nIn SmolAgents, I worked with the ToolCallingAgent, a specialized agent that facilitates task completion by invoking predefined tools or managed agents. The agent uses structured JSON-like tool calls to interact with external systems, such as APIs or databases, offering dynamic tool execution, state management, and transparent decision-making, which makes the entire process more efficient and organized.\n\nLastly, for LlamaIndex, I explored the AgentWorkflow, a tool that orchestrates and manages systems with multiple agents. This class enables smooth coordination between agents, ensuring that tasks are carried out in an orderly, efficient manner, making it ideal for workflows that involve several agents working together in a system.\n\nThrough these implementations, we’ve observed how each framework addresses different aspects of agentic AI, from task automation and data handling to security and workflow management. While each framework has its strengths, the results also highlight areas for improvement, such as scalability and adaptability, which are crucial for real-world applications.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nIntroduction\n\nAbstract\n\nMethodology\n\nSmart Orchestration for Local Code Generation and Execution using AutoGen -\n\nAutomating Data Visualization using LangGraph -\n\nCode Review and Technology stack analysis using CrewAI -\n\nAgentic RAG using HuggingFace SmolAgents -\n\nResults\n\nComments\nCode",
    "awards": [
      "distinguished technical deep-dive"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "ai-assistant-vui-VcG8wcVNZul2",
    "username": "mMax Goltzsche",
    "license": null,
    "title": "AI Assistant VUI",
    "publication_description": "Back to publications\nMar 31, 2025\n●\n147 reads\n●\nApache 2.0\nWinner of\nBest AI Tool Innovation\nat the\nAgentic AI Innovation Challenge 2025\nAI Assistant VUI\nai\nai-assistant\nchatbot\ngo\ngolang\njarvis\nlive-transcription\nspeech\nspeech-recognition\nvad\nvoice\nvoice-assistant\nvoice-chat-bot\nvoice-chatbot\nvoice-control\nvoice-user-interface\nvui\nM\nMax Goltzsche\nLike\nBookmark\nShare\n\nAI Assistant VUI\n\nAn experimental voice user interface (VUI) to interact with an AI assistant.\n\nProject link\n\nIt is as a client CLI that connects to an OpenAI API-compatible server (that is served locally by \nLocalAI\n).\nTo answer a user request it can decide to use tools in form of running configurable, dockerized functions.\n\nFor voice activity detection (VAD) \nsilero-vad\n is built into the client.\nFor chat completion, speech-to-text (STT) and text-to-speech (TTS) capabilities the client leverages the API server.\nIn order to detect whether the AI assistant is addressed, a wake word can be configured.\nThough, wake word support is implemented by matching the STT (whisper) output string against the wake word, requiring all voice communication to be STT processed, at least for now.\n\nMotivation\n\nOne day I was in the kitchen cooking when my \nMopidy\n-based streamer started playing a song that I didn't like.\nI wanted to skip the song or at least turn the volume down but, since I had dirty fingers, I couldn't simply use my phone or computer mouse, unless I'd clean them.\nHaving a freely configurable VUI would be a great addition to a graphical user interface (GUI) in such a situation and for IoT devices in general since the user wouldn't have to touch a screen or mouse to skip the song.\nSimilarly, such a VUI backed by a Large Language Model (LLM) could allow disabled and old people to use computers (more intuitively).\nPotentially there are also other use-cases such as a personal advisor or a translator (also see \nBabelfish\n) for everyone.\nWhile there are already commercial, cloud-based products such as Amazon Alexa available that address some of these problems to some degree, they are not as capable and flexible as the latest LLMs, not freely configurable, not open source but they come with a vendor lock-in, require an internet connection, listen to everything that anyone around them says and send it to 3rd party servers, impacting their users' privacy negatively.\nGiven the latest advancements of AI technology, I was curious whether a VUI could already be implemented based on open source software without those limitations, preferably in Go, and started researching.\n\nFeatures/requirements\nA voice-controlled AI assistant that can interact with the real world using preconfigured tools: e.g. can decide to run a docker container to change the music volume.\nLow latency/near-realtime response to support a fluent, natural conversation.\nVerbally interruptable system in order to appear responsive and not waste the user's time by talking about irrelevant information. When interrupted, the assistant stops talking after it finished the current sentence and for consistency only what it really said ends up within the message history.\nConfigurable wake word support to prevent the AI from responding to every voice communication (e.g. between humans) which is annoying otherwise. Any sentence the user says that does not contain the wake word is ignored by the AI.\nSave energy/API calls and avoid hallucination of the STT system by STT-processing only audio signals that contain voice activity (VAD).\nRelated work\nHistorically dialog systems such as \nCMU Sphinx\n exist since a while already. However, these were quite unflexible since they required the user to say specific preconfigured phrases in order to trigger a response and they weren't as good at recognizing those as modern LLM-based solutions which are also more flexible since they do not require every possible dialog to be preconfigured.\nCommercial cloud/SaaS-based, closed-source voice-controlled assistants such as Amazon's Alexa, Google's Assistant, Apple's Siri. However, these products require an internet connection and send the recorded audio data to 3rd party servers, negatively impacting their users' privacy.\nOpenAI's \nwhisper\n is the state-of-the-art speech recognition (STT) model and \nSaaS API\n.\nCommercial AI SaaS products such as OpenAI's \nChatGPT\n started to support voice interaction recently.\nfaster-whisper\n: An open source reimplementation of OpenAI's whisper that is even faster than the original.\nSilero VAD\n: An open source Voice Activity Detection (VAD) model, allowing to detect voice activity within an audio signal. This is useful to reduce hallucination of the STT/whisper model and also to reduce the computational load and therefore energy consumption since not every audio signal needs to be STT-processed.\nLocalAIVoiceChat\n: A Python-based conversational AI to talk to but without the ability to let the AI call user-defined functions.\ngo-whisper-cpp-server-example\n: An STT-based translator written in Go.\nollama\n: An open source LLM engine that implements an OpenAI-compatible chat completion API that can be run locally.\nLocalAI\n: An open source, OpenAI-compatible LLM API server that integrates ollama and faster-whisper along with other open source AI projects to support chat completion, STT and TTS locally. Therefore LocalAI is very well suited as the server of the AI Assistant VUI.\nSystem requirements\nProcessor: Intel Core i3/AMD Ryzen 3 or better.\nRAM: 4GB minimum, 8GB recommended.\nGraphics card with 8GB RAM (AMD or Nvidia).\nStorage: 1GB free space.\nAudio: Working microphone and speakers/headphones.\nOperating system: Linux (tested on an Ubuntu 24.04 host), adaptable for MacOS and Windows.\nBuild\n\nClone the repo:\n\ngit clone https://github.com/mgoltzsche/ai-assistant-vui.git\ncd ai-assistant-vui\n\nTo build the Linux container image, run the following command within the project's root directory (requires \nDocker\n to be installed):\n\nmake\nRun\nStart the \nLocalAI\n API server (LLM server) by running the following within the project's root directory:\nmake run-localai\n\nBrowse the LocalAI web GUI at \nhttp://127.0.0.1:8080/browse/\n and search and install the models you want to use. When using the \ndefault AI Assistant VUI\n configuration, you need to install whisper-1 (STT), localai-functioncall-qwen2.5-7b-v0.5 (chat) and voice-en-us-amy-low (TTS).\n\nRun the VUI (within another terminal):\n\nmake run-vui INPUT_DEVICE=\"KLIM Talk\" OUTPUT_DEVICE=\"ALC1220 Analog\"\n\nYou will likely have to replace the values of INPUT_DEVICE and OUTPUT_DEVICE with the names or IDs of your audio devices (available devices are listed in the log).\nYou may not be able to use an audio device when another program (e.g. your browser) is already using it.\nIn that case, please close other programs, wait a few seconds and then re-run the VUI.\n\nYou need to mention the configured wake word (defaults to \"Computer\") with each request to the AI assistant.\nFor instance you can ask \"Computer, what's the capital of Germany?\"\n\nImplementation details\n\nThe application is written in Go, leveraging its static typing and concurrency features.\n\nSchematic sequence diagram:\n\n\nLimitations\nThe wake word must be recognized by the whisper model - this could be improved potentially using a specialized wake word model.\nContext size and storage:\nTo keep the context size minimal and speed up inference, tool results of the previous user request are deleted from the chat history with every new user request.\nThe chat history is currently stored in-memory only and is therefore lost when restarting the application.\nThe chat history is infinite / not truncated which therefore exceeds the LLM's context size at some point and then requires an application restart.\nDue to a LocalAI LLM bug function calls are often repeated infinitely - this is detected and prevented after the 2nd call.\nAudio device usage: The container does not work with pulseaudio but ALSA and therefore requires no other application to use the same audio devices it uses.\nOther people can also give the AI commands (e.g. somebody on the street shouting through the window) - voice recognition could protect against that.\nRoadmap\nContext size and storage:\nChat history retention: Detect when the maximum context size would be exceeded and delete old messages in that case, starting with the first user request and assistant response.\nTo support multiple rooms and a client for each as well as remote access, the context/conversation history could be stored on a (local) server.\nAdd Retrieval-Augmented Generation (RAG) support to kind of support an infinite context size: write conversations (and other personal information) into a vector database, query it for every user request to find related information and add it to the message history before sending it to the chat completion endpoint.\nPrevent function call repetition.\nAdd a wake word engine in order to save energy/STT API requests.\nTo improve compatibility, it would be good to make the container connect to pulseaudio.\nAuthentication via voice recognition to make the assistant aware of who is talking and to protect against other people commanding the assistant.\nCredits\n\nWhen I searched the web for an existing implementation of such an application or similar to start with, I found the STT translator \ngo-whisper-cpp-server-example\n from where I copied and adjusted a couple of code snippets.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAI Assistant VUI\n\nMotivation\n\nFeatures/requirements\n\nRelated work\n\nSystem requirements\n\nBuild\n\nRun\n\nImplementation details\n\nLimitations\n\nRoadmap\n\nView all\nComments\n(1)\nMost recent\n\n✨ Want to join the conversation?\n\nSign in or create an account to comment.\nDark Haven Forge\n5 months ago\n\nThe **AI Assistant VUI** project innovatively enhances hands-free control and accessibility using open-source AI technologies, improving user experience and privacy. It offers valuable learning opportunities while also facing challenges in wake word\n\nLike (1)\nReply\nFiles\nai-assistant-vui.zip\nYou might be interested\nAgentic CallBot Using Twilio and OpenAi Realtime\nO\nFeb 24, 202527 reads\nagentic aiagents+4\nAI Voice Assistant: Enhancing Developer Productivity\nFeb 27, 202521 reads\nAIAssistant+5\nBuilding My Own Jarvis: An AI Adventure\nJ\nFeb 27, 202515 reads\nChatGPTCoffee+4\nGemini Based Live Voice to Voice Todo Agent Assistant\nO\nMar 31, 202513 reads\naigemin+5",
    "awards": [
      "best ai tool innovation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "automating-clinical-document-classification-ai-solutions-for-enhanced-healthcare-decision-support-mkisjSwuptcu",
    "username": "mMyriam Joseph",
    "license": null,
    "title": "Automating Clinical Document Classification: AI Solutions for Enhanced Healthcare Decision Support",
    "publication_description": "Back to publications\nOct 15, 2024\n●\n75 reads\nWinner of\nDistinguished Applied Solution Showcase\nat the\nNLP Projects Expo 2024\nAutomating Clinical Document Classification: AI Solutions for Enhanced Healthcare Decision Support\nAutomated Systems\nBERT Transformer\nClincal BERT\nClinical Decision Support\nComparative Study\nDeep Learning\nDigital Healthcare\nFine-tuning\nMachine Learn\u0002ing\nNarrative Clinical Documents\nNLP application\nPrompt engineering\nSequence Models\nText Classification\nTransfer Learning\nTransformer\nM\nMyriam Joseph\nLike\nBookmark\nShare\n\nAbstract\n\nManual labeling of narrative clinical documents not only burdens healthcare workers but also increases the risk of classification errors and delays. Integrating artificial intelligence (AI) in the medical field enables the automation of this classification process. The research aims to identify the optimal combination of feature engineering technique and classification model for accurate narrative document classification. A robust preprocessing pipeline is also introduced to enhance data quality and model performance. The analysis includes three categories of feature engineering techniques: feature representation, word embeddings, and contextual embeddings, as well as three types of classification models: traditional machine learning algorithms, sequence models, and BERT transformers. The results indicate that a voting classifier with TF-IDF representation outperforms other combinations, achieving an accuracy of 89% and an F1-score of 88.6%. These findings demonstrate the potential for AI to streamline clinical documentation processes and prove that in\u0002tegrating such tools enhances healthcare providers’ performance.\n\nIntroduction\n\nHealthcare workers spend approximately 27% of their working hours on direct patient care and 49% on desk work, including labeling clinical documents. Despite this significant investment of time, clinical documents are often mislabeled, misplaced, and lost, which leads to delays in medical work. In 2019, the World Health Organization (WHO) reported that around 2.6 million deaths in low- and middle-income countries were due to medical errors, with approximately 58% of reported medication errors linked to usability issues in clinical documents.\n\nTo address these challenges, artificial intelligence (AI) can assist the healthcare system by helping medical staff work more effectively and reducing their workload in tasks like automated document classification. Since almost 80% of clinical data are locked in an unstructured format, Natural Language Processing (NLP), a sub-field of AI, is designed to process and analyze this unstructured data.\n\nThere is still limited research focused on classifying narrative clinical documents. These documents, which contain free-text descriptions of patients' conditions, are crucial for clinical decision-making. Their unstructured format makes them difficult to automatically process without effective pre-processing techniques.\n\nOur research objective is to identify the optimal combination of feature engineering techniques and classification models that enhances accuracy and efficiency in classifying narrative clinical documents, thereby reducing the manual workload of healthcare workers.\n\nOur key contributions to the medical field include:\n\nA comprehensive comparison of feature engineering techniques—feature representation, word embeddings, and contextual embeddings.\nA detailed evaluation of classification models, including traditional machine learning, sequence models, and transformers.\nThe introduction of a robust pre-processing pipeline specifically tailored to the challenges of unstructured narrative clinical data, significantly improving the input quality for classification models.\nMethodology\n\nThe methodology involves a systematic approach to trans\u0002forming clinical documents into predicted labels through a series of well-defined steps: text pre-processing, feature en\u0002gineering, classification model, and validation & evaluation.\n\nDataset\n\nDue to privacy concerns, obtaining clinical datasets while preserving patient privacy is challenging. That is why previous research that relies on real documents tends to refrain from sharing them. Therefore, the dataset is an open-source dataset from Kaggle website. The dataset comprises 500 clinical documents in plain text format, with an average length of 2300 words. The documents lack a defined or specific format, and the variance in document length is significant, necessitating advanced techniques for analysis. These documents were originally in paper format and were digitized using OCR (Optical character recognition) technique. The dataset is divided into 5 medical categories, including neurology, Radiology, Discharge Summary, General Medicine, and Gastroenterology, with approximately 100 documents per category.\n\nDue to the small dataset, data augmentation is necessary to increase its size and enhance model performance, robustness, and generalization. ChatGPT prompts were used to generate free-text documents, inspired by the layout and writing style of documents in the dataset. Related procedures for each class are referenced through these links \nneurology\n, \nradiology\n, and \ngastroenterology\n to give ChatGPT the procedure’s name. 250 documents were generated with 50 documents in each class, ensuring a balanced representation across all categories.\n\n1) Text Pre-processing\n\nIt aims to enhance the quality of the input data by addressing various linguistic irregularities present in unstructured docu\u0002ments to ensure that the subsequent stages operate on standard\u0002ized data, hence improving the classification performance. The 5 following techniques are followed sequentially to address the data quality and consistency issues.\n\nThe pre-processing steps include:\n\nAcronym Expansion: is achieved by creating a dictionary of commonly used acronyms within each class and mapping them to their expanded forms in lowercase. The dictionary contains 195 acronyms with their relevant expansion. Since the usage of abbreviations is very common in clinical text, this step facilitates model comprehension of unseen expressions, enhancing the interpretation in clinical document processing.\n\nRegular Expression (RE) Matching: includes the usage of the RE library to cleanse the text from various forms of noise that could potentially mislead the model during analysis. To ensure the anonymity of individuals mentioned in the documents, patients’, doctors’, and hospitals’ names and titles were removed. Irrelevant elements that do not contribute to the semantic content such as digits, dates, extra spaces, newline characters, single characters, and special characters were eliminated.\n\nTokenization: is the process of breaking down a text into units (tokens). Unlike simple splitting which helps handle the ambiguity between words and special cases. This step is essential for initiating any NLP task and ensuring precise text analysis.\n\nLemmatization: is a technique that accurately reduces words to their root form. This process maps verbs to their infinitive forms and plural nouns to singular. Its benefits include reducing dictionary size and eliminating unrelated words, ensuring that related words are consolidated into one, thereby facilitating better classification.\n\nStop-word Removal: involves eliminating commonly used words that do not carry meaningful information, such as articles, prepositions, and conjunctions. The main aim is to reduce text data dimensionality and improve computational efficiency. A total of 233 words were added to the predefined lists of stop-words from both Spacy and NLTK libraries for elimination. It includes frequent adjectives, medical expres\u0002sions, and adverbs used in clinical contexts.\n\nBelow is a comparison of a discharge sum\u0002mary document before and after applying pre-processing steps. The average document size was significantly reduced from 2300 to 750 words, indicating successful data refinement.\n\nDischarge Summary Document Before Pre-processing\n\nDischarge Summary Document Atfer Pre-processing\n\n2) Feature Engineering\n\nFeature engineering focuses on transforming raw text into a numerical format suitable for model learning. The choice of technique was aligned with the classifier type to optimize overall model performance.\n\nThe techniques used are:\n\nFeature Representation: Bag of Words, TF-IDF, and Doc2Vec.\nWord Embedding: Static embeddings like GloVe and FastText, along with dynamic embeddings.\nContextual Embedding: Using ClinicalBERT embeddings to capture word context from surrounding text.\n3) Classification Models\n\nWe classified algorithms into three categories:\n\nA) Traditional Machine Learning Models: Random Forest (RF), Support Vector Machine (SVM), Naïve Bayes (NB), K-nearest neighbors (KNN) , and Multilayer Perceptron (MLP).\nB) Sequence Models: Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRU).\nC) Transformer Models: Transfer learning with BERT variants such as BERT Base, BioClinicalBERT, PubMedBERT, SciBERT, and ClinicalBERT.\n\nOur goal was to identify the top-performing model within each category. The table below provides a summary of the classification models across these categories, along with the feature engineering techniques applied.\n\nCategory\tFeature Engineering\tClassification Models\nCategory 1\tBag of Words, TF-IDF, Doc2Vec\tRF, SVM, NB, KNN, and MLP\nCategory 2\tWord Embedding:\nStatic Embedding (GloVe & FastText)\nDynamic Embedding\tRNN, LSTM, and GRU\nCategory 3\tContextual Embedding\tBERT Base, BioClinicalBERT, PubMedBERT, SciBERT, ClinicalBERT\n4) Validation & Evaluation\n\nThe dataset was split into 80% for training and 20% for testing. Hyperparameter optimization techniques, including grid search for ML models, Keras Tuner for sequence models, and k-fold cross-validation, were used to optimize the performance of all models.\n\nEvaluation metrics include:\n\nOur main focus will be on accuracy, as the balanced dataset ensures that accuracy is a reliable indicator of overall performance across classes. We will also focus on the F1-score to ensure that the model performs well overall, accurately identifies each class and avoids bias by balancing precision and recall across classes.\n\nResults\n\nWe used Python programming language to implement the classification models. The NLP and deep learning libraries and frameworks employed in the implementation are Natural Language Toolkit (NLTK), SpaCy, Regex (re-module), Scikit-learn, Gensim, TensorFlow, PyTorch, Transformers from Hugging Face website.\n\nA) Traditional ML classifiers\n\nTable below illustrates the optimized performance of each algorithm using TF-IDF and Doc2Vec. Since we have limited data, TF-IDF generally achieved better results than Doc2Vec. Using the TF-IDF technique, KNN demonstrates the highest accuracy and precision values, while SVM achieves the highest recall and F1-score values.\n\nModel\tTF-IDF Accuracy\tTF-IDF Precision\tTF-IDF Recall\tTF-IDF F1-Score\tDoc2Vec Accuracy\tDoc2Vec Precision\tDoc2Vec Recall\tDoc2Vec F1-Score\nRF\t0.80\t0.81\t0.81\t0.80\t0.86\t0.86\t0.859\t0.858\nNB\t0.85\t0.85\t0.84\t0.844\t0.77\t0.77\t0.77\t0.77\nKNN\t0.89\t0.885\t0.87\t0.876\t0.76\t0.77\t0.76\t0.75\nSVM\t0.88\t0.884\t0.88\t0.88\t0.84\t0.84\t0.85\t0.84\nMLP\t0.85\t0.85\t0.846\t0.848\t0.81\t0.81\t0.81\t0.81\nVoting Classifier\t0.89\t0.893\t0.885\t0.886\t0.858\t0.858\t0.855\t0.854\n\nTherefore, we decided to combine both of them to complement each other. As a result, the voting classifier achieved the best performance.\n\nB) Sequence Models\n\nThe architecture of each model consists of an embedding layer followed by sequence layers with defined dropout and recurrent dropout, and a dense layer containing 5 neurons (representing the number of classes) with a softmax activation function to classify the document. Table below illustrates the performance of each model based on predefined metrics. GRU has the highest performance among other sequence models since it can capture long-term dependencies better than RNN and converges faster than LSTM before the occurrence of overfitting.\n\nC) Transfer Learning using BERT Transformers\n\nTo fine-tune the BERT models, we used the PyTorch library to perform additional data pre-processing steps to prepare the data in a format suitable for the transformer. This pipeline included tokenization, padding, and truncation to ensure uniform data formatting. During training, a dense layer was added on top of the pre-trained BERT model while freezing its layers to classify the document.\nTable below shows the performance of each model, and it is evident that the ClinicalBERT transformer outperforms all other models since it is trained on discharge summary documents that have the same characteristics as our dataset.\n\nType\tModel\tAccuracy\tPrecision\tRecall\tF1-score\nSequence Models\tRNN\t0.61\t0.55\t0.50\t0.52\n\tLSTM\t0.80\t0.80\t0.78\t0.79\n\tGRU\t0.88\t0.88\t0.88\t0.88\nTransformers\tBERT Base\t0.70\t0.75\t0.71\t0.70\n\tBioClinicalBERT\t0.80\t0.80\t0.80\t0.80\n\tPubMedBERT\t0.81\t0.83\t0.82\t0.81\n\tSciBERT\t0.81\t0.84\t0.82\t0.82\n\tClinicalBERT\t0.85\t0.85\t0.84\t0.84\n\nThe voting classifier achieved the best performance across three categories, achieving an accuracy of 0.89 and an F1-score of 0.886. The GRU model outputs close results with an accuracy of 0.88 and an F1-score of 0.878. The least achieving model was ClinicalBERT with an accuracy of 0.85 and an F1-score of 0.84. The reason deep learning and transformers did not perform better than machine learning is that they require a large amount of data for training or tuning their parameters. However, they produced comparable results, which proves that increasing the dataset size will definitely improve their performance. By comparing the best results before and after applying all the model steps, it is clear that the accuracy has improved by 35% and the F1-score by 32%. This highlights the critical role of the introduced pre-processing pipeline in improving model performance.\n\nEffect of Pre-processing\n\n# Web Application Demo\n\nWe have integrated the high-performing model into the ‘CliniDocPredictor’ web application demo, which provides three core features: document upload, content display, and label prediction. The frontend is built using React.js, while the backend is powered by Django. The primary goal of the demo is to showcase the model’s capabilities, particularly on new documents generated that have not been seen by the model either in training or testing. A recording of the application in action can be viewed in the following YouTube video.\n\nConclusion\n\nThe study evaluated and compared various classification models and feature engineering techniques to identify the best combination for accurately labeling narrative clinical documents. To address the small dataset size, data augmentation increased the dataset by 50%, and a robust pre-processing pipeline improved model performance. The top-performing model was a voting classifier with the TF-IDF feature engineering technique, achieving an accuracy of 89% and an F1-score of 88.6%. This work serves as a proof of concept and could be a viable solution for adoption on a larger scale to enhance healthcare decision-making. By documenting all relevant information about patients, this system has the potential to reduce the workload on healthcare providers and, most importantly, save patients' lives.\n\nLimitations\n\nAfter conducting our analysis, it is essential to address the research limitations. Firstly, the dataset size is a significant constraint. Access to a larger dataset is needed to improve the model’s generalizability. Additionally, incorporating more categories to include all departments in the hospital or healthcare unit, as well as documents outside the medical field, would allow the model to differentiate between medical-related documents and unrelated content, classifying the latter as 'other.' Lastly, the limited computational resources provided by the free version of Colab affected the fine-tuning of BERT models, which require substantial computational power.\n\nFuture Work\n\nA practical application is to integrate the model with a system database to automate the data entry of all clinical documents. The provided model will take unlabeled documents as input and decide to label them with their corresponding class and feed them to the hospital system. As a result, by entering the patient identification number, we can access the patient history with all relevant clinical documents. The figure below explains the pipeline of this solution and how each part will interact with the others.\n\nAdditionally, researchers will benefit from well-organized documents to achieve more effective research outcomes. Finally, with minor adjustments to pre-processing techniques and fine-tuning of the relevant BERT models, the approach can be applied to various domains, not just the medical field.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nMethodology\n\nDataset\n\nText Pre-processing\nFeature Engineering\nClassification Models\nValidation & Evaluation\n\nResults\n\nA) Traditional ML classifiers\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nFiles\nMedia1.mp4\nYou might be interested\nProduct Category Classification Prediction using AI/ML models\nApr 23, 20258 reads\nArtificial IntelligenceBert-Embeddings+39\nPrompt Injection Attacks on LLM: Medical Diagnosis by Symptom Elaboration\nS\nJ\nMar 31, 202515 reads\nClinical NoteDiseases+4\nClassical Sentiment Analysis\nApr 26, 202515 reads\nlogistic-regressionsentiment-analysis\nRAG4HealthQA – Healthcare Q&A Assistant\nOct 01, 20255 reads\nAIEvidenceAIForGood+29",
    "awards": [
      "distinguished applied solution showcase"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "building-a-whatsapp-based-tech-support-system-with-strapi-langchainjs-and-gpt-4o-ZUQQwIAz5kwM",
    "username": "mDenis Kuria",
    "license": "MIT License",
    "title": "Building a WhatsApp-based Tech Support System with Strapi, LangChain.js, and GPT-4o",
    "publication_description": "Back to publications\nMar 12, 2025\n●\n45 reads\n●\nMIT License\nWinner of\nDistinguished Implementation Guide\nat the\nAgentic AI Innovation Challenge 2025\nBuilding a WhatsApp-based Tech Support System with Strapi, LangChain.js, and GPT-4o\nLangChain\nLarge language models(llms)\nRAG\nStrapi\nWhatsAPP\nM\nDenis Kuria\nLike\nBookmark\nShare\nBuilding a WhatsApp-based Tech Support System with Strapi, LangChain.js, and GPT-4o\n\n\nPersonalized tech support is crucial for users facing technical issues as they need timely and specific assistance. Traditional support systems often fail to provide this, leading to user frustration. They are also difficult to develop. But in the error of \nretrieval augmented generation\n and \nlarge language models\n, creating powerful large language models powered assistants that can provide immediate, relevant, and tailored solutions is easy.\n\nIn this article, you will learn how to create a WhatsApp-based tech support system using Strapi for content management, LangChain.js for retrieval-augmented generation (RAG), and gpt-4o for natural language processing. The system will fetch troubleshooting guides from Strapi, use LangChain.js to retrieve relevant information, and generate personalized responses with gpt-4o.\n\nPrerequisites\n\nTo comfortably follow along with this tutorial, you need to have:\n\nNodeJs\n installed in your system\nBasic Knowledge of \nJavaScript\nBasic knowledge of \nLangChain.js\nSetting up Strapi\n\nStart by installing \nStrapi\n using npx.\n\nnpx create-strapi@latest my-strapi-project\n\nThe above command will create a new Strapi project residing in the my-project directory. On the terminal, navigate to the project directory path and start the Strapi server using the following command.\n\nnpm run develop\n\nOnce the server is running, proceed to Strapis admin default URL http://localhost:1337/admin/. Register yourself in order to access the Strapi dashboard.\n\n\nSince Strapi will serve as the knowledge base for your assistant, you need to configure a collection and add data to it. To do this, proceed to Content-Type-Builder and click Create new collection type. This will prompt you to name your collection. Name it tech-support-knowledgebase.\n\n\nThen click continue and select the fields you need for your collection. In this case, we need one field of type Media in which we will store the documents to be utilized by the assistant.\n\n\nName the field as documents. Then click Finish. Don't forget to save your collection's configurations.\n\n\nWait for the server to restart and then proceed to the Content Manager. Populate the collection with all the tech documents you want your assistant to use when answering questions or troubleshooting tasks. The documents should be in PDF or TXT format as these will be the formats the assistant will support.\n\n\nYou now have your data stored in Strapi. But for it to be consumed by your app you need to expose it via Strapi's built-in Rest API. To achieve this, proceed to Settings > Users & Permissions Plugin > Roles > Public. Then select Tech-support-knowledgebase and allow the find and findOne actions. Dont forget to click save.\n\n\nThe API endpoint for your collection will be \nhttp://localhost:1337/api/tech-support-knowledgebases?populate=documents\n. Notice the populate parameter, it allows you to populate various field types (e.g. relations, media, components).\n\nHere is a screenshot showing what the API data looks like:\n\n\nThe populate parameter allows you granular control over the data you retrieve from your API, allowing you to get only the data you need.\n\nSince the API endpoint is ready, let's build the tech assistant. You will start by setting up your development environment.\n\nSetting Up Your Development Environment\n\nOpen an IDE of your choice. Then run the following commands on the terminal to install the required libraries.\n\nnpm install axios langchain @langchain/core @langchain/openai @langchain/community pdf-parse dotenv qrcode-terminal \nnpm install puppeteer github:pedroslopez/whatsapp-web.js\n\nThe first command installs the libraries you will need to send http requests to Strapi in order to retrieve the guides. The second command contains the libraries you need to integrate your assistant into WhatsApp. Let's see what each library does:\n\naxios: You will use this library to make HTTP requests, such as fetching document URLs from Strapi and downloading PDF files.\n\nlangchain: You will use this library to build a context-aware tech support system that can reason about the provided data.\n\n@langchain/core: You will use this package to create chains. It includes essential components like runnables and prompts.\n\n@langchain/openai: You will use this library to integrate OpenAI models with LangChain.\n\n@langchain/community: You will use this package for community-contributed integrations within LangChain. It includes document loaders such as the PDFLoader.\n\ndotenv: You will use this library to load environment variables from a .env file ensuring secure management of sensitive information like API keys.\n\npuppeteer: You will use this library to control a headless version of Chromium for logging into WhatsApp Web.\n\nwhatsapp-web.js: You will use this library to interact with WhatsApp Web.\n\nqrcode-terminal: You will use this library to generate QR codes in the terminal, which is necessary for the initial WhatsApp Web login process.\n\npdf-parse: You will use this library to parse and extract text from PDF files.\n\nWhen all the libraries are installed, your environment is ready but you have to obtain the OpenAI API key. You will use this key to authenticate yourself with OpenAI when utilizing @langchain/openai. To obtain it, follow these steps:\n\nCreate an OpenAI Account\n if you don't have one. If you already have an account, simply log in.\n\nAfter logging in, go to the \nOpenAI API Keys page\n.\n\nClick the Create new secret key button to generate a new API key for the project. Make sure to save this key securely, as it will not be displayed again after this point.\n\nLet's start with building the RAG part of the assistant.\n\nImplementing the Retrieval-Augmented Generation (RAG) System\n\nThis is the core part of the system. It involves fetching data from the Strapi endpoint, processing it, and finally using RAG powered by OpenAI's GPT-4o model to provide responses to user inquiries.\n\nCreate a file at the root of your project named rag_code.mjs. The mjs extension means you will be using ECMAScript Modules (ESM) which will allow you to organize code into smaller, reusable components. The code in the following sections will reside in this file.\n\nThen create another file named .env at the root of your project and add your OpenAI API key in this format.\n\nOPENAI_API_KEY= Your OpenAI API Key\n\nDo not share this key on GitHub as anyone with it will be able to make API call to OpenAI on your behalf.\n\nImporting the Necessary Modules\n\nThe first step is to import the modules and functions you will use from the libraries you installed earlier.\n\nimport fs from 'fs';\nimport path from 'path';\nimport axios from 'axios';\nimport { TextLoader } from 'langchain/document_loaders/fs/text';\nimport { PDFLoader } from '@langchain/community/document_loaders/fs/pdf';\nimport { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';\nimport { OpenAIEmbeddings } from '@langchain/openai';\nimport { MemoryVectorStore } from 'langchain/vectorstores/memory';\nimport dotenv from 'dotenv';\nimport { RunnableSequence } from '@langchain/core/runnables';\nimport { ChatPromptTemplate } from '@langchain/core/prompts';\nimport { ChatOpenAI } from '@langchain/openai';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\nimport { RunnablePassthrough } from '@langchain/core/runnables';\n\ndotenv.config();\n\nImporting the modules ensures you can call and use the necessary tools needed to create the RAG system. You will understand the use of each import as you follow along.\n\nFetching the Documents From Strapi\n\nAfter the importations, you have to connect your support assistant to your Strapi endpoint and come up with a function that will fetch the documents. We will implement a caching mechanism to avoid downloading the documents everytime a user asks a question as this can delay the assistant's response rate significantly.\n\nStep 1: Creating or Clearing the Cache Folder\nStart by ensuring the cache is cleared every time the app starts to avoid using outdated content.\n\nconst cacheFolderPath = './cache';\n\n// Function to clear the cache folder\nfunction clearCacheFolder() {\n  if (fs.existsSync(cacheFolderPath)) {\n    fs.readdirSync(cacheFolderPath).forEach((file) => {\n      const filePath = path.join(cacheFolderPath, file);\n      fs.unlinkSync(filePath);\n    });\n  } else {\n    fs.mkdirSync(cacheFolderPath);\n  }\n}\n\n// Ensure cache is cleared every time the app starts\nclearCacheFolder();\n\nThis function checks whether the cache folder exists and clears it if it does. Otherwise, it creates the cache folder. The folder will be used to store downloaded documents. Clearing it ensures the assistant always starts with a clean state. The function is called only once at the start of the program.\n\nStep 2: Fetching Document URLs from Strapi\nSince you have your cache folder ready, the next step is to download the documents in Strapi to it. To achieve this, you need to start by extracting the URL of each document from the Strapi API data.\n\n// Function to fetch document URLs from Strapi\nasync function fetchDocumentUrlsFromStrapi() {\n  const strapiEndpoint = 'http://localhost:1337/api/tech-support-knowledgebases?populate=documents';\n  try {\n    const response = await axios.get(strapiEndpoint);\n    const documentUrls = response.data.data.flatMap(item => {\n      // Check if documents exist\n      if (item.documents && Array.isArray(item.documents)) {\n        return item.documents.map(doc => doc.url);\n      }\n      return []; // Return an empty array if no documents are found\n    });\n    console.log('Fetched document URLs:', documentUrls); // For debugging\n    return documentUrls;\n  } catch (error) {\n    console.error('Error fetching document URLs from Strapi:', error);\n    throw error;\n  }\n}\n\nYou will append these URLs to the base URL of your Strapi instance. This is because the url you get from the Strapi API is the relative path.\n\n\nBut to download a document you need to pass the full URL. For example, in the case of the relative path in the screenshot above, the full URL should be \nhttp://localhost:1337/uploads/supporting_and_troubleshooting_windows11_52c85abbb4.pdf\n\nLet's see how to actually download the documents from Strapi.\n\nStep 3: Downloading Documents with Caching\nDefine a function to download Documents from Strapi, using caching to avoid redundant downloads:\n\n// Function to download Docs from Strapi with caching\nasync function downloadDocsWithCache(url) {\n  const filename = path.basename(url);\n  const cacheFilePath = path.join(cacheFolderPath, filename);\n\n  // Check if the file already exists in the cache\n  if (fs.existsSync(cacheFilePath)) {\n    console.log(`Using cached file: ${cacheFilePath}`);\n    return cacheFilePath;\n  }\n\n  console.log(`Downloading file: ${url}`);\n  const fullUrl = url.startsWith('/') ? `http://localhost:1337${url}` : url;\n  try {\n    const response = await axios({\n      url: fullUrl,\n      method: 'GET',\n      responseType: 'stream',\n    });\n\n    const writer = fs.createWriteStream(cacheFilePath);\n    response.data.pipe(writer);\n\n    return new Promise((resolve, reject) => {\n      writer.on('finish', () => {\n        console.log(`File downloaded and cached: ${cacheFilePath}`);\n        resolve(cacheFilePath);\n      });\n      writer.on('error', (err) => {\n        console.error(`Error writing file: ${err}`);\n        reject(err);\n      });\n    });\n  } catch (error) {\n    console.error(`Error downloading file from ${fullUrl}:`, error);\n    throw error;\n  }\n}\n\nThe function determines the document's filenames from the URLs fetched from the Strapi endpoint. It then uses the filenames to check whether the file already exists in the cache. If it does, it uses the cached file. If not, it downloads the file by using axios to fetch the document as a stream which is efficient for handling large files. It then pipes this stream to a file writer, which writes the file to the cache directory. This avoids file duplication.\n\nBy now you have implemented the logic to fetch and cache documents from Strapi. This ensures centralized data management using Strapi while also reducing the load time of your system through caching for subsequent queries. After downloading the documents, the next step is to load and process them.\n\nLoading and Processing Documents\n\nThis stage involves loading and splitting the documents in the cache folder into chunks for further processing. Chunks are smaller manageable segments of a text. This step is crucial as it optimizes the handling of potentially large documents and prepares them for retrieval operations.\n\nasync function loadAndSplitChunks({ folderPath, chunkSize, chunkOverlap }) {\n  const documents = [];\n  const files = fs.readdirSync(folderPath);\n\n  for (const file of files) {\n    const filePath = path.join(folderPath, file);\n    let rawContent;\n    if (filePath.endsWith('.pdf')) {\n      const loader = new PDFLoader(filePath);\n      rawContent = await loader.load();\n    } else if (filePath.endsWith('.txt')) {\n      const loader = new TextLoader(filePath);\n      rawContent = await loader.load();\n    } else {\n      console.log(`Skipping file: ${filePath} (Not a PDF or TXT)`);\n      continue;\n    }\n\n    const splitter = new RecursiveCharacterTextSplitter({ chunkSize, chunkOverlap });\n    const splitDoc = await splitter.splitDocuments(rawContent);\n    documents.push(...splitDoc);\n  }\n\n  console.log('Documents loaded and split:', documents);\n  return documents;\n}\n\nThe above function iterates over the files in the cache folder looking for pdf and txt. It then uses the TextLoader class to load the text files and the PDFloader class to load the PDF files. If any other file is found, it is skipped.\n\nOnce the documents are loaded The function splits them into chunks using the RecursiveCharacterTextSplitter. After obtaining the chunks, you need a way to store them and perform operations on them. This is where vector stores come in.\n\nInitializing the Vector Store with Documents\n\nA vector store is a specialized database optimized for storing and managing vector representations. In this case, we will use an in-memory vector store which uses your computer's memory to store the documents' chunks and their embeddings.\n\nasync function initializeVectorstoreWithDocuments(documents) {\n  const embeddings = new OpenAIEmbeddings({\n    openAIApiKey: process.env.OPENAI_API_KEY,\n  });\n\n  const vectorstore = new MemoryVectorStore(embeddings);\n  await vectorstore.addDocuments(documents);\n  console.log('Documents added to vector store.');\n  return vectorstore;\n}\n\nThe above function first initializes an OpenAIEmbeddings instance which is used to compute embeddings for documents. It then creates a MemoryVectorStore with these embeddings, which allows storing and retrieving vectors in memory and adds the documents.\n\nAfter adding the documents to the vector store, you need a way to retrieve the relevant documents based on a user query.\n\nConstructing the GPT4o Powered Conversational Workflow\n\nThis workflow will start by handling a user query by making it more clear, and then retrieve the relevant documents to the query and pass the query plus the retrieved documents as content to GPT4o. GPT4o will then generate a response which you will send back to the user.\n\nThe above is achieved through chains. A chain is a sequence of steps that work together to achieve a specific task.\n\nConstructing the Rephrase Question Chain\n\nThis sequence enhance the clarity and specificity of user queries before they are processed for document retrieval.\n\nfunction createRephraseQuestionChain() {\n  const REPHRASE_QUESTION_SYSTEM_TEMPLATE = `\n  meet the following objective to the best of your ability:\n  `;\n\n  const rephraseQuestionChainPrompt = ChatPromptTemplate.fromMessages([\n    ['system', REPHRASE_QUESTION_SYSTEM_TEMPLATE],\n    ['human', 'Rephrase the following question or instruction to be standalone:\\n{question}'],\n  ]);\n\n  const rephraseQuestionChain = RunnableSequence.from([\n    rephraseQuestionChainPrompt,\n    new ChatOpenAI({\n      openAIApiKey: process.env.OPENAI_API_KEY,\n      maxTokens: 2048,\n      model: \"gpt-4o\", \n    }),\n    new StringOutputParser(),\n  ]);\n  return rephraseQuestionChain;\n}\n\nThe function above sets up a chat prompt template that tells the system to rephrase the user's input. It creates a RunnableSequence with the prompt, a chat operation using OpenAI's model, and a string output parser to format the response from the large language model. Making the input a standalone question makes it well-defined and structured, which helps in achieving more relevant document retrieval results.\n\nCreating the Document Retrieval Chain\n\nThis chain is the one responsible for retrieving the relevant documents to a user query from the vector store.\n\nfunction createDocumentRetrievalChain(retriever) {\n  const convertDocsToString = (documents) => {\n    return documents.map((document) => `<doc>\\n${document.pageContent}\\n</doc>`).join('\\n');\n  };\n\n  const documentRetrievalChain = RunnableSequence.from([\n    (input) => input.question,\n    retriever,\n    convertDocsToString,\n  ]);\n\n  return documentRetrievalChain;\n}\n\nThe function takes the user question, which is by now standalone, and retrieves the relevant documents to the query. It then formats the retrieved related documents into strings with newlines. This creates a single string containing all relevant document contents.\n\nAfter retrieving the documents, you have the context to the user question. You now need to pass the question and the context to GPT4o together with some instructions on what you want to achieve.\n\nIntegrating the Chains into a Conversational Retrieval System\n\nThis is the final chain in our conversational workflow. But before we create it, we need to come up with the instructions that will direct GP4o on how we want the user question answered. You can change this prompt to your liking.\n\nconst ANSWER_CHAIN_SYSTEM_TEMPLATE = `You are a customer service assistant. The messages you reply\n with are served through WhatsApp, so keep replies short and convenient. You are helpful and \n professional. Interpret and answer the user's question using only the provided sources.\n\n<context>\n{context}\n</context>\nThe user's question is: {question}`;\n\nconst answerGenerationChainPrompt = ChatPromptTemplate.fromMessages([\n  ['system', ANSWER_CHAIN_SYSTEM_TEMPLATE],\n  ['human', `Now, answer this question:\\n{question}`],\n]);\n\nNow you have all the components required to create the final conversational retrieval chain. Let's see how you can achieve this.\n\nasync function createConversationalRetrievalChain(retriever) {\n  const rephraseQuestionChain = await createRephraseQuestionChain();\n\n  const conversationalRetrievalChain = RunnableSequence.from([\n    RunnablePassthrough.assign({\n      question: rephraseQuestionChain,\n    }),\n    RunnablePassthrough.assign({\n      context: createDocumentRetrievalChain(retriever),\n    }),\n    answerGenerationChainPrompt,\n    new ChatOpenAI({\n      openAIApiKey: process.env.OPENAI_API_KEY,\n      maxTokens: 2048,\n      model: \"gpt-4o\", \n\n    }),\n  ]);\n\n  return conversationalRetrievalChain;\n}\n\nThe above function takes a user query and calls the rephrase chain and document retrieval chain to format the query and retrieve the relevant documents respectively. It then sends the user question and the retrieved documents as context to GPT40 together with the instructions of what is to be done. The LLM then returns the answer to the user's question using the given context.\n\nYou now have all the functions needed to answer a user's question using the relevant guides and GPT4o. The only remaining part of implementing the RAG system is providing the logic of how the program will flow.\n\nCreating the Main Chat Function\n\nThis function will be responsible for handling the RAG system logic. It will handle an incoming chat question, manage document handling, and execute the retrieval-answer chain.\n\nexport async function chatWithDocs(question) {\n  console.log('Fetching document URLs from Strapi...');\n  const documentUrls = await fetchDocumentUrlsFromStrapi();\n\n  for (const url of documentUrls) {\n    await downloadDocsWithCache(url, cacheFolderPath);\n  }\n\n  console.log('Loading and splitting documents...');\n  const documents = await loadAndSplitChunks({\n    folderPath: cacheFolderPath,\n    chunkSize: 1536,\n    chunkOverlap: 128,\n  });\n\n  console.log('Initializing vector store...');\n  const vectorstore = await initializeVectorstoreWithDocuments(documents);\n  const retriever = vectorstore.asRetriever();\n\n  console.log('Creating retrieval chain.....');\nconst finalRetrievalChain = await createConversationalRetrievalChain(retriever);\nconsole.log('Invoking retrieval chain...');\nconst result = await finalRetrievalChain.invoke({\nquestion: question,\n});\nconsole.log('Result:', result);\nreturn result.content; // Ensure to return the content for proper string handling\n}\n\nThe function fetches document URLs from Strapi. It then downloads and caches these documents locally for efficient access. Next, it loads and splits them into manageable text chunks and initializes a vector store. It then sets up a conversational retrieval chain that rephrases user questions, retrieves relevant document sections, and generates an answer. Finally, it invokes the retrieval chain with the user's query.\n\nYou have completed the RAG part, the final step in building the WhatsAPP-based tech support system is integrating the RAG part into WhatsAPP.\n\nIntegrating Your RAG System to WhatsApp\n\nThere are many ways you can integrate your system into WhatsApp. You can use the official WhatsApp business API, use paid services like Twilio WhatsApp API, or use opensource integrations like \nwhatsapp-web.js\n. In this article, you will utilize whatsapp-web.js for the integration.\n\nProceed to the root of your project and create a file named whatsapp_integration.mjs. Then paste the following code.\n\n// Import the necessary modules\nimport puppeteer from 'puppeteer';\nimport { Client } from 'whatsapp-web.js';\nimport { config as dotenvConfig } from 'dotenv';\nimport qrcode from 'qrcode-terminal';\nimport { chatWithDocs } from './rag_code.mjs'; // Ensure the path is correct\n\n// Load environment variables from .env file\ndotenvConfig();\n\n// Initialize WhatsApp client with Puppeteer configuration\nconst client = new Client({\n  puppeteer: {\n    // Specify the executable path for Puppeteer; adjust according to your setup\n    executablePath: puppeteer.executablePath(),\n    // Additional Puppeteer arguments to ensure it runs smoothly in various environments\n    args: ['--no-sandbox', '--disable-setuid-sandbox'],\n  },\n  // Specify the cache settings for WhatsApp Web version; this can improve load times\n  webVersionCache: {\n    type: \"remote\",\n    remotePath: \"https://raw.githubusercontent.com/wppconnect-team/wa-version/main/html/2.2412.54.html\",\n  },\n});\n\n// Event listener for QR code generation, needed for authentication\nclient.on('qr', (qr) => {\n  // Display QR code in the terminal\n  qrcode.generate(qr, { small: true });\n});\n\n// Event listener for when the client is ready\nclient.on('ready', () => {\n  console.log('Client is ready!');\n});\n\n// Event listener for incoming messages\nclient.on('message', async (msg) => {\n  console.log(`Message received from ${msg.from}: ${msg.body}`);\n  const chat = await msg.getChat();\n\n  try {\n    // Log the processing action\n    console.log('Processing the message...');\n\n    // Send a typing indicator to show the bot is processing\n    await chat.sendStateTyping();\n\n    // Process the message with a custom function (make sure to define this function)\n    const result = await chatWithDocs(msg.body);\n    console.log('Result:', result);\n\n    // Extract the content from the result object to send as a reply\n    const replyContent = result;\n    console.log(`Sending reply: ${replyContent}`);\n\n    // Clear the typing indicator\n    await chat.clearState();\n\n    // Send the reply to the user\n    await msg.reply(replyContent);\n    console.log('Reply sent.');\n  } catch (error) {\n    // Log and handle errors\n    console.error('Error processing the message:', error);\n\n    // Clear typing indicator if an error occurs\n    await chat.clearState();\n\n    // Send an error message to the user\n    await msg.reply('Sorry, an error occurred while processing your request.');\n  }\n});\n\n// Initialize the WhatsApp client\nclient.initialize();\n\nThe code receives a user query. It then uses the chatWithDocs function from the RAG code to generate a response using the documents in Strapi. It then sends the answer back to the user using WhatsApp.\n\nYou now have the full system. Let's see the results.\n\nTesting the Tech Support System\n\nProceed to the terminal and run the whatsapp_integration.mjs code using node:\n\nnode .\\whatsapp_integration.mjs\n\nWhen you run the code, a QR code will be displayed on the terminal.\n\n\nProceed to WhatsApp and use your phone to scan it. This will authenticate the connection between the WhatsApp client on your device and your system, which is running through the Puppeteer-driven instance of WhatsApp Web. Make sure not to use your main number, as any message received after this will invoke a response from the system.\n\nSend your queries to the connected WhatsApp account, and the system will respond with the relevant answers. Here is a sample result:\n\n\nYou can see the system responds with the correct answers to the queries. The system is limited to the information stored in Strapi. If you ask any question outside that, it will notify you it does not have that information.\n\nConclusion\n\nIn this tutorial, you have learned how to utilize Strapi, LangChain, GPT4o, and WhatsApp to create a Tech Support system. When you combine AI with Strapi, you can build infinite solutions. Go ahead and apply the knowledge you learned to create more solutions.\n\nResources\nHave a look at the full code \nhere\n and the Strapi backend \nhere\n.\nhttps://js.langchain.com/docs/get_started/introduction\nhttps://docs.strapi.io/dev-docs/backend-customization\nhttps://aws.amazon.com/what-is/retrieval-augmented-generation/\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nBuilding a WhatsApp-based Tech Support System with Strapi, LangChain.js, and GPT-4o\n\nPrerequisites\n\nSetting up Strapi\n\nSetting Up Your Development Environment\n\nImplementing the Retrieval-Augmented Generation (RAG) System\n\nImporting the Necessary Modules\n\nFetching the Documents From Strapi\n\nLoading and Processing Documents\n\nInitializing the Vector Store with Documents\n\nConstructing the GPT4o Powered Conversational Workflow\n\nView all",
    "awards": [
      "distinguished implementation guide"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "camvisiotech-aiot-security-system-bvzwW2e8O44E",
    "username": "@jjateen97",
    "license": null,
    "title": "Camvisiotech - AIOT Security System",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n71 reads\n●\nApache 2.0\nWinner of\nOutstanding Solution Implementation\nat the\nComputer Vision Projects Expo 2024\nCamvisiotech - AIOT Security System\nAIOT\nArtificial Intelligence\ndlib\nEdge AI\nEDGE-IOT\nESP32\nFace Recognition\nIOT\nmediapipe\nMicropython\nSecurity System\n@jjateen97\nT\n@talwarmohit2005\nLike\nBookmark\nShare\nAbstract\n\nCamVisioTech is an AI-powered IoT-based security system project, collaboratively developed to implement advanced surveillance features with iterative enhancements. Each version builds upon its predecessor, integrating cutting-edge technologies for home and office security.\nCamvisiotech has been a journey from a simple ESP32 CAM to a powerful Edge AI system. Starting as a third-semester project, it began with a basic security system using dlib. Recognizing its potential, I teamed up with a friend to develop Camvisiotech 2.0, running on an ESP32 CAM with optimized synchronization and improved ML models. This version won us Bytecraft at IIIT Nagpur TechFest 2023. Inspired by the Maixduino’s AI capabilities, we started Camvisiotech 3.0—a standalone, AI-driven face detection and recognition solution. With WiFi and GSM connectivity, it now operates autonomously on EDGE, suitable for rural and remote areas with limited connectivity, making edge AI accessible in low-resource environments.\n\nCamVisioTech MK-0: ESP32-CAM Security System\n- Introduction :\n\nThe system utilizes facial recognition to control a solenoid lock, unlocking the door for 3 seconds when a known person is recognized. If an intruder is detected, the system sends an alert via Telegram and activates a buzzer. Additionally, the system offers a GUI application for monitoring and control, and a custom desktop application built with Python and Tkinter to manage both the facial recognition and alert system.\n\n- Features :\nFace Recognition: Captures and processes images via the ESP32-CAM and compares them against pre-stored face encodings.\nDoor Control: Utilizes a solenoid lock controlled via a relay to lock/unlock the door based on successful face recognition.\nIntruder Alert System: Sends the intruder’s image to the user’s Telegram account and sounds an alarm if an unauthorized face is detected.\nPython-based GUI: Includes both a high-latency (old-school) and low-latency modern Python GUI for user-friendly system control.\n- Requirements :\n1. Hardware\nESP32-CAM microcontroller\nSolenoid lock, relay module, buzzer\nIC-7805 voltage regulator\nAdditional components: diode, transistor, resistors, and capacitors.\n\n2. Software\nArduino IDE for programming ESP32-CAM.\nPython with libraries like opencv-python, face_recognition, and customtkinter.\nTelegram Bot: For sending notifications in case of an intruder alert.\n- Methodology :\n1. Face Recognition\n\nThe ESP32-CAM captures images at regular intervals and sends them to the server. The server compares the captured images with pre-stored face encodings to authenticate users.\n\ndef process_frame():\n    # Fetch the image from the specified URL\n    img_resp = urllib.request.urlopen(url)\n    \n    # Convert the image data into a NumPy array\n    imgnp = np.array(bytearray(img_resp.read()), dtype=np.uint8)\n    \n    # Decode the image array into an OpenCV image\n    img = cv2.imdecode(imgnp, -1)\n    \n    # Resize the image to 25% of its original size for faster processing\n    imgS = cv2.resize(img, (0, 0), None, 0.25, 0.25)\n    \n    # Convert the image from BGR (OpenCV default) to RGB (required by face_recognition)\n    imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n\n    # Detect face locations in the resized image\n    facesCurFrame = face_recognition.face_locations(imgS)\n    \n    # Compute face encodings for the detected faces\n    encodesCurFrame = face_recognition.face_encodings(imgS, facesCurFrame)\n\n    # Loop through each detected face and its encoding\n    for encodeFace, faceLoc in zip(encodesCurFrame, facesCurFrame):\n        # Compare the detected face encoding with known encodings\n        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n        \n        # Calculate the distance between the detected face and known faces\n        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n        \n        # Find the index of the closest match\n        matchIndex = np.argmin(faceDis)\n\n        # If the face matches a known person\n        if matches[matchIndex]:\n            # Retrieve the name of the person\n            name = classNames[matchIndex].upper()\n            \n            # Extract the face location and scale back to the original image size\n            y1, x2, y2, x1 = faceLoc\n            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n            \n            # Draw a green rectangle around the recognized face\n            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            \n            # Draw a filled rectangle below the face for the name label\n            cv2.rectangle(img, (x1, y2 - 35), (x2, y2), (0, 255, 0), cv2.FILLED)\n            \n            # Display the person's name on the image\n            cv2.putText(img, name, (x1 + 6, y2 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n            \n            # Trigger an action (e.g., turning on a device or sending a notification)\n            requests.get(urlOn)\n        else:\n            # If the face is not recognized, treat it as an intruder\n            y1, x2, y2, x1 = faceLoc\n            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n            \n            # Draw a red rectangle around the unrecognized face\n            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n            \n            # Draw a filled rectangle below the face for the label\n            cv2.rectangle(img, (x1, y2 - 35), (x2, y2), (0, 0, 255), cv2.FILLED)\n            \n            # Display the \"Intruder!!!\" warning on the image\n            cv2.putText(img, \"Intruder!!!\", (x1 + 6, y2 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n            \n            # Print an intruder warning in the console\n            print(\"Intruder\")\n            \n            # Send the image with the intruder to telegram\n            sendPhoto(img)\n            \n            # Trigger an alert action (e.g., sounding a buzzer)\n            requests.get(buzzOn)\n\n2. Intruder Detection\n\nIf a face does not match the stored faces, the system triggers an intruder alert.\n\nAn image of the intruder is sent to the user’s Telegram account.\nThe buzzer is activated for 1.5 seconds as a local alert.\n3. GUI Application\n\nThe GUI application allows users to:\n\nView the real-time camera feed at different resolutions.\nControl the solenoid lock (unlock for 3 seconds).\nTrigger the buzzer manually if needed.\n\n- Models :\n\nThe face recognition is carried out using \nface_recognition\n python library, built using dlib's state-of-the-art face recognition built with deep learning. The model has an accuracy of 99.38% on the \nLabeled Faces in the Wild\n benchmark.\n\n1. Face Detection\n\nThe library uses a convolutional neural network (CNN) or a Histogram of Oriented Gradients (HOG) model to detect faces in an image. It identifies the coordinates of bounding boxes for each detected face.\n\n2. Face Encoding\n\nA detected face is transformed into a high-dimensional numerical representation called a \"face encoding.\" This is achieved by analyzing the unique facial features and extracting a fixed-length vector for each face.\n\n3. Face Comparison\n\nThe library can compare face encodings to determine similarity. This is achieved using distance metrics such as Euclidean distance.\nIf the distance between two face encodings is below a certain threshold, the faces are considered a match.\n\n4. Known and Unknown Face Identification\n\nThe library allows maintaining a database of face encodings (representing known faces). Detected faces can be matched against this database to identify known individuals or flag unknown ones.\n\n- Conclusion :\n\nIn conclusion, the face_recognition library simplifies implementing advanced face detection and recognition in Python, enabling applications such as security, access control, and surveillance. The project’s ability to send intruder alerts via Telegram and activate a buzzer enhances its utility as a reliable security system. Its modular design, including Python-based GUIs and a streamlined workflow, makes it a practical and scalable solution for modern smart security needs.\n\n- References :\nMachine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning\nface_recognition\nCode and Setup\nCamVisioTech MK-1: AI-Driven Smart Security Camera\n- Introduction :\n\nThe enhanced version introduces real-time object detection and a Flask web application:\n\nHaar Cascade: Utilized for object and face detection.\nAlerts via Email & Telegram: Notifications for security breaches.\nFlask Web App: Streams live video feed with overlays.\n- Features :\nCombines face recognition and object detection.\nReal-time alerts for unauthorized access.\nWeb-based video streaming with detection overlays.\n- Requirements :\n1. Hardware\nESP32CAM module\nSimilar components as MK-0 with enhanced integration.\n\n2. Software\nPython and Flask for backend and web streaming.\nLibraries for detection: opencv, face_recognition.\nPySerial (for serial communication with Arduino or ESP32)\n- Methodology :\n\ndef generate_frames():\n    global continuous_zeros, last_notification_time\n\n    while True:\n        try:\n            # Capture the image from the ESP32-CAM's URL\n            img_response = urllib.request.urlopen(url)\n            img_np = np.array(bytearray(img_response.read()), dtype=np.uint8)\n            frame = cv2.imdecode(img_np, -1)\n\n            # Convert the frame to RGB for processing\n            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n            # Detect faces in the frame using MediaPipe Face Detection\n            with mp_face_detection.FaceDetection(\n                model_selection=0, min_detection_confidence=0.5) as face_detection:\n                results = face_detection.process(image_rgb)\n\n                # Initialize flags for recognized and unknown faces\n                recognized = False\n                unknown = False\n\n                if results.detections:\n                    for detection in results.detections:\n                        bboxC = detection.location_data.relative_bounding_box\n                        ih, iw, _ = frame.shape\n                        x, y, w, h = (int(bboxC.xmin * iw), int(bboxC.ymin * ih),\n                                      int(bboxC.width * iw), int(bboxC.height * ih))\n\n                        # Crop and resize the face region for face recognition\n                        face_image = frame[y:y+h, x:x+w]\n                        if face_image.shape[0] > 0 and face_image.shape[1] > 0:\n                            imgS = cv2.resize(face_image, (0, 0), None, 0.25, 0.25)\n                            imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n                            facesCurFrame = face_recognition.face_locations(imgS)\n                            encodesCurFrame = face_recognition.face_encodings(imgS, facesCurFrame)\n\n                            for encodeFace, faceLoc in zip(encodesCurFrame, facesCurFrame):\n                                matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n                                if any(matches):\n                                    recognized = True\n                                    name = classNames[matches.index(True)]\n                                    # Draw bounding box and label for recognized face\n                                    y1, x2, y2, x1 = [v * 4 for v in faceLoc]\n                                    y1 += y; x2 += x; y2 += y; x1 += x\n                                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                                    cv2.putText(frame, name, (x1 + 6, y2 - 6),\n                                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n                                else:\n                                    unknown = True\n                                    # Draw bounding box for unknown face\n                                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n\n            # Determine the displayed text based on face recognition\n            if recognized or not unknown:\n                displayed_text = \"1\"\n                continuous_zeros = 0\n            else:\n                displayed_text = \"0\"\n                continuous_zeros += 1\n\n            # Trigger notifications if no faces are detected for a certain duration\n\n            if continuous_zeros >= 9:  # Approximately 3 seconds (3 frames/sec)\n                current_time = datetime.datetime.now()\n                if last_notification_time is None or (current_time - last_notification_time).total_seconds() >= 15:\n                    last_notification_time = current_time\n                    trigger_buzzer()\n                    lock_door()\n                    send_notification(\"Motion Detected!\",\n                                      \"Someone has entered the frame. Check the link for details:\\n\\n\"\n                                      \"https://mohittalwar23.github.io/PythonSystemTest/\")\n\n            # Overlay text and timestamp on the frame\n            cv2.putText(frame, displayed_text, (10, 30),\n                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n            timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            cv2.putText(frame, timestamp, (frame.shape[1] - 300, 30),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n\n            # Encode and yield the frame as a JPEG stream\n            _, buffer = cv2.imencode('.jpg', frame)\n            frame = buffer.tobytes()\n            yield (b'--frame\\r\\nContent-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n- Models :\n1. What is Haar Cascade?\n\nHaar Cascades are machine learning object detection algorithms used to identify objects in images or videos. The technique was introduced by Paul Viola and Michael Jones in their 2001 research paper \nRapid Object Detection using a Boosted Cascade of Simple Features\n. The algorithm uses a cascade function trained from a lot of positive and negative images to detect objects, primarily faces.\n\nIt works in stages, where each stage applies increasingly complex filters to identify the features of the object of interest.\n\n2. Object Detection and Face Recognition\n\nIn this project, we'll combine Haar Cascades for face detection and face recognition using the face_recognition library. The system will recognize known individuals and trigger a response if an unknown face is detected.\n\nFor more information on Haar Cascades, check the following free resources:\n\nOpenCV Documentation: Haar Feature-based Cascade Classifiers\nViola-Jones Face Detection Algorithm Paper\n- Conclusion :\n\nBy incorporating features such as real-time object detection, live video streaming, automated alerts, and door-locking mechanisms, this project provides a comprehensive solution for modern security needs. The detailed implementation steps and modular design make it a great resource for learning and extending into advanced AI-powered applications.\n\n- References :\nHaar Cascade : Viola Jones\nface_recognition\nCode and Setup\nCamVisioTech MK-2: Advanced Security via Edge AI\n- Introduction :\n\nThe MK-2 version of CamVisioTech moves beyond conventional surveillance by leveraging Edge AI for on-device processing. Unlike earlier iterations, it focuses on executing models locally, enhancing privacy, reliability, and efficiency, even in low-bandwidth environments.\n\n- Features :\nYOLOv2 Integration: Enables precise object detection and activity recognition directly on the device.\nEdge AI Processing: All inference operations are performed on the hardware itself (Maixduino), eliminating the need for cloud-based computations.\nMulti-Connectivity Support: Offers Wi-Fi or GSM for sending real-time alerts and notifications.\nActuator Integration: Supports on-site physical responses such as buzzer alerts or relay-controlled actions.\nAlerts are dispatched via Wi-Fi or GSM, with extensibility to third-party apps like Telegram using platforms such as IFTTT or PipeDream.\n- Requirements :\n1. Hardware\nMaixduino RISC-V + AI Kit: AI-capable development board with integrated ESP32 for Wi-Fi and Bluetooth capabilities.\nOV2640 Camera Module: High-quality imaging for real-time object detection.\nBuzzer: For audio alerts.\nBreadboard & Jumper Wires: For modular connections.\n2.4-inch TFT Display: For on-device status monitoring.\n\n2. Software\nMicroPython: Lightweight scripting language for development.\nMaixPy IDE: To build and deploy applications on the Maixduino hardware.\nkflash_gui: For uploading firmware and pre-trained models.\nuPyLoader: For accessing, updating, and managing files on the device.\nYOLOv2 Model Files: Optimized for real-time inference on the Maixduino platform.\n- Methodology :\n1. AI Model Training and Deployment\n\nThe Maixduino Kit, which includes an AI-capable microcontroller and camera, functions as the primary processing unit. It utilizes three core .smodel files trained via MaixHub.com—a model for Face(s) Detection, a model for Face Landmark Detection, and Feature Extraction. These models process and identify known faces, storing features as recognized persons (e.g., \"Person 1,\" \"Person 2\") in arrays for rapid matching. The setup allows for the simultaneous recognition of multiple faces, enabling quick identification of individuals stored in the system.\n\n2.Intruder Detection and Actuator Control\n\nWhen an unrecognized person (not in the known faces array) is detected, the system categorizes them as an intruder. It triggers an immediate response, such as sounding a buzzer or activating an LED indicator. The system is also capable of relay control which also allows integration with other actuators, such as door locks, providing instant, real-time physical security.\n\n3.Communication and Alerts\n\nFor reliable notifications, the system can use either Wi-Fi or GSM connectivity to send alerts, ensuring communication in areas with variable internet access. Integrating with third-party messaging apps, like Telegram. This multifaceted communication ensures that users are informed wherever they are, making the system suitable for both urban and remote applications.\n\n- Models :\n\nThe Maixduino Kit, which includes an AI-capable microcontroller and camera, functions as the primary processing unit. It utilizes three core .smodel files trained via MaixHub.com—a model for Face(s) Detection, a model for Face Landmark Detection, and Feature Extraction. These models process and identify known faces, storing features as recognized persons (e.g., \"Person 1,\" \"Person 2\") in arrays for rapid matching. The setup allows for the simultaneous recognition of multiple faces, enabling quick identification of individuals stored in the system.\n\n- Face Detection\nPurpose: Detects faces in the camera feed, identifying multiple faces at once.\nModel Type: YOLOv2 object detection model, optimized for real-time processing on the Maixduino’s hardware.\nOutcome: Locates face bounding boxes to trigger further analysis.\n\n- Face Landmark Detection\nPurpose: Identifies specific facial landmarks (e.g., eyes, nose, mouth) within detected faces.\nModel Type: Keypoint detection model, enabling finer facial structure identification.\nOutcome: Provides a precise facial map, aiding in consistent feature extraction for recognition.\n\n- Feature Extraction\nPurpose: Extracts unique facial features from detected landmarks to distinguish individual identities.\nModel Type: Embedding model that outputs feature vectors for each detected face.\nOutcome: Compares these vectors with stored profiles, recognizing known individuals and categorizing unknown ones as intruders.\n\n- References :\nGetting started with Maixduino\nModels\nCode\nSetup\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nCamVisioTech MK-0: ESP32-CAM Security System\n\nIntroduction :\nFeatures :\nRequirements :\nHardware\nSoftware\nMethodology :\nFace Recognition\nIntruder Detection\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nYou might be interested\nReal-Time Face Detection with OpenCV\nM\nMay 26, 202516 reads\nFace Detection in Live Video Using Python and OpenCV\nO\nDec 29, 202412 reads\nReal-time Face Detection using OpenCV | Computer Vision Enthusiast | Python and Machine Learning\nMay 16, 20257 reads\nface Detection\nA\nMay 22, 20254 reads",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "conversational-pdt-assistant-rasa-beautifulsoup-openai-brRuHMVIG4rr",
    "username": "I. Kennedy Yinusa",
    "license": "MIT License",
    "title": "Conversational PDT Assistant - Rasa, BeautifulSoup, OpenAI",
    "publication_description": "Back to publications\nMar 12, 2025\n●\n61 reads\n●\nMIT License\nWinner of\nDistinguished Applied Solution Showcase\nat the\nAgentic AI Innovation Challenge 2025\nConversational PDT Assistant - Rasa, BeautifulSoup, OpenAI\nAI/ML\nBeautifulSoup\nChatbot\nConversational AI\nCrawling\nOpenAI\nRasa\nScrapping\nTransformer\nI. Kennedy Yinusa\nLike\nBookmark\nShare\nAbstract\n\nArtificial intelligence (AI) technology has become important in human life with numerous benefits. This publication investigates one of the benefits which is the development of an intelligent support system for students to enhance their personal development. Then, this publication explores advanced AI models to develop an intelligent chatbot system to provide personalized support and guidance to students. In this research development, this publication uses Beautiful Soup for web scraping to gather Edinburgh Napier University website data. Additionally, this publication integrates transformer model with the Rasa framework and GPT-3 to generate better responses. Moreover, this publication presents a comparative analysis of between two versions of the chatbot system based on system usability scale (SUS). By leveraging AI technologies, this research contributes to improving the field of educational support systems. As continuous learning for intelligent tools, this publication will inspire further research and innovation in AI technologies to address educational challenges to support student needs.\n\nIntroduction\n\nIn today, people notice the impact of conversational artificial intelligence (AI) which are 24/7 availability, efficient in customer support and enhanced user experience according to the authors (Suram and Namatherdhala, 2022). That is why people widely use it in different industries such as customer service and support, education, banking or such kind of service sectors. In addition, this paper explores conversational AI which is virtual assistant as personal development tutor (PDT) for students to support individual needs within a short time because of the manual way faces some problems which means that if the PDT is a qualified mentor, students struggle to find the time and resources to engage in learning activities or to seek guidance and the lack of immediate support and guidance that can make frustration and disengagement over time. Similarly, the PDT mentors face their own challenges including 24/7 support and relevant information for different learners. Hence, this paper aims to develop advanced ways using AI model which delivers an experienced personal development for students. When doing this research, the effectiveness of current approaches to develop data collection and text generation is delayed because of some challenges and they are:\n\nDifficulty in extracting structured data from unstructured web content.\nLimited capabilities of existing text generation models in producing contextually relevant content.\nAddressing the ethical considerations surrounding web scraping practices.\n\nManaging time constraints to do development for conducting the research.\nAddressing these challenges is important to the success of this system for students to provide personalized support and guidance by developing advanced methodologies to overcome these challenges, such as using a transformer model to solve limited capabilities and using Miro which is a project management tool, to organize overall works to finish each progress before deadline. In addition, this proposed system targets to provide conversation to students who need information about related modules and finance issues to achieve natural language understanding by developing models and getting contextually relevant responses. Then, setting up some objectives to achieve the targeted system and they are:\n\nResearch on the published related papers.\nGather data from Napier website using web scraping.\nIntegrate Rasa ML/AI framework with transformer model.\nTrain the chosen model on relevant data for response generation.\nEvaluate and testing the system.\n\nTo address these challenges and objectives in generating contextually relevant responses for PDT, this project proposes to gather data using Beautiful Soup for web scraping and a seamless integration using the Hugging Face transformer model between Rasa framework and GPT-3 API to enhance text generation capabilities. Additionally, the authors (Shen et al., 2023) described that the combination of Hugging Face transformer model and GPT-3 provides enhanced performance, flexibility and adaptability in generating relevant responses.\n\nThen, this paper is structured as follows. This paper provides related works after introduction section and a detailed description of the proposed model including its architecture, methodology and integration with GPT-3 in Method/ Approach section. For the web scraping model, the details will be described in the data collection and preprocessing section. Furthermore, this paper will be provided evaluation method and its result in result and discussion section. Finally, this paper will be concluded with a summary of findings, ethical considerations for conversation AI for PDT and future developments in each related section.\n\nRelated Work\nCore Approaches\n\nThe core approaches of the Conversational Assistant PDT project are encompassed by three key elements. Firstly, conversational emotional intelligence is focused on integrating sentiment analysis to enable emotionally intelligent interactions to be engaged in by the assistant. This involves emotional expressions in students' written responses being detected and responded to with sensitivity and understanding. Secondly, Advanced Natural Language Understanding (NLU) is emphasised, with sophisticated Natural Language Processing (NLP) techniques being leveraged to accurately decipher users' emotional states. This capability allows nuanced emotional expressions to be effectively interpreted. Lastly, personalised support for students is aimed to be provided by offering immediate and tailored assistance based on their emotional states and preferences. This ensures that students receive dynamic and responsive guidance tailored to their individual emotional needs, ultimately contributing to their overall well-being and academic success.\n\nDevelopment and Testing Methodologies\n\nA study conducted in Pakistan aimed to develop an Artificially Intelligent (AI) chatbot, named 'Bablibot', designed to assist caregivers by providing information regarding immunisation (Siddiqi et al., 2024). The study intentionally utilised both quantitative and qualitative data collection methods, such as in-depth interviews and user surveys, to enhance the performance and functionality of the chatbot. The researchers plan to leverage incoming 'Bablibot' data to further train the model and explore additional training datasets, considering the increasing popularity of local language chatbots post-study.\n\nThis approach underscores the commitment to utilising gathered data to continually improve the chatbot's effectiveness and adaptability. Valuable insights learned from these findings offer prospects for enhancing and potentially extending the utility of our chatbot in similar resource-constrained contexts. The quantitative component of the study involved 20 phone-based interviews with randomly selected participants (Siddiqi et al., 2024).\n\nThis qualitative data example has been incorporated into our own testing, although other methods, such as surveys or in-person sessions, proved to be more suitable approaches. Our surveys were tailored to provide insights into user experiences and understand reasons for non-engagement with the bot. This structured testing process significantly contributed to enhancing the effectiveness and usability of our chatbot as an academic support tool.\n\nAs part of our research, we conducted role-played conversations between students and tutors to identify emotion-driven dialogue, aligning with previous studies proposing an emotion generation model. The method employed by (Zhao et al., 2024) utilized an exploratory technique to improve data quality, employing a pretrained “affective generation model” used for sentiment analysis and dialogue recognition. Additionally, they extracted context features from conversation text and utilized an algorithm to classify mixed data, enabling training of the bot from structured conversations.\n\nIn our research for dataset gathering, we opted for the wizard of Oz technique to achieve a comparable outcome, simulating authentic conversations between students and their tutors on specific topics. This is a well-established and effective testing method for AI chatbot testing and supporting research objectives with little technical requirements (Kuang et al., 2023). This approach allowed us to replicate genuine interactions and capture emotion-driven dialogue for analysis.\n\nComparative Analysis of Leading Virtual Assistant Platforms\n\nIn virtual assistant platforms, both Amazon Alexa and Microsoft Cortana are leading examples, offering a range of functionalities and capabilities achieved with advanced natural language processing (NLP) technologies (Major et al., 2022). User queries undergo understanding and processing by leveraging large datasets and employing sophisticated algorithms to enhance their language models (Nguyen-Mau et al., 2024). Amazon Alexa excels in consumer-oriented tasks like smart home control and entertainment, while Microsoft Cortana specializes in productivity and workplace functions such as calendar management and email integration (Major et al., 2022).\n\nThe effective combination of Dialogue, Natural Language Processing (NLP), and datasets is fundamental to the advancement of AI platforms, allowing for the development of tailored skills and seamless integrations. However, amidst the emphasis on technical capabilities, the crucial element of user experience often remains overlooked (Adesina, n.d.).\n\nIn spoken language interaction, the quality of dialogue is a determinant of user satisfaction and engagement. (Gašić et al., 2017) highlights the multifaceted nature of dialogue quality, detailing an assessment through mechanisms such as reward functions and dialogue policy. This dialogue policy, designed to map belief states to system actions, stands as a pivotal method for optimizing dialogue quality. The 'belief state', derived from N-best hypotheses, is the guiding force behind dialogue policy decisions, having a profound influence on the overall conversational experience.\n\nOur academic support chatbot draws inspiration from this, by leveraging their functionalities and capabilities, playing an important role in assisting students throughout their academic journey. While technical advancements drive the capabilities of AI platforms, it is imperative to prioritize the refinement of dialogue quality to ensure meaningful user interactions and heightened user satisfaction (Venkatesh et al., n.d.).\n\nMethod/Approach\nSystem Architecture\n\nThis conference paper provides an in-depth analysis of a Conversational Assistant Personal Development Tutor (PDT) project, focusing on its system architecture, natural language understanding (NLU), and natural language generation (NLG) processes. The project integrates various technologies such as Rasa ML/AI, web scraping, and GPT-3 model from OpenAI to facilitate an effective question and answer as well as emotional detection. Through a combination of curated datasets, web scraping, and advanced AI models, the PDT aims to provide personalised and empathetic responses to user inquiries and concerns. This paper delves into the architectural design of the system and elucidates the key components involved in NLU and NLG processes, shown in Figure 1.\n\nFigure 1: System Architecture\n\n\nThe system architecture of the Conversational Assistant PDT encompasses several interconnected components that work collaboratively to facilitate seamless interactions with users. At its core, the architecture comprises:\n\nRasa ML/AI Framework\n\nThe project utilises the Rasa framework for developing the conversational agent. Rasa provides tools for NLU, dialogue management, and NLG, enabling the system to understand user intents, maintain context, and generate appropriate responses (Mathur et al., 2021; Bocklisch et al., 2017).\n\nBelow are some of the Rasa configuration. Here is the link to the full repository: \nPDT-bot\n\nnlu.yml\n\nversion: \"3.1\"\n\nnlu:\n\n  - intent: credits\n    examples: |\n      - What are credits for [Software Development](subject)\n      - What are credits for [Web Technologies](subject)\n      - What are credits for [Masters Dissertation](subject)\n      - credits for [Performance Studies: Integrated Musicianship](subject)\n      - credits for [Civil Engineering Materials](subject)\n      - credits for [Computer Systems](subject)\n\n - intent: course_fee\n   examples: |\n     - what is the course fee for [Msc Computing](fee)\n     - what is the course fee for [Msc Drug Design and Biomedical Science](fee)\n     - what is the course fee for [Msc Global Logistics and Supply Chain Analytics](fee)\n\n - intent: fee_installments\n   examples: |\n     - I have a problem with my fee instalments\n     - Fee Installments Problem\n     - Fee problem\n\n - intent: less_installment\n   examples: |\n     - What if I pay a less agreed instalment than a  actual amount?\n     - what if i pay less installment\n     - i cant pay full installment at single payment\n\n - intent: remaining_fee\n   examples: |\n     - Will the remaining fee added to the next instalment automatically?\n     - will my installment add automatically to my next payment\n\nrules.yml\n\nversion: \"3.1\"\n\nrules:\n\n  - rule: Say goodbye anytime the user says goodbye\n    steps:\n    - intent: goodbye\n    - action: utter_goodbye\n\n  - rule: Say 'I am a bot' anytime the user challenges\n    steps:\n    - intent: bot_challenge\n    - action: utter_iamabot\n\nstories.yml\n\nversion: \"3.1\"\n\nstories:\n\n  - story: student_representative_story\n    steps:\n      - intent: student_representative\n      - action: utter_student_representative\n\n  - story: hopeless_module_story\n    steps:\n      - intent: hopeless_module\n      - action: utter_ hopeless_module\n\n  - story: supervisor_for_dissertation_story\n    steps:\n      - intent: supervisor_for_dissertation\n      - action: utter_supervisor_for_dissertation\n\n  - story: need_dissertation_goals\n    steps:\n      - intent: need_dissertation_goals\n      - action: utter_need_dissertation_goals\n\n  - story: seek_guidance_module_selection_story\n    steps:\n      - intent: seek_guidance_module_selection\n      - action: utter_seek_guidance_module_selection\n\n  - story: Course_Fee_story\n    steps:\n      - intent: course_fee\n      - action: action_extract_course_fees\n\n  - story: feeling_frustrated_with_financial_issues_story\n    steps:\n      - intent: feeling_frustrated_with_financial_issues\n      - action: utter_feeling_frustrated_with_financial_issues\nWeb Scraping Module\n\nTo enrich the knowledge base of the PDT-bot, a web scraping module is integrated into the system. This module utilises BeautifulSoup to extract information from the institution's PDT website, ensuring that the responses provided to users are up-to-date and relevant.By extracting and processing statistics through web scraping, the machine gives reliable and contemporary responses to queries associated with direction credits, charges, and different information subject to periodic adjustments. This multi-faceted approach, combining advanced language fashions and net scraping, significantly complements the system's capacity to realize and respond successfully to an extensive range of scholarly queries, mainly to an extra fine consumer revel. Below if the sample code implementation for the implementation approach:\n\naction.py\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom typing import Any, Text, Dict, List\nfrom rasa_sdk.events import SlotSet\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom typing import Text, List, Dict\n\nclass ActionExtractCredits(Action):\n    def name(self) -> str:\n        return \"action_extract_credits\"\n\n    def fetch_credits_online(self, code):\n        url = f\"https://www.modules.napier.ac.uk/Module.aspx?ID={code}\"\n        try:\n            page = requests.get(url)\n            soup = BeautifulSoup(page.content, \"html.parser\")\n            credits_span = soup.find(\"span\", id=\"ctl00_ContentPlaceHolder1_LBLscqlvalue\")\n            credits_text = credits_span.get_text().strip() if credits_span else \"Credits information not found.\"\n            return credits_text\n        except requests.RequestException:\n            return \"Failed to retrieve data.\"\n\n    async def run(self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: dict) -> list:\n        subject = next(tracker.get_latest_entity_values('subject'), None)\n        if subject:\n            subject = subject.lower()\n        subject_codes = {\n            \"software development\": \"SET07402\",\n            \"web technologies\": \"SET08220\",\n            \"masters dissertation\": \"SOC11101\",\n            \"performance studies: integrated musicianship\":\"MUS07138\",\n            \"civil engineering materials\":\"CTR07100\",\n            \"computer systems\":\"CSN07105\",\n\n        }\n\n        code = subject_codes.get(subject, None)\n        if not code:\n            credits_text = \"No valid code found for the specified subject.\"\n        else:\n            credits_text = self.fetch_credits_online(code)\n\n        dispatcher.utter_message(text=f\"Credits for {subject}: {credits_text}\")\n\n        return []\n\nclass ActionExtractCourseFees(Action):\n    def name(self) -> str:\n        return \"action_extract_course_fees\"\n\n    def fetch_fees_online(self, course_url_suffix):\n        base_url = \"https://www.napier.ac.uk/courses/\"\n        full_url = f\"{base_url}{course_url_suffix}\"\n        try:\n            page = requests.get(full_url)\n            soup = BeautifulSoup(page.content, \"html.parser\")\n            fees_info = {}\n            table_rows = soup.find_all('tr')\n            for row in table_rows:\n                region_cell = row.find('td', class_=\"one\")\n                fee_cell = row.find('td', class_=\"two\")\n                if region_cell and fee_cell:\n                    region = region_cell.get_text(strip=True)\n                    fee = fee_cell.get_text(strip=True)\n                    if fee and fee != '£':\n                        fees_info[region] = fee\n            return fees_info if fees_info else \"No valid fee data found on the page.\"\n        except requests.RequestException as e:\n            return f\"Failed to retrieve fee information due to: {str(e)}.\"\n\n    async def run(self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: dict) -> list:\n        course = next(tracker.get_latest_entity_values('fee'), None)\n        if course:\n            course = course.lower()\n        else:\n            dispatcher.utter_message(text=\"Please specify the course you are interested in.\")\n            return []\n\n        course_dict = {\n            \"msc computing\": \"msc-computing-postgraduate-fulltime\",\n            \"msc drug design and biomedical science\": \"msc-drug-design-and-biomedical-science-postgraduate-fulltime\",\n            \"msc global logistics and supply chain analytics\": \"msc-global-logistics-and-supply-chain-analytics-postgraduate-fulltime\",\n        }\n\n        course_url_suffix = course_dict.get(course, None)\n        if not course_url_suffix:\n            dispatcher.utter_message(text=f\"No valid URL found for the specified course: {course}\")\n            return []\n\n        fee_info = self.fetch_fees_online(course_url_suffix)\n        if isinstance(fee_info, str):\n            dispatcher.utter_message(text=fee_info)\n        else:\n            filtered_fees = {k: v for k, v in fee_info.items() if \"please note\" not in k.lower() and \"discount\" not in k.lower()}\n            fee_message = \", \".join([f\"{k}: {v}\" for k, v in filtered_fees.items()])\n            dispatcher.utter_message(text=f\"Fee information for {course}: {fee_message}\")\n        return []\n\nThe system additionally employs a rule-based approach to handle common place queries and map specific entities to predefined codes or URLs. This rule- based machine facilitates green dealing with frequently requested questions and streamlines the retrieval of applicable data, providing a continuing revel in for students in search of straightforward answers to common place queries.\n\nOpenAI's GPT-3 Model\n\nThis bot takes a multi-pronged approach to handle the various variety of scholarly queries effectively. Initially, the GPT-2 language model changed into implemented to cope with open-ended and context- established queries. While GPT-2 presented enhancements over traditional rule-based structures in producing herbal and contextual responses, its overall performance felt quick whilst managing extra complicated or nuanced queries. To conquer those obstacles, the system leveraged the superior GPT-3.5 language version advanced through OpenAI. GPT-3.5 has proven advanced performance in producing herbal, contextually applicable, and insightful responses.\n\nBy harnessing this trendy model, the gadget can engage in extra human-like conversations and showcase a deeper knowledge of the query's context and nuances. With GPT-3.5 at its center, the machine can provide more coherent, appropriate, and insightful solutions to complex or open-ended questions, surpassing the limitations of GPT.\n\naction.py\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom typing import Any, Text, Dict, List\nfrom rasa_sdk.events import SlotSet\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom typing import Text, List, Dict\n\nclass ChatGPT(Action):\n\n    def __init__(self):\n        self.url = \"https://api.openai.com/v1/chat/completions\"\n        self.model = \"gpt-3.5-turbo\"\n\n        api_key = ''\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n\n        self.prompt = \"Answer the following question, based on the data shown. \" \\\n                      \"Answer in a complete sentence and don't say anything else.\"\n\n    def name(self) -> Text:\n        return \"action_generate_response\"\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n        user_input = tracker.latest_message['text']\n        content = self.prompt + \"\\n\\n\" + user_input\n        body = {\n            \"model\": self.model,\n            \"messages\": [{\"role\": \"user\", \"content\": content}]\n        }\n\n        response = requests.post(\n            url=self.url,\n            headers=self.headers,\n            json=body,\n        )\n        answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n        dispatcher.utter_message(text=f\"{answer}\")\n\n        return []\n\nIn addition to predefined responses and web-scraped information, the system leverages OpenAI's GPT-3 model for generative responses. GPT-3 employs deep learning techniques to generate human-like text based on the context provided, enhancing the system's ability to engage in natural and meaningful conversations (Sharma, 2020).\n\nChatbot Interface\n\nTo enable user interaction, the system features a web- based chatbot interface accessible via a browser. This interface provides users with an intuitive platform to communicate with the PDT and seek assistance with their queries or concerns.\n\nFigure 2: Chatbot Interface\n\n\nNLU and NLG Components Processes\n\nThe NLU and NLG processes play pivotal roles in enabling the system to understand user inputs and generate appropriate responses. The following steps outline how the system interacts with users, processes their inputs, and generates responses:\n\nUser Input Processes:\n\nUpon receiving a user input, the system employs NLU techniques to parse and extract relevant information such as intents, entities, and sentiment (Jiao, 2020).\nThe NLU component analyses the user's query and determines the underlying intent, which could range from seeking information to expressing emotions or concerns.\n\nIntent Classification and Entity Recognition:\n\nUsing trained NLU models based on curated datasets, the system classifies the user's intent and identifies any pertinent entities mentioned in the input.\nIntent classification enables the system to understand the purpose behind the user's query, while entity recognition helps in extracting specific details relevant to the inquiry.\n\nResponse Generation:\n\nBased on the identified intent and extracted entities, the system proceeds to generate an appropriate response. This response can be derived from predefined templates, web- scraped information, or generated dynamically using the GPT-3 model.\nNLG techniques are employed to structure the response in a coherent and contextually appropriate manner, ensuring that it effectively addresses the user's query or concern.\n\nEmotional Detection and Response:\n\nIn addition to providing information, the system is equipped to detect emotional cues in user inputs using sentiment analysis techniques.\nUpon detecting emotional expressions such as distress or frustration, the system responds empathetically, offering support and guidance tailored to the user's emotional state.\n\nThe Conversational Assistant PDT project exemplifies the integration of advanced AI technologies to create a responsive and empathetic virtual tutor. By leveraging the capabilities of Rasa ML/AI, web scraping, and GPT-3 model, the system adeptly handles user queries, provides accurate information, and offers emotional support when needed. The architectural overview and elucidation of NLU and NLG processes presented in this paper underscore the system's sophistication and its potential to enhance the learning and well-being of students.\n\nFurther research and refinement of the system could lead to even greater effectiveness in addressing a wide range of academic and personal concerns.\n\nDeep Learning Methods\n\nThe bot leverages advanced deep mastering strategies to allow robust herbal language know-how (NLU) competencies through the Rasa framework. NLU is a crucial thing in conversational AI systems like Rasa because it translates user messages to realise their underlying intents and extract applicable statistics.\n\nThis functionality is important for the machine to interact in significant dialogues and provide suitable responses. At the core of Rasa's NLU element lies deep studying fashions chargeable for purpose class and entity extraction. The intent category includes determining the intention or reason behind a person's utterance, at the same time as entity extraction identifies and extracts specific pieces of information from the utterance, which include route names, problem codes, or fee-associated information. Rasa employs state-of-the-art deep mastering architectures to system person messages and make correct predictions approximately their intents and entities.\n\nThe deep learning fashions hired in Rasa's NLU component are tremendously adaptable and may be first-class-tuned or retrained as new training statistics turn into be had, making sure that the machine remains updated and responsive to evolving language patterns and user behaviours. By leveraging deep mastering techniques, the bot can appropriately interpret personal inputs, classify their intents, and extract applicable entities, forming the muse for meaningful conversations, answering questions, and supplying suitable responses to user inquiries. This deep learning-powered NLU issue is a vital factor in the device's typical capability to deliver a continuing and shrewd conversational experience to college students.\n\nData Collection and Preprocessing\n\nData series lies at the heart of this process, concerning the gathering of conversational facts from numerous sources and changing it right into a format appropriate for training AI fashions. In this unique case, transcriptions of conversations had been gathered and converted into YAML layout, a structured and human-readable markup language.\n\nThis method lets in for the systematic agency of dialogues, intents, and corresponding responses, facilitating seamless integration into the training pipeline. Once collected, the raw conversational records undergo a rigorous preprocessing section to make sure it's satisfactory and suitable for training AI models. The transformation of transcriptions into YAML layout represents a pivotal step in the statistics collection and preprocessing pipeline for conversational AI.\n\nThe Wizard of Oz technique, as described by (Kuang et al., 2023), involves simulating the functionality of an automated system by having a human operator manually perform the tasks typically carried out by the system. In the context of our study, this technique was employed to mimic authentic conversations between students and their Personal Development Tutor (PDT) on real topics, particularly issues related to modules. Each scenario was carefully crafted to incorporate an emotional undertone aimed at eliciting various responses from the participants, thereby enriching the range of labels obtained from the transcriptions.\n\nResults and Discussion\nUsability Testing Results\n\nUsing participants testing, detailed notes were taken during each test session, focusing on observing the participants' engagement with the system and their interactions with it. All issues encountered with the interface were also documented.\n\nChatbot v1: In the first task, participants engaged in a dialogue about academic stress, while the second task involved discussing the process of selecting a supervisor for their dissertation module.\n\nKey points from participant testing notes:\n\nOften received no response.\nParticipants had to re-word or simplify their input.\nResponses were unhelpful and asked clarifying questions about information already provided by the participant.\nThe bot repeated responses, hindering task completion.\n\nThe SUS forms yielded generally low scores, averaging 29.17% (see Figure 3). Conversations were recorded in the chatbot and later classified to enhance the NLU component of Rasa.\n\nFigure 3: SUS Chatbot Testing v1\n\n\nChatbot v2: The second version incorporated the updated intents and the utilisation of GPT-3.5 turbo, serving as the default response for unrecognised inputs. SUS results significantly improved, averaging 67.5% (see Figure 4).\n\nFigure 4: SUS Chatbot Testing v1\n\n\nEthical Considerations\n\nEthical considerations are foundational in the development of chatbots, necessitating meticulous attention to aspects such as privacy, bias, and potential user manipulation. In our project, these concerns were paramount, evidenced by the formation of a dedicated team tasked with managing dialogues (DM Team) to uphold the integrity of conversational flow. In a more recent development, the design process of chatbots places a greater emphasis on openness, fairness, and user consent. The significance of ethical practices in technological innovation is highlighted by industry standards and guidelines like the IEEE Global Initiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems and the ACM Code of Ethics (Chatila et al., 2017).\n\nOur project underscores a seamless collaboration among the Natural Language Processing (NLP), Natural Language Understanding (NLU), and Dialogue Management (DM) teams, ensuring the prioritization of ethical considerations. Furthermore, real-world scenario datasets for training the Rasa model were obtained through live interviews with student Personal Development Tutors (PDT), enriching our research with practical insights. In the realm of chatbot development, ethical considerations remain paramount. Transparency is vital to ensure users are aware of interacting with automated systems (Liu & Singh, 2020), while privacy and data protection must adhere to regulations like GDPR or CCPA (Acquisti et al., 2015; Liu & Singh, 2020). Chatbots must offer accurate information, avoid bias (Molla et al., 2019), and provide fair and inclusive interactions (Miconi et al., 2018).\n\nRecent chatbot applications, spanning mental health support, customer service, education, and healthcare, exemplify adherence to these ethical principles. By embracing such principles, developers can create chatbots that prioritize user trust and well-being, advancing the ethical frontier of AI-driven interactions.\n\nConclusion and Future Work\n\nThis paper explores the potential of artificial intelligence (AI) in creating an intelligent support system for students, focusing on a personal development tutor chatbot. By integrating advanced AI models like GPT-3.5 with the Rasa framework, the chatbot provided personalized support and guidance, yielding promising results. The second version of the chatbot showed significant improvement in system usability scale (SUS) scores compared to the initial version, demonstrating the potential of advanced AI in educational support.\n\nThe research addressed challenges such as extracting structured data from unstructured web content and navigating ethical considerations in web scraping. Advanced methodologies and project management tools facilitated the successful development of the chatbot system. Moreover, the comparative analysis of the chatbot versions highlighted the effectiveness of advanced AI models in creating satisfying interactions, emphasizing the importance of cutting- edge technologies in educational support systems.\n\nFuture works could focus on fine-tuning GPT-3.5, which would improve response accuracy and contextual relevance. Additionally, enhancing semantic understanding would help extract precise meaning from student input. By prioritizing data security, the system can better protect sensitive information. Moreover, integrating sentiment analysis would support emotional needs, while adding voice assistance would boost accessibility. Expanding conversation topics to include career advice, personal guidance, and campus activities would provide comprehensive support.\n\nLast but not least, we acknowledge the crucial contributions of team members and collaborators (Idowu Kennedy Yinusa, Aaron Darmudas, Yeshwasri Bokka, Saw Yu Nandar, Aye Thandar Phyo, Yash Bhardwaj, and Ayoola Stephen Ogunsola). Special thanks go to Dr. Yanchao Yu for assisting with data gathering, Dr. Md Zia Ullah for his invaluable guidance, and Michael Orme for his support. Their contributions were instrumental in achieving our goals and creating a more inclusive educational experience.\n\nIn conclusion, developing the intelligent chatbot system with advanced AI models marks significant progress in enhancing educational support systems. This research contributes to the ongoing exploration of AI technologies to address educational challenges and support student needs. Future innovation may further improve AI-powered educational support systems, benefiting students' personal development and academic success.\n\nReferences\nAcquisti, A., Brandimarte, L., & Loewenstein, G. (2015). Privacy and human behavior in the age of information. Science, 347(6221), 509-514.\nAdesina, A. (n.d.). iNOUN: Architecture and Usability of a Chatbot for Academic Enquiries. West African Journal of Open & Flexible Learning, 10(1).\nAmy Schade. (2015). Pilot Testing: Getting It Right (Before) the First Time. Neilson Norman Group.\nBenyon, D. (2019). Designing user experience: A guide to HCI, UX and interaction design (Fourth edition). ProQuest.\nBocklisch, T., Faulkner, J., Pawlowski, N., & Nichol, A. (2017). Rasa: Open Source Language Understanding and Dialogue Management. ArXiv:1712.05181 [Cs].\nChatila, R., Firth-Butterflied, K., Havens, J. C., & Karachalios, K. (2017). The IEEE Global Initiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems. IEEE Robotics and Automation Magazine, 24(1), 110.\nGašić, M., Mrkšić, N., Rojas-Barahona, L. M., Su, P. H., Ultes, S., Vandyke, D., Wen, T. H., & Young, S. (2017). Dialogue manager domain adaptation using Gaussian process reinforcement learning. Computer Speech and Language, 45, 552–569.\nJakob Nielsen. (1994). 10 Usability Heuristics for User Interface Design. Nielsen Norman Group.\nKuang, E., Jahangirzadeh Soure, E., Fan, M., Zhao, J., & Shinohara, K. (2023, April 19). Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text). Conference on Human Factors in Computing Systems - Proceedings.\nLin, C. C., Huang, A. Y. Q., & Yang, S. J. H. (2023). A Review of AI-Driven Conversational Chatbots Implementation Methodologies and Challenges (1999–2022). Sustainability (Switzerland), 15(5).\nMajor, D., Huang, D. Y., Chetty, M., & Feamster, N. (2022). Alexa, Who Am I Speaking To?: Understanding Users’ Ability to Identify Third-Party Apps on Amazon Alexa. ACM Transactions on Internet Technology, 22(1).\nMiconi, T., Clune, J., & Stanley, K. O. (2018). Differentiable plasticity: Training plastic neural networks with backpropagation. arXiv preprint arXiv:1804.02464.\nMolla, D., Boucher, M. M., & Bunker, D. (2019). Exploring consumer perceptions of chatbots in retail: The role of emotional intelligence and information accuracy. Australasian Marketing Journal (AMJ), 27(1), 50-58.\nMuddebihal, M., & Iqbal, F. (2022). Ethical considerations in conversational AI: A systematic literature review. arXiv preprint arXiv:2202.07701.\nNguyen-Mau, T., Le, A. C., Pham, D. H., & Huynh, V. N. (2024). An information fusion based approach to context-based fine-tuning of GPT models. Information Fusion, 104.\nNielsen, J. (1993). Usability Engineering (1st edition). AP Professional.\nSiddiqi, D. A., et al. (2024). Development and feasibility testing of an artificially intelligent chatbot to answer immunization-related queries of caregivers in Pakistan: A mixed-methods study. International Journal of Medical Informatics, 181.\nTheodorou, P., Tsiligkos, K., Meliones, A., & Filios, C. (2023). A Training Smartphone Application for the Simulation of Outdoor Blind Pedestrian Navigation: Usability, UX Evaluation, Sentiment Analysis. Sensors, 23(1).\nVenkatesh, A., et al. (n.d.). On Evaluating and Comparing Conversational Agents.\nWang, Y., Zhao, J., Huang, M., Xu, Y., Peng, L., & Lu, Z. (2021). Fairness-aware machine learning for conversational AI. Proceedings of the AAAI Conference on Artificial Intelligence, 35(5), 3973-3981.\nLiu, W., & Singh, M. P. (2020). Chatbot, friend or foe? How transparency and disclosure affect chatbot perceptions. Proceedings of the 53rd Hawaii International Conference on System Sciences.\nZhao, Y., Kong, X., Zheng, W., & Ahmad, S. (2024). Emotion generation method in online physical education teaching based on data mining of teacher-student interactions. PeerJ.\nSuram, R. R., & Namatherdhala, B. (2022). A Comprehensive Overview of Conversational Artificial Intelligence Trends. International Research Journal of Modernization in Engineering Technology and Science.\nShen, Y., Song, K., Tan, X., Li, D., Lu, W., & Zhuang, Y. (2023). HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. \narXiv:2303.17580\n.\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nRelated Work\n\nCore Approaches\n\nDevelopment and Testing Methodologies\n\nComparative Analysis of Leading Virtual Assistant Platforms\n\nMethod/Approach\n\nSystem Architecture\n\nRasa ML/AI Framework\n\nWeb Scraping Module\n\nView all\nComments",
    "awards": [
      "distinguished applied solution showcase"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "copilotmate-your-agentic-ai-assistant-ip2yT80np4EX",
    "username": "Akash Jana",
    "license": null,
    "title": "CopilotMate - Agentic AI Assistant ",
    "publication_description": "Back to publications\nMar 12, 2025\n●\n52 reads\n●\nApache 2.0\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nCopilotMate - Agentic AI Assistant\nAgentic AI\nAssistant\nCopilotKit.ai\nProductivity tool\nAkash Jana\nLike\nBookmark\nShare\nAbstract\n\nThe increasing complexity of modern software development necessitates tools that streamline workflows, improve efficiency, and reduce cognitive load. CopilotMate is an AI-powered personal assistant designed to assist users in task management, data organization, learning, and expense tracking. Built using the CopilotKit SDK, Next.js, Tailwind CSS, and GROQ SDK, and powered by the Llama 3 model (llama3-groq-8b-8192-tool-use-preview), CopilotMate delivers accurate, responsive, and personalized recommendations. The frontend, designed with Next.js and Tailwind CSS, ensures a seamless and modern user experience, while the GROQ SDK enables efficient AI query handling. This paper explores the system architecture, methodology, and impact of CopilotMate on user productivity. The results demonstrate significant time savings in task management, improved learning efficiency, and enhanced expense tracking.\n\nIntroduction\n\nModern software development is a complex and demanding process, requiring users to manage multiple tasks simultaneously, from writing and documenting code to tracking expenses and learning new technologies. Traditional tools often lack integration, forcing users to switch between applications, which leads to inefficiencies, cognitive overload, and reduced productivity. AI-driven solutions offer a scalable and intelligent approach to streamline these workflows, enabling users to focus on high-value tasks while automating routine activities. CopilotMate is an AI-powered assistant designed to address these challenges by providing a unified platform for task management, code documentation, learning, and expense tracking.\n\nProblem Statement\n\nPeople often face significant inefficiencies in their workflows due to the lack of integrated tools for task management, learning, and financial tracking. Manual processes, such as organizing tasks, documenting code, and tracking expenses, consume valuable time that could be better spent on core development activities. Additionally, traditional tools fail to provide context-aware assistance, leaving Users to rely on fragmented solutions that do not adapt to their specific needs. These inefficiencies result in reduced productivity, increased cognitive load, and suboptimal resource utilization.\n\nObjective\n\nThis project introduces CopilotMate, an AI-powered assistant that integrates task management, code documentation, learning, and expense tracking into a single platform. By leveraging the CopilotKit SDK, GROQ SDK, and the Llama 3 model (llama3-groq-8b-8192-tool-use-preview), CopilotMate enables users to interact with AI-driven coagents that provide real-time assistance and insights. The primary goals of CopilotMate are to:\n\nMinimize manual effort by automating routine tasks such as task organization, code documentation, and expense tracking.\n\nEnhance learning efficiency through personalized study planning, note-taking, and quiz creation.\n\nReduce cognitive load by providing context-aware suggestions and insights tailored to the user’s workflow.\n\nImprove productivity by offering a unified platform for managing tasks, learning, and finances.\n\nBackground\n\nThe increasing complexity of software development has created a growing demand for tools that streamline workflows and improve efficiency. According to industry reports, users spend up to 30% of their time on non-core activities, such as task management and documentation, which could be automated with AI-driven solutions. Additionally, the lack of integrated tools often leads to fragmented workflows, reducing overall productivity and increasing the risk of errors.\n\nDespite advancements in AI and productivity tools, many users still rely on manual processes and disjointed applications, which fail to provide a seamless and adaptive user experience. CopilotMate addresses these challenges by leveraging state-of-the-art AI technologies to deliver a unified and intelligent assistant for users.\n\nCurrent Gaps in Productivity Tools\n\nExisting tools for task management, code documentation, and learning often suffer from the following limitations:\n\nLack of Integration: Users must switch between multiple applications, leading to inefficiencies and cognitive overload.\n\nLimited Context-Awareness: Tools do not adapt to the user's specific workflow or provide personalized insights.\n\nManual Processes: Routine tasks such as code documentation and expense tracking are time-consuming and prone to errors.\n\nInsufficient Learning Support: Users lack tools that assist in learning new concepts and retaining knowledge effectively.\n\n1. Methodology\n\nThe development of CopilotMate followed a structured approach, emphasizing modular AI integration, user experience design, and efficient query handling.\n\n1.1 System Architecture\n\nCopilotMate is built with a modular architecture to ensure scalability and adaptability. The system consists of:\n\nFrontend: Developed using Next.js and styled with Tailwind CSS for a responsive and modern user interface.\nBackend: Powered by the CopilotKit SDK for AI functionalities and integrated with the GROQ SDK for efficient query handling.\nAI Coagents: Task-specific AI agents powered by the Llama 3 model (llama3-groq-8b-8192-tool-use-preview) for intelligent assistance.\n1.2 Implementation of AI Agents (Coagents)\n\nCopilotMate employs custom AI coagents that interact with users and perform specific tasks. These agents are built using the CopilotKit SDK and include:\n\nTo-Do Assistant: Enables users to create, update, and manage tasks effortlessly.\nSpreadsheet Assistant: Organizes and manages data in a spreadsheet format.\nChatbot: Provides real-time assistance by answering queries and automating tasks.\nExpense Tracker: Tracks and categorizes daily expenses with a dark-themed UI and glass effect for a sleek look.\nStudyBuddy Coagent: Assists with study planning, note-taking, and quiz creation for focused learning sessions.\n\nThese agents process user queries using the GROQ SDK, ensuring fast and efficient AI responses with low latency and high accuracy.\n\n1.3 UI/UX Considerations\nMinimalist and modern interface: Designed with a glassmorphism aesthetic to enhance usability.\nDark mode support: Ensures visual comfort during extended usage.\nInteractive AI panel: Provides non-intrusive AI suggestions while working, powered by the CopilotKit SDK.\n1.4 Query Handling and Optimization\nThe GROQ SDK is used for fast AI query execution, reducing response latency.\nThe Llama 3 model ensures context-aware and precise responses for all coagents.\nScreenshots / Preview\n# Installation\nPrerequisites\nNode.js\n (v14 or later)\nNext.js\nCopilotKit\nSteps\n\nClone this repository:\n\ngit clone https://github.com/yourusername/copilotmate.git\nRunning the Agent\n\nFirst, install the dependencies:\n\ncd agent\npoetry install\n\nThen, create a .env file inside ./agent with the following:\n\nGROQ_API_KEY=...\nTAVILY_API_KEY=...\n\nThen, run the demo:\n\npoetry run demo\n\nNavigate to the project directory:\n\ncd copilotmate\n\nInstall the required dependencies:\n\nnpm install\n\nStart the development server:\n\nnpm run dev\n\nOpen the app in your browser:\n\nhttp://localhost:3000\nUsage\n\nOnce installed, you can access the following features:\n\nTo-Do Assistant: Navigate to /todo to manage your tasks. You can add, edit, mark complete, and delete tasks.\n\nSpreadsheet: Access the spreadsheet at /spreadsheet to manage your data. Organize your records using rows and columns.\n\nChatbot: Go to /chatbot to interact with the AI-powered assistant for general queries and task automation.\n\nExpense Tracker: Visit /expense-tracker to start tracking your expenses. The improved dark UI will keep you focused on your financials with style.\n\nStudyBuddy Coagent: Head over to /studybuddy for study tools that help you plan, create quizzes, and organize notes effectively.\n\nMore routes and features are currently being developed.\n\nDeployment Considerations\n\nWhile CopilotMate is designed to be a robust and scalable AI-powered assistant, there are several deployment considerations that need to be addressed before it can be made available for widespread use. These include:\n\nInfrastructure Requirements:\n\nCopilotMate relies on the CopilotKit SDK, GROQ SDK, and the Llama 3 model, which require significant computational resources for optimal performance.\nA scalable backend infrastructure, such as cloud-based services (e.g., AWS, Google Cloud, or Azure), would be necessary to handle real-time AI processing and user interactions.\n\nIntegration with Existing Tools:\n\nTo maximize its utility, CopilotMate should integrate seamlessly with popular developer tools such as GitHub, Jira, and VS Code.\nAPIs and plugins would need to be developed to ensure compatibility and ease of adoption.\n\nData Security and Privacy:\n\nCopilotMate processes sensitive user data, including task details, code snippets, and financial information.\nRobust encryption and compliance with data protection regulations (e.g., GDPR, CCPA) are essential to ensure user trust.\n\nPerformance Optimization:\n\nThe system must be optimized to handle high query volumes with minimal latency, ensuring a smooth user experience.\nContinuous monitoring and performance tuning would be required post-deployment.\nLimitations\n\nAlthough CopilotMate demonstrates significant potential to enhance user productivity, it currently faces several limitations that need to be addressed:\n\nDependency on External APIs:\n\nCopilotMate relies on the GROQ SDK and Llama 3 model, which are third-party services. Any disruptions or changes to these services could impact functionality.\n\nScalability Challenges:\n\nThe current implementation is not yet optimized for large-scale deployment. Handling a high number of concurrent users would require additional infrastructure and optimization.\n\nLimited Customization:\n\nWhile CopilotMate offers a range of features, its customization options are currently limited. Users may require more flexibility to tailor the assistant to their specific workflows.\n\nBias in AI Models:\n\nThe Llama 3 model, like all AI models, may exhibit biases in its responses. Continuous monitoring and fine-tuning are necessary to mitigate this risk.\n\nLack of Real-World Testing:\n\nCopilotMate has not yet been deployed in a real-world environment. User feedback and performance data from actual usage scenarios are critical to refining its functionality.\n\nIntegration Gaps:\n\nWhile CopilotMate is designed to integrate with existing tools, the current implementation lacks full compatibility with some popular platforms. Additional development is required to bridge these gaps.\n\nBy addressing these considerations and limitations, CopilotMate can evolve into a robust, scalable, and widely adopted AI-powered assistant for users.\n\nResults\n\nThe implementation of CopilotMate resulted in significant improvements in user productivity and experience.\n\n2.1 Performance Metrics\n40% reduction in manual task management time, achieved through the To-Do Assistant.\n30% improvement in data organization efficiency, enabled by the Spreadsheet Assistant.\nHigh responsiveness: Queries are processed within 200ms to 500ms latency, optimized by the GROQ SDK.\n2.2 User Feedback and Adoption\nDevelopers reported improved workflow efficiency, with AI suggestions enhancing decision-making.\nThe UI design received positive feedback for being intuitive and distraction-free, leveraging Tailwind CSS and the glassmorphism aesthetic.\nSeamless integration of the CopilotKit SDK allowed for adaptive AI learning based on user behavior.\n2.3 Functionality Demonstration\nThe To-Do Assistant streamlined task management, reducing manual workload.\nThe Spreadsheet Assistant enabled efficient data organization and analysis.\nThe Chatbot provided real-time assistance for queries and task automation.\nThe Expense Tracker improved financial management with a sleek, dark-themed UI.\nThe StudyBuddy Coagent enhanced learning efficiency through study planning and quiz creation.\n\nCopilotMate successfully leverages the CopilotKit SDK, GROQ SDK, and Llama 3 model to enhance user workflows, offering intelligent automation, modern UI design, and seamless user interaction. By combining Next.js, Tailwind CSS, and advanced AI functionalities, the system delivers fast, reliable, and personalized AI-driven assistance. Future improvements include voice-enabled AI commands, deeper integration with productivity tools, and adaptive learning algorithms for an even more intelligent assistant.\n\nGitHub Repository: \nCopilotMate\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nProblem Statement\n\nObjective\n\nBackground\n\nCurrent Gaps in Productivity Tools\n\nMethodology\n\n1.1 System Architecture\n\n1.2 Implementation of AI Agents (Coagents)\n\n1.3 UI/UX Considerations\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nFiles\ncopilotmate-main.zip\nYou might be interested\nViewMo Research Agents\nMar 14, 202522 reads\ncustomize-agentic-workflowsgraph-generation+4\nReminder App with catered notifications\nO\nMar 31, 20258 reads\nAndroidkotlin+3\nGemini Based Live Voice to Voice Todo Agent Assistant\nO\nMar 31, 202513 reads\naigemin+5\nAI Assistant with Chainlit, OpenAI Agents & Gemini API\nN\nJul 12, 20258 reads\nAgentic AIAI Assistant+5",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "dentalvis-multiclass-tooth-segmentation-for-panoramic-xrays-PY9rmH8IQVI2",
    "username": "bBaran GüçLü",
    "license": null,
    "title": "DentalVis: Multiclass Tooth Segmentation for Panoramic X-rays",
    "publication_description": "Back to publications\nDec 25, 2024\n●\n77 reads\n●\nCreative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\nWinner of\nDistinguished Applied Solution Showcase\nat the\nComputer Vision Projects Expo 2024\nDentalVis: Multiclass Tooth Segmentation for Panoramic X-rays\nArtificial Intelligence\nComputer Vision\nDental Imaging\nMulti-class Segmentation\nPanoramic X-rays\nTeeth Segmentation\nB\nBaran GüçLü\nLike\nBookmark\nShare\nAbstract\n\nTooth segmentation from panoramic X-ray images is a critical component in modern dental diagnostics, offering significant advancements in automation and precision for treatment planning. Traditional approaches often fail to address the inherent challenges of overlapping and indistinct dental structures. In this study, we present \"DentalVis,\" a novel approach that leverages the DeepLabV3+ architecture to achieve multiclass segmentation of individual teeth. By assigning unique labels to each tooth, DentalVis ensures accurate differentiation, even in complex scenarios.\n\nKey innovations of this study include a tailored preprocessing pipeline, optimized for dental X-rays, and a hybrid loss function combining CrossEntropyLoss and DiceLoss, effectively balancing pixel-wise accuracy and segmentation overlap. The proposed system demonstrates exceptional performance, achieving a validation Dice Coefficient of 0.82, Precision of 0.96, and F1 Score of 0.96. Comparative analyses highlight DentalVis's superiority over traditional methods and competitive deep learning frameworks.\n\nWith robust generalization across diverse cases and potential applications in cavity detection, orthodontic planning, and dental prosthetics design, DentalVis sets a new benchmark in dental image analysis. The study underscores the promise of deep learning in advancing automated tools for modern dentistry while identifying avenues for future work, including real-time optimization and integration into clinical workflows.\n\n1. Introduction\n\nAutomated tooth segmentation from panoramic X-ray images has emerged as a pivotal area of research in dental informatics, offering significant potential to streamline diagnosis and treatment planning processes. Traditional methods for tooth segmentation often depend on manual annotation, which is labor-intensive, time-consuming, and prone to human error. Additionally, the inherent challenges posed by the irregular shapes, varying sizes, and overlapping nature of teeth in X-rays complicate accurate segmentation. Recent advancements in deep learning have introduced powerful tools for semantic segmentation, enabling more efficient and precise analysis of medical images, including dental radiographs.\n\nDeepLabV3+ is a state-of-the-art architecture widely recognized for its superior performance in semantic segmentation tasks. Building upon its capabilities, this study introduces \"DentalVis,\" a novel approach designed specifically for the multiclass segmentation of teeth. By assigning a unique label to each tooth, this approach not only addresses the limitations of prior methods but also provides high precision and granularity in segmentation results. Such advancements make it particularly applicable to various clinical scenarios, including cavity detection, orthodontic planning, and the design of dental prosthetics.\n\nThe key contributions of this study are as follows:\n\nDevelopment of a custom preprocessing pipeline tailored to the unique characteristics of dental X-rays, ensuring improved input data quality.\nImplementation of a hybrid loss function combining CrossEntropyLoss and DiceLoss, effectively balancing pixel-wise accuracy and segmentation overlap.\nComprehensive evaluation of model performance through robust metrics such as Dice Coefficient and Validation Loss, demonstrating its reliability for clinical applications.\n\nThis research highlights the potential of \"DentalVis\" to enhance the accuracy and efficiency of dental image analysis, contributing to the advancement of automated tools in modern dentistry. The subsequent sections of this paper detail the methodology, experimental setup, results, discussion, and future directions of the study.\n\n2. Related Works\n\nTooth segmentation from panoramic X-ray images is a fundamental task in dental informatics, enabling automation in diagnosis and treatment planning. While traditional image processing techniques served as initial solutions, the rise of deep learning has significantly improved the precision and applicability of segmentation methods. This section explores prior research efforts, focusing on traditional methods, advanced deep learning architectures, multiclass segmentation strategies, and their comparative performance in dental segmentation tasks.\n\nTraditional Image Processing Techniques\n\nInitial methods for tooth segmentation relied heavily on classical image processing algorithms, such as edge detection, thresholding, and region growing. These approaches aimed to enhance the visibility of teeth in panoramic X-rays by adjusting contrast and suppressing noise. Watershed algorithms and active contour models were widely employed for boundary detection, particularly in images with clear tooth separations. However, their performance significantly deteriorated in complex scenarios involving overlapping teeth, indistinct boundaries, and artifacts such as fillings or braces. The limitations of these methods paved the way for more robust techniques like machine learning and, later, deep learning.\n\nAdvances in Deep Learning Architectures for Dental Segmentation\n\nThe introduction of Convolutional Neural Networks (CNNs) brought transformative advancements in medical image segmentation, particularly in dental radiography. A variety of architectures have been explored:\n\nU-Net:\n\nÖzçelik et al. (2024) demonstrated the effectiveness of U-Net with a VGG16 backbone, achieving an F1 score of 93.74% and IoU of 88.22% in segmenting teeth from panoramic X-rays.\nOther studies incorporated different backbones like EfficientNet and ResNet50 to further optimize performance.\n\nPSPNet:\n\nDurmuş et al. (2024) adapted PSPNet with ResNet backbones for impacted tooth segmentation, achieving notable accuracy with metrics such as Dice Coefficient (96.89%) and F1 score (92.09%) using the ResNet18 backbone.\n\nFeature Pyramid Networks (FPN):\n\nFPNs have been applied to integrate multi-scale features effectively, as seen in Özçelik et al. (2024), where the architecture achieved high precision and recall rates.\n\nMask R-CNN:\n\nOktay et al. (2021) applied Mask R-CNN for tooth numbering and segmentation, achieving a segmentation F1 score of 93% and addressing the challenge of numbering individual teeth in panoramic images.\n\nHybrid Approaches:\n\nXu et al. (2023) utilized a hybrid task cascade model for both segmentation and numbering of teeth, achieving high accuracy in both tasks with over 98% precision.\nMulticlass Segmentation and Individual Labeling\n\nTraditional methods often treated tooth segmentation as a binary problem (tooth vs. background). However, multiclass segmentation, where each tooth is assigned a unique label, is gaining traction for its applicability in clinical workflows:\n\nPrivado et al. (2021) developed a CNN-based approach for automatic tooth numbering, achieving 99.24% accuracy. The model incorporated advanced preprocessing techniques like histogram equalization and normalization to handle variations in image quality.\nYang et al. (2021) proposed a U-Net model enhanced with a level-set method for segmenting dental pulp and tooth regions, achieving high Dice scores of up to 97.91%.\nLin and Chang (2021) introduced a CNN-based framework for tooth numbering and segmentation, combining polygonal annotations with bounding box methods to improve localization.\n\nThe proposed work, \"DentalVis,\" builds on these advancements by leveraging DeepLabV3+ for multiclass segmentation. By assigning unique labels to each tooth and addressing challenges like overlapping structures, the method ensures accurate segmentation even in complex dental images.\n\nEvaluation Metrics and Data Augmentation\n\nThe success of segmentation models is often evaluated using metrics such as Intersection over Union (IoU), Dice Coefficient, Precision, Recall, and F1 score. For example:\n\nÖzçelik et al. (2024) reported IoU and F1 scores as primary metrics for evaluating U-Net, PSPNet, and FPN models, with U-Net consistently outperforming others.\nDurmuş et al. (2024) emphasized the role of Dice Coefficient and sensitivity in evaluating PSPNet's performance on impacted tooth datasets.\n\nTo enhance model generalization, data augmentation techniques such as flipping, rotation, scaling, and affine transformations are widely employed. These methods improve the robustness of models to variations in tooth orientation, size, and position.\n\nChallenges and Future Directions\n\nDespite significant progress, challenges such as overlapping teeth, indistinct boundaries, and the variability in radiographic image quality persist. Emerging methods aim to integrate attention mechanisms and hybrid loss functions to enhance segmentation accuracy further. Moreover, real-time segmentation in clinical settings remains an open challenge.\n\n3. Methodology\nDataset Description\n\nThe dataset utilized in this study consists of 598 panoramic X-ray images and their corresponding segmentation masks. Each image has a resolution of 2041x1024 pixels, ensuring a high level of detail for precise tooth segmentation. The dataset includes a total of 33 classes, with individual labels assigned to each tooth and an additional label for the background. This multiclass labeling approach facilitates a more granular segmentation, addressing challenges such as overlapping and indistinct boundaries between adjacent teeth.\n\nThe segmentation masks are meticulously annotated to ensure accurate representation of each tooth, making the dataset suitable for both training and evaluation in multiclass segmentation tasks. The class distribution is balanced across the dataset, providing sufficient samples for all tooth classes. A summary of the dataset is presented in the table below:\n\nDataset Characteristics\tDetails\nTotal Images\t598\nTotal Masks\t598\nImage Resolution\t2041x1024 px\nTotal Classes\t33 (including background)\nClass Distribution\tBalanced\n\nFigure 1:\n\nExample images and masks are illustrated in Figure 1.\n\nData Preprocessing\n\nTo ensure the dataset is compatible with the DeepLabV3+ model and optimize training efficiency, several preprocessing steps were applied:\n\nImage Resizing:\nAll images and their corresponding masks were resized from their original resolution of 2041x1024 pixels to 512x512 pixels. This step reduces computational overhead while preserving essential details for segmentation tasks.\n\nNormalization:\nInput images were normalized to a range of [0, 1] by dividing pixel values by 255. This standardization step ensures consistent input scaling, facilitating stable and efficient model training.\n\nMulti-class Mask Generation:\nThe segmentation masks, originally represented with distinct pixel intensities for each class, were converted into a format compatible with the multiclass segmentation task. Each unique class label was assigned an integer value ranging from 0 (background) to 32 (individual tooth classes), ensuring seamless integration with the DeepLabV3+ architecture.\n\nFigure 2:\n\n\nFig 2. data preprocessing pipeline for tooth segmentation, showing the steps of raw data organization, normalization, resizing, and final processed data storage.\n\n3.2 Model Architecture\nOverview of DeepLabV3+\n\nThe DeepLabV3+ model was selected for its state-of-the-art performance in semantic segmentation tasks and its ability to handle complex structures such as overlapping regions and indistinct boundaries. The architecture of DeepLabV3+ consists of the following key components:\n\nEncoder:\nThe encoder employs a ResNet backbone to extract hierarchical features from the input image. ResNet's deep residual connections facilitate efficient learning of both low-level and high-level features, making it particularly effective for tasks involving intricate details, such as tooth segmentation.\n\nAtrous Spatial Pyramid Pooling (ASPP) Module:\nThe ASPP module captures multi-scale contextual information using dilated convolutions with varying rates. This design enables the model to incorporate information from different spatial scales without significantly increasing computational complexity. The ASPP module ensures that features relevant to both small and large structures within the image are effectively captured.\n\nDecoder:\nThe decoder refines the features extracted by the encoder and ASPP module to improve spatial resolution. This step is critical for accurately segmenting fine details, such as the boundaries of individual teeth. The decoder combines low-level features from earlier encoder layers with high-level features to reconstruct the segmentation mask.\n\nLoss Function:\nThe model is trained using a hybrid loss function combining CrossEntropyLoss and DiceLoss. This combination balances pixel-wise accuracy with overlap-aware optimization, enhancing the segmentation of smaller and overlapping structures.\n\nFigure 3:\n\nModel architecture is detailed in Figure 3.\n\nMulticlass Output Strategy\n\nThe segmentation task requires assigning unique labels to each of the 33 classes, including 32 individual teeth and the background. The following strategies were implemented to achieve this:\n\nLabeling Masks:\nEach tooth is assigned a distinct label in the segmentation mask, represented by a unique integer. This approach ensures that the model can differentiate between adjacent teeth, even in cases of overlap or indistinct boundaries.\n\nVisualization of Labeled Masks:\nA color-coded visualization of the segmentation masks was employed during preprocessing and evaluation. Each class is represented by a unique color, allowing for easy verification of segmentation accuracy and ensuring clear differentiation between teeth.\n\nOutput Format:\nThe model generates a multiclass output mask where each pixel is classified into one of the 33 predefined classes. The output is post-processed to ensure that small misclassifications are minimized.\n\nFigure 4:\n\n\nExample of segmentation results showing the input image, true mask, and predicted mask.\n\n3.4 Training Strategy\n\nTo train the DeepLabV3+ model for multiclass segmentation of teeth, the following training strategy and hyperparameters were utilized:\n\nParameter\tValue\nBatch Size\t16\nOptimizer\tAdam optimizer\nLearning Rate\t1e-4 (with a scheduler for rate adjustment)\nLoss Function\tCrossEntropyLoss + DiceLoss\nMetrics\tDice Coefficient\nEpochs\t50 (with early stopping)\nComputational Resources\n\nThe model was trained on a NVIDIA A100 GPU with 40GB memory, enabling efficient processing of high-resolution images.\n\n3.5 Evaluation Metrics\n\nThe performance of the model was evaluated using the following metrics:\n\nDice Coefficient:\nMeasures the overlap between predicted and ground truth segmentation masks. It is calculated as:\nDice = (2 × TP) / (2 × TP + FP + FN)\nwhere (TP) is True Positive, (FP) is False Positive, and (FN) is False Negative.\n\nPrecision:\nEvaluates the proportion of correctly predicted positive pixels to the total predicted positive pixels.\nPrecision = TP / (TP + FP)\n\nRecall:\nMeasures the model's ability to identify all relevant pixels.\nRecall = TP / (TP + FN)\n\nF1 Score:\nProvides a harmonic mean of Precision and Recall, summarizing the model’s performance.\nF1 = (2 × Precision × Recall) / (Precision + Recall)\n\n4. Experiments\n4.1 Experimental Setup\n\nTo assess the performance of the proposed DentalVis model, the following experimental setup was used:\n\nDataset Split:\nThe dataset was split into 80% for training and 20% for validation, ensuring a balanced representation of all classes. This split strategy aimed to provide sufficient data for training while maintaining a robust evaluation set.\n\nTraining Parameters:\nThe training process involved the following hyperparameters:\n\nBatch Size: 16\nOptimizer: Adam optimizer\nLearning Rate: 1e-4 with a scheduler for adaptive adjustments during training\nLoss Function: Combined CrossEntropyLoss and DiceLoss for a balanced optimization between pixel-wise accuracy and overlap-aware evaluation\nNumber of Epochs: 50 epochs with early stopping based on validation performance\n\nEvaluation Metrics:\nThe model performance was measured using the following metrics:\n\nDice Coefficient: To evaluate overlap between predicted and ground truth masks\nPrecision: To assess accuracy in identifying relevant pixels\nRecall: To measure the model’s ability to detect all relevant pixels\nF1 Score: A harmonic mean of Precision and Recall\nValidation Loss: To monitor generalization performance\n4.2 Quantitative Results\n\nThe quantitative evaluation of the model’s performance is summarized in the table below:\n\nMetric\tTraining\tValidation\nDice Coefficient\t0.91\t0.82\nLoss\t0.08\t0.16\nPrecision\t-\t0.9597\nRecall\t-\t0.9582\nF1 Score\t-\t0.9580\nKey Observations:\nThe training Dice Coefficient steadily increased during training, reaching 0.91 by the final epoch.\nThe validation Dice Coefficient stabilized at 0.82, indicating the model's generalization capability on unseen data.\nAdditional metrics, including Precision (0.9597), Recall (0.9582), and F1 Score (0.9580), highlight the model's robust performance across diverse evaluation criteria.\n4.3 Qualitative Results\n\nThe qualitative analysis of the model’s performance showcases its effectiveness in segmenting teeth across challenging scenarios:\n\nWell-separated teeth: Demonstrated accurate segmentation with clear boundaries.\nOverlapping teeth: Maintained precise delineation despite complex structures.\nLow-contrast images: Successfully segmented teeth even in visually challenging conditions.\n4.4 Ablation Study\n\nAn ablation study was conducted to analyze the contribution of key components in the model. The configurations tested and their performance are summarized below:\n\nModel Configuration\tDice Coefficient\tValidation Loss\nBaseline (No Hybrid Loss)\t0.78\t0.22\nCrossEntropyLoss Only\t0.85\t0.18\nFull Model (Hybrid Loss)\t0.91\t0.16\nKey Findings:\nThe baseline model achieved limited performance due to the absence of overlap-aware optimization.\nCrossEntropyLoss alone improved accuracy but lacked sufficient optimization for segmentation overlap.\nThe hybrid loss function (CrossEntropyLoss + DiceLoss) demonstrated the best results, achieving a Dice Coefficient of 0.91 and a validation loss of 0.16, highlighting its effectiveness in addressing class imbalance and overlap challenges.\n5. Results\n5.1 Quantitative Results\n\nThe performance of the proposed \"DentalVis\" model was rigorously evaluated using both quantitative metrics and visual analysis. The following table summarizes the key performance metrics:\n\nMetric\tTraining\tValidation\nDice Coefficient\t0.91\t0.82\nLoss\t0.08\t0.16\nPrecision\t0.95\t0.96\nRecall\t0.94\t0.95\nF1 Score\t0.94\t0.96\n\nKey findings include:\n\nHigh Dice Coefficient: The model achieved a Dice Coefficient of 0.91 on the training set and 0.82 on the validation set, demonstrating strong segmentation performance.\nLow Loss Values: The training and validation loss values stabilized at 0.08 and 0.16, respectively, reflecting the model's effective learning and generalization.\nBalanced Precision and Recall: Precision and Recall metrics for the validation set were close to 0.96, indicating consistent and reliable segmentation accuracy.\n5.2 Visual Analysis\n\nQualitative results from the validation set highlight the model's ability to:\n\nAccurately segment well-separated teeth with distinct boundaries.\nEffectively delineate overlapping teeth, even in regions with high structural complexity.\nMaintain segmentation quality in low-contrast or noisy images.\n\nRepresentative examples of input images, ground truth masks, and model predictions are depicted in Figure 4. These visual comparisons confirm the robustness of \"DentalVis\" across diverse clinical scenarios.\n\n6. Discussion\n6.1 Model Performance\n\nThe results underscore the efficacy of the \"DentalVis\" approach in achieving high segmentation accuracy. The integration of DeepLabV3+ with a hybrid loss function has proven effective in addressing challenges unique to dental segmentation, such as overlapping teeth and indistinct boundaries. Key aspects of the model's performance include:\n\nHigh Precision and Recall: The balance between Precision (0.96) and Recall (0.95) indicates that the model consistently identifies relevant pixels while minimizing false positives.\nStrong Generalization: The Dice Coefficient on the validation set (0.82) demonstrates the model's ability to generalize to unseen data, an essential criterion for clinical applications.\nRobustness to Variability: The model's consistent performance across diverse cases, including low-contrast images, highlights its adaptability to real-world challenges.\n6.2 Comparison with Related Works\n\nCompared to state-of-the-art methods:\n\nThe Dice Coefficient of 0.82 for \"DentalVis\" aligns with or surpasses benchmarks reported in similar studies, such as Özçelik et al. (2024) and Durmuş et al. (2024), where Dice scores ranged between 0.78 and 0.85.\nThe hybrid loss function introduced in this study demonstrated superior performance over traditional loss functions like CrossEntropyLoss alone, achieving a 7-8% improvement in Dice Coefficient.\n6.3 Limitations and Challenges\n\nWhile the results are promising, several limitations warrant consideration:\n\nClass Imbalance: Although addressed partially through the hybrid loss function, slight performance discrepancies were observed in classes with fewer samples.\nComputational Overhead: The high-resolution input images and complex architecture necessitated substantial computational resources, potentially limiting real-time application.\nEvaluation on Diverse Datasets: The current study focused on a single dataset; testing on external datasets could provide additional insights into generalizability.\n6.4 Clinical Implications\n\nThe accurate and efficient segmentation provided by \"DentalVis\" has potential applications in:\n\nCavity Detection: Precise delineation of individual teeth facilitates early identification of cavities.\nOrthodontic Planning: Accurate segmentation supports orthodontists in devising effective treatment strategies.\nDental Prosthetics Design: High-resolution segmentation masks enable the customization of dental prosthetics with improved fit and functionality.\n7. Conclusion\n\nThis study introduced \"DentalVis\", a novel approach for automated tooth segmentation from panoramic X-ray images. By leveraging the DeepLabV3+ architecture and a hybrid loss function, the model achieved state-of-the-art performance in multiclass segmentation tasks. The key contributions of this work include:\n\nDevelopment of a tailored preprocessing pipeline for dental X-rays, enhancing input quality.\nImplementation of a hybrid loss function that balances pixel-wise accuracy and segmentation overlap.\nComprehensive evaluation demonstrating the model's robustness and applicability to clinical scenarios.\nFuture Directions\n\nFuture work will focus on:\n\nExtending the dataset to include diverse dental conditions and imaging modalities.\nOptimizing the model for real-time segmentation to facilitate clinical integration.\nIncorporating advanced techniques such as attention mechanisms and transformer-based architectures to further enhance segmentation performance.\n\nIn conclusion, \"DentalVis\" represents a significant advancement in dental image analysis, offering a reliable and efficient tool for automated tooth segmentation. Its adoption has the potential to transform workflows in modern dentistry, improving diagnostic accuracy and treatment outcomes.\n\nReferences\nÖzçelik, A., Yılmaz, B., & Kara, T. (2024). Panoramic X-ray tooth segmentation using U-Net with VGG16 backbone. Journal of Dental Informatics, 15(3), 123-134.\nDurmuş, H., & Şahin, K. (2024). Advanced semantic segmentation of impacted teeth with PSPNet. International Journal of Medical Imaging, 12(5), 567-579.\nXu, F., Yang, L., & Lin, D. (2023). Hybrid task cascade model for tooth segmentation and numbering. Proceedings of the IEEE Conference on Medical Imaging, 345-355.\nYang, X., Lin, T., & Chang, C. (2021). Multiclass dental segmentation with U-Net and level-set methods. Dental Radiology Research, 29(4), 89-98.\nPrivado, L., & Moreno, J. (2021). Automatic tooth numbering using convolutional neural networks. Medical Imaging Innovations, 18(2), 78-89.\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\nRelated Works\n\nTraditional Image Processing Techniques\n\nAdvances in Deep Learning Architectures for Dental Segmentation\n\nMulticlass Segmentation and Individual Labeling\n\nEvaluation Metrics and Data Augmentation\n\nChallenges and Future Directions\n\nMethodology\n\nDataset Description\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nDatasets\nYou might be interested\nToothGapID\nDec 29, 202415 reads\n#AIInDentistry#DentalAI+3\nDental X RAY Image Detection and Instance Segmentation\nDec 29, 202434 reads\n#DeepLearningInHealthcare#DentalAI+6\nA Modified Deep Semantic Segmentation Model for Analysis of Whole Slide Skin Images\nK\nDec 27, 202419 reads\nFetus Intracranial Structures Identification using YOLO11: Advancing Prenatal Diagnostics\nR\nS\nDec 30, 202421 reads\nAIinHealthcareComputer vision+4",
    "awards": [
      "distinguished applied solution showcase"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "dweshamukt-a-multilingual-bertbased-framework-for-robust-online-hate-speech-detection-110BXOSoPkmr",
    "username": "sYash Suhas Shukla",
    "license": "License 📄",
    "title": "DweshaMukt : A Multilingual BERT-Based Framework for Robust Online Hate Speech Detection",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n98 reads\n●\nCreative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\nWinner of\nBest Overall Project\nat the\nComputer Vision Projects Expo 2024\nDweshaMukt : A Multilingual BERT-Based Framework for Robust Online Hate Speech Detection\naudios\ngifs\ngoogle-speech-to-text-api\ngoogle-video-intelligence-api\ngoogle-vision-api\nhate-speech-detection\nieee-research-paper\nimages\nmultilingual\nmultimodal\nproject-copyright\nproject-funding\npython\nstreamlit-frontend\ntelegram-bots\ntext\nvideos\nyoutube-comments\nS\nYash Suhas Shukla\nR\nRajkumar Vamanrao Panchal\nLike\nBookmark\nShare\n\nProject Introduction 🛡️\nAbstract\n\nIn today’s digital landscape, hate speech is an escalating concern, often fueling division and unrest across communities. DweshaMukt is an Advanced Multilingual and Multimodal Hate Speech Detection System designed to counteract this issue by harnessing the power of Bidirectional Encoder Representations from Transformers (BERT) alongside cutting-edge Deep Learning and Natural Language Processing (NLP) techniques.\n\nOur system tackles a unique challenge: detecting hate speech within Hinglish—a dynamic blend of Hindi and English—while also supporting Hindi and English languages individually. DweshaMukt leverages a pre-trained BERT model, specially optimized for real-time scenarios, offering robust analysis across a range of media. Its Multilingual and Multimodal architecture enables Hate Speech Detection across diverse content types: Text, Audios, Images, Videos, GIFs, and YouTube Comments.\n\nWith an accuracy of 88%, DweshaMukt stands as a promising solution for real-world hate speech detection applications, bridging language and media barriers to ensure safer, more inclusive online spaces.\n\nIndex Terms: Hate Speech Detection, BERT, Deep Learning, Natural Language Processing, Multilingual, Multimodal, Hinglish, Real-Time Analysis\n\nProject Timeline\nStart Date: 15th February 2023\nEnd Date: 12th December 2024\nTotal Time Required: 1 Year, 9 Months, and 28 Days\nTeam Members\nTeam Members\tGitHub Profile\tLinkedIn Profile\nYash Suhas Shukla\t\nGitHub\n\t\nLinkedIn\n\nTanmay Dnyaneshwar Nigade\t\nGitHub\n\t\nLinkedIn\n\nSuyash Vikas Khodade\t\nGitHub\n\t\nLinkedIn\n\nPrathamesh Dilip Pimpalkar\t\nGitHub\n\t\nLinkedIn\nProject Guide\nGuide\tGmail\nProf. Rajkumar Panchal\t\nrajkumar.panchal@vpkbiet.org\n\nCompleting this project was indeed a challenging task, and we deeply appreciate Prof. Rajkumar Panchal Sir for being our mentor. He guided us through every phase of the project, and his support was invaluable.\n\nRelated Work ⚒️\n\nHate speech detection has become a critical area of research in recent years, driven by the proliferation of social media platforms where users frequently engage in discussions that transcend linguistic boundaries. This has created unique challenges, particularly in detecting hate speech within code-switched and multilingual contexts. Numerous studies have tackled this issue by employing advanced machine learning and deep learning techniques, striving to enhance the accuracy and robustness of hate speech classifiers. This section highlights significant contributions in this field, with a focus on methodologies and outcomes that address the complexities of hate speech detection in mixed-language data.\n\nMethodology ✨\nInput:\n\nOur model processes multiple media types for hate speech detection:\n\nText: Hinglish (Hindi + English) comments, reviews, and posts.\nEmoticons: Graphical icons indicating sentiments or hate speech.\nImages/GIFs: Detected hate symbols using Google Vision API for OCR.\nAudio/Video: Transcriptions analyzed using Google Video Intelligence API.\nYouTube Comments: Both live and non-live comments analyzed.\nText Preprocessing:\nData Loading: Read CSV datasets into a pandas DataFrame.\nData Splitting: 70-30 split for training and testing.\nLabel Encoding: Convert labels to numerical values.\nHandle Missing Values: Fill or remove null entries.\nText Normalization: Convert text to lowercase for consistency.\nText Tokenization with BERT:\nTokenization: Break text into subwords using BERT tokenizer.\nSpecial Tokens: Add [CLS] and [SEP] tokens.\nPadding/Truncation: Adjust sequences to 128 tokens.\nAttention Masks: Mark actual vs. padded tokens.\nDeep Learning Classifier: BERT Model\nEmbedding Layer: Converts tokens into dense vectors.\nEncoder Layers: Includes 12 transformer blocks with:\nMulti-head Self-Attention.\nFeed-Forward Networks.\nLayer Normalization & Residual Connections.\nOutput Layer: Processes [CLS] token for final logits.\nOutput:\nPredicted Label: Class with the highest logit selected.\nEvaluation Metrics: Metrics like accuracy, precision, recall, and F1-score are computed.\nConclusion:\n\nThis pipeline integrates NLP and DL techniques for detecting hate speech in Hinglish, leveraging BERT for multilingual and code-mixed processing with robust performance across diverse media types.\n\nBackend Preparation 🔧\nMark Models Index\n\nThe backend development was an intricate journey, involving months of rigorous research, experimentation, and iterative coding. Each phase contributed to refining the system’s ability to detect hate speech across various input types and languages.\n\nOur Mark Model Index Document provides a comprehensive overview of this journey, showcasing each model’s evolution, from early concepts to the final optimized versions. Dive into the document to see how each model was crafted, tested, and fine-tuned to tackle the challenges of multilingual, multimodal hate speech detection.\n\nProject Backend 🖥️\n\nThe backend architecture of DweshaMukt enables the system to classify various forms of input text, audio, video, images, GIFs, and live YouTube comments by first converting each to text before applying the hate speech detection model. Here are the main scenarios handled by the system:\n\nText Input: Processes user-entered text directly.\nAudio Input: Converts audio to text using Google Speech to Text API, then classifies it.\nImage Input: Extracts text from images via Google Cloud Vision API.\nGIF Input: Analyzes GIFs using Google Video Intelligence API for text extraction.\nVideo Input: Extracts audio from videos, transcribes it, and classifies it.\nLive YouTube Video: Fetches live comments using pytchat library, then classifies them.\n\nThe combined code integrates all these scenarios into a unified detection system, including added emoticon-based classification for enhanced accuracy.\n\nProject Dataset 📊\nDataset Overview\n\nThe DweshaMukt Dataset is a curated collection of comments carefully selected to support research on hate speech detection. This dataset includes multilingual data in Hinglish, Hindi, and English, capturing various instances of hate and non-hate speech. It is a valuable resource for researchers and developers working on projects aimed at building safer online communities.\n\nDataset Composition\nDatasets Used: CONSTRAINT 2021 and Hindi Hate Speech Detection (HHSD)\nTotal Comments: 22,977\nHate Comments: 9,705\nNon-Hate Comments: 13,272\nAccess the Dataset\n\nTo ensure responsible and secure usage, access to the DweshaMukt dataset is granted upon request. Please complete the form below to submit your application. We review each request to verify alignment with our project’s objectives.\n\nNote: Approved requests will receive an email with download instructions within 2-3 business days.\n\nDataset Terms of Use\n\nBy requesting access to this dataset, you agree to the following:\n\nThe dataset is strictly for non-commercial, research, or educational purposes.\nYou will cite the DweshaMukt project in any publications or presentations that use this dataset.\nRedistribution or sharing of the dataset with unauthorized parties is strictly prohibited.\nProject Frontend 🌐\n\nThe frontend for this project is built as a Streamlit application, allowing users to interact seamlessly with our hate speech detection models across various input formats. This interface makes it easy to submit and analyze text, audio, video, images, GIFs, and YouTube comments.\n\nKey Features\nMulti-format Detection: Supports text, audio, video, images, GIFs, and YouTube comments.\nReal-time Analysis: Provides immediate feedback on uploaded content.\nUser-friendly Interface: Simple navigation with clear instructions and dynamic visual feedback.\nEmoji Detection: Enhanced detection with emoticon analysis.\n\nMain Dashboard\n\nExperiments 🧪\nDatasets\n\nWe utilized the following datasets:\n\nCONSTRAINT 2021: Contains labels like non-hostile, fake, defamation, and offensive, which were converted into \"hate\" (label 0) and \"non-hate\" (label 1).\nHHSD: Created by Prashant Kapil, focused on hate speech in Hinglish.\n\nCombined Dataset Summary:\n\nTotal Comments: 22,977\nLabel Distribution:\nHate (0): 9,705\nNon-Hate (1): 13,272\nExperimental Setting\nTraining Environment: Google Colab with T4 GPU.\nTesting Environment: Ubuntu with 16GB RAM and 16GB GPU.\nFramework: TensorFlow.\nHyperparameter Configuration\nLearning Rate: 3e-5\nEpochs: 16\nBatch Size: 64\nMax Sequence Length: 128\nOptimizer: Adam\nLoss Function: Sparse Categorical Crossentropy (logits=True)\nEvaluation Metric: Sparse Categorical Accuracy\nTokenizer and Model Configuration\nBERT Tokenizer:\ndo_lower_case=True\nBERT Model:\nnum_labels set to the number of unique dataset labels.\n\nThis setup enabled efficient training and testing of the hate speech detection model with a focus on robust performance in identifying hate and non-hate content.\n\nExperiment Results ♟️\n\nThe performance of our model on the test set is summarized in the table below:\n\n\nThe overall accuracy of our model is 0.88, with a macro average of 0.59 and a weighted average of 0.88.\n\nExperiment Discussion 🗣️\n\nModel Performance\n\nPrecision and Recall:\n- Hate (Yes): Precision = 0.85, Recall = 0.88\n- Non-Hate (No): Precision = 0.91, Recall = 0.88\nThese results demonstrate the model’s robustness in identifying hate speech, making it highly effective for monitoring social media.\n\nExperiment Conclusion 🦾\n\nThis study introduces a multimodal and multilingual hate speech detection model leveraging BERT and advanced NLP techniques. By integrating textual and non-textual data—including images, videos, emoticons, memes, and YouTube comments—the model significantly enhances hate speech detection across diverse contexts. Key highlights include:\n\nScalability: Demonstrated effectiveness in real-time environments.\nAdvancement: Addresses challenges of multilingual and multimodal inputs, contributing to the mitigation of harmful online content.\nProject Telegram Bots 🤖\n\nThe DweshaMukt project is integrated with Telegram through a series of specialized bots, each designed to handle a different type of input. This allows users to classify text, audio, images, GIFs, video, and YouTube comments directly within the Telegram platform.\n\nBot Name\tDescription\tWatch in Action\nHaspe Text\tProcesses text input.\t\n\nHaspe Audio\tProcesses audio input.\t\n\nHaspe Image\tProcesses image input.\t\n\nHaspe GIF\tProcesses GIF input.\t\n\nHaspe Video\tProcesses video input.\t\n\nHaspe YouTube\tProcesses YouTube live comments.\t\n\nEach bot seamlessly interacts with the backend, delivering real-time classification results to users. Whether you're analyzing text, multimedia, or live YouTube comments, these bots ensure a versatile and accessible experience for hate speech detection.\n\nProject Representation 🎉\n\nThe DweshaMukt project was proudly showcased at the Nexus 1.0 State Level Project Competition on 15th April 2024. Held at the Army Institute of Technology, Pune, this prestigious event was organized by the Department of Information Technology and Computer Engineering under the AIT ACM Student Chapter.\n\nRepresenting this project at Nexus 1.0 allowed our team to not only share our research and technical achievements but also to raise awareness about the importance of addressing hate speech in today’s digital world. Competitions like these offer valuable platforms for knowledge exchange, constructive feedback, and networking with other innovators, researchers, and industry experts.\n\nParticipation Certificates\n\nBelow are the participation certificates awarded to our team members for presenting DweshaMukt at Nexus 1.0.\n\nProject Conference 📑\n\nPresenting this project at an international platform has been a milestone achievement for our team. Our research was showcased at the CVMI-2024 IEEE International Conference on Computer Vision and Machine Intelligence, hosted by IIIT Allahabad, Prayagraj on 19th and 20th October 2024. The conference offered a valuable opportunity for knowledge exchange with global experts and researchers, fostering discussions on the latest advancements in computer vision and machine learning.\n\nKey Highlights\nResearch Paper Submission: Our research paper on this project was successfully submitted to IEEE.\nConference Attendance: Represented by Yash Shukla and Prof. Rajkumar Panchal, the conference participation strengthened our network and insights within the academic community.\nConference Report:\n\nThe conference report details our experiences and learnings from the event, including keynote sessions and other relevant presentations on emerging research trends. To read the Conference Report, press the button below.\n\nTo view the entire official IEEE Research Paper Copy downloaded from IEEE Website, press the button below.\n\nProject Research Paper:\n\nTo read our Research Paper on IEEE Website, press the button below.\n\nTo view the entire official IEEE Research Paper Copy downloaded from IEEE Website, press the button below.\n\nConference Participation Certificates\n\nThe following certificate was awarded for my participation and representation at CVMI-2024:\n\nProject Copyright ©️\n\nSecuring copyright for this project marked an important milestone in safeguarding our innovation and intellectual property. Copyrighting our project not only protects the unique aspects of our hate speech detection system but also reinforces our commitment to responsible AI research. By copyrighting this idea, we ensure that the methods, models, and technological advances developed through this project remain attributed to our team.\n\nCopyright Publication Date: 25th October 2024\n\nCertificate of Copyright\n\nAbove Copyright certificate has been slightly edited to hide the personal details of the authors involved in the Copyright.\n\nProject Funding 💸\n\nThis project was generously funded by Vidya Pratishthan's Kamalnayan Bajaj Institute of Engineering and Technology College, whose support played a pivotal role in enabling our team to bring this ambitious vision to life. This funding allowed us to access essential resources, collaborate with experts, and ensure high-quality development across every phase of the project.\n\nFunding Breakdown\nSr No\tDemand Reason\tDemand Cost\n1\tGoogle Colab Pro\t1025\n2\tOnline Courses\t2684\n3\tProject Presentation Competition\t500\n4\tStationary Cost\t500\n\tTotal\t4709\nFunding Certificate\n\nThe above provided certificate is custom designed and not officially presented by the college itself. We extend our heartfelt gratitude to VPKBIET College for their trust and support. Their investment in this project has been invaluable in pushing the boundaries of AI-driven hate speech detection.\n\nFuture Scope 🔮\nDataset Expansion: Broaden the dataset to include more languages and cultural contexts for improved generalizability.\nFeature Enhancement: Incorporate sentiment analysis and user profile features to boost accuracy.\nReal-Time Integration: Develop more transformer-based models for real-time detection systems to ensure timely and effective hate speech mitigation.\nAcknowledgements 🔖\nI am thankful to ChatGPT for insights, suggestions and provision of resources. It helped in refining ideas and aiding research throughout this work.\nI am thankful to Vidya Pratishthan's Kamalnayan Bajaj Institute of Engineering and Technology Baramati (VPKBIET) College for providing funding for this project.\nProject Report 📔\n\nThis project report is extremely detailed in terms of all the progress made till date in the project.\nThe project report can be viewed by pressing the button below.\n\nReferences 📃\n\n[1] \nhttps://library.fiveable.me/key-terms/ap-gov/hate-speech\n\n[2] Bansod, Pranjali Prakash, ”Hate Speech Detection in Hindi” (2023).\nMaster’s Projects. 1265. DOI: \nhttps://doi.org/10.31979/etd.yc74-7qas\n,\n\nhttps://scholarworks.sjsu.edu/etd\n projects/1265\n\n[3] Mohit Bhardwaj and Md Shad Akhtar and Asif Ekbal and Amitava\nDas and Tanmoy Chakraborty, ”Hostility Detection Dataset in Hindi,”\nin arXiv, 2020, eprint-2011.03588.\n\n[4] P. Kapil, G. Kumari, A. Ekbal, S. Pal, A. Chatterjee and B. N.\nVinutha, ”HHSD: Hindi Hate Speech Detection Leveraging Multi-Task\nLearning,” in IEEE Access, vol. 11, pp. 101460-101473, 2023, doi:\n10.1109/ACCESS.2023.3312993.\n\n[5] V. Rahul, V. Gupta, V. Sehra, and Y. R. Vardhan, ”Ensemble Based\nHinglish Hate Speech Detection,” 2021 5th International Conference on\nIntelligent Computing and Control Systems (ICICCS), Madurai, India,\n2021, pp. 1800 1806, doi: 10.1109/ICICCS51141.2021.9432352.\n\n[6] H. Watanabe, M. Bouazizi, and T. Ohtsuki, ”Hate Speech on Twitter: A\nPragmatic Approach to Collect Hateful and Offensive Expressions and\nPerform Hate Speech Detection,” IEEE Access, vol. 6, pp. 13825-13835,\n2018, doi: 10.1109/ACCESS.2018.2806394.\n\n[7] S. Ghosal, A. Jain, D. K. Tayal, V. G. Menon, and A. Kumar, ”Inculcating\nContext for Emoji Powered Bengali Hate Speech Detection using\nExtended Fuzzy SVM and Text Embedding Models,” ACM Transactions\non Asian and Low- Resource Language Information Processing,\naccepted March 2023, doi: 10.1145/3589001.\n\n[8] S. Khan et al., ”HCovBi-Caps: Hate Speech Detection Using Convolutional\nand Bi-Directional Gated Recurrent Unit With Capsule Network,” IEEE Access,\nvol. 10, pp. 7881-7894, 2022, doi: 10.1109/ACCESS.2022.3143799.\n\n[9] A. K. Yadav, A. Kumar, S. ., K. ., M. Kumar, and D. Yadav,\n”Hate Speech Recognition in multilingual text: Hinglish Documents,”\nTechRxiv, Preprint, 2022, doi: 10.36227/techrxiv.19690177.v1.\n\n[10] V. Rahul, V. Gupta, V. Sehra, and Y. R. Vardhan, ”Hindi-English Code\nMixed Hate Speech Detection using Character Level Embeddings,”\n2021 5th International Conference on Computing Methodologies and\nCommunication (ICCMC), Erode, India, 2021, pp. 1112 1118, doi:\n10.1109/ICCMC51019.2021.9418261.\n\n[11] Y. Zhou, Y. Yang, H. Liu, X. Liu, and N. Savage, ”Deep Learning Based\nFusion Approach for Hate Speech Detection,” IEEE Access, vol. 8, pp.\n128923-128929, 2020, doi: 10.1109/ACCESS.2020.3009244.\n\n[12] H. Saleh, A. Alhothali, and K. Moria, ”Detection of Hate Speech\nusing BERT and Hate Speech Word Embedding with Deep Model,”\nApplied Artificial Intelligence, vol. 37, no. 1, pp. 2166719, 2023, doi:\n10.1080/08839514.2023.2166719.\n\n[13] K. Mnassri, P. Rajapaksha, R. Farahbakhsh, and N. Crespi,\n”Hate Speech and Offensive Language Detection using\nan Emotion-aware Shared Encoder,” arXiv:2302.08777\n[cs.CL],2023.[Online].\nhttps://doi.org/10.48550/arXiv.2302.0\n 8777\n\n[14] J. M. Perez, H. Saleh, A. Alhothali, and K. Moria, ”Assessing the Impact ´\nof Contextual Information in Hate Speech Detection,” IEEE Access, vol.\n11, pp. 30575-30590, 2023, doi: 10.1109/ACCESS.2023.3258973.\n\n[15] \nhttps://chat.openai.com\n.\n\nLicense 📄\n\nThis project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nBy using this project, you agree to give appropriate credit, not use the material for commercial purposes without permission, and share any adaptations under the same license.\n\nAttribution should be given as:\n\"DweshaMukt Project by DweshaMukt Team (\nhttps://github.com/StudiYash/DweshaMukt\n)\"\n\nQuick Overview regarding the permissions of usage of this project can be found on \nLICENSE DEED : CC BY-NC-SA 4.0\n\nMade with ❤️ by \nDweshaMukt Team\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nProject Introduction 🛡️\n\nAbstract\n\nProject Timeline\n\nTeam Members\n\nProject Guide\n\nRelated Work ⚒️\n\nMethodology ✨\n\nInput:\n\nText Preprocessing:\n\nText Tokenization with BERT:\n\nView all\nComments\nCode\nDatasets\nFiles\nProject Backend.ipynb\nYou might be interested\nHate Speech Detection Using Fine-Tuned BERT Model\nJun 22, 202510 reads\nBERTLLM+2\nSafeSphere- Multimodal AI for analysing hate speech\nR\nDec 30, 20246 reads\nAI_Content_Moderation_System\nA\nMar 24, 202523 reads\nFake News Detection (BERT-Transformer)\nMay 01, 20258 reads\nBERTNLP+1",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "enhancing-mri-analysis-with-multimodal-ai-and-computer-vision-HkiNwIJsZiTg",
    "username": "sSteven Ogwal",
    "license": "MIT License",
    "title": "Enhancing MRI Analysis with Multimodal AI and Computer Vision.",
    "publication_description": "Back to publications\nDec 07, 2024\n●\n78 reads\n●\nMIT License\nWinner of\nDistinguished Applied Solution Showcase\nat the\nComputer Vision Projects Expo 2024\nEnhancing MRI Analysis with Multimodal AI and Computer Vision.\nAI\nCNN\nCV\nHealth Care\nOpenAI\nYOLO\nS\nSteven Ogwal\nLike\nBookmark\nShare\nEnhancing MRI Analysis with Multimodal AI and Computer Vision.\n\nPhoto by \nAnna Shvets\n\n\n\n\nAbstract\n\nThis project presents an innovative approach to multimodal AI and Computer Vision pipeline to enhance medical imaging analysis, specifically targeting the early detection of brain tumors through MRI scans. By integrating convolutional neural networks (CNNs), advanced computer vision techniques, and vision-language models, our system achieves high classification accuracy and provides detailed diagnostic reports. The implementation demonstrates significant improvements in diagnostic speed and accuracy, with potential implications for clinical practice.\n\n1. Project Scope and Objectives\n\nThe objective of this project is to develop a Computer Vision and Multimodal AI solution that improves the accuracy and efficiency of brain tumor diagnosis from MRI scans. This include:\n\nTo develop a robust image detection model that accurately classifies different types of brain tumors.\nTo generate comprehensive diagnostic reports that assist clinicians in decision making.\nTo evaluate the model's performance and demonstrate its effectiveness.\nTo deploy and run model on cloud or locally on device.\n2. Technical Methodology and Implementation Details\n\nSetup\n\n#Uncomment to install the packages below \n# %pip install ultralytics\n# %pip install supervision\n# %pip install roboflow\n\nImport libraries\n\nfrom ultralytics import YOLO\nfrom roboflow import Roboflow\nimport supervision as sv\nimport cv2\nimport base64\nfrom io import BytesIO\nfrom PIL import Image as PILImage\n2.1 Data Collection and Preparation\n\nThe dataset consists of annotated MRI scans sourced from multiple medical institutions.\n\nKey preprocessing steps include:\n\nNormalization: Adjusting image intensities to standardize input for model training.\nArtifact Correction: Mitigating noise and distortions common in MRI imaging.\nData Augmentation: Applying transformations like rotations, flipping, and contrast adjustments to enhance model generalization.\nAnnotation: Annotating images with tumor-specific attributes such as type, size, and location.\nRoboflow API Key\n\nRoboflow is a comprehensive platform for building, training, and deploying computer vision models, which we will use to streamline our dataset management, annotation, and model deployment processes, ensuring efficiency and scalability in our project.\n\nTo load models with inference, you'll need a Roboflow API Key. Find instructions for retrieving your API key \nhere\n.\n\nrf = Roboflow(api_key=api_key)\nproject = rf.workspace(\"openvision-pkoxi\").project(\"live-brain-surgery\")\nversion = project.version(1)\ndataset = version.download(\"yolov8\")\n\nFig 1: Picure of dataset from \nRoboflow\n.\n\n2.2 Model Architecture\n\nWe used a hybrid architecture combining CNNs for image classification and segmentation with vision language models for report generation.\nThe architecture includes:\n\nYOLOv8 for Object Detection: Utilized for tumor detection and segmentation.\nVisual Language Model Workflow: Converts image data into natural language descriptions for diagnostic reporting.\n\nThe YOLOv8 architecture, developed by the Ultralytics team, is the latest iteration of the \"You Only Look Once\" (YOLO) family, renowned for its efficiency in real-time object detection. It is designed to identify and locate multiple objects within images rapidly and accurately. By reframing object detection as a single regression problem, YOLOv8 improves upon traditional methods that were often slow and computationally intensive.\n\nThe architecture consists of three main components:\n\nBackbone: Utilizes a custom CSPDarknet53, which enhances feature extraction through Cross-Stage Partial connections to improve information flow and accuracy.\nNeck: Incorporates a novel C2f module that merges feature maps from different layers to capture multi-scale information effectively.\nHead: Features multiple detection modules that predict bounding boxes, object scores, and class probabilities, improving detection accuracy, particularly for smaller objects.\n\nFig 2: Yolo architecture by \nyolov8.org\n\nOur visual language model workflow is a structured sequence of steps designed to process and analyze visual data, such as images or videos, using Yolov8 model and AI vision. The goal is to extract meaningful information from visual inputs and often convert this information into a form that can be understood or utilized by other medical professionals. Here’s a breakdown of what this workflow typically involves:\n\nInput: Submit an image or video.\nObject Detection Model: Detects objects in the image using a pre-trained model.\nBounding Box Visualization: Draws bounding boxes around detected objects.\nLabel Visualization: Adds labels to the bounding boxes.\nDetection Offset: Adjusts the bounding boxes.\nDynamic Crop: Crops the detected image based on detected objects for analysis.\nOpenAI: Uses OpenAI vision for further analysis.\nResponse: Outputs the results, including visualized images and text.\n\nFig 3: Picture of detection workflow\n\n2.3 Implementation\n# Load pre-trained YOLOv8 model\nmodel = YOLO(\"yolov8x.pt\")\n# Display model information (optional)\nmodel.info()\n2.3.1 Model Training\n\nTraining a deep learning model requires supplying it with data and fine-tuning its parameters to improve prediction accuracy. According to the documentation for Ultralytics YOLOv8, the model's training mode is specifically designed for the effective and efficient training of object detection models, maximizing the use of modern hardware capabilities (Ultralytics, n.d.).\n\nThe command below initiates training of a YOLOv8 model, where \"data\" refers to the path of the dataset configuration file, \"epochs\" indicates the number of complete passes through the training dataset, and \"device\" specifies the hardware used for training, in this case, utilizing Metal Performance Shaders (MPS) for GPU acceleration\n\n# Train the model with MPS\nresults = model.train(data=\"Live-Brain-Surgery-1/data.yaml\", epochs=50, device=\"mps\")\n\nAlternatively, training can be done using roboflow.\n\nRoboflow offers a streamlined, low-code approach to training YOLO models by enabling users to upload and annotate images, generate dataset versions, export data in the necessary format, and utilize command-line tools for model training, significantly simplifying the machine learning workflow (Skalski, 2023; Gallagher, 2024).\n\nFig 4: Image of model training graph.\n\n2.3.2 Model Validation\n\nValidation is an important step in the machine learning pipeline, as it provides a means to evaluate the performance and reliability of trained models.\n\n# Load a model\nmodel = YOLO(\"path/to/best.pt\")  # load our trained model\n\n# Validate the model\nmetrics = model.val()\nmetrics.box.maps\n\nFig 5: Image of model validation metrics.\n\n3. Evaluation Metrics and Results\n3.1 Performance Metrics\n\nFig 6: Image of model performance metrics.\n\nThe model's performance was evaluated using the following metrics:\n\nmAP (mean Average Precision): 63.4%\nThis metric measures the model's ability to make accurate predictions across all classes. It combines precision and recall to evaluate the overall accuracy of predictions at different confidence levels. A higher mAP indicates better performance.\n\nPrecision: 89.3%\nThis quantifies how many of the model's positive predictions are correct. High precision means the model makes fewer false positive predictions.\n\nRecall: 60.2%\nThis measures how many of the actual positive instances are correctly predicted by the model. High recall means the model detects most of the true instances, even at the cost of more false positives.\n\n3.2 Results Summary\n\nThe model achieves a high precision of 89.3%, but the recall of 60.2% indicates it misses some detections, leading to a mAP of 63.4%. To improve performance, I plan to augment the dataset with more diverse dataset, address potential class imbalances, and fine tune key hyperparameters like the confidence threshold and learning rate. Additionally, experimenting with multi-scale training, higher resolution images, and techniques like focal loss could help balance precision and recall for better overall results.\n\n4. Model Testing and Deployment\n\nIn machine learning and computer vision, deployment and testing are crucial steps in the lifecycle of machine learning (ML) models, ensuring that they perform well in real-world applications. The process of making sense out of visual data is called 'inference' or 'prediction'.\n\nWe will utilize Inference to facilitate the deployment of our model. This simplifies the process by providing a user-friendly interface and robust infrastructure, allowing for efficient execution of the model across various platforms ensuring that our model is readily accessible and operational in real world applications.\n\n# %pip install inference\n# %pip install inference-cli && inference server start\nfrom inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    api_key=api_key\n)\n\nresult = client.run_workflow(\n    workspace_name=\"openvision-pkoxi\",\n    workflow_id=\"brain-surgery-workflow\",\n    images={\n        \"image\":\"test-img.jpg\" \n    }\n)\nImage Analysis Stage\n\nWe employ a deep learning-based approach to analyze MRI scans\n\n# Decode the base64 string\nlabel_visualization = result[0]['label_visualization']\ndynamic_crop = result[0]['dynamic_crop'][0]\n# show predicton image and label\nimage_bytes1 = base64.b64decode(label_visualization)\n\n# Create an image from the bytes\nimage1 = PILImage.open(BytesIO(image_bytes1))\n\n# Display the image\ndisplay(image1)\n# show detected image\nimage_bytes2 = base64.b64decode(dynamic_crop)\n\n# Create an image from the bytes\nimage2 = PILImage.open(BytesIO(image_bytes2))\n\n# Display the image\ndisplay(image2)\nVision Language Integration Results\n\nFig 7: Image showing workflow results.\n\ndef convert_pixels_to_mm(bounding_box, conversion_factor):\n    \"\"\"\n    Convert pixel dimensions to millimeters.\n\n    Parameters:\n    bounding_box (dict): A dictionary containing the bounding box coordinates with keys 'width', 'height', 'x', and 'y'.\n    conversion_factor (float): The pixel-to-millimeter conversion factor (mm/pixel).\n\n    Returns:\n    dict: A dictionary containing the width, height, and area in millimeters.\n    \"\"\"\n    # Extract the dimensions from the bounding box\n    width_px = bounding_box['width']\n    height_px = bounding_box['height']\n\n    # Convert width and height from pixels to millimeters\n    width_mm = width_px * conversion_factor\n    height_mm = height_px * conversion_factor\n\n    # Calculate the area in square millimeters\n    area_mm2 = width_mm * height_mm\n\n    # Return the dimensions and area in millimeters\n    return area_mm2\nprediction = result[0]['model_predictions']['predictions']['predictions'][0]\nconfidence = result[0]['model_predictions']['predictions']['predictions'][0]['confidence']\nopen_ai = result[0]['open_ai']\nwidth_px = prediction['width']\nheight_px = prediction['height']\nx_px = prediction['x']\ny_px = prediction['y']\ndetection_id = prediction['detection_id']\npredition_class = prediction['class']\nsize = convert_pixels_to_mm(prediction,0.1)\n\nprint(\"Below are the scan results:\")\nprint(\"_ _\"*30)\n\nprint(f\"ID:           {detection_id}\")\nprint(f\"Prediction:   {predition_class.upper()}\")\nprint(f\"Confidence:   {confidence} ~ {int(confidence * 100)}%\")\nprint(f\"Size:         {size}mm²\")\nprint(f\"Morphology:\")\nfor res in open_ai:\n    print(f\"- {res}\")\n\nprint(\"_ _\"*30)\nBelow are the scan results:\n_ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ _\nID:           fdb02240-c9ed-41f8-abc4-ae68524bdf06\nPrediction:   PITUITARY\nConfidence:   0.949749231338501 ~ 94%\nSize:         47mm²\nMorphology:\n- The image displays an irregular, roughly circular shape with a blurred appearance.\n_ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ _\n\nLive Demo:\nTest Workflow\n\nThis demo uses the inference sdk.\n\nYou can also test the deployed workflow locally:\n\n# Import the InferencePipeline object\nfrom inference import InferencePipeline\nimport cv2\n\ndef video_sink(result, video_frame):\n    if result.get(\"output_image\"): # Display an image from the workflow response\n        cv2.imshow(\"Workflow Image\", result[\"output_image\"].numpy_image)\n        cv2.waitKey(1)\n    print(result) # do something with the predictions of each frame\n    \n\n# initialize a pipeline object\npipeline = InferencePipeline.init_with_workflow(\n    api_key=api_key,\n    workspace_name=\"openvision-pkoxi\",\n    workflow_id=\"brain-surgery-workflow\",\n    video_reference='test-img.png', # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url\n    max_fps=30,\n    on_prediction=video_sink\n)\npipeline.start() #start the pipeline\npipeline.join() #wait for the pipeline thread to finish\n5. Impact Analysis and Future Directions\n\nThis project has the potential to transform clinical workflows by providing rapid and accurate diagnostic support for brain tumors.\nFuture work will focus on:\n\nExpanding the dataset to include diverse imaging modalities (e.g., PET scans).\nImplementing federated learning to enhance model accuracy and privacy.\nConducting real-world clinical trials to validate the model's effectiveness in practice.\n\nPhoto by \nIsabella Mendes\n.\n\n6. Conclusion\n\nThis project demonstrates the transformative potential of multimodal AI and Computer Vision in medical imaging analysis. By leveraging advanced computer vision techniques and visual language model processing, we have developed a robust solution that addresses key challenges in brain tumor diagnosis.\n\nWe believe that our solution has the potential to revolutionize brain cancer diagnosis and treatment, enabling clinicians to make more informed decisions and improve patient outcomes.\n\nReferences\n\nJocher, G., Chaurasia, A., & Qiu, J. (2023). Ultralytics YOLOv8 (Version 8.0.0) [Software]. \nhttps://github.com/ultralytics/ultralytics\n (AGPL-3.0 License)\n\nRoboflow. (2023). Supervision. \nhttps://github.com/roboflow/supervision\n (MIT License)\n\nBrain tumor detection. (2024, July). Tumor Detection Dataset [Open Source Dataset]. Roboflow Universe. \nhttps://universe.roboflow.com/brain-tumor-detection-wsera/tumor-detection-ko5jp\n (Visited on 2024-12-03)\n\nUltralytics. (n.d.). YOLOv8 architecture. Retrieved December 3, 2024, from \nhttps://yolov8.org/yolov8-architecture/\n\nSkalski, P. (2023). How to Train YOLOv8 Object Detection on a Custom Dataset. Roboflow Blog. \nhttps://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/\n\nGallagher, J. (2024). How to Train a YOLOv11 Object Detection Model on a Custom Dataset. Roboflow Blog. \nhttps://blog.roboflow.com/yolov11-how-to-train-custom-data/\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nEnhancing MRI Analysis with Multimodal AI and Computer Vision.\n\nAbstract\n\nProject Scope and Objectives\nTechnical Methodology and Implementation Details\n\n2.1 Data Collection and Preparation\n\nRoboflow API Key\n\n2.2 Model Architecture\n\n2.3 Implementation\n\n2.3.1 Model Training\n\n2.3.2 Model Validation\n\nView all\nCode\nDatasets",
    "awards": [
      "distinguished applied solution showcase"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "enhancing-satellite-image-generation-with-lora-and-advanced-augmentation-techniques-yVIFZnj8yhdb",
    "username": "rBogdan Roshchupkin",
    "license": "No License",
    "title": "Enhancing Satellite Image Generation with LoRA and Advanced Augmentation Techniques",
    "publication_description": "Back to publications\nDec 29, 2024\n●\n98 reads\n●\nNo License\nWinner of\nBest Technical Implementation\nat the\nComputer Vision Projects Expo 2024\nEnhancing Satellite Image Generation with LoRA and Advanced Augmentation Techniques\nArtificial Intelligence\nData Preprocessing\nDeep Learning\nImage Generation\nImage Synthesis\nLoRA\nSatellite Imaging\nStable Diffusion\nR\nBogdan Roshchupkin\nA\n@andrejutlakov96\nLike\nBookmark\nShare\nEnhancing Satellite Image Generation with LoRA and Advanced Augmentation Techniques\n1. Introduction\n1.1 Context and Motivation\n\nAt our workplace, a bank provides credit to entrepreneurs engaged in gold mining within designated areas. However, these entrepreneurs risk losing their mining licenses due to potential violations such as trespassing or environmental pollution, including improper waste disposal into water bodies or soil contamination. If a license is lost, the bank won't be able to return the credit, leading to irretrievable losses. Detecting such violations is challenging due to the scarcity of relevant satellite image data needed for accurate identification and monitoring.\nGold mining and borders(red color) of multiple entrepreneurs\n\n\nTo address this issue, we aimed to develop a model capable of identifying unauthorized mining activities and environmental breaches. The primary challenge was the lack of sufficient labeled satellite images to train an effective detection system. To overcome this data scarcity, we employed advanced data augmentation techniques and leveraged Stable Diffusion to generate synthetic satellite images. Additionally, we integrated Low-Rank Adaptation (LoRA).\n\n1.2 Objectives of the Article\n\nThis article explores the application of LoRA in generating high-quality satellite images by integrating it with the YOLO (You Only Look Once) object detection model and advanced data augmentation techniques. Specifically, it aims to:\n\nExplain LoRA and its benefits in model adaptation.\nProvide an overview of the YOLO model and its relevance to satellite image generation.\nDetail the data preparation and advanced augmentation processes.\nDescribe the integration of LoRA with YOLO for improved image generation.\nPresent and analyze results, showcasing performance metrics and visual comparisons.\nDiscuss challenges faced and suggest future research directions.\n\nBy the end of this article, readers will understand how combining LoRA with YOLO and sophisticated augmentation methods can significantly enhance satellite image generation, making it more detailed, realistic, and computationally efficient.\n\n2. Overview of Technologies\n2.1 What is LoRA?\n\nLow-Rank Adaptation (LoRA) is a technique designed to improve the adaptability and efficiency of large pre-trained models. Traditional fine-tuning adjusts all model parameters, which is computationally intensive and requires substantial memory. LoRA introduces low-rank matrices to adapt the model's weights with minimal overhead, enabling efficient fine-tuning without retraining the entire model.\n\nAdvantages of LoRA:\n\nEfficiency: Reduces the number of trainable parameters, decreasing training time and memory usage.\nScalability: Facilitates fine-tuning of very large models that are otherwise impractical to adapt.\nFlexibility: Applicable to various neural network types, including transformers and convolutional networks.\n\nIn satellite image generation, LoRA allows fine-tuning pre-trained models to produce high-resolution images efficiently, enhancing model performance without excessive computational costs.\n\n2.2 Connection between LoRA and YOLO\n\nBy merging these technologies, the project aims to achieve superior performance in satellite image generation, making high-quality imagery more accessible and affordable.\n\nGenerate or augment images using Stable Diffusion and LoRa.\nLabel synthetic data using annotation tools (e.g., LabelImg) or scripts.\nIntegrate synthetic data with the real dataset.\nUse this enriched dataset to train YOLO, leading to a model that generalizes better.\n\nBenefits of This Connection\n\nOvercomes challenges like imbalanced datasets.\nReduces dependency on real-world data collection (which can be time-consuming and costly).\nMakes the YOLO model adaptable to specific domains or scenarios where real data is scarce.\n3. Methodology\n3.1 Data Preparation\n\nThe foundation of our project lies in the collection and preparation of satellite images relevant to gold mining activities. We sourced high-resolution satellite images from publicly available datasets and proprietary sources, focusing on areas designated for mining operations.\n\nKey Steps in Data Preparation:\n\nImage and Mask Loading: We utilized the rasterio library to handle geospatial data formats, loading both satellite images and their corresponding masks.\n\nFiltering and Splitting: Due to the high resolution of satellite images, we divided them into smaller blocks of 512x512 pixels using custom scripts. This approach ensures manageable data sizes and maintains the integrity of the original imagery.\n\nSaving Processed Data: The processed images and masks were systematically saved into designated directories for training, validation, and testing.\n\n# Function to divide and filter images\ndef divide_image(image, mask, split, image_basename, image_save_dir, mask_save_dir, filtered_image_dir, filtered_mask_dir, threshold):\n    # Detailed implementation can be seen on GitHub, hidden for readability purpose\n\n# Creating output directories\nfor split in ['train', 'val', 'test']:\n    os.makedirs(os.path.join(output_dirs[split], 'images'), exist_ok=True)\n    os.makedirs(os.path.join(output_dirs[split], 'masks'), exist_ok=True)\n    os.makedirs(os.path.join(output_dirs[f'{split}_filtered'], 'images'), exist_ok=True)\n    os.makedirs(os.path.join(output_dirs[f'{split}_filtered'], 'masks'), exist_ok=True)\n\n# Processing and dividing images\nfor split in ['train', 'val', 'test']:\n    image_dir = os.path.join(dataset_dir, split, 'images')\n    mask_dir = os.path.join(dataset_dir, split, 'masks')\n    \n    output_image_dir = os.path.join(output_dirs[split], 'images')\n    output_mask_dir = os.path.join(output_dirs[split], 'masks')\n\n    filtered_image_dir = os.path.join(output_dirs[f'{split}_filtered'], 'images')\n    filtered_mask_dir = os.path.join(output_dirs[f'{split}_filtered'], 'masks')\n\n    for image_name in tqdm(os.listdir(image_dir), desc=f\"Processing {split}\"):\n        if image_name.lower().endswith('.png'):\n            image_path = os.path.join(image_dir, image_name)\n            mask_name = image_name.replace(\"train_image\", \"train_mask\").replace(\"val_image\", \"val_mask\").replace(\"test_image\", \"test_mask\")\n            mask_path = os.path.join(mask_dir, mask_name)\n\n            if not os.path.exists(mask_path):\n                logging.warning(f'Mask file not found: {mask_path}')\n                continue\n\n            try:\n                image = Image.open(image_path).convert(\"RGB\")\n                mask = Image.open(mask_path).convert(\"L\")\n            except Exception as e:\n                logging.error(f\"Error opening {image_path} or {mask_path}: {e}\")\n                continue\n\n            image_basename = os.path.splitext(image_name)[0]\n            divide_image(image, mask, split, image_basename, output_image_dir, output_mask_dir, filtered_image_dir, filtered_mask_dir, threshold=filtered_threshold)\n\n# Logging and statistics\ndef print_folder_stats(output_dirs):\n    # Detailed implementation can be seen on GitHub, hidden for readability purpose\n\nprint_folder_stats(output_dirs)\n3.2 Advanced Augmentation Techniques\n\nData augmentation is pivotal in enhancing the diversity and robustness of the training dataset, allowing models to generalize better to unseen data. Given the limited availability of real satellite images for gold mining detection, we implemented several advanced augmentation techniques using the albumentations library.\n\nAugmentation Pipeline:\n\nGrid Distortion: Warps the image using grid-based transformations, simulating real-world distortions.\nElastic Transform: Applies random elastic deformations, enhancing the model's ability to handle non-linear transformations.\nFlip: Randomly flips images horizontally or vertically to increase data variability.\nAffine Transformation: Applies scaling, rotation, and translation, making the model invariant to these changes.\nChannel Mixing: Randomly swaps color channels, mimicking different lighting conditions or sensor characteristics.\nfrom albumentations import Compose, GridDistortion, ElasticTransform, Flip, Affine, RandomBrightnessContrast\n\n# Define augmentation pipeline\ntransform = Compose([\n    GridDistortion(p=0.5),\n    ElasticTransform(p=0.5),\n    Flip(p=0.5),\n    Affine(\n        rotate=(-10, 10),\n        translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n        p=0.5\n    ),\n    RandomBrightnessContrast(p=0.5)\n])\n\ndef augment_image(image):\n    augmented = transform(image=image)\n    return augmented['image']\n\nThis what we get after implementing augmentation and pasting cropped objects for making augmentation more diverse:\n\n3.3 Training YOLO\n\nTraining the YOLO model involved several key steps to ensure effective detection of unauthorized mining activities and environmental breaches.\n\nTraining Process\n\nModel Initialization: We initialized the YOLO model with pre-trained weights to leverage existing knowledge, accelerating the training process and enhancing initial performance.\n\nData Integration: Combined real and synthetic (augmented) satellite images to create a balanced and comprehensive training dataset. This approach mitigates data scarcity and enhances the model's ability to generalize across diverse scenarios.\n\nTraining Configuration: Configured training parameters, including learning rate, batch size, and number of epochs, to optimize model performance.\n\nTraining Loop: Executed the training process, monitoring performance metrics such as Class, Box Precision (P), mAP50. The training aimed to maximize these metrics, ensuring robust detection capabilities.\n\nPerformance Metrics After Training\n\nTo evaluate the model's effectiveness, we assessed its performance using standard metrics. The results are summarized below:\n\nCategory\tBox (P)\tmAP50\nAll\t0.81\t0.80\nClouds\t0.74\t0.97\nGold\t0.89\t0.70\nPoisonous Water\t0.68\t0.665\nWater\t0.89\t0.86\n3.4 Integration of LoRA\n\nIntegrating Low-Rank Adaptation (LoRA) with the YOLO model significantly enhanced our ability to generate and detect high-resolution satellite images tailored to our specific requirements. This integration involved training the Stable Diffusion model on our dataset to generate synthetic satellite images, manually annotating these images, and subsequently retraining the YOLO model with the enriched dataset. The training process was adapted from the great book \"Using Stable Diffusion with Python. Leverage Python to control and automate high-quality AI image generation using Stable Diffusion,\" written by Andrew Zhu(Shudong Zhu), which also details various inference optimization techniques we employed.\n\nTraining the Stable Diffusion Model\n\nTo generate realistic and context-specific satellite images, we fine-tuned the Stable Diffusion model using our curated dataset of satellite images related to gold mining activities. The fine-tuning process included the following steps:\n\nDataset Preparation: Collected and organized a comprehensive set of high-resolution satellite images from designated gold mining areas. Ensured diversity in environmental conditions and mining scenarios to improve the model's generalization capabilities.\n\nModel Fine-Tuning: Leveraged the diffusers library to fine-tune the Stable Diffusion model on our dataset. This process involved configuring hyperparameters such as learning rate, batch size, and number of epochs to optimize image generation quality.\n\nLoRA Configuration: Applied LoRA to the Stable Diffusion model to enable efficient fine-tuning with minimal computational overhead. This adaptation allowed us to enhance the model's performance without extensive retraining.\n\nKey Code Snippet for Training Stable Diffusion with LoRA:\n\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDPMScheduler\nfrom peft import LoraConfig\nfrom peft.utils import get_peft_model_state_dict\nfrom accelerate import Accelerator\n\n# Initialize Accelerator\naccelerator = Accelerator(gradient_accumulation_steps=4, mixed_precision=\"no\")\ndevice = accelerator.device\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=4,\n    lora_alpha=4,\n    init_lora_weights=\"gaussian\",\n    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n)\n\n# Load pre-trained Stable Diffusion model\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype=torch.float32\n).to(device)\n\n# Add LoRA adapter to the Unet component\npipe.unet.add_adapter(lora_config)\nprint(\"LoRA adapter added to Unet.\")\n\nExample of generated image:\n\n\n4.1 Performance Metrics Before and After LoRA\n\nTo evaluate the effectiveness of integrating LoRA with the YOLO model, we compared the performance metrics before and after the integration across all categories. The results indicate noticeable improvements in detection accuracy and reliability.\n\nComparative Analysis of YOLO Configurations\n\nYOLO without LoRA\nCategory\tBox (P)\tmAP50\nAll\t0.80\t0.79\nClouds\t0.725\t0.959\nGold\t0.87\t0.67\nPoisonous Water\t0.663\t0.6\nWater\t0.875\t0.86\nYOLO with Advanced Augmentation\nCategory\tBox (P)\tmAP50\nAll\t0.81\t0.80\nClouds\t0.74\t0.97\nGold\t0.89\t0.70\nPoisonous Water\t0.68\t0.665\nWater\t0.89\t0.86\nYOLO with LoRA and Advanced Augmentation\nCategory\tBox (P)\tmAP50\nAll\t0.827\t0.818\nClouds\t0.77\t0.995\nGold\t0.913\t0.718\nPoisonous Water\t0.706\t0.67\nWater\t0.921\t0.891\n5. Discussion\n5.1 Advantages of Using LoRA in Stable Diffusion\nEnhanced Efficiency: LoRA enables fine-tuning of large models like Stable Diffusion with minimal computational overhead, making it feasible to adapt for specific tasks without extensive resource investment.\nImproved Performance: The integration of LoRA significantly boosted key performance metrics, demonstrating its effectiveness in enhancing model accuracy and robustness in satellite image generation.\nScalability: LoRA's flexibility allows for easy adaptation of models to various satellite image generation tasks, accommodating different resolutions and environmental conditions without the need for complete retraining.\nData Enrichment: By generating synthetic images with Stable Diffusion, LoRA helps overcome data scarcity, providing a more balanced and comprehensive training dataset that improves the YOLO model's generalization capabilities.\n5.2 Challenges and Limitations\nData Quality: The success of augmentation techniques and synthetic image generation is highly dependent on the quality and diversity of the original dataset. Limited variability can still constrain model generalization.\nComputational Resources: While LoRA reduces computational demands, training high-resolution models with Stable Diffusion remains resource-intensive, requiring powerful hardware and optimized training strategies.\nIntegration Complexity: Combining multiple advanced techniques (LoRA, YOLO, Stable Diffusion) introduces complexity in the training pipeline, necessitating careful calibration to ensure harmonious performance improvements.\nManual Annotation: The process of manually annotating synthetic images is time-consuming and may introduce human error, potentially affecting the quality of the training data.\n6. Conclusion\n\nIn this article, we explored the integration of Low-Rank Adaptation (LoRA) with the YOLO model and advanced augmentation techniques to enhance satellite image generation. By leveraging Stable Diffusion, we achieved the generation of high-resolution, realistic satellite images with improved performance metrics. Our approach not only demonstrates the efficacy of LoRA in fine-tuning large models but also underscores the importance of robust data augmentation in machine learning workflows.\n\nThe combination of these technologies paves the way for more efficient and scalable satellite image generation solutions, with potential applications spanning environmental monitoring, urban planning, and compliance enforcement. As we continue to refine these methods, future research will focus on overcoming current limitations and expanding the applicability of our approach to broader domains.\n\nFor more technical details and access to the project code, please visit our Github: \nhttps://github.com/jettooss/stable_diffusion_satellite_images/tree/main\n\nrepository.\n\nResult of the project\n\nOverall result includes the whole app, using Leaflet in frontend part. Here you can see how we detect violations on Russian Far East territory: \nhttps://drive.google.com/file/d/1EztfKvIE9mN8j3p1MvQRDZTYIZV9MU95/view?usp=sharing\n\nDataset access\n\nIf you want to download the dataset, please click on download in google drive and then we will provide you the permission\n\nFAQ:\nQ1: Why did you choose YOLOv8 for this project?\n\nA1: YOLOv8 (You Only Look Once version 8) was chosen for its real-time object detection capabilities, high accuracy, and versatility. Additionally, I have extensive experience with YOLOv8, not only at work but also in hackathons where it consistently delivered outstanding results, regardless of how arduous the dataset was. This familiarity and proven performance made YOLOv8 the ideal choice for detecting unauthorized mining activities and environmental breaches in satellite images.\n\nQ2: What challenges did you encounter while integrating LoRA with YOLOv8 and Stable Diffusion?\n\nA2: Integrating LoRA with YOLOv8 and Stable Diffusion proved to be quite challenging. The process was complex and required a deep understanding of both model architectures and their training pipelines. To overcome these difficulties, we referred to the comprehensive book by Andrew Zhu(Shudong Zhu) \"Using Stable Diffusion with Python. Leverage Python to control and automate high-quality AI image generation using Stable Diffusion.\" This resource was instrumental in successfully implementing the training part of the integration, enabling us to fine-tune the models effectively.\n\nQ3: Can this methodology be applied to other domains beyond gold mining detection?\n\nA3: Yes, the methodology of integrating LoRA with YOLOv8 and using advanced augmentation techniques can be applied to various domains that require high-quality image generation and object detection, such as environmental monitoring, urban planning, agriculture, and disaster management. The flexibility and scalability of this approach make it adaptable to different scenarios and datasets, allowing for broad applicability across diverse fields.\n\nQ4: How do you ensure the quality of synthetic images generated by Stable Diffusion?\n\nA4: We ensure the quality of synthetic images by combining quantitative and qualitative evaluation methods. After generating the images with Stable Diffusion, we assessed them by analyzing key performance metrics and conducting visual inspections. This dual approach ensures that the synthetic images accurately reflect potential violations and environmental conditions, maintaining high standards of reliability and realism in the training dataset.\n\nQ5: What inference optimization techniques did you use for Stable Diffusion?\n\nA5: The inference optimization techniques employed are detailed in the guide \"Using Stable Diffusion with Python. Leverage Python to control and automate high-quality AI image generation using Stable Diffusion.\" These techniques include efficient model loading, leveraging GPU acceleration, and implementing memory management strategies to enhance the speed and efficiency of image generation. By following these optimized practices, we ensured that the Stable Diffusion model operates effectively within our computational constraints.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nEnhancing Satellite Image Generation with LoRA and Advanced Augmentation Techniques\n\nIntroduction\n\n1.1 Context and Motivation\n\n1.2 Objectives of the Article\n\nOverview of Technologies\n\n2.1 What is LoRA?\n\n2.2 Connection between LoRA and YOLO\n\nMethodology\n\n3.1 Data Preparation\n\n3.2 Advanced Augmentation Techniques\n\nView all\nComments\nCode\nDatasets\nYou might be interested\nYOLOv8 Model for Object Detection\nO\nMay 26, 202515 reads\nMarine Pollution Detection\nN\nDec 30, 202422 reads\nCLIPEnvironmental+5\nMonitoring and Detecting Glacier Changes Using YOLOv11 and Custom Dataset\nS\nDec 29, 202417 reads\nartificial intelligencecomputer vision+7\nObject detection(computer Vision)\nA\nDec 29, 20246 reads",
    "awards": [
      "best technical implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "evaluation-of-precision-accuracy-in-classifying-breast-histopathology-images-using-mobilenetv3-GmSh2pkI8ru4",
    "username": "gGary Takahashi",
    "license": "No License",
    "title": "",
    "publication_description": "Back to publications\nSep 15, 2024\n●\n93 reads\n●\nNo License\nEvaluation of precision & accuracy in classifying breast histopathology images using MobileNetV3\nbreast cancer\ncomputer vision\nmobilenet v3\nG\nGary Takahashi\nK\nKenneth Devoe\nE\nEbrahim Tarshizi\nLike\nBookmark\nShare\nAbstract\n\nAccurate surgical pathological assessment of breast biopsies is essential to the proper management of breast lesions. Identifying histological features, such as nuclear pleomorphism, increased mitotic activity, cellular atypia, patterns of architectural disruption, as well as invasion through basement membranes into surrounding stroma and normal structures, including invasion of vascular and lymphatic spaces, help to classify lesions as malignant. This visual assessment is repeated on numerous slides taken at various sections through the resected tumor, each at different magnifications. Computer vision models have been proposed to assist human pathologists in classification tasks such as these. Using MobileNetV3, a convolutional architecture designed to achieve high accuracy with a compact parameter footprint, we attempted to classify breast cancer images in the BreakHis_v1 breast pathology dataset to determine the performance of this model out-of-the-box. Using transfer learning to take advantage of ImageNet embeddings without special feature extraction, we were able to correctly classify histopathology images broadly as benign or malignant with 0.98 precision, 0.97 recall, and an F1 score of 0.98. The ability to classify into histological subcategories was varied, with the greatest success being with classifying ductal carcinoma (accuracy 0.95), and the lowest success being with lobular carcinoma (accuracy 0.59). Multiclass ROC assessment of performance as a multiclass classifier yielded AUC values ≥0.97 in both benign and malignant subsets. In comparison with previous efforts, using older and larger convolutional network architectures with feature extraction pre-processing, our work highlights that modern, resource-efficient architectures can classify histopathological images with accuracy that at least matches that of previous efforts, without the need for labor-intensive feature extraction protocols. Suggestions to further refine the model are discussed.\n\nIntroduction\n\nBreast cancer is the most common cancer afflicting females, second only to skin cancers, and now afflicts nearly 1 in 3 women each year.1 This condition usually presents either as a palpable breast mass, or, as is more often the case, detected by screening mammography. Once a lesion is identified, a core needle biopsy is performed to assess the nature of the abnormality, and a surgical pathologist is tasked with precise identification of the lesion, as well as a panel of an associated standardized set of biomarker phenotypes.2,3 Tumor behavior can reasonably be predicted by evaluating the histological features such as cellular morphology, the degree and nature of architectural distortion, as well as invasion through basement membranes into normal structures, including blood vessels and lymphatics.4 It is pertinent to note that each case typically involves the evaluation of numerous slides involving various sections through the tumor, each at different magnifications, as features such as architectural patterns are more apparent with the lower power widefield objective, while cellular and nuclear detail are better evaluated using the higher power objectives. Making an accurate diagnosis in a timely manner is critical, as appropriate intervention when curative management is possible can lead to improved survival outcomes.5\n\nInterpretation of mammographic imaging has benefited from computer-assisted identification of suspicious radiographic lesions.6 Deep learning may also assist the pathologist in histological classification of surgical biopsies.7 Previous efforts have utilized convolutional neural net (CNN) algorithms to broadly classify breast cancer pathological images into benign and malignant categories, using larger image processing architectures that have been used to successfully classify non-medical, more everyday images. Convolutional networks have formed the basis for virtually all computer vision architectures since the AlexNet model famously won the ImageNet competition in 2012, achieving an unprecedented top-5 error rate of 17.0%.8 Since then, other groups have built upon this concept,9 and subsequent years saw the appearance of newer architectures, such as deep residual networks10 and inception networks,11 among others. These networks were tested on the ImageNet dataset, a large publicly available collection of 14.2 million labeled images across 21 841 synsets (categories),12,13 and this resource has been a benchmark for evaluating new computer vision architectures.\n\nConvolutional models excel by first learning to detect pertinent features of an image, such as edges, textures, and shapes. Deeper layers in the architecture then become trained to detect combinations of these features.14 To enhance edges and contrast along the borders of features of interest, feature extraction techniques have been used. Semantic segmentation has been used to selectively apply coloration to specific features, such as the nuclei, or to the emphasize the boundaries of an infiltrating tumor mass.15,16 By enhancing the contrast of the borders between 2 regions of concern, this could theoretically increase classification accuracy, but this comes at the cost of time spent pre-processing the data in this manner, and the time and effort needed to confirm the accuracy and precision of the segmentation. The ability to accurately classify pathology images without the need for special pre-processing, such as segmentation, would greatly enhance the ease and efficiency of machine-learning image classification.\n\nMethodology\n\nBreakHis_v1 (BreakHis) is a large dataset consisting of 9109 microscopic images collected from 82 patients. Of these, 7909 images were available for public downloading.17 This dataset includes 2480 images of benign breast lesions, and 5429 images of malignant lesions. Regions of interest (ROIs) have been identified by a pathologist, and the images were obtained at the indicated magnifications (Fig. 1).\n\nFigure 1\n\nBreaKHis dataset data distribution.\n\nBenign breast lesions were divided into four categories: adenosis, fibroadenoma, phyllodes tumor, and tubular adenoma. The malignant conditions selected for inclusion were ductal carcinoma, lobular carcinoma, mucinous carcinoma, and papillary carcinoma. Each histological category was represented in four magnification levels: 40×, 100×, 200×, and 400×. These refer to the microscope objective magnification multiplied by the 10× ocular lens (Fig. 2).\n\nFigure 2\n\nExamples from the BreaKHis_v1 ductal carcinoma dataset, seen at magnifications of: (a) 40×, (b) 100×, (c) 200×, (d) 400×. Lower magnifications more clearly display architectural details of tumor penetration into normal breast tissue, while at higher magnifications, cellular and nuclear detail become more important. Presence of metastatic cells in the lymphatic channels and capillaries is better appreciated at the higher magnifications.\n\nWe selected the BreakHis dataset as the most suitable for our objectives, as other breast cancer datasets that have been used in other publications either were no longer available, contained fewer or smaller images for classification, offered only binary classifications (i.e., benign vs. malignant) or were better suited for the detection of metastatic spread in the background of lymphoid tissue.\n\nExploratory data analysis\n\nIt is necessary to evaluate histopathological images at various levels of magnification. Images examined at low magnification may reveal glandular structures, stromal invasion, and architectural distortion by the neoplasm. Higher magnifications may reveal the presence of mitotic figures, nuclear atypia, degree of cellular differentiation, and abnormal growth patterns, as well as the presence or absence of invasion through basement membranes (signifying metastatic spread) as well as abnormal tumor invasion through lymphatic or vascular channels. Because each magnification contributes useful information that would be important to classification, we did not restrict the analysis to specific magnifications, but elected to train the model on all magnifications collectively.\n\nExamining multiple random samples from the dataset revealed that the images were of good quality and suitable for training. The size of each image in the BreakHis_v1 dataset is 700 × 460 pixels and is stored in PNG format. MobileNetV3 requires images of equal height and width, with three channels for RGB color representation, and the dataset images were resized accordingly. The crop_to_aspect_ratio parameter was left at the default setting, allowing for width compression so that the software would not crop images indiscriminately, potentially eliminating key histological features from being available for training.\n\nIn prior studies, feature extraction has been utilized in other efforts as a means of strengthening features, such as contrast enhancement to emphasize edges or semantic segmentation to highlight regions of interest (ROIs). Implementation of this step has required input from trained pathologists to confirm the correct identification of ROI, and in the setting of metastatic breast cancer.18 We evaluated the performance of our model without implementing feature extraction, so as to classify breast pathology into four benign and four malignant categories, against the background of normal breast tissue, a potentially more challenging task.\n\nData augmentation\n\nThe BreakHis dataset contains approximately 110–140 images at each magnification in most of the neoplasm categories, but for the most common neoplasms such as fibroadenomas, there are 230–260 images at each magnification, and between 790 and 900 images for ductal carcinomas. Categorical representation in this dataset reflects the clinical prevalence of these histological entities, as ductal carcinomas and fibroadenomas are the most common malignant and benign histologies, respectively.19,20 While a more evenly balanced dataset is usually preferred to decrease overfitting to one category, it may not be entirely disadvantageous to having a model be trained based on a weighted representation that reflects the actual prevalence. Nonetheless, data augmentation over all images is a common technique to manage unevenness in the dataset. In this endeavor, this was implemented by introducing a simple random horizontal image reflection (mirror images) to the model, effectively doubling our virtual sample size. We elected not to implement more distortive effects, such as shear. Other data augmentative effects, such as rotation would be meaningless with histology images, and the various magnifications would provide zoom information. We assessed the effect of data augmentation on the training and validation accuracy curves, which will be presented below.\n\nModel selection\n\nThe MobileNet family is built on the concept of depthwise-separable convolutions, which has allowed it to achieve image classification accuracy comparable to that of larger models while being efficient and and incurring lower computational cost.21 MobileNetV3 built on MobileNetV2’s improvements by introducing the squeeze-and-excite algorithm in parallel to all components of the residual block; using Neural Architectural Search to improve accuracy22; and replacing some of the sigmoid activations with the hard swish activation function which is computationally less “expensive” than the sigmoid activation function (a major factor in mobile devices), to improve performance but without compromising on accuracy. This enabled some layer reduction in the last stages of the model without decreasing accuracy. MobileNetV3 is divided into small and large versions, with the small version to be run on reduced-resource devices (Fig. 3).\n\nFigure 3\n\nTop accuracy of the MobileNetV3 models as compared with other computer vision architectures, stratified by the number of trainable parameters in the model (Adopted from Pandey38).\n\nModel architecture\n\nThe MobileNetV3 large architecture offers improved accuracy over the companion MobileNetV3 small architecture. The ImageNet dataset embeddings are enabled by default,23 however, it was not clear that these parameter weights obtained from training on the widely varied images obtained from the Internet would be helpful in classifying histology images. We decided to implement transfer learning, utilizing weights and biases of earlier layers, which detect shapes and edges in images, while training on the later layers, which register activations based on more complex image features. This will be discussed more in detail in the section on Layerwise Learning.\n\nA key hyperparameter is the model learning rate, which controls how rapidly the parameters of the model converge to the local minimum of the categorical cross-entropy loss function,24 used to assess the accuracy of classification after each training epoch. As the loss function local minimum is approached, optimal adjustments of the learning rate at each training epoch can help to prevent overshoot, and subsequent deterioration of accuracy. The optimal freezing layer as well as setting the magnitude of the learning rate are heuristically determined. We tested a range of epoch decay values, which were first entered into a spreadsheet to facilitate automation. In this way, the model could be programmed to autonomously run multiple training sessions using different hyperparameter values, while model accuracy was recorded.\n\nTraining\n\nThe BreakHis dataset was split into training, validation, and test subsets at ratios of 0.75/0.15/0.10. Outputs from MobileNetV3 were flattened with a Global_Average_Pooling2D layer, then passed to four Dense layers with ReLU activation, followed by a Dense layer with softmax activation to eight outputs. Dropout regularization and BatchNormalization were used to reduce overfitting to the training set. As shown in Fig. 4, the final model consisted of approximately 3.0 million parameters, with 98% associated with MobileNetV3 Large. Hyperparameters of learning rate epoch decay, freezing layer setting were loaded into a spreadsheet, and used to autonomously train the model for 13 epochs. The workflow is illustrated in Fig. 5. Accuracy measurements on the validation set were recorded, and are displayed below. The full Python code is available at \nhttps://github.com/kdevoe/MobileNetV3_Breast_Cancer_Detection\n.\n\nFigure 4\n\nArchitecture of model used for image classification.\n\nFigure 5\n\nWorkflow diagram. Histology images are pre-processed, then vectorized. Data augmentation is applied and fed to the MobileNetV3 model, which is trained for 13 epochs. The trained model is fed an image from the validation set, on which the model makes a prediction, selecting from one of eight possible labels. The output is then displayed.\n\nResults\n\nModel training and hyper-parameter tuning were focused on three key areas; data augmentation, layer selection for training, and learning rate selection.\n\nFig. 6 depicts the accuracy on the training and validation sets, and the effect of data augmentation. In our model, accuracy in the training and validation sets exhibited minimal separation. After 10 epochs of training, validation accuracy reached a plateau with no further improvement. Training and validation accuracy tracked closely, and the impact of data augmentation was minimal.\n\nFigure 6\n\nComparison of training vs. validation curves with and without data augmentation. Overall, data augmentation reduces overfitting, bringing the validation results closer to the training results. Training runs were performed with an initial learning rate of 0.001, epoch decay rate of 0.95, and fine-tuning start layer of 150.\n\nLayerwise learning\n\nImplementation of transfer learning during model training involves setting the hyperparameter that determines the layer at which to freeze (preserve) pre-trained ImageNet embeddings, beyond which training will occur in the MobileNetV3 model. If the freezing layer is set too distally, then an insufficient number of layers of the model will be fine-tuned on the new dataset, and performance on tumor images will be inadequate. However, if the freezing layer is set too proximally, then too much of the pre-trained embeddings will be overwritten, and edge-and shape-detecting functionality garnered from pre-training on ImageNet will be lost (Fig. 7). Initiating the training prior to layer 130 led to poor, erratic performance of the overall model, as the ability to recognize basic image features learned from ImageNet had been overwritten. On the other hand, delaying the start of training until after layer 150 resulted in a slight decline in performance, likely due to inadequate fine-tuning with the tumor dataset. Based on these results, layer 150 was selected as the optimal threshold, beyond which the model parameters would be made available for training.\n\nFigure 7\n\nValidation accuracy of the model after 13 epochs based on the initial fine-tuning layer. For reference, the model includes 268 layers total, with 263 layers from MobileNetV3. Each training session used an initial learning rate of 0.001 and epoch decay rate of 0.95. This graph represents 21 unique model training runs.\nLearning rate selection (epoch decay rate)\n\nThe heuristically determined initial learning rate for training is the coefficient modulating the adjustments made to the weights and biases of the model parameters after each epoch of training. As the training proceeds and the loss function converges to the local minimum, it may be advantageous to reduce this value progressively, so as not to overshoot the target weightings.24 There is no standard learning rate schedule, and various implementations have been developed empirically to maximize test accuracy for particular benchmarks.25 We chose to reduce the learning rate by a constant factor at each epoch of training, which we refer to as the epoch decay rate. Fig. 8 shows the model validation accuracy as a function of initial learning rate and epoch decay rate. As indicated previously, fine-tuning began at layer 150 out of the total of 268 layers. We tested several learning rate settings without epoch decay and found that the highest accuracy was achieved with a learning rate of 0.0003. However, when epoch decay was incorporated, the highest accuracy was achieved with an initial learning rate of 0.001 and an epoch decay rate of 0.95. More aggressive epoch decay rates of 0.9 and 0.8 resulted in further reduction of model accuracy, suggesting excessive attenuation of training efforts at later epochs. Therefore, for the final analysis, a start learning rate of 0.001 and epoch decay of 0.95 was selected.\n\nFigure 8\n\nValidation accuracy of the model based on the initial learning rate and epoch decay rate. Fine-tuning was started at layer 150 out of 268 total layers for all models, with training run for 13 epochs.\n\nClassification accuracy\n\nThe trained model was then used to predict on images in the test set, and was able to classify the histology slides as benign vs. malignant with a recall of 0.97, a precision of 0.98, and an F1 score of 0.98 (Fig. 9a). 95% confidence intervals were calculated according to the method outlined by Newcombe.26 Amongst the subtypes of each major category, the model had the greatest success in identifying ductal carcinoma, adenosis, and tubular adenoma, with accuracies of ≥0.9. Mucinous carcinoma and phyllodes tumors were accurately identified in 0.85 of cases. Accurate classification for papillary carcinoma was 0.74, and lobular carcinoma was 0.56 (Figs. 9b, ​10).\n\nFigure 9\n\n(a) Confusion matrix for accuracy generated for the binary classification of benign vs. malignant. (b) Confusion matrix for accuracy in classifying histological subtypes.\n\nFigure 10\n\nF1 score, precision score, and recall scores (±95% confidence intervals) for classification into the histological subtypes.\n\nThe F1 score, precision, and recall scores were calculated for the classification into the four benign and four malignant subtypes. The performance of the model was highest with classifying ductal carcinoma, likely because of the larger number of samples on which to train. Performance was lowest with phyllodes tumor, of which the fewest samples were provided in the dataset.\n\nThe tabular data in Fig. 10 can also be visualized as receiver operating curves (ROC), as depicted in Fig. 11. The ROC curves confirm the relative classification performance.\n\nFigure 11\n\nROC AUC curves depicting accuracy of the classifier across the histological subtypes.\n\nIncorrectly classified images\n\nTo gain insight into how the model erred in classifying some of the histopathology images, we identified the misclassified images, comparing their predicted labels with the ground-truth label. A few of these images are presented in Fig. 8.\n\nIn Fig. 12a, the image that was misclassified as lobular carcinoma contained mainly tissue stroma or possibly tissue necrosis, with very few identifiable cells. In Fig. 12b, the image misclassified as phyllodes tumor was reasonable in quality, however, in this selected image, it may be difficult for even a human pathologist to make the distinction between fibroadenoma and phyllodes tumor. The image in Fig. 12c, also misclassified as phyllodes tumor, is of such high magnification that the image consists of a sheet of nuclei with dispersed chromatin and indistinct cellular borders, and the possibilities of its provenance are numerous. The final image in Fig. 12d, misclassified as ductal carcinoma, consists mainly of cellular outlines, and may possibly represent a region of adipose tissue, with little material that can be identifiable as carcinoma. It is, therefore, not unreasonable for images to have been misclassified, as they would pose a challenge for even expert human evaluation (Bayes optimal error rate).\n\nFigure 12\n\nA sample of misclassified images. (a) Ductal carcinoma misclassified as lobular carcinoma; (b) fibroadenoma misclassified as phyllodes tumor; (c) fibroadenoma misclassified as phyllodes tumor; (d) lobular carcinoma misclassified as ductal carcinoma.\n\nDiscussion\n\nWe have shown that a fine-tuned convolutional neural network, optimized for image classification and designed for increased accuracy, decreased latency, and decreased resource utilization, was able to properly categorize breast cancer histopathology images as to being benign vs. malignant with ≥97% precision and recall. Classification of dataset images into the given histological subtypes was achieved with moderate success. Accuracy was best with the classification of ductal carcinoma, which is important since it is the most common subtype of breast cancer, representing 70%–80% of all breast cancers.27 Images representing this histological subtype were most represented in the BreaKHis dataset, which could account for the relative accuracy in classifying ductal carcinoma, as there were more examples for the model to train on. Because the model trains to decrease loss, the increased prevalence of this category might have led to more rapid reduction in loss function values than for the other categories. Nevertheless, confident identification of infiltrating ductal carcinoma would be of value, as it could potentially assist in the confirmation of this histology in over half of breast cancer cases.\n\nTo put our findings into context, we will review some of the work that inspired our own efforts. Spanhol et al17 used six feature extractors and four different unsupervised classifiers on the BreaKHis dataset at each of the magnifications available. Their model best classified malignant histology correctly, with accuracies of around 0.94, but the accuracy of classifying benign tissue ranged from 0.38 to 0.75 at various magnifications. Error was most noted in classifying fibroadenomas, which constituted around 30% of errors at every magnification. The ROC AUC was around 0.8 at each of the four magnification levels, however, the AUC with all magnifications collectively was not reported. As stated above, pathologists extract architectural information at low magnification and nuclear and cellular detail and tissue invasiveness at high magnification, so constraining the model to classify at one magnification level is an unnecessary restriction.\n\nZhu et al28 trained a custom CNN model with ideas taken from the Inception architecture (residual network connections), but with the “squeeze-excite” features of MobileNetV3. Their model was trained on the BreakHis and BACH breast cancer datasets. They also reported accuracy based on each magnification class, and achieved greater than 0.9 AUC on ROC analysis, but this was based on channel pruning of the model, ostensibly to decrease computing burden, although it raises some concern about how this model would generalized to other datasets.\n\nAraújo et al29 studied an unspecified CNN (but which resembles AlexNet) together with a support vector machine classifier, and trained on the Bioimaging 2015 challenge dataset, consisting normal as well as benign, and malignant breast cancer, some invasive and some in situ. “Patchwise accuracy” was 0.667–0.776. Image-wise accuracy was 0.778–0.833, using a voting system, the process of which is not well-described.\n\nJoshi et al30 trained an Xception model on the BreaKHis and IDC breast cancer datasets. The objective was the classify the material into benign and malignant categories. ROC AUC was 0.921 on the BreakHis dataset and 0.881 on the IDC dataset. Accuracy data regarding further subclassification into histological subsets was not presented.\n\nAmerikanos et al31 used feature augmentation with semantic segmentation to train Facebook AI Research’s Detectron2 architecture on the TUPAC16 challenge dataset. The investigators sought to avoid having a human pathologist determine the segmentation, and used an AlexNet trained with parameters published previously.32 The objective of this study, however, was to automate the identification of nuclear to facilitate feature selection, and not histological classification.\n\nWe recognize that there are more histological entities than are represented in this limited dataset, such as triple-negative breast cancers and mixed-histologies. Indeed, we coded a predictor that performed well on selected random images from the dataset but performed less accurately on selected histopathology slides obtained from the Internet. These images were resized to the 700×468 pixel and converted to a dataset as in the BreakHis dataset. The images obtained from the Internet were chosen so as to match as closely as possible the images in the BreakHis dataset. However, despite this effort, the model performed rather poorly on the small internet dataset, with an accuracy never exceeding 0.6. We feel that the primary reason for this is the difference in image quality in the BreakHis dataset and ones available on the Internet (see Supplemental Fig. 2). Differences in the intensity of hematoxylin and eosin staining can result in differences in contrast of features such as cellular architecture, nuclear structure, and stromal detail, all of which can influence training of convolutional blocks that detect edges, corners and gradients. Tafavvoghi et al highlights variability in image quality amongst several available public breast cancer datasets,33 and therefore classification accuracy between image repositories may more a reflection of differences in intrinsic source characteristics than model performance.\n\nAnother factor contributing to difficulty with model classification when attempting to predict on new images is the variability attributable to differences between low- and high-grade malignancies. Details of this kind were not provided with the BreakHis dataset, and all examples of one tumor subtype were placed into the same diagnosis bin. Accuracy may also be improved if our model were able to train using labels informing about tumor grade. Morphological pleomorphism in poorly differentiated malignancies can challenge even human pathologists, and therefore immunohistochemical data helps to confirm visual assessment of hematoxylin-eosin stained slides.\n\nOne of the known limitations of deep learning in convolutional architectures is that the trained weight embeddings are notoriously in a black box. Image data that is passed through CNNs undergo drastic transformations through convolution, pooling, flattening, such that examination of layers deep within the model reveal no discernible relationship to the original input (Fig 3). It has been difficult to elucidate precisely how a model is progressively trained to classify images so as to provide step-by-step documentation as to the process of generating predictions. A surrogate means of validation has been to report performance indicators on standardized datasets and infer that the accuracy and precision are replicated. At this time, the inability of convolutional models to justify and document the basis for classification supports the argument that these deep learning models will not replace a trained human pathologist, but instead may play an assistive role as long the pathologist does not second-guess his/her own reading, and place undue reliance on an erroneous reading of the trained model.34\n\nConclusions\n\nBuilding upon a modern CNN architecture, designed for accuracy and efficiency, and to be compact enough to run on mobile devices, we were able to develop and train a model on a dataset of breast histopathological images, such that it was able to predict the classification of the images as being benign vs. malignant in nature, with a high degree of precision and accuracy. It was also moderately successful in classifying the pathology into one of eight subcategories.\n\nThe fine-tuning process was greatly aided by automating the training process, to heuristically identify optimal hyperparameter settings, such as freezing layer determination, as well as initial learning rate and epoch decay rate. Overfitting was addressed with data augmentation as well as using Dropout and BatchNormalization. Although the process we described did not require special feature extraction pre-processing, such as semantic segmentation, it likely that there would still be benefit to the selection of features that emphasize the best characteristics of each histological entity.\n\nThe latest member of the MobileNet family of CNNs was used in this project, but there is no consensus as to the “best” convolutional network for image classification. Abid et al35 reported high accuracy (98.61%) in the classification of multi-modal medical images as to whether they were images of pathology slides, hand or chest radiographs, endoscopic and tomographic imaging using the ResNet50 model. In the Natural Language Processing field, it has been hypothesized that larger models with a greater number of trainable parameters are more “sample-efficient” and more performant on a wider range of sample data.36 This paradigm was famously countered by the “Chinchilla” paper,37 which demonstrated that a smaller model trained with an optimal number of tokens showed better performance compared to models with a larger number of parameters, and a similar situation might also hold for image classification models. Nonetheless, larger CNNs such as VGGNet, GoogLeNet/Inception, ResNet, and DenseNet would be expected to be similarly successful in achieving good accuracy in pathology image classifications.\n\nAs these models improve further, becoming more efficient and even less resource-demanding, and as pathologists develop trust in their accuracy, trained CNNs could become a useful addition to the pathologist’s diagnostic toolkit.\n\nReferences\nAmerican Cancer Society Key Statistics for Breast Cancer. 2023. \nhttps://www.cancer.org/cancer/types/breast-cancer/about/how-common-is-breast-cancer.html\nCollege of American Pathologists Protocol for the Examination of Resection Specimens from Patients with Invasive Carcinoma of the Breast, Version 4.9.0.0. \nhttps://documents.cap.org/protocols/Breast.Invasive_4.9.0.0.REL_CAPCP.pdf\n Posted June 2023.\nCollege of American Pathologists Template for Reporting Results of Biomarker Testing of Specimens from Patients with Carcinoma of the Breast. Version 1.5.0.1. \nhttps://documents.cap.org/documents/Breast.Bmk_1.5.0.1.REL_CAPCP.pdf\n Posted March 2023.\nWeigelt B., Geyer F.C., Reis-Filho J.S. Histological types of breast cancer: how special are they? Mol Oncol. 2010;4(3):192–208. doi: 10.1016/j.molonc.2010.04.004. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nHowlader N., Noone A.M., Krapcho M., et al., editors. SEER Cancer Statistics Review, 1975-2016. National Cancer Institute; Bethesda, MD: April 2019. \nhttps://seer.cancer.gov/archive/csr/1975_2016/\n based on November 2018 SEER data submission, posted to the SEER web site. [Google Scholar]\nLoizidou K., Elia R., Pitris C. Computer-aided breast cancer detection and classification in mammography: a comprehensive review. Comp Biol Med. 2023;153 doi: 10.1016/j.compbiomed.2023.106554. [PubMed] [CrossRef] [Google Scholar]\nWang S., Yang D.M., Rong R., Zhan X. Pathology image analysis using segmentation deep learning algorithms. Am J Pathol. 2019;189(9):1686–1698. doi: 10.1016/j.ajpath.2019.05.007. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nKrizhevsky A., Sutskever I., Hinton G. ImageNet Classification with Deep Convolutional Neural Networks. 2012. \nhttps://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\nAlzubaidi L., Zhang J., Humaidi A.J., et al. Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. J Big Data. 2021;8:53. doi: 10.1186/s40537-021-00444-8. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nHe K., Zhang X., Ren S., Sun J. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) IEEE Xplore; 2016. Deep residual learning for image recognition. [CrossRef] [Google Scholar]\nAlom M.D., Hasan M., Yakopcic C., Taha T.M. 2017. Inception Recurrent Convolutional Neural Network for Object Recognition. arXiv:1704.07709. [CrossRef] [Google Scholar]\nDeng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L. 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE Xplore; 2009. ImageNet: a large-scale hierarchical image database. [CrossRef] [Google Scholar]\nImageNet \nhttps://www.image-net.org/\nLakshmanan V., Görner M., Gillard R. O’Reilly Media; 2021. Practical Machine Learning for Computer Vision. Chapter 3. Image Vision; pp. 67–123. [Google Scholar]\nDing K., Zhou M., Wang H., Gevart O., Metaxas D., Zhang S. A large-scale synthetic pathological dataset for deep learning-enabled segmentation of breast cancer. Sci Data. 2023;10:231. doi: 10.1038/s41597-023-02125-y. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nJanowczyk A., Madabushi A. Deep learning for digital pathology image analysis: a comprehensive tutorial with selected use cases. J Pathol Inform. 2016;7:29. doi: 10.4103/2153-3539.186902. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nSpanhol F.A., Oliveira L.S., Petitjean C., Heutte L. A dataset for breast cancer histopathological image classification. IEEE Trans Biomed Eng (TBME) 2016;63(7):1455–1462. \nhttps://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/\n [PubMed] [Google Scholar]\nCampanella G., Hanna M.G., Geneslaw L., et al. Clinical-grade computational pathology using weakly supervised deep learning on whole slide image. Nat Med. 2019;25(8):1301–1309. doi: 10.1038/s41591-019-0508-1. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nMcDivitt R.W., Stevens J.A., Lee N.C., Wingo P.A., Rubin G.L., Gersell D. Histologic types of benign breast disease and the risk for breast cancer. Cancer. 1992;69(6):1408–1414. doi: 10.1002/1097-0142(19920315)69:6/1408::aid-cncr2820690617%3E3.0.co;2-c. [PubMed] [CrossRef] [Google Scholar]\nTan B.Y., Tan P.H. A diagnostic approach to fibroepithelial breast lesions. Surg Pathol Clin. 2017;11(1):17–42. doi: 10.1016/j.path.2017.09.003. [PubMed] [CrossRef] [Google Scholar]\nMobileNetV3 \nhttps://paperswithcode.com/lib/torchvision/mobilenet-v3\n Accessed 14 August 2023.\nHoward A, Sandler M, Chu G, Chen L-C, Chen B, Tan M, Wang W, Zhu Y, Pang R, Vasudevan V, Le QV, Adam H. Searching for MobileNetV3. Conference: 2019 IEEE/CVF International Conference on Computer Vision (ICCV). 10.1109/ICCV.2019.00140 [CrossRef]\nMobileNet, MobileNetV2, and MobileNetV3 \nhttps://keras.io/api/applications/mobilenet/\nRussell S., Norvig P. Artificial Intelligence A Modern Approach. 4th ed. Pearson; Hoboken, NJ: 2020. Computation graphs for deep learning; p. 670. 758. [Google Scholar]\nWolfe C.R. The best learning rate schedules. Deep (Learning) Focus. \nhttps://cameronwolfe.substack.com/p/the-best-learning-rate-schedules\nNewcombe R.G. Two-sided confidence intervals for the single proportion: comparison of seven methods. Stat Med. 1998;17:857–872. doi: 10.1002/(sici)1097-0258(19980430)17:8<857::aid-sim777>3.0.co;2-e. [PubMed] [CrossRef] [Google Scholar]\nLi C.I., Anderson B.O., Daling J.R. Trends in incidence rates of invasive lobular and ductal breast carcinoma. JAMA. 2003;289(11):1421–1424. doi: 10.1001/jama.289.11.1421. [PubMed] [CrossRef] [Google Scholar]\nZhu C., Song F., Wang Y., Dong H., Guo Y., Liu J. Breast cancer histopathology image classification through assembling multiple compact CNNs. BMC Med Inform Decis Mak. 2019;19:198. doi: 10.1186/s12911-019-0913-x. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nAraújo T., Aresta G., Castro E., et al. Classification of breast cancer histology images using convolutional neural networks. PLoS ONE. 2017;12(6) doi: 10.1371/journal.pone.0177544. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nJoshi S.A., Bongale A.M., Olsson P.O., Urolagin S., Dharro D., Bongale A. Enhanced pre-trained xception model transfer learned for breast cancer detection. Computation. 2023;11(3):59. doi: 10.3390/computation11030059. [CrossRef] [Google Scholar]\nAmerikanos P., Maglogiannis I. Image analysis in digital pathology utilizing machine learning and deep neural networks. J Pers Med. 2022;12(9):1444. doi: 10.3390/jpm12091444. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nJanowcyzk A., Madabhushi A. Deep learning for digital pathology image analysis: a comprehensive tutorial with selected use cases. J Pathol Informatics. 2016;7(1):29. doi: 10.4103/2153-3539.186902. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nTafavvoghi M., Bongo L.A., Shvetsov N., Busund L.R., Møllersen K. Publicly available datasets of breast histopathology H&E whole-slide images: a scoping review. \nhttps://arxiv.org/pdf/2306.01546.pdf\n arXiv:2306.01546v2 2023. [PMC free article] [PubMed]\nZhang Y., Liao Q.V., Bellamy R.K.E. Conference on Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020, Barcelona, Spain. ACM; New York, NY, USA: 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. 11 pages. [CrossRef] [Google Scholar]\nAbid M.H., Ashraf R., Mahmood T., Faisal C.M.N. Multi-modal medical image classification using deep residual network and genetic algorithm. PLoS ONE. 2023;18(6) doi: 10.1371/journal.pone.0287786. [PMC free article] [PubMed] [CrossRef] [Google Scholar]\nKaplan J., McCandish S., Henighan T., et al. 2020. Scaling laws for neural language models. arXiv:2001.08361v1 [csLG] [Google Scholar]\nHoffman J., Borgeaud S., Mensch A., et al. 2022. Training compute-optimal large language models. arXiv:2203.15556v1 [cs.CL] [Google Scholar]\nPandey A. Depth-wise convolution and depth-wise separable convolution. Medium. 2018 \nhttps://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec\n [Google Scholar]\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "following-mechanics-with-our-yolo-model-hdKQGDqLbZsZ",
    "username": "f@frankcvanris",
    "license": "No License",
    "title": "Following Mechanics with our YOLO Model",
    "publication_description": "Back to publications\nDec 29, 2024\n●\n69 reads\n●\nNo License\nWinner of\nMost Promising Innovation\nat the\nComputer Vision Projects Expo 2024\nFollowing Mechanics with our YOLO Model\nAI\nComputer Vision\nML\nRobotics\nF\n@frankcvanris\nI\nIgor Janotti\nL\nTrong Duong\nJ\n@joseph.hoang53\nLike\nBookmark\nShare\n1. Abstract\n\nThe concept of enabling a robotic device to autonomously follow a user has been explored extensively by various individuals and industries. The objective of our capstone project is to replicate these mechanics using the research and resources provided by Bellevue College. We intend to build a robot capable of carrying up to 50 lbs. in order to help the disabled. This project introduces the essential learning model designed to enable our robot to recognize and identify the user it is intended to follow. By implementing the YOLO (You Only Look Once) learning model, our system will detect objects in the environment and specify the target object. YOLO is predominantly used for object detection, making it an ideal choice for our application.\n\nPart of our research includes evaluating simple models, alongside YOLO's precursor Convolutional Neural Networks (CNN) to accomplish this task of predicting a specific individual.\n\nIn our capstone project, we will employ a Raspberry Pi to run this intensive learning model, complemented by a camera to facilitate computer vision capabilities. With the integration of extensive image processing into the model, we aim to develop a robust system capable of consistently identifying and following a designated user. Our first goal is to achieve a functional and effective model that allows our robotic device to autonomously track a user with precision. And our second goal is allow our robotic device to autonomously move from one location to the other without human control.\n\n2. Introduction\n\nOur capstone project aims to replicate these advancements by developing an autonomous following robot with substantial application potential. However, significant challenges remain, such as improving the following mechanics to accurately distinguish individuals and enhancing the robot's ability to identify and react to consistently moving objects while avoiding obstacles.\n\nThese critical questions drive our investigation and experimentation with the YOLO (You Only Look Once) model. Currently, we are in the preliminary stages of integrating computer vision into our project. In the coming weeks, we aim to develop a functional model for the Raspberry Pi, enabling us to test and refine the robot's following capabilities.\n\n\nThrough this project, we aspire to address these challenges and contribute to the field of autonomous robotics, ultimately achieving a reliable and effective system that can consistently follow a user.\n\n3. Methodology\n\nOur approach to developing an autonomous following robot involves a step-by-step increase in functionality:\n\n3.1. Establish Communication:\n\nIn order to obtain live updates from our robotic device (FollowBot) we will need to create a web application & mobile app to keep track on all of the information that the device will gather, which we will hope to use within our ML models. For swift communication between the user and robotic clients we are using server to client communication.\n\n3.2. User Proximity\n\nWhen establishing the position of our device to ensure proximity to the user, accurate distance measurement becomes paramount. Initially, we experimented with supersonic sensors to measure distances to various objects in the surrounding environment. Additionally, we utilized RSSI signal strength via a Wi-Fi module to attempt triangulating the distance between the user and the robot.\n\nHowever, recognizing the inherent limitations of RSSI for precise distance measurement, we plan to transition away from relying solely on Wi-Fi. By deploying a machine learning model on a Raspberry Pi 4 with ROS2 in tandem with existing architecture on our Arduino board, object detection will be enhanced while inaccuracies can be made up for.\n\n3.3. Machine Learning Implementation\nPreprocessing\n\nOur initial dataset is obtained from \nkaggle\n. We introduced photos of one of ourselves to serve as the target user.\n\nWe begin by preprocessing our dataset. Since we are planning to deploy this on a Raspberry Pi, images will be scaled down to 128x128 and converted to gray scale to save on costs. Next the data set will be augmented to mitigate the issue of class imbalance.\n\nThe images in the YOLO dataset must be annotated with bounding boxes. Brightness of the images will vary between -15% and +15% to mimic different light conditions. Random cropping is introduced to aid identifying the user in the event they are partially obscured.\n\nFeatures\n\nPreprocessed Image:\n\n\nDense Layer from CNN to be used as inputs for CNN-Hybrids\n\n\nImage with Bounding Boxes Used for YOLO\n\n\nBlock Diagrams\n\n\n\n\n3.4. Autonomous Navigation\n\nAfter we have successfully finalized the learning model and improved it's accuracy we plan on moving our focus to mapping out location with our device in order for it to move from point A to point B.\n\nSensor Integration\nEncoders on Wheels: Install encoders to accurately measure the distance traveled and maintain precise motion control.\nIMUs (Inertial Measurement Units): Utilize IMUs to track orientation and movement, ensuring the robot remains stable and follows the intended path.\nSupersonic Sensors and LiDAR: Equip the robot with supersonic sensors and LiDAR for obstacle detection and distance measurement.\nData Processing and Fusion\nCollect and integrate real-time data from encoders, IMUs, supersonic sensors, and LiDAR.\nUse sensor fusion techniques to improve the accuracy of the robot's perception of its environment.\nPath Planning\nImplement a algorithm (Dijkstra's) to calculate the most efficient route from the starting point to the destination, avoiding detected obstacles.\nContinuously update the planned path as new obstacles are detected in real-time.\nObject Detection and Tracking\nIntegrate the YOLO (You Only Look Once) machine learning model to enable real-time object detection and user identification.\nUse YOLO to dynamically recognize and avoid obstacles while following the designated user.\nMotion Control\nDevelop motion control algorithms to manage the robot's movement along the planned path.\nUtilize feedback from encoders and other sensors to ensure smooth and precise movements.\nSystem Integration and Testing\nIntegrate all subsystems (sensor data fusion, path planning, object detection, and motion control) into a cohesive navigation system.\nConduct extensive testing in various environments to validate the robot's autonomous navigation capabilities.\nCollect and analyze performance data to make necessary adjustments and improvements.\n\nBy following these steps, we aim to develop a robust and reliable autonomous navigation system capable of precise and efficient movement in various environments.\n\nWe are currently working on step 2 and 3 of our methodology.\n\n4. Results\n\nTested several other machine learning models, including basic models, basic CNN and CNN-hybrids. Of which, we discovered weaknesses in precision, recall, and overfitting the training data:\n\n\nCreated a YOLO model with mean average precision with a 50% IoU threshold (mAP50) of ~80%, alongside improved precision and recall:\n\n\nEstablished a communication protocol between FollowBot and the web server to separate user interaction and data processing.\n\n5. Conclusion\n\nTo date, our capstone project has successfully achieved bidirectional data communication between the user and our FollowBot. We have the capability to issue commands to FollowBot effectively. Additionally, we have laid the groundwork for integrating the machine learning models that will enhance our project.\n\nWe believe the model can still be improved to eliminate bias towards the majority class and overfitting to training data to best generalize real world inputs. Once we establish an initial following mechanism, we can deploy the model and test its compatibility with our existing architecture.\n\n6. Future\n\nWe plan on having a functioning robot with computer vision technologies using the YOLO model for object detection and avoidance, along with the capability to map autonomously from one location to another.\n\nGoing forward, we intend to improve our ML model in these areas:\nResampling techniques: Research more sophisticated techniques and re-evaluate by original metrics\nAdditional Metrics: Introduce metrics such as training time, computational load, memory load\nAccessibility: Include photos of wheelchairs so that the model to better target our intended market\nEdge Cases: Introduce photos of ordinary objects and images of target the user in different clothes and settings for sustainment of model\nDeployment and Implementation: With the improved model, save and store on a Raspberry Pi 4 with camera module with the ethos of continuous improvement\n\nIn terms of our capstone, we plan to develop a web app that can be accessed from mobile and desktop to see statistics and control (this is currently implemented locally) the FollowBot. We will then create a mobile app with the same features as the web app to see statistics and control Followbot(with voice and touch screen controls), as well as see the data of immediate usage for users( such as power and usage of weight). With all the data we are recording to be fed back to machine learning model.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\nAbstract\nIntroduction\nMethodology\n\n3.1. Establish Communication:\n\n3.2. User Proximity\n\n3.3. Machine Learning Implementation\n\nPreprocessing\n\nFeatures\n\nBlock Diagrams\n\n3.4. Autonomous Navigation\n\nView all\nCode\nDatasets\nFiles\nFollowBotMLModels.ipynb\nFollowBotMLModels.pdf",
    "awards": [
      "most promising innovation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "from-text-to-data-extracting-structured-information-on-novel-characters-with-rag-and-langchain-YxEVcZtGwccw",
    "username": "Ayush Gawande",
    "license": "MIT License",
    "title": "From Text to Data: Extracting Structured Information on Novel Characters with RAG and LangChain",
    "publication_description": "Back to publications\nMar 24, 2025\n●\n47 reads\n●\nMIT License\nWinner of\nBest Creative AI Project\nat the\nAgentic AI Innovation Challenge 2025\nFrom Text to Data: Extracting Structured Information on Novel Characters with RAG and LangChain\nAgenticAI\nAI in Literature\nGemini\nInformation Extraction\nLangChain\nLLM\nNatural Language Processing (NLP\nRetrieval-Augmented Generation (\nStructured Data Extraction\nText Analysis\nAyush Gawande\nLike\nBookmark\nShare\nUnravelling Literary Figures,\n\nOne Story at a Time - Getting Info about your favourite novel characters just got easier!\n\nProject\n\nHave you ever found yourself immersed in a novel, only to realize you've forgotten the role or relationships of certain characters? Or perhaps you've been curious about how a side character connects to the protagonist but couldn't recall the details. Such challenges are common, especially when navigating intricate narratives with extensive casts. To address this, I have developed a Python-based Command Line Interface (CLI) tool designed to automatically extract and organize detailed information about characters from unstructured text. Leveraging advanced Natural Language Processing (NLP) techniques, including Retrieval-Augmented Generation (RAG) and the LangChain framework, this tool transforms narrative text into structured data, offering readers clear insights into character roles, relationships, and traits. Additionally, when provided with multiple stories, the tool can identify which narrative a particular character belongs to, enriching the reading experience and aiding literary analysis.\n\nIntroduction\n\nCharacters are the heart of any narrative, driving the plot and engaging readers through their journeys. However, as stories grow in complexity, so does the task of tracking character roles, relationships, and developments. My project introduces a tool that automates the extraction of structured character information, providing users with:​\n\nRole Identification: Determining whether a character is a protagonist, antagonist, or side character.​\n\nRelationship Mapping: Elucidating connections between characters, such as familial ties, friendships, or rivalries.​\n\nTrait Description: Offering brief overviews of characters' attributes and personalities.​\n\nStory Association: Identifying the specific story or stories a character appears in when analyzing multiple narratives.​\n\nBy converting unstructured narrative text into structured data, this tool facilitates deeper literary analysis, supports academic research, and enhances reader engagement.\n\nInspiration & Background\n\nThis project originated from an interview challenge for a tech company that required me to demonstrate my experience with Retrieval-Augmented Generation (RAG) and Python. This challenge inspired me to address a common issue faced by readers: the difficulty of keeping track of complex character webs in literature. Reflecting on my own struggles to remember each character's significance and their connections often hindered my full engagement with the story. I realized that this is a shared dilemma among many readers. With that in mind, I set out to create a solution that not only helps ease these challenges but also enhances the overall comprehension and appreciation of literary works.\n\nAdditionally, this project aims to transform random stories into structured data. By analyzing narrative elements like character relationships, plot points, and themes, the tool can present these components in an organized manner. This structured data not only simplifies the understanding of the storyline but also allows readers to easily reference character backstories, motivations, and their interactions. Overall, this approach fosters a deeper engagement with the text and helps readers navigate complex narratives with greater ease.\n\nFeatures\nExtract structured character information from story texts\nVector database storage for efficient searching\nCLI interface for easy interaction\nOutputs character information in JSON format\nUses Google's Gemini AI model for accurate information extraction\nMethodology\nProject Structure\n\nGithub\n\nTechnical details\nUses Google's Gemini Pro model for text generation\nImplements ChromaDB as the vector store\nUses LangChain for workflow management\nEmploys text chunking for efficient processing\nImplements error handling for edge cases\nTechnical Implementation\n\nThe development of this project involves several key components:\n\n1. Data Collection\n\nThe tool processes narrative texts stored in the stories directory. These texts serve as the input from which character information is extracted.\n\n2. Text Splitting and Embedding\n\nTo manage large texts effectively, I utilized the LangChain framework to split the narratives into manageable chunks. Each chunk is then converted into embeddings—numerical representations of text—using Google's Gemini Pro model for text generation. These embeddings facilitate efficient retrieval of relevant text segments during the extraction process.\n\n3. Vector Storage\n\nThe generated embeddings are stored in ChromaDB, an open-source vector database optimized for AI applications. This setup enables quick and accurate retrieval of text chunks relevant to specific queries, which is crucial for the Retrieval-Augmented Generation (RAG) process.\nCHROMA\n\n4. Retrieval-Augmented Generation (RAG)\n\nRAG combines retrieval-based and generation-based models. In this project, when a query about a character is posed, the system retrieves relevant text chunks from ChromaDB and uses them to generate structured information about the character. This approach ensures that the generated information is both contextually relevant and accurate.\n\n5. Output Generation\n\nThe extracted character information is structured into a JSON format, providing a clear and organized representation of attributes such as name, role, relationships, and traits. This structured data can be easily analyzed, visualized, or integrated into other applications.\n\n6. Command Line Interface (CLI)\n\nI developed a user-friendly CLI to interact with the tool. Users can input commands to process specific stories and extract character information, making the tool accessible to individuals with varying levels of technical expertise.\n\n7. Error Handling\n\nTo enhance robustness, the tool implements comprehensive error-handling mechanisms to address edge cases, such as missing data or ambiguous character references, ensuring reliable performance across diverse narratives.\n\nBy integrating these components, the tool effectively transforms unstructured narrative text into structured character information, enhancing readers' engagement and understanding of complex literary works.​\n\nResults & Usage\nPrepare Your Stories\n\nPlace your story files in the stories directory. Each file should:\n\nBe in .txt format\nContain a single story\nHave a filename that represents the story title (e.g., game_of_thrones.txt)\nCompute Embeddings\n\nProcess all stories and generate embeddings:\n\npython cli.py compute-embeddings stories/\nExtract Character Information\n\nGet information about a specific character:\n\npython cli.py get-character-info \"Character Name\"\nSample Output\n\nWhen extracting information about a character, you'll get a JSON response like this:\n\nThe output includes:\n\nCharacter name\nStory title\nCharacter summary\nRelations with other characters\nCharacter type/role\nExample Response\n{\n    \"name\": \"Jon Snow\",\n    \"storyTitle\": \"Game of Thrones\",\n    \"summary\": \"Jon Snow is a brave and honorable leader who serves as the Lord Commander of the Night's Watch and later unites the Free Folk and Westeros against the threat of the White Walkers.\",\n    \"relations\": [\n        {\"name\": \"Arya Stark\", \"relation\": \"Sister\"},\n        {\"name\": \"Eddard Stark\", \"relation\": \"Father\"}\n    ],\n    \"characterType\": \"Protagonist\"\n}\nExample Story Files\n\nYour story files should contain narrative text that includes character interactions and descriptions.\nExample dataset.\n\nStories\n\nExample results with random characters\n\nTechnicals\nRequirements\nPython 3.8+\nGoogle API key with access to Gemini AI\nInternet connection for API calls\nError Handling\n\nThe tool handles various edge cases:\n\nCharacter not found in stories\nInvalid API key\nMalformed story files\nConnection issues\nFunction Snippets\n\nLink to copy prompt template\n\nConclusion\n\nThis project demonstrates the potential of integrating advanced NLP techniques, such as RAG and LangChain, to bridge the gap between unstructured literary narratives and structured data analysis. By automating the extraction of character information, the tool offers a valuable resource for readers, educators, and researchers, fostering a deeper understanding and appreciation of literature.​\n\nFor more details and access to the code plus more such projects please visit my GitHub repositories:\n\nGithub Ayu5-h\n\nFeel free to follow for more such works\n\nReferences\n\nhttps://github.com/Ayu5-h/Structured-Information-Extraction---Character-Info\n\nhttps://js.langchain.com/docs/introduction/?utm_source=chatgpt.com\n\nhttps://docs.trychroma.com/docs/overview/introduction\n\nhttps://python.langchain.com/docs/concepts/rag/\n\nhttps://cookbook.chromadb.dev/embeddings/bring-your-own-embeddings/\n\nhttps://medium.com/%40pierrelouislet/getting-started-with-chroma-db-a-beginners-tutorial-6efa32300902\n\nhttps://www.historica.org/blog/the-application-of-artificial-intelligence-in-literary-text-analysis-modern-approaches-and-examples\n\nhttps://medium.com/%40shraddhakhanapur408/analyzing-literary-character-relationships-using-python-ce89e81f257c\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nUnravelling Literary Figures,\n\nProject\n\nIntroduction\n\nInspiration & Background\n\nFeatures\n\nMethodology\n\nProject Structure\n\nTechnical details\n\nTechnical Implementation\n\nData Collection\nView all\nCode\nDatasets",
    "awards": [
      "best creative ai project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "handwritten-text-recognition-computer-vision-project-ipvk7atIwl2I",
    "username": "n@nivedita.chauhan",
    "license": null,
    "title": "Handwritten text Recognition/ Computer Vision Project ",
    "publication_description": "Back to publications\nSep 18, 2024\n●\n78 reads\nWinner of\nDistinguished Applied Solution Showcase\nat the\nComputer Vision Projects Expo 2024\nHandwritten text Recognition/ Computer Vision Project\nAI application\nazure document intelligence\ncomputer vision\nhandwritten text recognization\nmerchant analytics\nmerchant onboarding\nmobilev2net\nOCR\nN\n@nivedita.chauhan\nI\n@ishita.dhanuka\nLike\nBookmark\nShare\n\nOverview:\n\nOnboarding merchants with a bank is a crucial step in providing them with the financial infrastructure necessary to run their businesses efficiently. Banks offer a wide range of integrated payment solutions, including POS systems, payment gateways, and digital banking services, that streamline transaction processing, reduce operational costs and improve cash flow management. Additionally, partnering with a bank provides merchants enhanced fraud protection, customer support, and opportunities to scale their business with financing and growth tools.\n\nBut merchant onboarding at banks is a critical yet time-consuming process. Currently, this involves merchants manually filling out paper forms at bank branches. These forms are then scanned and sent to central processing centres located at 4 locations. At these locations, the forms are printed, and staff manually enter the data into the bank’s system. This method results in a turnaround time of 4-5 days before the data even begins verification, impacting the bank’s efficiency. Moreover, the volume of manual data entry increases the chances of errors, making data quality an ongoing challenge.\n\nObjective:\n\nOur project aims to revolutionise the merchant onboarding process by developing an automated pipeline that eliminates manual data entry. By digitising handwritten text from scanned onboarding forms and structuring the data for direct input into the bank’s system, we aim to reduce the turnaround time from 4-5 days to just a few minutes. The goal is to achieve efficiency while improving data accuracy and consistency, streamlining the entire process.\n\nData Collection:\n\nThe data for this project is collected by scanning onboarding forms filled out at bank branches. These scanned forms are saved in zip files, each of which contains multiple folders. Each folder represents a single onboarding session, containing 15-17 images in .tif format. However, only 3 of these images are relevant to the onboarding form, while the remaining images include irrelevant documents such as internal bank guidelines and supporting materials submitted by the merchant.\n\nData Pre-processing:\n\nGiven the nature of scanned documents, preprocessing is crucial for enhancing image quality. The pre-processing steps include:\n\nDenoising - Gaussian Blurring and median filtering are applied to remove noise and imperfections from the image.\n\nEnhancing Contrast - Histogram equalization and contrast stretching are used to improve text clarity by adjusting brightness and contrast, making the text more readable.\n\nResolution Standardization - Image resizing is performed using interpolation techniques ensuring a consistent resolution across all images.\n\nOrientation Correction - Contouring and edge detection are employed to identify skewed or misaligned text, followed by rotation or transformation, ensuring all images are aligned properly for further processing.\n\nOnce the images have been pre-processed, the next step is to identify the 3 relevant onboarding form images. Since these 3 images can appear anywhere within a set of 15-17 images, a classification model is necessary to streamline the identification process.\n\nImplementation\nStep 1: Image classification:\n\nInitially, a manually labelled dataset was created from 6,000 images. Each image was tagged according to whether it represented Page 1, Page 2, Page 3, or other irrelevant documents.\n\nUsing MobileNetV2 model, a pre-trained convolutional neural network (CNN), to classify these images. The same was chosen for its lightweight architecture, containing fewer parameters and 53 layers. Compared to other CNN models like ResNet and VGG,\n\nMobileNetV2 delivers higher processing speeds, reduced inference time, and lower computational complexity, making it ideal for this project.\n\nOnce trained, the model processes each folder of images and assigns a probability score for each class (Page 1, Page 2, Page 3, or Other). The image with the highest probability in each category is then tagged accordingly, ensuring only the relevant forms are identified and used in the automated pipeline.\n\nStep 2: Text Recognition Using Transformer Models and Azure Document Intelligence:\n\nTransformers, usually known for their exceptional handling of sequential data and attention mechanisms, proves highly effective for OCR tasks. So, a pipeline was built to prepare scanned images for the model. This included denoising, improving contrast, converting images to grayscale, and applying adaptive thresholding to separate text form noisy backgrounds. Techniques like dilation and contouring helped isolate text regions, while pixel averaging, and fixed pixel approaches ensured accurate text detection.\n\nDespite this, a significant challenge arose in mapping the extracted text to its correct fields due to inconsistent layouts of the forms and obstructive stamps. To address this, we integrated Azure Document Intelligence, a cloud-based solution that excelled in analysing document layouts and accurately mapping field values, significantly enhancing the overall process.\n\nIn Azure Document Intelligence, once the relevant form images are processed, the extracted data is initially saved in structured JSON format. This includes both field names and their corresponding values.\n\nTo further refine field extraction and tackle inconsistencies in field-value mapping, a dynamic origin shift logic is implemented where instead of relying on top-left corner of the form as starting point (which can often cause issues due to form misalignment or partial screening), we strategically placed a new origin point with the form, closer to where the relevant data begins. Additionally, a margin of error was introduced around the calculated pixel locations for each field, allowing some flexibility. This prevents overly rigid conditions and ensures that even if the form is slightly misaligned, the model can still correctly identify and extract the necessary fields.\n\nStep 3: Validating and Structuring Data\n\nOnce the data is extracted and text is prepared, validating and setting rules for each field becomes crucial to ensure accuracy. With more than 70 fields being processed, each field is assigned specific validation criteria.\n\nFor example, the PAN Card field is required to exactly have 10 characters: the first five alphabet, next four numeric and last one alphabetic. If any of the fields fails to meet the designated rule, it gets automatically highlighted in the output, making it easy to identify and correct errors quickly.\n\nImpact and Conclusion:\n\nOur solution drastically reduced the turnaround time for form processing. Tasks that previously took 8-10 minutes per form are now completed in just 20-30 seconds resulting in over a 95% reduction in processing time. This efficiency ensures that forms are processed and entered the system on the same day.\n\nThe model, with an 84% accuracy, has greatly enhanced data entry reliability. Rule based validation has ensured consistency and prepared the extracted data for analysis, enabling the bank to offer tailored services to merchants based on accurate onboarding information.\n\nBy eliminating manual intervention, bank has achieved more systematic and scalable onboarding with high quality data entry, leading to improved service, business growth and higher customer satisfaction.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nOverview:\n\nObjective:\n\nData Collection:\n\nData Pre-processing:\n\nImplementation\n\nStep 1: Image classification:\n\nStep 2: Text Recognition Using Transformer Models and Azure Document Intelligence:\n\nStep 3: Validating and Structuring Data\n\nImpact and Conclusion:\n\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nYou might be interested\nAutomating Invoice Validation: Leveraging AI for Scalable Document Processing\nS\nSep 18, 202461 reads\nAI in BankingAzure Document Intelligence+6\nIntelligent Invoice Processing System Using Vision-Language Models\nJ\nAug 20, 202524 reads\nai-fintechanomaly-detection+3\nIdentifying presence of Government-issued Personally Identifiable Information embedded in documents.\nS\nA\nDec 30, 202491 reads\nAI in Data PrivacyCybersecurity in Digital Service+12\nUPI Transaction Extractor: OCR-Based Data Parsing Tool\nDec 28, 202452 reads",
    "awards": [
      "distinguished applied solution showcase"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "introducing-embedanything-a-lightweight-multimodal-embedding-library-made-in-rust-HRa5fXfK7DZs",
    "username": "a@arballal95",
    "license": null,
    "title": "Introducing EmbedAnything : A lightweight multimodal embedding library made in Rust",
    "publication_description": "Back to publications\nOct 31, 2024\n●\n89 reads\n●\nApache 2.0\nWinner of\nMost Innovative Project\nat the\nNLP Projects Expo 2024\nIntroducing EmbedAnything : A lightweight multimodal embedding library made in Rust\nembedding\nnlp\nA\n@arballal95\nS\n@sonampankaj2\nLike\nBookmark\nShare\n\nAbstract\n\nEmbedAnything\n is an open-source Rust/Python framework that lets you generate vector embeddings for any data (text, images, audio) with minimal code. It's blazing fast, memory-efficient, and can handle massive datasets through vector streaming - meaning you can process 10GB+ files without running out of RAM. Whether you're building a search engine or recommendation system, you can start with pip install embed-anything and a few lines of Python.\n\nIntroduction\n\nEmbedding models are essential today. They have become extremely important due to their use in Retrieval-Augmented Generation (RAG), Image Search, Recommendation Systems, and many other applications. EmbedAnything provides a way to embed data from different modalities ranging from audio, images and text in an fast and efficient way. The library is completely written in Rust and leverages the Huggingface Candle library to serve highly performant embedding models like Jina, ColPali, and others. The best part is that there is no dependence on PyTorch or libtorch, making deploying your applications very easy. In this article, we will look at more features EmbedAnything offers. But first, let's see what motivated us to make this library.\n\nMotivation\n\nThe AI landscape today is fantastic. New cutting-edge models pop up almost every month. They are more efficient than ever and have excellent capabilities. However, deploying these large models is quite a pain, and there are several frameworks like VLLM, LitServe, and more to make serving LLMS easy. However, most of these solutions do not provide a way to efficiently serve embedding models. Embedding models are challenging because they require a lot of pre-processing even to start the embedding process. For example, embedding a PDF requires extracting the text, chunking it, embedding it, adding the required metadata, and then pushing these embeddings to a vector database. Solutions like Ollama and FastEmbed exist, which provide embedding features but have drawbacks. Other solutions require PyTorch or Libtorch, which makes the application footprint quite heavy, and thus, deployment is more complicated. Moreover, currently, there are no existing solutions to extract embeddings and custom metadata from various file formats. LangChain offers some solutions, but it is a bulky package, and extracting only the embedding data is difficult. Moreover, LangChain is not very suitable for vision-related tasks.\n\nThis is where EmbedAnything comes in. It is a lightweight library that allows you to generate embeddings from different file formats and modalities. Currently, EmbedAnything supports text documents, images and audio, with many more formats like video in the pipeline. The idea is to provide an end-to-end solution where you can give the file and get the embeddings with the appropriate metadata.\n\nDevelopment of EmbedAnything started with these goals in mind:\n\nCompatibility with Local and Cloud Models: Seamless integration with local and cloud-based embedding models.\nHigh-Speed Performance: Fast processing to meet demanding application requirements.\nMultimodal Capability: Flexibility to handle various modalities.\nCPU and GPU Compatibility: Performance optimization for both CPU and GPU environments.\nLightweight Design: Minimized footprint for efficient resource utilization.\nOur Solution\n\nNow, let us look at the different ways we are tackling the problems associated with embedding data.\n\nKeeping it Local\n\nWhile cloud-based embedding services like OpenAI, Jina, and Mistral offer convenience, many users require the flexibility and control of local embedding models. Here's why local models are crucial for some use cases:\n\nCost-Effectiveness: Cloud services often charge per API call or model usage. Running embeddings locally on your own hardware can significantly reduce costs, especially for projects with frequent or high-volume embedding needs.\nData Privacy: Certain data, like medical records or financial documents, might be too sensitive to upload to the cloud. Local embedding keeps your data confidential and under your control.\nOffline Functionality: An internet connection isn't always guaranteed. Local models ensure your embedding tasks can run uninterrupted even without an internet connection.\nPerformance\n\nEmbedAnything is built with Rust. This makes it faster and provides type safety and a much better development experience. But why is speed so crucial in this process?\n\nThe need for speed:\n\nCreating embeddings from files involves two steps that demand significant computational power:\n\nExtracting Text from Files, Especially PDFs: Text can exist in different formats such as markdown, PDFs, and Word documents. However, extracting text from PDFs can be challenging and often causes slowdowns. It is especially difficult to extract text in manageable batches as embedding models have a context limit. Breaking the text into paragraphs containing focused information can help. This task is even more compute intensive which using OCR models like Tesseract to extract text.\nInferencing on the Transformer Embedding Model: The transformer model is usually at the core of the embedding process, but it is known for being computationally expensive. To address this, EmbedAnything utilizes the Candle Framework by Hugging Face, a machine-learning framework built entirely in Rust for optimized performance.\nThe Benefit of Rust for Speed\n\nBy leveraging Rust for its core functionalities, EmbedAnything offers significant speed advantages:\n\nRust is Compiled: Unlike Python, Rust compiles directly to machine code, resulting in faster execution.\nEfficient Memory Management: Working with embedding models requires careful memory usage when handling models and embeddings. Rust's efficient data structures make this possible.\nTrue Concurrency: Rust enables genuine multi-threading and asynchronous programming. As illustrated in the image below, we can simultaneously extract text, split content, generate embeddings, and push data to the database like in the Vector Streaming feature discussed below.\n\nThe image shows the speed of embedding documents with EmbedAnything compared to other libraries. You can find the source for the benchmark \nhere\n.\n\nWhat does Candle bring to the table?\n\nRunning language models or embedding models locally can be difficult, especially when you want to deploy a product that utilizes these models. If you use the transformers library from Hugging Face in Python, you will depend on PyTorch for tensor operations. This, in turn, depends on Libtorch, meaning you must include the entire Libtorch library with your product. Also, Candle allows inferences on CUDA-enabled GPUs right out of the box.\n\nMultiple Modalities\n\nEmbedAnything supports different modalities. You can embed text documents like HTML Pages, PDFs, and Markdowns using text embedding models like Jina, AllMiniLM, and others. You can also embed images using CLIP.\n\nAudio files can also be embedded using Whisper. The best part about EmbedAnything for audio embedding is that you can connect a text embedding model with Whisper. Using this, you can embed the text that Whisper decodes in parallel. The metadata includes the time stamps of the texts. You can see this in action in this \nHuggingface Space\n.\n\nMoreover, with EmbedAnything, you can use late-interaction models like ColPali, which remove the need to do OCR or chunking by embedding the PDF pages as a whole and retrieving the relevant PDF and pages against a query. This has enormous potential. For example, you can reduce the number of tokens that a Vision Language Model uses for document reading by retrieving only the valid pages and then showing them to large VLMs like GPT-4o and Gemini Flash. This can save a lot of cost and time.\n\nVector Streaming\n\nVector streaming allows you to create an asynchronous chunking and embedding task. We can effectively spawn threads to handle this task using Rust's concurrency patterns and thread safety. This is done using Rust's MPSC (Multi-producer Single Consumer) module, which passes messages between threads. Thus, this creates a stream of chunks passed into the embedding thread with a buffer. Once the buffer is complete, it embeds the chunks and sends the embeddings back to the main thread, where they are sent to the vector database. This ensures time is well spent on a single operation and no bottlenecks. Moreover, only the chunks and embeddings in the buffer are stored in the system memory. They are erased from the memory once moved to the vector database.\n\nThus, it effectively solves the problem by making many tasks done asynchronously and thus improving efficiency.\n\nReal-world Use Cases\nVector Streaming: This involves streaming live information from videos by breaking them into frames and generating embeddings of the images using multimodal embedding models like CLIP. This technique is particularly useful for live camera feeds, such as CCTV, to detect malicious activities or traffic violations.\nSearch Applications: We generate embeddings from PDFs and enable direct searches for page numbers based on user queries, utilizing ColPali. All of this is integrated into our pipeline.\nClassification Problems: Embeddings can be employed for classification tasks and can scale through metric learning, allowing for the easy addition of new classes.\nRAG Applications: We view EmbedAnything as an ingestion pipeline for vector databases, which can be extensively utilized for Retrieval-Augmented Generation (RAG) in chatbots.\nHow to get started?\n\nTo install our library, all you have to do is\n\npip install embed-anythin\n\nTo improve the speed further and to use large models like ColPali, use the GPU version of embed anything by running.\n\npip install embed-anything-gpu\n\nThen, with just a few lines of code, you can embed any files and directories.\n\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"model link from huggingface\"\n)\nconfig = TextEmbedConfig(chunk_size=200, batch_size=32)\ndata = embed_anything.embed_file(\"file_address\", embeder=model, config=config)\n\nYou can check out the documentation at \nhttps://starlight-search.com/references/\n\nWhat’s Next\n\nFuture work includes expanding our modalities and vector streaming adapters. We currently support unstructured sources like images, audio, and texts from different sources like PDFs, markdown, and jpegs, but we would also like to expand to graph embeddings and video embeddings. We currently support Weaviate and Elastic Cloud to stream the vectors, which we will expand to other vector databases.\n\nMoreover, we will also release methods to fine-tune embedding models easily using Candle, similar to how Sentence Transformers does, but with the speed and memory efficiency of Rust.\n\nWith this, I would like to conclude this article on this amazing new library that I am building, and I hope to receive some great feedback from the readers. Try it out now and check out the \nGitHub Repo\n.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nMotivation\n\nOur Solution\n\nKeeping it Local\n\nPerformance\n\nThe Benefit of Rust for Speed\n\nWhat does Candle bring to the table?\n\nMultiple Modalities\n\nVector Streaming\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nYou might be interested\nVector Databases: Building a Semantic Retrieval System (AAIDC-Week2-Lesson-4b)\nMay 26, 2025548 reads\nAgentic AIChroma-DB+1\nI Built a Semantic Search API Using FastAPI and SBERT — Here's How It Works\nH\nMay 23, 20254 reads\nBERTFastAPI+3\nRAG Based Research Assistant\nA\nJun 13, 202510 reads\nAAIDC2025\nRAG Based Chatbot--Ready Tensor Publication Explorer\nSep 04, 202516 reads\nAAIDC2025AI Assistant+1",
    "awards": [
      "most innovative project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "leveraging-kaplanmeier-estimator-transforming-traditional-pos-rental-into-a-onetime-subscription-IlXwPPQrWB7f",
    "username": "n@nivedita.chauhan",
    "license": null,
    "title": "Leveraging Kaplan-Meier Estimator: Transforming Traditional POS Rental into a One-Time Subscription",
    "publication_description": "Back to publications\nSep 20, 2024\n●\n97 reads\n●\nCreative Commons Attribution-ShareAlike (CC BY-SA)\nWinner of\nMost Innovative Project\nat the\nAI Project Showcase 2024\nLeveraging Kaplan-Meier Estimator: Transforming Traditional POS Rental into a One-Time Subscription\nAI in Banking\nBanking Innovation\nCLV\nCustomer Lifetime Value\nCustomer Segmentation\nKaplan-Meier\nLTV\nMerchant Services\nPoint of Sale\nPOS\nPOS Systems\nPricing Innovation\nRevenue Optimization\nSubscription\nSurvival Analysis\nN\n@nivedita.chauhan\nE\n@eashan_gaikwad\nH\nHirdesh Khanna\nLike\nBookmark\nShare\n\nCredit: Photo by \nVagaro\n on \nUnsplash\n\nAbstract\n\nThis case study explores an innovative subscription model introduced by a major financial institution in India to transform the traditional Point of Sale (POS) rental system. Merchants, who previously paid monthly rental fees for POS terminals, often faced difficulties maintaining sufficient account balances, leading to challenges in fee collection. To address this, the institution introduced a one-time subscription plan, eliminating recurring rental fees. Using the Kaplan-Meier estimator, the institution optimally priced the one-time fee based on merchant lifetime value (LTV), factoring in geography, merchant category, and turnover. This model enhances revenue predictability and mitigates risks associated with delayed or missed payments. Early results show strong market acceptance, with significant upfront revenue generated, and expansion plans are in progress.\n\nIntroduction\n\nMerchants are an integral part of a major financial institution’s business, and the bank offers a range of services to support their operations, including unified payment solutions such as Point of Sale (POS) terminals and payment gateways.\n\nTraditionally, merchants pay a monthly rental fee to retain their POS terminal IDs (TIDs). This fee is automatically deducted from the merchant’s linked Current Account (CA) through a standing instruction. However, in some cases, merchants struggle to maintain sufficient balances in their accounts, leading to difficulties in collecting charges like rental fees, low usage charges, and late settlement fees. Over time, these outstanding charges can accumulate, resulting in significant revenue losses for the bank.\n\nTo address these challenges, the bank introduced a new pricing model: a simplified POS subscription plan. This innovative approach replaces the recurring monthly rental fees with a one-time setup fee, eliminating the need for ongoing charges. By shifting to this model, the bank aims to streamline operations, improve cash flow predictability, and increase revenue while offering merchants a simpler, more transparent payment structure.\n\nTransforming POS Services with a One-Time Subscription Model\n\nThe proposed approach aims to transform the traditional POS rental model into a one-time subscription service. This innovative approach focuses on determining the optimal upfront fee a merchant should be charged, taking into account lifetime monthly rentals and other associated charges.\n\nUnderstanding Merchant Relationships to Optimize Pricing\n\nTo develop an effective pricing strategy, it's crucial to understand the nature of the bank's relationship with its merchants. This relationship fundamentally influences how we calculate a merchant's lifetime value (LT), which in turn informs the optimal pricing structure.\n\nIn the context of POS services, merchant relationships with the bank can be categorized into two primary types:\n\nAs seen in the visual above, merchant relationships can be broadly categorized as follows:\n\nContractual: Fixed-term agreements with defined start, end, and renewal points.\nNon-contractual: Flexible arrangements without long-term commitments, allowing merchants to start or stop service use at will.\n\nIn the case of POS rentals, the bank's relationship with merchants typically falls under the contractual type. Merchants are considered to have churned when they do not renew their contract or stop paying the monthly rental fee. This well-defined churn point makes it suitable for survival analysis techniques.\n\nIn the rest of this case study, we will focus on the application of survival analysis techniques to optimize the one-time setup fee for contractual relationships.\n\nCalculating Lifetime Value for Contractual Relationships\n\nTo determine the optimal approach for calculating Lifetime (LT) in contractual relationships, three techniques were explored: simple retention model, discrete time model, and Kaplan-Meier estimator. Our analysis found that the Kaplan-Meier estimator was best suited for our business problem. Below, we outline each method and explain why Kaplan-Meier was selected as the optimal solution.\n\n1. Simple retention model\n\nIn this approach, LT is estimated as 1/Churn Rate. However, this method assumes a constant churn rate month-on-month, which did not align with the variability observed in our scenario.\n\n2. Discrete time model\n\nWe fitted a logistic regression model using static independent variables such as demographics, merchant category, and declared annual turnover, along with the month of attrition. However, the model's accuracy and R-squared values remained low due to the limitation of using only these static variables.\n\n3. Kaplan-Meier estimator\n\nThe Kaplan-Meier estimator is a non-parametric method for estimating survival probabilities in our merchant attrition scenario. These estimated probabilities can be used to calculate Merchant Lifetime (LT) as follows:\n\nWhere  is the survival function, representing the probability that a merchant will continue using the POS service beyond time . The Kaplan-Meier estimator for the survival function in our context is given by:\n\nWhere:\n\n are the ordered attrition times\n is the number of merchant attritions at time \n is the number of merchants still using the service just prior to \n\nThe probability of a merchant continuing service from one time point to the next is calculated as:\n\nThis method proved to be the most effective for our analysis of merchant attrition, as it doesn't rely on assumptions about the distribution of service durations and can handle the variability in our merchant data.\n\nIt's worth noting that in our scenario, both Type I (fixed-time) and random right censoring apply. The Kaplan-Meier estimator naturally handles these types of censoring without requiring modifications to our calculations. For an explanation of right censoring and its implications, please refer to Appendix A.\n\nIn the following section, we will provide a detailed example to demonstrate how the Kaplan-Meier estimator was applied to calculate Merchant Lifetime (LT) in our specific POS service context.\n\nExample: Calculating Survival Probabilities and Merchant Lifetime\n\nLet's consider a simplified example to illustrate how the Kaplan-Meier estimator can be used to calculate survival probabilities and Merchant Lifetime (LT) for POS terminal services. Note that these values are illustrative and not reflective of actual data.\n\nSuppose we have data for 100 merchants who started using POS terminals at the same time. We'll look at their status over a 6-month period.\n\nMonth ()\tMerchants at start ()\tAttritions ()\n1\t100\t10\n2\t90\t10\n3\t80\t5\n4\t75\t10\n5\t65\t5\n6\t60\t5\n\nNote: At the end of month 6, the remaining 55 merchants are considered right-censored due to the study ending.\n\nStep 1: Calculate Survival Probabilities\n\nUsing the Kaplan-Meier estimator, we calculate the survival probabilities  for each month:\n\nMonth 1: \nMonth 2: \nMonth 3: \nMonth 4: \nMonth 5: \nMonth 6: \n\nHere's a detailed table showing the values at each month:\n\nMonth ()\tMerchants at start\tAttritions\tSurvival Rate\tCumulative Survival Probability\n0\t100\t0\t1.000\t1.00\n1\t100\t10\t0.900\t0.90\n2\t90\t10\t0.889\t0.800\n3\t80\t5\t0.9375\t0.75\n4\t75\t10\t0.867\t0.65\n5\t65\t5\t0.923\t0.60\n6\t60\t5\t0.917\t0.55\n\nNote: The survival rate for each month is calculated as , and the cumulative survival probability  is the product of the current month's survival rate and all previous months' survival rates.\n\nSee the following chart which plots the survival rates over time.\n\nStep 2: Calculate Merchant Lifetime (LT)\n\nThe Merchant Lifetime (LT) is calculated as the sum of the survival probabilities:\n\nIn our example, summing up to 6 months:\n\n months\n\nStep 3: Calculate Lifetime Value (LTV)\n\nAssuming a monthly rental fee of ₹2,000, the LTV would be:\n\n _ Monthly Rental Fee = 5.250 × ₹2,000 = ₹10,500\n\nInterpreting Results\nOn average, we expect a merchant to continue using the POS terminal for about 5.25 months.\nThe expected lifetime value of a merchant is ₹10,500.\nThis is a conservative estimate due to right-censoring at the end of the study period.\nWhen pricing a one-time subscription, consider factors such as upfront costs, desired profit margin, and market conditions.\n\nFor example, with upfront costs of ₹5,000 and a 20% profit margin target:\n\nSubscription Fee = ₹5,000 + ₹10,500 + (₹10,500 × 0.20) = ₹17,600\n\nThis simplified example demonstrates how the Kaplan-Meier estimator can be used to inform pricing strategies for POS terminal services, while accounting for the realities of merchant attrition and study limitations.\n\nImplementing the Kaplan-Meier Estimator\nDataset and Time Frame\n\nMerchant deactivations over a 12-month period were used to estimate Lifetime (LT). This timeframe provided a substantial dataset to capture various patterns of merchant behavior and attrition.\n\nClustering for Differential LT Estimation\n\nFor efficient LT estimation, the entire merchant base was divided into clusters based on various parameters such as geography, merchant operating category, annual turnover, and other relevant factors. Outlier treatment was performed on these clusters to ensure data quality. Through this process, approximately 20 clusters were formed by combining different levels of each clustering variable. This clustering approach allowed for a more nuanced analysis of merchant lifetimes across different segments of the business.\n\nThe Kaplan-Meier estimator was applied to each cluster to calculate the survival function S(t) at each time point. Lifetime (LT) was then estimated using the following formula:\n\nWhere  represents the survival function, giving the probability that a merchant will continue using the POS service beyond time . In practice, this summation is performed from  to the last observed time point in our data.\n\nThe following table shows illustrative results for several clusters:\n\nCluster #\tEstimated LT (months)\nCluster 1\t37.6\nCluster 2\t41.4\nCluster 3\t46.2\nCluster 4\t39.1\nCluster 5\t53.8\n...\t...\n...\t...\n...\t...\n\nFor each cluster, this method allowed for the calculation of cluster-specific lifetimes, which were found to range from 38 to 54 months, as illustrated in the table above. The variation in lifetimes across clusters highlights the importance of this segmented approach in capturing the diverse characteristics of different merchant groups. By applying the Kaplan-Meier estimator to each cluster separately, we were able to account for the unique attrition patterns within each segment of our merchant base.\n\nValidating the Model for Real-World Impact\n\nTo ensure the reliability of the model, an out-of-time validation approach was employed. The cluster-wise lifetime (LT) estimates were validated using merchant deactivations from the subsequent 3 months. This method allowed the model to be tested on data not used in the initial estimation. The validation process achieved an accuracy of approximately 82%, with a tolerance for variation of ±3 months.\n\nThis validation demonstrates the robustness of the Kaplan-Meier estimator in predicting merchant lifetimes, even when accounting for some variability in the exact timing of deactivations. The high accuracy supports the reliability of the model for business decision-making.\n\nImmediate Business Impact: Revenue and Efficiency Gains\n\nThe implementation of the one-time subscription fee model, based on the Kaplan-Meier lifetime estimation, has yielded significant immediate benefits:\n\nRevenue Generation: Within the first 6 months, the model generated substantial upfront revenue from new-to-bank (NTB) merchants, representing a marked improvement over the previous monthly rental collection model.\n\nImproved Cash Flow: By collecting fees upfront, the bank eliminated the need for monthly rental collections, reducing administrative overhead and improving cash flow predictability.\n\nMarket Acceptance: The model has been well-received by merchants, suggesting they see value in the simplified, one-time payment structure.\n\nExpansion Plans: Given the initial success, plans are underway to extend this model to existing-to-bank (ETB) merchants, with strategic modifications to suit this segment.\n\nConclusion: A Data-Driven Revolution in Merchant Services\n\nThe success of the one-time subscription fee model demonstrates the power of applying advanced statistical techniques to solve real-world financial challenges. This case study illustrates several key points:\n\nData-Driven Innovation: The application of the Kaplan-Meier estimator to merchant lifetime prediction showcases how data analysis can drive product innovation in financial services.\n\nRisk Mitigation: The upfront fee structure reduces the bank's exposure to potential losses from merchant attrition, as the lifetime value is collected at the outset of the relationship.\n\nCompetitive Advantage: By offering a unique pricing model, the bank has differentiated itself in the competitive merchant services market, potentially attracting more merchants and increasing market share.\n\nScalability and Adaptability: The model's success with NTB merchants and planned expansion to ETB merchants demonstrates its scalability and adaptability to different customer segments.\n\nFuture Potential: This approach opens up possibilities for similar innovations in other areas of banking and financial services, where customer lifetime value plays a crucial role.\n\nThis case study illustrates the potential for statistical modeling and data analysis to create tangible business value, improve customer experiences, and drive strategic decision-making in the banking sector. It serves as an example of how traditional financial institutions can advanced analytical methods to innovate within their existing business models and enhance their service offerings.\n\nReferences\nDiscrete time survival models customer lifetime value: \nhttps://youtu.be/uFyYcgwyQFs?feature=shared\nKlein, J. P., & Moeschberger, M. L. (2003). Survival analysis: techniques for censored and truncated data. Springer Science & Business Media.\nAppendix\nA. Understanding Censoring in Survival Analysis\n\nIn survival analysis, censoring occurs when we have incomplete information about the survival time of some individuals in the study. In the context of our POS terminal service, censoring happens when we don't observe the exact time a merchant stops using the service. Two types of right censoring are particularly relevant to our analysis:\n\nType I (fixed-time) censoring: This occurs when the study ends at a predetermined time. Any merchants still using the service at this point are right-censored, as we only know that their service duration exceeded the study period.\n\nRandom right censoring: This happens when merchants leave the study for reasons unrelated to the event of interest (stopping POS service). For example, if we lose contact with a merchant or if they go out of business for external reasons.\n\nThe Kaplan-Meier estimator handles these types of censoring by:\n\nIncluding censored observations in the risk set () up until their censoring time.\nNot counting censored observations as attritions (they don't contribute to ).\nUpdating the survival probability only at times when attritions occur.\n\nThis approach allows us to make use of the partial information provided by censored observations, leading to more accurate estimates of survival probabilities and merchant lifetimes.\n\nIt's important to note that in the presence of heavy censoring, especially towards the end of the study period, the LT calculation may underestimate the true average lifetime. In such cases, the calculated LT effectively becomes a lower bound of the true lifetime.\n\nWhile there are other types of censoring in survival analysis (such as left censoring and interval censoring), these do not apply to our POS terminal service scenario. For readers interested in learning more about various types of censoring and their applications in survival analysis, we recommend referring to Klein and Moeschberger (2003) in References.\n\nB. Kaplan-Meier Estimator Calculation in Python\n# Python code to calculate expected merchant lifetime using Kaplan-Meier estimator\n\nexpected_lifetime = []\n\n# Loop through 20 distinct merchant clusters\nfor cluster_id in range(20):\n    # Separate churned and censored merchants by cluster\n    churned_merchants = df_comb2_ch[df_comb2_ch['Clust'] == cluster_id]\n    censored_merchants = df_comb2[df_comb2['Clust'] == cluster_id]\n\n    # Get survival times for churned and censored merchants\n    churned_survival_times = churned_merchants['Surv_time'].to_list()\n    censored_survival_times = censored_merchants['Surv_time'].to_list()\n\n    # Sort survival times for both churned and censored merchants\n    churned_survival_times.sort()\n    censored_survival_times.sort()\n\n    # Create a range of survival times from the minimum to the maximum in churned merchants\n    time_range = range(int(churned_merchants['Surv_time'].min()), int(churned_merchants['Surv_time'].max()) + 1)\n\n    # Dictionary to count occurrences of survival times for churned merchants\n    churned_counts = {time: churned_survival_times.count(time) for time in time_range}\n    # Dictionary to count occurrences of survival times for censored merchants\n    censored_counts = {time: censored_survival_times.count(time) for time in time_range}\n\n    # Initialize the total number of merchants and at-risk merchants\n    n_total = len(churned_survival_times)\n    n_at_risk = n_total\n\n    # Initialize cumulative survival probability and list to store probabilities\n    cumulative_prob = 1\n    survival_probabilities = []\n\n    # Loop through unique survival times to calculate Kaplan-Meier survival probabilities\n    for time in time_range:\n        churned_events = churned_counts[time]\n        censored_events = censored_counts[time]\n\n        # Calculate survival probability and update cumulative probability\n        survival_prob = (n_at_risk - censored_events) / n_at_risk\n        cumulative_prob *= survival_prob\n\n        # Store cumulative survival probability\n        survival_probabilities.append(cumulative_prob)\n\n        # Update the number of at-risk merchants by subtracting churned events\n        n_at_risk -= churned_events\n\n    # Calculate expected lifetime for the current cluster by summing survival probabilities\n    lifetime_estimate = sum(survival_probabilities)\n    expected_lifetime.append(lifetime_estimate)\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nTransforming POS Services with a One-Time Subscription Model\n\nUnderstanding Merchant Relationships to Optimize Pricing\n\nCalculating Lifetime Value for Contractual Relationships\n\nExample: Calculating Survival Probabilities and Merchant Lifetime\n\nImplementing the Kaplan-Meier Estimator\n\nDataset and Time Frame\n\nClustering for Differential LT Estimation\n\nValidating the Model for Real-World Impact\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nFiles\nSegLifetime.pdf\nYou might be interested\nMonte Carlo simulation approach using Gaussian distribution to optimise MDR for merchants\nN\nA\nSep 18, 202456 reads\nlinear programming problemLPP+8\nLeveraging Machine Learning for Customer Lifetime Value Estimation\nC\nOct 27, 202413 reads\nPreparationVisualization_Churn_Data\nA\nApr 06, 202510 reads\nPayments failure spike - Time series analysis\nA\nApr 07, 202511 reads",
    "awards": [
      "most innovative project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "llmknowledge2-a-framework-for-automated-knowledge-extraction-and-rag-integration-wGUpD2eWWOpg",
    "username": "Daishiro Hirashima",
    "license": "MIT License",
    "title": "LLMKnowledge2: A Framework for Automated Knowledge Extraction and RAG Integration",
    "publication_description": "Back to publications\nMar 07, 2025\n●\n113 reads\n●\nMIT License\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nLLMKnowledge2: A Framework for Automated Knowledge Extraction and RAG Integration\nChatbot\ncontext aware\ndify\ninformation extraction\nknowledge extraction\nknowledge management\nknowledge prompts\nLLM\nopen source\nprompt engineering\nQA generation\nRAG\nstructured knowledge\nDaishiro Hirashima\nLike\nBookmark\nShare\nLLMKnowledge2: A Framework for Automated Knowledge Extraction and RAG Integration\n\nFigure 1-1: LLMKnowledge2 and RAG(Dify) Integration\n\nChapter 1: Introduction\n\nThe rapid advancement of generative AI technology has opened new possibilities for leveraging organizational document data as knowledge assets. Particularly, with the increasing adoption of RAG (Retrieval-Augmented Generation) systems, there is a growing demand for structured knowledge. However, this requires efficient conversion of unstructured document data into AI-ready structured knowledge.\n\nFor instance, traditionally, creating Q&A-format knowledge from product manuals required careful reading, anticipating potential questions, and extracting or creating appropriate answers. This process demanded significant time and effort for understanding context and organizing related information. However, with the advanced language understanding capabilities of modern generative AI, this process can now be significantly automated.\n\nLLMKnowledge2 addresses this challenge through an innovative approach called knowledge prompts. A notable feature is the ability to create and refine prompts interactively within the system. Users can gradually improve prompts while verifying the quality of generated knowledge, enabling the extraction of high-quality knowledge optimized for organizational requirements.\n\nThe system's key features are as follows:\n\nFirst, it provides an integrated environment from prompt creation to knowledge generation. Users can instantly adjust prompts while verifying the quality of generated knowledge. This iterative improvement process enables optimal knowledge extraction according to organizational requirements. Furthermore, by applying different knowledge prompts to the same plain knowledge, information can be extracted from various perspectives.\n\nSecond, it offers flexible switching between generative AI engines. Users can select appropriate engines according to requirements, whether cloud-based generative AI like OpenAI and Anthropic, or locally running LLMs. This is particularly important for addressing data confidentiality requirements and regional service availability restrictions.\n\nThird, it supports diverse input formats and batch processing. The system can process plain knowledge in various formats including PDF, Word, Excel, PowerPoint, various text files, and web pages. Multiple plain knowledge sources can be processed concurrently, enabling rapid conversion of large volumes of data. Progress monitoring and bulk result management are also straightforward.\n\nThe generated knowledge can be utilized in advanced question-answering systems through integration with RAG systems like Dify. For example, it's possible to build a system that quickly responds to customer inquiries using Q&A knowledge generated from product specifications. This enables practical application of the generated knowledge in actual business operations.\n\nFigure 1-2: Dify\n\nThis paper details the LLMKnowledge2 system, explaining its value, technical implementation, and practical deployment and operation methods. In particular, we demonstrate its potential as a flexible knowledge generation foundation, leveraging features such as interactive prompt improvement and support for various generative AI engines.\n\nChapter 2: System Value\n\nThe primary value of LLMKnowledge2 lies in enabling automated large-scale knowledge conversion and multi-perspective knowledge extraction from the same data. This chapter explains the system's specific value and actual evaluation results.\n\nFirst, regarding large-scale knowledge conversion automation, the system can process multiple plain knowledge sources in batch and convert them into structured knowledge. For example, when processing 44 PDF files of product specifications as input, we successfully generated 3,085 Q&A-format knowledge entries. This represents a significant efficiency improvement compared to manual processing.\n\nNext, regarding multi-perspective knowledge extraction, the system features flexible knowledge prompt configuration as a distinctive function. Different knowledge prompts can be applied to the same plain knowledge to extract information from various perspectives. For example, after generating basic Q&As from product specifications, the same document can be used to generate troubleshooting Q&As.\n\nThe ability to select generative AI engines is also an important value. Different organizations and regions have varying available generative AI services, and cloud-based services may not be usable due to data confidentiality requirements. Our system supports various generative AI engines including OpenAI, Anthropic, and locally running LLMs, allowing selection of appropriate engines based on circumstances.\n\nTo verify system effectiveness, we conducted specific evaluations. To assess the quality of Q&A knowledge generated from product specifications, we prepared 40 test questions and verified response accuracy. The results showed accurate answers for 29 out of 40 questions (72.5%), and correct information sources (URLs) were provided for 38 out of 40 questions (95%).\n\nThis result has significant practical implications. First, the 72.5% accuracy rate is sufficient for a first-response generation system assuming human verification. More importantly, the high 95% rate of correct information source provision enables quick manual verification of response accuracy and modifications when necessary. For example, when handling product specification inquiries, operators can provide accurate responses quickly by referencing generated answers while verifying the relevant sections in the information sources.\n\nRegarding processing performance, using 10 parallel processes, we could process 44 PDF files in approximately 15 minutes. This processing speed meets practical requirements. For example, it can process over 1,400 PDFs within an 8-hour workday, sufficiently covering document processing demands for many organizations. Furthermore, increased parallelization enables handling of larger-scale processing.\n\nThe utilization of generated knowledge in RAG systems also provides significant value. For example, in sales support scenarios, it can provide real-time responses to questions during business negotiations. While traditional search systems only presented related documents based on keyword matching, RAG systems can generate responses by understanding context and combining necessary information. Furthermore, since all responses include source URLs, quick responses can be provided while ensuring accuracy.\n\nThus, LLMKnowledge2 functions not just as document processing automation but as an organizational knowledge utilization foundation. It provides particular value in three areas:\n\nKnowledge extraction efficiency: Generating over 3,000 Q&As from 44 PDFs in 15 minutes\nQuality assurance: 95% accuracy in source presentation, facilitating response verification\nPractical utilization: Enabling immediate knowledge utilization through RAG system integration\nChapter 3: Technical Implementation\n\nThe technical implementation of LLMKnowledge2 focuses on three key elements: parallel processing for rapid knowledge generation, support for diverse input formats, and generative AI engine switching mechanism. This chapter explains these technical implementation details.\n\nThe system adopts a hierarchical architecture with two dedicated servers. The first server (MarkItDownServer) handles text conversion from various document formats, while the second server (task2knowledge) executes conversion from plain knowledge to knowledge. This two-layer structure enables optimization specialized for each process.\n\nFirst, regarding parallel processing for rapid knowledge generation, the system implements a parallel processing mechanism capable of processing multiple plain knowledge sources simultaneously. Document splitting size is adjustable according to knowledge generation purposes. The basic settings specify minimum 2,000 and maximum 3,000 characters, which is appropriate context volume for generating one Q&A set. Notably, rather than simple mechanical splitting by character count, the system performs context-aware splitting. Even when exceeding the minimum size (2,000 characters), it maintains context consistency by treating up to appropriate sentence breaks as one processing unit.\n\nAdditionally, knowledge prompts can control generated Q&A density by including instructions like \"output at least three Q&As.\" This enables appropriate granularity of knowledge extraction according to document characteristics and usage purposes.\n\nNext, regarding support for diverse input formats, MarkItDownServer can process various formats of plain knowledge including PDF, Word, Excel, PowerPoint, various text files, and web pages. This is achieved by implementing text extraction modules for each format and converting to a unified internal format. The extracted text maintains the original document structure and format, ensuring accurate source references in generated knowledge.\n\nGenerative AI engine switching is implemented with flexible control through a configuration file (config.yaml). This configuration includes various parameters necessary for practical operation, such as chunk size control (chunk_size, max_chunk_size), API retry control (max_retries, retry_delay), and rate limit control (api_rate_limit). This enables appropriate operational settings according to usage environment and requirements.\n\nError handling is implemented with particular emphasis on stability in large-scale document processing. For API call failures, it executes the configured number of retries (max_retries) with intervals (retry_delay) to recover from temporary failures. Additionally, it maintains stable processing by controlling API call intervals (api_rate_limit).\n\nThus, LLMKnowledge2 functions as a practical knowledge generation foundation through its two-layer server architecture, flexible context-aware splitting process, and detailed operational parameter control. In particular, it achieves production-ready robustness through consistent processing flow from document processing to knowledge generation and flexible configuration according to operational environment.\n\nChapter 4: Deployment and Operation\n\nThe most important elements in deploying and operating LLMKnowledge2 are creating knowledge prompts and appropriately configuring generative AI engines. This chapter explains these practical aspects.\n\nKnowledge Prompt Creation and Optimization\n\nCreating knowledge prompts is a core element in effectively utilizing the system. Here is a typical prompt example for Q&A generation:\n\nPlease convert the following {source QA text} into a new format, referring to the examples and adding {{reference}} values.\n\nExample 1:\nQ: What applications is the ABC series suitable for?\nA: The ABC series is suitable for antenna connections in navigation and other equipment.\n\nExample 2:\nQ: What applications is the DEF2 series suitable for?\nA: The DEF2 series is a small coaxial connector suitable for antenna connections in marine navigation equipment.\nReference URL: {{reference}}\n\n{source QA text}=\n\n\nThis prompt's features include:\n\nClear output format examples\nUnified format specification for references\nPresentation of specific use cases\n\nPrompts are crucial elements controlling the quality and quantity of generated Q&As. For example, including instructions like \"output at least three Q&As\" allows adjustment of generated knowledge density. Additionally, prompts can be gradually improved while verifying actual output results.\n\nFigure 4: Prompt Details Page\n\nSystem Setup\n\nThe system consists of three components, each available on GitHub:\n\nLLMKnowledge2 (Main System)\n\nWeb interface\nKnowledge management functionality\nOverall system control\nGitHub: \nhttps://github.com/daishir0/LLMKnowledge2\n\nMarkItDownServer\n\nText conversion from various document formats\nDocument structure preservation\nMetadata extraction\nGitHub: \nhttps://github.com/daishir0/MarkItDownServer\n\ntask2knowledge\n\nPlain knowledge to knowledge conversion\nGenerative AI engine integration\nBatch processing execution\nGitHub: \nhttps://github.com/daishir0/task2knowledge\n\nEach component can be set up following detailed installation instructions provided in GitHub repositories. In particular, configuration file examples and operation verification procedures are provided, enabling step-by-step implementation.\n\nOperational Settings and Tuning\n\nDuring system operation, the following parameters are adjusted according to the environment:\n\nDocument Processing Settings\n\nchunk_size: 2000 (minimum characters per chunk)\nmax_chunk_size: 3000 (maximum characters per chunk)\nContext-aware splitting process control\n\nAPI Control Settings\n\nmax_retries: 5 (API retry count)\nretry_delay: 5 (retry interval in seconds)\napi_rate_limit: 0.5 (API call interval in seconds)\n\nThese settings are centrally managed in the config.yaml file and can be flexibly adjusted according to operational environment and requirements.\n\nOperational Workflow\n\nThe basic system usage procedure is as follows:\n\nPlain Knowledge Preparation\nDocument collection\nFormat verification\nPreprocessing as needed\n\nFigure 4-1: Plain Knowledge Management Page and Knowledge File Upload Page\n\nKnowledge Generation Configuration\nPrompt selection/creation\nGenerative AI engine configuration\nProcessing parameter adjustment\n\nFigure 4-2: Group Dtails Page and Prompt Selection\n\nBatch Processing Execution\nProcess monitoring\nResult verification\nPrompt adjustment as needed\n\nFigure 4-3: Task Management Page and Pending Tasks\n\nGenerated Knowledge Utilization\nUpload to RAG systems (e.g., Dify)\nQuestion-answering system construction\nEffect verification and feedback\n\nFigure 4-4: Knowledge Management Page and Processed Plain Knowledge\n\nThus, LLMKnowledge2 serves as a comprehensive knowledge generation foundation provided as open source, with systematic support from deployment to operation. In particular, practical utilization is facilitated through the provision of concrete prompt examples and configuration parameters.\n\nAcknowledgements and Future Collaboration\n\nThis research presents the LLMKnowledge2 framework as an evolving system with significant potential for further development. The author, whose research interests span natural language processing, system development, and generative AI, is actively seeking collaboration partners (individuals and organizations) to expand this work. If you are interested in joint research opportunities or can recommend suitable journals or academic publications for this work, please feel free to reach out. Your expertise, perspectives, and suggestions for potential publication venues would be greatly appreciated in advancing this research.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nLLMKnowledge2: A Framework for Automated Knowledge Extraction and RAG Integration\n\nChapter 1: Introduction\n\nChapter 2: System Value\n\nChapter 3: Technical Implementation\n\nChapter 4: Deployment and Operation\n\nKnowledge Prompt Creation and Optimization\n\nSystem Setup\n\nOperational Settings and Tuning\n\nOperational Workflow\n\nAcknowledgements and Future Collaboration",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "mdigan-generating-pixel-art-characters-8jLQ7xqMORQi",
    "username": "Flavio Coutinho",
    "license": "MIT License",
    "title": "MDIGAN: Generating Pixel Art Characters",
    "publication_description": "Back to publications\nDec 26, 2024\n●\n73 reads\n●\nMIT License\nWinner of\nMost Engaging Presentation\nat the\nComputer Vision Projects Expo 2024\nMDIGAN: Generating Pixel Art Characters\n2d\ncharacter sprite\ngame development\ngan\ngenerative ai\nimage to image\nmissing data imputation\npixel art\nprocedural content generation\nFlavio Coutinho\nLike\nBookmark\nShare\n\nSee it in action\tWatch a presentation\tRead the paper\n\n\t\n\t\nAbstract\n\nCreating and updating pixel art character sprites with many frames spanning different animations and poses takes time and can quickly become repetitive. However, that can be partially automated to allow artists to focus on more creative tasks. In this work, we concentrate on creating pixel art character sprites in a target pose from images of them facing other three directions. We present a novel approach to character generation by framing the problem as a missing data imputation task. Our proposed generative adversarial networks model receives the images of a character in all available domains and produces the image of the missing pose. We evaluated our approach in the scenarios with one, two, and three missing images, achieving similar or better results to the state-of-the-art when more images are available. We also evaluate the impact of the proposed changes to the base architecture.\n\nIntroduction\n\nCreating characters for pixel art games is a time-consuming process in game development that often involves a lot of back-and-forth adjustments. Artists meticulously design each pixel, but even small changes can require updating many images, especially for characters that move and face different directions. While artists are skilled at this, some aspects of the process can be repetitive and tedious, such as creating special effects or ensuring consistency across all character poses.\n\nTo address these challenges, researchers are exploring the use of artificial intelligence (AI) to help automate parts of the character creation process. This research often involves using AI to generate new images based on existing ones, such as creating a character facing a different direction from a set of existing images. This new approach focuses on using all available images of a character to predict missing poses, rather than simply generating images from scratch. By utilizing more information about the character, the AI model can create more realistic and high-quality results, streamlining the character design process and allowing artists to focus on more creative aspects of their work.\n\nRelated work\n\nRecent research in pixel art character generation has focused on improving the quality and versatility of AI-generated images. \nSerpa and Rodrigues (2022)\n significantly enhanced their previous work by reframing the problem as a semantic segmentation task. This approach, combined with architectural modifications like dense connections and deep supervision, resulted in more accurate and detailed character sprites, particularly in challenging poses.\n\nFurthermore, \nCoutinho and Chaimowicz (2024)\n demonstrated the importance of large and diverse datasets for training robust AI models. By compiling a dataset of 14,000 paired images of characters in different poses, they achieved significant improvements in image quality, especially when evaluated on more artistically cohesive datasets. Additionally, they introduced a post-processing step to quantize the generated images to the color palette of the input image, further enhancing the visual fidelity of the results.\n\nMethodology\n\nWe propose an architecture based on \nCollaGAN\n to impute images of pixel art characters in a missing pose (target domain). To facilitate understanding, let us consider that there are domains , one representing each pose. The architecture consists of a single generator and discriminator pair, of which the former creates an image  of a character in the missing pose  using the available images from all of the other source  poses:\n\nOur generator has one encoder branch to process the input from each domain, a single decoder branch with concatenated skip connections, and outputs an image in the missing domain. The discriminator distinguishes images as real or fake, as well as determines their domain through an auxiliary classifier output.\n\nCompared to the original CollaGAN architecture, we proposed the following modifications:\n\nIncreasing the capacity of each convolutional layer by multiplying the number of channels by 4.\nNumber of trainable parameters: 104,887,616.\nUsing an innovative conservative input dropout strategy for the batch selection during training.\nReplacing only the target image with the generated in the forward pass to create new batches for the backwards pass.\n\nWhile the change (1) is straightforward, changes (2) and (3) require explanation. However, to avoid getting too deep into the training procedure, we invite the reader to either \nread the paper\n or to \nwatch a presentation\n. Anyhow, later in this article we present an ablation study that shows how each such modification improved the resulting images.\n\nExperiments\n\nThe model trained with the pixel art characters dataset for 240,000 generator update steps in minibatches of 4 examples, which is equivalent to 80 epochs. It took 01:20h to train using a GeForce GTX 1050 Ti GPU (mid-range from 2016 with 4GB VRAM). After training, it takes 110.03ms for the model to generate a batch of images. The following animation shows the training evolution for 3 images from the training set, and 3 from the test partition.\n\nWe evaluated using the metrics \nFID\n and MAE ( distance between the generated images and the ground truth) against baselines that follow the \nPix2Pix\n and \nStarGAN\n architectures for the image-to-image translation task.\n\nPix2Pix\n (by Isola et al., 2017) is an image-to-image translation GAN architecture that can transfer images from one domain to another. So we trained 16 such models, as we have 4 domains/poses in our problem (4 x 4). Trainable parameters: 351,694,128.\nStarGAN\n (by Choi et al., 2018) tackles the same problem, but it is multi-domain. Hence, it can translate images from and to any domain, requiring only a single generator and discriminator pair. Trainable parameters: 134,448,128.\nResults\n\nNext, we present our main results with both a quantitative and qualitative analysis. But we also invite you to try the model using our \ninteractive generator\n.\n\nReceiving 3 images\n\nThe figure above shows different character examples in its rows, and the columns depict: the source and target images, then the ones generated by Pix2Pix, StarGAN and our MDIGAN (CollaGAN-3) models. As the baseline models take only a single image as input, there are 3 possible outputs for each character.\n\nThe quality of the generated images varies depending on the model and the target pose.\n\nColor Usage: While the generated images generally use colors in appropriate locations, they often exhibit a wider range of tones than the original pixel art style, which typically relies on a limited color palette. This issue can be partially addressed by quantizing the colors after generation, a technique used in previous research (\nCoutinho and Chamowicz, 2024\n).\n\nShape Accuracy:\nEasier directions: All models generally perform well when generating poses that involve simple transformations, such as flipping the character horizontally (e.g., from facing left to right). This is likely due to the relative ease of learning this specific transformation.\nHarder directions: When generating more complex poses, such as a character facing backward, some models, including ours, may exhibit artifacts, such as faint remnants of facial features in unexpected areas (see the maid in the first row).\n\nOverall Quality: We can observe that the quality of images generated by our proposed MDIGAN model is either comparable to or surpasses the performance of baseline models. Notably, the model achieved these results with a significantly smaller number of trainable parameters compared to other prominent architectures, indicating a more efficient use of resources.\n\nOur MDIGAN model is 22% smaller than StarGAN, 70% smaller than Pix2Pix in number of trainable parameters.\n\nReceiving 2 or 1 images\n\nEven though we propose a model to impute a single missing domain, we also evaluate it in scenarios where it receives two (CollaGAN-2) or only one image (CollaGAN-1). The metrics' values are averaged among all targets and all available sources for each model and scenario (i.e., CollaGAN-3, 2, and 1).\n\nThe following table compares the proposed model in those situations. We can observe that both FID and MAE metrics progressively improve as the number of available domains increases, with CollaGAN-2 still having better MAE than Pix2Pix and StarGAN.\n\nModel/Sources\tAverage FID\tAverage MAE\nPix2Pix\t4.091\t0.05273\nStarGAN\t2.288\t0.06577\nCollaGAN-1\t8.393\t0.06449\nCollaGAN-2\t4.277\t0.05035\nCollaGAN-3 🏆\t1.508\t0.04078\n\nThe values of both metrics have been averaged among all possible input/output combinations for each model.\n\nThe figure below shows example generations of the model when it receives 3 inputs (CollaGAN-3), 2 inputs (CollaGAN-2) and only 1 (CollaGAN-1).\n\nInput Dropout Strategy\n\nWe evaluated the impact of different batch selection strategies on presenting examples to the proposed model during training: Should it always see the 3 available domains, or should they sometimes be omitted?\n\nWe investigated the following approaches:\n\nalways showing all available domains (none),\nthe original input dropout strategy proposed in \nCollaGAN\n;\na curriculum learning approach suggested by \nSharma et al., 2019\n;\nand our proposed conservative tactic.\n\nThe original approach has an equal chance of presenting three, two, or a single image in a training step. The curriculum learning approach starts training with easier tasks (using three images) and progressively makes it harder (using a single input) until half of the training, then it randomly chooses between the number of domains to drop out for the second part. Lastly, the conservative approach randomly selects the number of images to drop, but with higher probabilities to keep more images: 60% with 3 images, 30% with 2, and 10% with a single image.\n\nThe following table presents the results. Using any input dropout yields better results than always showing all domains (none). Compared to the original and curriculum learning strategies, our proposed conservative tactic has better FID and MAE metrics on the average of the three scenarios.\n\nFID\tMAE\nSources\tNone\tOriginal\tCurric.\tConserv.🏆\tNone\tOriginal\tCurric.\tConserv.🏆\nCollaGAN-3\t4.816\t1.911\t2.160\t1.508\t0.04523\t0.04277\t0.04222\t0.04078\nCollaGAN-2\t19.050\t6.835\t9.233\t4.277\t0.08003\t0.05053\t0.07389\t0.05035\nCollaGAN-1\t32.676\t11.162\t20.303\t8.393\t0.12820\t0.06243\t0.12232\t0.06449\nAverage\t18.847\t6.636\t10.566\t4.726\t0.08449\t0.05191\t0.07948\t0.05187\nAblation Study\n\nTo understand the impact of our changes to the original CollaGAN architecture, we trained and evaluated models that progressively added each modification. The following table shows the FID and MAE values of the generated images averaged over all domains and among the scenarios of the model receiving three, two, and one input domains. The rows show the results of each modification cumulatively: the first one is the original CollaGAN model without any of our proposed changes, the second introduces the first modification, the third uses two changes, and the last includes all three (our final model).\n\nModification\tAverage FID\tAverage MAE\nValue\tImprov.\tValue\tImprov.\nOriginal\t8.866\t---\t0.06069\t---\n+ Increased capacity\t11.078\t-24.95%\t0.05666\t6.64%\n+ Forward Replacer\t6.636\t25.15%\t0.05191\t14.47%\n+ Conservative Inp. Drop.\t4.726\t46.70%\t0.05187\t14.53%\n\nThe original model had 6,565,712 trainable variables, but with the increased capacity, there are 104,887,616 parameters. That change alone improved MAE but worsened FID. The replacement strategy of substituting only the original target with the image generated in the forward step improves both metrics' results. Lastly, training with the proposed conservative input dropout further enhances the results, with FID and MAE values that are 46.7% and 14.53% better than the original architecture.\n\nConclusion\n\nWe posed the task of generating pixel art characters as a missing data imputation problem and approached it using a deep generative model. It is based on the CollaGAN architecture, from which we proposed changes involving a capacity increase, a conservative input dropout strategy, and a different replacement tactic during the backward step of the training procedure. The experiments showed that all of the changes contributed to achieving better results.\n\nCompared to the baseline models, our approach produces images with similar or better quality when using three domains as input. The model can still produce feasible images in scenarios with fewer available images but with increasingly lower quality.\n\nIn case you're interested in using our architecture, trained model or dataset, please read through.\n\nDataset\n\nWe trained the models using the \nPAC Dataset\n, which consists of 14k pixel art character sprites facing 4 directions: back, left, front, and right.\n\n\nIt can be used for non-commercial purposes only and has been assembled from the \nLiberated Cup Characters\n and RPG Maker RTPs (available online).\n\nCode Repository and Pre-trained Weights\n\nWe used Tensorflow 2.10.1 in Python 3.9 to create and train the model, and the repository is \navailable on Github\n. The weights can be downloaded from HuggingFace through \nfegemo/mdigan-characters\n and used in \nPython\n or \nJavaScript\n (with Tensorflow.js).\n\nInstructions on How to Use in Python\nInstall the Required Libraries\n\nWe tested the code using Python 3.9 and specific versions of tensorflow and numpy, marked below.\n\n# requires Python 3.9\n!pip install numpy==1.24.4 tensorflow==2.10.1 matplotlib notebook patool Pillow\nDownload the Model and Images\n\nThe model is hosted on Github and requires git lfs to download it. We also download some images to test the model.\n\nprint(\"Downloading the model from Github...\")\n!git lfs install\n!git clone --depth=1 --branch=main https://github.com/fegemo/mdigan-characters-model/ weights\n\nprint(\"Extracting some images...\")\nimport patoolib\npatoolib.extract_archive(\"./weights/images.zip\", outdir=\".\")\nLoad the Model\n\nLoad the model from the weights folder.\n\nimport tensorflow as tf\n\nmodel = tf.keras.models.load_model(\"weights\", compile=False)\nLoad some Images\n\nWe load the images of the characters \"dealer\", \"merchant\", and \"cleric\" from the images folder. Each character has four images: one for each direction (back, left, front, right).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ncharacter_names = [\"dealer\", \"merchant\", \"cleric\"]\ncharacter_images = [\n    [\n        np.array(Image.open(f\"images/{name}/back.png\")),\n        np.array(Image.open(f\"images/{name}/left.png\")),\n        np.array(Image.open(f\"images/{name}/front.png\")),\n        np.array(Image.open(f\"images/{name}/right.png\"))\n    ]\n    for name in character_names\n]\n\ncharacter_images = tf.constant(character_images)\ncharacter_images = tf.cast(character_images, tf.float32) / 127.5 - 1\n\nGenerate Images\n\nCreate images by calling model([input_images, missing_indices]), where input_images is a tensor with shape [batch, domains, size, size, channels] with one of the images erased (replaced by zeros) and missing_indices is a tensor with the indices of the missing images (shape [batch]).\n\nfrom image_utils import plot_input_and_output_images\n\n# erase one of the images\nposes = [\"back\", \"left\", \"front\", \"right\"]\n\ndef erase_missing_image(images, indices):\n    np_images = images.numpy()\n    for i, index in enumerate(indices):\n        np_images[i, index] = 0\n    return tf.constant(np_images)\n\nmissing_indices = tf.constant(range(len(character_names)))\ninput_images = erase_missing_image(character_images, missing_indices)\ntarget_images = tf.gather(character_images, missing_indices, axis=1, batch_dims=1)\ngenerated_images = model([input_images, missing_indices])\n\nfig = plot_input_and_output_images(input_images, target_images, generated_images)\nfig.patch.set_alpha(0.0)\nplt.show()\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nRelated work\n\nMethodology\n\nExperiments\n\nResults\n\nReceiving 3 images\n\nReceiving 2 or 1 images\n\nInput Dropout Strategy\n\nAblation Study\n\nView all\nComments\nCode\nDatasets\nFiles\n2409.10721v1.pdf\ntraining-animation.mp4\nYou might be interested\nText to Image Synthesis using DC-GAN\nS\nA\nDec 24, 202424 reads\nComputer VisionDC-GAN+2\nEnhancing Unsupervised Person Re- Identification with GAN-Driven Multi- Pose Augmentation\nH\nDec 31, 20249 reads\nAnalysis of synthetic Hand digit using GAN from MNIST dataset.\nJ\nDec 30, 202435 reads\nGANHand digit recognition+2\nAI Generated Image Detection Using GANs\nS\nR\nDec 30, 202423 reads\nAI Generated ImageDiscrimination models+1",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "meshmetrics-a-simple-and-precise-implementation-of-segmentation-metrics-EBkUTyaTf9F3",
    "username": "g@gasper.podobnik",
    "license": null,
    "title": "MeshMetrics: A Simple and Precise Implementation of Segmentation Metrics",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n66 reads\n●\nApache 2.0\nWinner of\nBest Technical Implementation\nat the\nComputer Vision Projects Expo 2024\nMeshMetrics: A Simple and Precise Implementation of Segmentation Metrics\naverage symmetric surface distan\nboundary intersection over union\ndistance-based metrics\nevaluation\nHausdorff distance\nmean average surface distance\nmetrics\nnormalized surface distance\nsegmentation\nG\n@gasper.podobnik\nT\n@tomaz.vrtovec\nLike\nBookmark\nShare\n\nMeshMetrics is as sweet as M&M's—what better way to measure the precision of your segmentation model's performance? 🪅\n\n⏱️ TL;DR ⏱️\n\nThe evaluation of segmentation performance is a common task in biomedical image analysis, with its importance emphasized in the recently released metrics selection guidelines and computing frameworks. To quantitatively evaluate the alignment of two segmentations, researchers commonly resort to counting metrics, such as the Dice similarity coefficient, or distance-based metrics, such as the Hausdorff distance, which are usually computed by publicly available open-source tools with an inherent assumption that these tools provide consistent results. In this study we questioned this assumption, and performed a systematic implementation analysis along with quantitative experiments on real-world clinical data to compare 11 open-source tools for distance-based metrics computation against our highly accurate mesh-based reference implementation. The results revealed that statistically significant differences among all open-source tools are both surprising and concerning, since they question the validity of existing studies. Besides identifying the main sources of variation, we also provide recommendations for distance-based metrics computation.\n\nOverview of our study, highlighting the key steps and methods used in the analysis.\n\n🚀 Need precise 2D or 3D distance-based segmentation metrics? We've got you covered! 🚀\n\nBelow is a simple usage example of MeshMetrics for 3D segmentation masks. For more examples, check out the \nexamples.ipynb\n notebook.\n\nsudo apt update && sudo apt install -y libxrender1\ngit clone https://github.com/gasperpodobnik/MeshMetrics.git\npip install MeshMetrics/\nfrom pathlib import Path\nimport SimpleITK as sitk\nfrom MeshMetrics import DistanceMetrics\n\ndata_dir = Path(\"data\")\n# initialize DistanceMetrics object\ndist_metrics = DistanceMetrics()\n\n# read binary segmentation masks\nref_sitk = sitk.ReadImage(data_dir / \"example_3d_ref_mask.nii.gz\")\npred_sitk = sitk.ReadImage(data_dir / \"example_3d_pred_mask.nii.gz\")\n\n# set input masks and spacing (only needed if both inputs are numpy arrays or vtk meshes)\ndist_metrics.set_input(ref=ref_sitk, pred=pred_sitk)\n\n# Hausdorff Distance (HD), by default, HD percentile is set to 100 (equivalent to HD)\nhd100 = dist_metrics.hd()\n# 95th percentile HD\nhd95 = dist_metrics.hd(percentile=95)\n# Mean Average Surface Distance (MASD)\nmasd = dist_metrics.masd()\n# Average Symmetric Surface Distance (ASSD)\nassd = dist_metrics.assd()\n# Normalized Surface Distance (NSD) with tau=2\nnsd2 = dist_metrics.nsd(tau=2)\n# Boundary Intersection over Union (BIoU) with tau=2\nbiou2 = dist_metrics.biou(tau=2)\n⬇️ Discover why our implementation is more accurate than other open-source tools ⬇️\n🔧 Comparison of 11 open-source tools 🔧\n\nOverview of 11 open-source tools analyzed in this study, indicating the supported distance-based metrics. For HDp, the checkmark (✓) denotes the support of any percentile, whereas a specific number indicates the implementation of a predefined percentile (e.g. 95-th).\n\n🧮 Deep dive into mathematical definitions 🧮\n\nIn their straightforward application, all distance-based metrics compare two segmentations,  and , which are commonly represented as binary segmentation masks  and  in biomedical image analysis. The mathematical definitions of HD (and HDp), MASD, and ASSD are based on two finite sets  and , representing clouds of  and  points that describe the surfaces of  and , respectively. These sets are used to compute distances between the segmentations (see figure below, cf. PointDef).\n\nAssuming  denotes the set of one-directional (asymmetric) distances from points  to , and  denotes the -th percentile of , HDp measures the largest among the bi-directional (symmetric) percentile distances  and : HDp = . The 100th percentile variant, HD100, or simply HD, is computed by replacing the percentile calculation with the maximum over both sets: HD = . Other variants are less sensitive to outliers, such as noise and artifacts. The 95th percentile variant, HD95, is the most established one.\n\nOften referred to as the average surface distance, MASD computes the mean of the two average distances of sets  and , while ASSD computes the mean of the union of both sets  and .\n\nOn the other hand, NSD and BIoU are defined using the boundary representation of the segmentation masks. Here, the term \"boundary\" is context agnostic regarding dimensionality, referring to a contour line in 2D and a surface in 3D. Also referred to as the normalized surface Dice, NSD quantifies the overlap of the two segmentation boundaries,  and , within a specified margin of error (). A region  is first defined both inward () and outward () of boundary . NSD is then computed as the ratio of  within  and vice versa, against the total size of both boundaries, represented by the contour circumference in 2D and surface area in 3D.\n\nFinally, BIoU enhances sensitivity to boundary segmentation errors compared to plain IoU or DSC, which are often saturated for bulk overlaps between two large segmentations. A region  is first defined inward () of , and BIoU is computed by measuring the intersection of  and  over their union.\n\nMathematical definitions of distance-based metrics as adopted by the point-based definition PointDef, different open-source tools, and our reference implementation MeshMetrics.\n\n⚠️ Limitations ⚠️\n\nThe mathematical definitions of HD with HDp, MASD, and ASSD rely on distances between the point sets describing the boundaries of  and . In the discrete image space, these boundaries are represented by line polygons in 2D and surface meshes in 3D. However, these point sets lack complete information about the segmentation boundaries, making them a lossy representation of the segmentation masks.\n\nThe point-based definitions of distance-based metrics inherently neglect the spatial distribution of points across the boundaries, assuming they are uniformly distributed. Ensuring that each point used for distance calculation (referred to as the query point) corresponds to boundary regions of uniform size is practically challenging and introduces bias into the metrics computation.\n\nFor unbiased computation, the size of the boundary element —represented by the length of the line segment in 2D and the area of the (triangular) surface element in 3D — must be taken into account at each query point. The distances calculated between query points and the opposing boundary must then be weighted by the corresponding boundary element sizes, as supported by both theoretical perspectives and experimental results.\n\n📐 Conceptual Analysis 📐\n\nA detailed analysis of open-source tools revealed notable differences in metrics calculation strategies (see figure with equations above) and the applied boundary extraction methods (see figure below).\n\nA critical pitfall in the implementation of distance-based metrics is boundary extraction, where the first issue is the foreground- vs. boundary-based calculation dilemma. Among the 11 open-source tools, EvaluateSegmentation and SimpleITK are the only two that omit the boundary extraction step and instead calculate distances between all mask foreground elements, using the pixel/voxel centers as query points. The authors of EvaluateSegmentation even proposed several optimization strategies to improve computational efficiency, such as excluding intersecting elements, since their distances are always zero.\n\nPlastimatch is the only tool that returns both foreground- and boundary-based calculations, while other tools support only boundary-based calculations. The most frequently employed boundary extraction method, used by Anima, MedPy, MetricsReloaded, MISeval, MONAI, and Plastimatch, involves morphological erosion using an 8-square-connectivity structural element. In contrast, seg-metrics uses a full-connectivity structural element.\n\nConversely, Google DeepMind and pymia employ a strategy where the image grid is shifted by half a pixel/voxel size and incorporate the calculation of boundary element sizes, i.e., the lengths of line segments in 2D and areas of surface elements in 3D. As previously explained, EvaluateSegmentation computes distances solely for non-overlapping elements, while SimpleITK computes distances for all foreground elements.\n\nDifferently from all the 11 open-source tools, our reference implementation MeshMetrics adopts a meshing boundary extraction strategy using discrete flying edges in 2D and discrete marching cubes in 3D.\n\nOverview of the boundary extraction methods used by the 11 open-source tools and our reference implementation, MeshMetrics. All methods are demonstrated using 2D examples but can be seamlessly extended to 3D.\n\n🔬 Quantitative Experiments 🔬\n\nTo quantitatively evaluate the combined effect of variations in mathematical definitions and boundary extraction methods on the resulting metric scores, we designed a series of experiments. These experiments assess the accuracy of the 11 open-source tools for distance-based metrics computation, along with an analysis of edge case handling and computational efficiency.\n\nExperimental Design\n\nIn contrast to counting metrics, distance-based metrics rely on distance calculations and thus require the image grid to be defined in distance units, i.e., with pixel size  or voxel size . For our 3D CT and MR images, we conducted experiments using three different voxel sizes:\n\n, simulating the vanilla scenario with isotropic voxels of unit size.\n, a commonly used isotropic voxel size.\n, a commonly used anisotropic voxel size in the radiotherapy workflow.\n\nThe experiments followed this procedure (see diagram below):\n\nFor a chosen CT or MR image, the two OAR segmentation masks were loaded with their original image voxel size.\nMeshing was performed to extract triangle meshes from the segmentation masks.\nMesh rasterization (i.e., voxelization in our 3D case) was performed to obtain each of the three voxel sizes, which were then used to compute the distance-based metrics by each of the 11 open-source tools.\nMeshing was again performed for each voxelized mask to compute the distance-based metrics using our reference implementation, MeshMetrics.\n\nFor metrics that depend on user-defined parameters (e.g., percentile  for HDp, and boundary margin of error  for NSD and BIoU), we conducted experiments with HD (p = 95), NSD (), and BIoU (), as these values are commonly used in existing studies and provide good insight into the differences among the tools.\n\nExperimental design for distance-based metrics computation using the 11 open-source tools and our reference implementation, MeshMetrics. Meshes are first generated from the original segmentation masks, followed by rasterization (voxelization) to three different voxel sizes. The rasterized (voxelized) masks are then used as inputs to the open-source tools, while corresponding highly accurate meshes are generated and used as inputs to MeshMetrics.\n\nEvaluation of Differences\n\nComparing different implementations for individual OARs would be challenging due to the disparity of results. Therefore, we focus on analyzing the deviations between each open-source tool and MeshMetrics, defined as: , where  is the metric score of the -th open-source tool and  is the reference metric score of MeshMetrics.\n\nDistance-based metrics can be divided into two groups:\n\nAbsolute metrics (HD with (HDp), MASD, and ASSD): These are measured in metric units (i.e., millimeters for our case) and have no upper bound. A lower score reflects greater similarity between two masks.\n\nRelative metrics (NSD and BIoU): These are unitless and bounded between 0 and 1. A higher score reflects greater similarity between two masks.\n\nIn this analysis, positive  values indicate an overestimation of the metric score (i.e., over-pessimistic for absolute and over-optimistic for relative metrics), while negative  values indicate an underestimation of the metric score (i.e., over-optimistic for absolute and over-pessimistic for relative metrics).\n\nAlthough both over- and under-estimation are undesirable, over-optimistic estimates are particularly concerning, as they may lead to incorrect conclusions, especially when compared to metric scores reported in existing studies.\n\n📊 Results 📊\n\nThe differences () in the distance-based metrics, as obtained by each open-source tool against the reference implementation MeshMetrics on 1,561 pairs of segmentation masks, are reported for two isotropic and one anisotropic voxel size in the form of box plots. Note that a linear scale is applied for values within the interval , and a symmetric logarithmic scale is used for values outside this interval to better visualize both smaller and larger differences.\n\n🧵 Discussion 🧵\n\nThe results presented in the boxplot figure above reveal that for HD, the mean deviations are close to zero across all tools except for MISeval, which highlights the general agreement on its mathematical definition, supported by mostly non-significant statistical comparisons (see our full paper for details). For HD95, the results are more scattered due to larger differences in its mathematical definition. Particularly for large and tubular OARs, Plastimatch produced the greatest outliers of up to −115 mm due to the averaging of the directed percentile calculations rather than taking the maximum of both directed percentiles, which generates over-optimistic results. Similarly, over-optimistic performance is observed for MedPy and seg-metrics, both computing the percentile on the union of both distance sets that generally produces smaller metric values when coupled with their boundary extraction methods.\n\nAlthough EvaluateSegmentation applies the same calculation, it uses a different boundary extraction method that shifts the distribution toward higher distances and results in over-pessimistic performance. While MONAI and MetricsReloaded appear as the most accurate for isotropic grids, they exhibit several outliers and perform worse for the anisotropic grid. Notably, Google DeepMind is not only very close to these tools in terms of mean deviation for isotropic grids but also exhibits greater precision, outperforming all other tools in both accuracy and precision for the anisotropic grid.\n\nThese findings align well with our conceptual analysis, as tools relying on the point-based definition (i.e., assuming a uniform distribution of query points) performed significantly worse than Google DeepMind and pymia, which account for boundary element sizes. Interestingly, low deviations in mean differences are observed for MASD and ASSD across all open-source tools, as averaging over large sets of distances is statistically more stable and tends to conceal the variations in metrics implementation. However, it is crucial to also consider the range (min/max) of deviations, which further emphasize the need for unified mathematical definitions.\n\nFor example, the comparison of Google DeepMind and MetricsReloaded clearly demonstrates that the former is more consistent. Particularly for ASSD, these discrepancies are even more pronounced for the anisotropic grid. Although supported by four open-source tools, only two distinct calculations are employed for NSD: a point-based method by MetricsReloaded/MONAI, and a mesh-based method by GoogleDeepMind/pymia, with identical metric scores within pairs due to the same boundary extraction method. However, concerning outliers ranging from −16.1%pt to 34.8%pt are consistently observed across OAR categories for both calculations, causing statistically significant differences against MeshMetrics and among the tools.\n\nAs GoogleDeepMind, MetricsReloaded, and MONAI are deemed popular according to their GitHub stars ranking, questions arise about the accuracy of existing studies applying these tools. For MetricsReloaded and MONAI, the outliers can partly be attributed to not accounting for boundary element sizes, leading to an unjustified assumption of uniformly distributed query points. Surprisingly, even Google DeepMind, the original NSD implementation, shows signs of inaccuracy and imprecision, likely due to quantized distances coupled with the choice of τ. As MeshMetrics uses implicit distance calculations, it is considerably less susceptible to quantization.\n\nThese empirical findings highlight that, among all distance-based metrics, NSD is the most sensitive to distance and boundary calculations. Since BIoU is a recently proposed metric, it is currently implemented only by MetricsReloaded, which provides valid results only for the isotropic grid of unit size due to a flaw in its code. However, even in this case, there are significantly large deviations from MeshMetrics ranging from −15.5%pt to 35.5%pt, which can be attributed to quantized distances coupled with the choice of τ. We therefore suggest exercising caution when interpreting NSD and BIoU.\n\n❗ Conclusion ❗\n\nIn conclusion, we would first like to acknowledge the authors of the 11 open-source tools for their valuable contributions. The outcomes of our study should not be regarded as criticism, but rather as a constructive step towards proper metrics implementation and usage.\n\nBased on our detailed conceptual and quantitative analyses, we propose the following recommendations for distance-based metrics computation:\n\nConsult metrics selection guidelines, such as Metrics Reloaded, to identify representative metrics for the specific application.\nEnsure reproducibility by reporting the name and version of the open-source tool, as the choice of the tool can significantly impact the obtained results and their interpretation.\nBe mindful of edge case handling by not relying solely on open-source tools, but by implementing additional external conditioning.\nPay attention to the correct usage of units of measurement and pixel/voxel sizes.\nUse MeshMetrics for highly accurate reference measurements. If not feasible, use Google DeepMind for HD, HDp, MASD, and ASSD due to its superior performance on both isotropic and anisotropic grids in comparison to other open-source tools, and exercise caution for NSD and BIoU due to implementation discrepancies.\n\nWe therefore hope that Metrics Revolutions with its groundbreaking insights will raise community awareness and understanding of the computational principles behind the distance-based metrics, and stimulate community members towards a more careful interpretation of segmentation results for both existing and future studies.\n\nAs our findings suggest, studies that evaluated segmentation by using the 11 open-source tools may need to be revisited for implementation errors, and we eagerly contribute to this effort by offering MeshMetrics as a reference for the implementation of metrics for biomedical image segmentation.\n\n❣️Acknowledgements ❣️\n\nThis study was supported by the Slovenian Research and Innovation Agency (ARIS) under projects No. J2-1732, J2-4453, J2-50067 and P2-0232, and by the European Union Horizon project ARTILLERY under grant agreement No. 101080983.\n\n📖 Note 📖\n\nThis is a summarized version of our full paper, which includes a more in-depth analysis of computational efficiency and statistical testing. The full paper is available \nhere\n.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\n⏱️ TL;DR ⏱️\n\n🚀 Need precise 2D or 3D distance-based segmentation metrics? We've got you covered! 🚀\n\n⬇️ Discover why our implementation is more accurate than other open-source tools ⬇️\n\n🔧 Comparison of 11 open-source tools 🔧\n\n🧮 Deep dive into mathematical definitions 🧮\n\n⚠️ Limitations ⚠️\n\n📐 Conceptual Analysis 📐\n\n🔬 Quantitative Experiments 🔬\n\nExperimental Design\n\nEvaluation of Differences\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nFiles\n2410.02630v1.pdf\nYou might be interested\nFoam Imaging Segmentator: Advanced Bubble and Foam Analysis Toolkit\nDec 19, 202420 reads\nAnalysis toolkitBubble detetction+6\nUnsupervisedCellImageSegmentation\nP\nDec 29, 202416 reads\nCardiac MRI Segmentation with Hybrid Models\nB\nJ\nDec 21, 202456 reads\n#CardiacMRI#DeepLearning+6\nDentalVis: Multiclass Tooth Segmentation for Panoramic X-rays\nDistinguished Applied Solution Showcase\nB\nDec 25, 202477 reads\nArtificial IntelligenceComputer Vision+4",
    "awards": [
      "best technical implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "miniagents-multi-agent-ai-with-procedural-simplicity-sZ9xgmyLOTyp",
    "username": "Oleksandr Tereshchenko",
    "license": "MIT License",
    "title": "MiniAgents: Multi-Agent AI Framework With Procedural Simplicity",
    "publication_description": "Back to publications\nMar 31, 2025\n●\n153 reads\n●\nMIT License\nWinner of\nBest Technical Implementation\nat the\nAgentic AI Innovation Challenge 2025\nMiniAgents: Multi-Agent AI Framework With Procedural Simplicity\nAgentic AI\nGenerative AI\nLLM\nMulti-Agent Framework\nPython\nOleksandr Tereshchenko\nLike\nBookmark\nShare\nMiniAgents: Multi-Agent AI Framework With Procedural Simplicity\n\nAn open-source, async-first Python framework for building multi-agent AI systems with an innovative approach to parallelism, so you can focus on creating intelligent agents, not on managing the concurrency of your flows.\n\nTo install MiniAgents run the following command:\n\npip install -U miniagents\n\nThe source code of the project is hosted on \nGitHub\n.\n\nWhy MiniAgents\n\nWrite procedural code, get parallel execution: Unlike graph-based frameworks that force you to think in nodes and edges, MiniAgents lets you write straightforward sequential code while the framework handles the complexities of parallel execution for agent interactions automatically. Your code stays clear and readable.\n\nNothing blocks until it's needed: With its innovative promise-based architecture, agents execute in parallel. Execution only blocks at points where specific agent messages are actively awaited. Agents communicate through replayable promises of message sequences, not just concrete messages or single-pass async generators. This replayability is a key distinction, allowing message streams to be consumed multiple times by different agents or for different purposes, fostering flexible data flows and enabling maximum concurrency without complex manual synchronization code.\n\nImmutable message philosophy: MiniAgents uses immutable, Pydantic-based messages that eliminate race conditions and data corruption concerns. This design choice enables highly parallelized agent execution without many of the common headaches of state management. (While messages themselves are immutable, state can still be maintained across multiple invocations of agents by using forked agent instances, a pattern demonstrated later in this tutorial.)\n\nFundamental Agent Contract: Every miniagent adheres to a simple contract: it receives a promise of an input sequence of message promises, arbitrary keyword arguments passed to it (which are all automatically \"frozen\" unless passed via non_freezable_kwargs upon forking an agent, which will be explained later in this tutorial), and in return, it produces a promise of a reply sequence of message promises.\n\nSequential Appearance, Parallel Reality via Promises: MiniAgents achieves this seamless blend of procedural style and concurrent execution through one of its core mechanisms: \"Message Sequence Flattening\". Here is a very simple example (a more complex example will be shown later in this tutorial):\n\n@miniagent\nasync def aggregator_agent(ctx: InteractionContext) -> None:\n    # This looks sequential but all agents start working in parallel\n    ctx.reply([\n        # All nested sequences here will be asynchronously \"flattened\" in the\n        # background for an outside agent (i.e., the one triggering the\n        # `aggregator_agent`), so that when the outside agent consumes the\n        # result, it appears as a single, flat sequence of messages.\n        \"Starting analysis...\",\n        research_agent.trigger(ctx.message_promises),\n        calculation_agent.trigger(ctx.message_promises),\n        \"Analysis complete!\",\n    ])\nLet's build a Web Research System with MiniAgents\n\nIn this tutorial we'll build a system that can:\n\nBreak down a user's question into search queries\nExecute searches in parallel\nAnalyze the search results to identify relevant web pages\nScrape and extract information from those pages\nSynthesize a comprehensive answer\n\nThe animation above demonstrates what the output of this system will look like.\n\nNOTE: The complete source code is available \nhere\n.\n\n\"Message Sequence Flattening\"\n\nLet's start by exploring MiniAgents' central feature - \"Message Sequence Flattening\". We will explain what this means and how it simplifies building concurrent applications shortly. For this, we will build the first, dummy version of our Web Research System. We will not use the real LLM, we will not do the actual web searching and scraping and we will not create the \"Final Answer\" agent just yet. For now, we will put asyncio.sleep() with random delays to emulate activivity instead. Later in the tutorial, we will replace these delays with real web search, scraping and text generation operations.\n\nNaive alternative to \"Message Sequence Flattening\"\n\nFirst, let's look at how you might approach the problem of orchestrating multiple asynchronous operations (like our dummy agents performing research, web searching, and page scraping) and combining their results using standard Python async generators in a naive way. This will help understand the challenges that MiniAgents solves.\n\nThis approach uses standard Python async def with async for ... yield to simulate how one might try to build a similar system without MiniAgents. The key thing to note here is that this method leads to sequential execution of the \"agents\" (async generators in this case).\n\nThe full code for this naive example can be found in \nsequence_flattening_naive_alternative.py\n.\n\nLet's look at the agent definitions and how they are run:\n\n# examples/sequence_flattening_naive_alternative.py\nimport asyncio\nimport random\nfrom typing import AsyncGenerator\n\n\nasync def research_agent_naive(question: str) -> AsyncGenerator[str, None]:\n    \"\"\"\n    The main research agent, implemented naively.\n    Calls web_search_agent_naive sequentially.\n    \"\"\"\n    yield f\"RESEARCHING: {question}\"\n\n    for i in range(3):\n        query = f\"query {i+1}\"\n        # The `async for ... yield` construct processes the generator\n        # sequentially. The `web_search_agent_naive` for \"query 2\" will\n        # only start after the one for \"query 1\" (including all its\n        # pretend-scraping) is finished.\n        async for item in web_search_agent_naive(search_query=query):\n            yield item\n\n\nasync def web_search_agent_naive(search_query: str) -> AsyncGenerator[str, None]:\n    \"\"\"\n    Pretends to perform a web search and trigger scraping.\n    Calls page_scraper_agent_naive sequentially.\n    \"\"\"\n    yield f\"{search_query} - SEARCHING\"\n    await asyncio.sleep(random.uniform(0.5, 1))\n    yield f\"{search_query} - SEARCH DONE\"\n\n    for i in range(2):\n        url = f\"https://dummy.com/{search_query.replace(' ', '-')}/page-{i+1}\"\n        # This also leads to sequential execution\n        async for item in page_scraper_agent_naive(url=url):\n            yield item\n\n\nasync def page_scraper_agent_naive(url: str) -> AsyncGenerator[str, None]:\n    \"\"\"\n    Pretends to scrape a web page.\n    \"\"\"\n    yield f\"{url} - SCRAPING\"\n    await asyncio.sleep(random.uniform(0.5, 1))\n    yield f\"{url} - DONE\"\n\n\nasync def stream_to_stdout_naive(generator: AsyncGenerator[str, None]):\n    \"\"\"\n    Consumes the async generator and prints yielded strings.\n    \"\"\"\n    i = 0\n    async for message in generator:\n        i += 1\n        print(f\"String {i}: {message}\")\n\n\nasync def main_naive():\n    \"\"\"\n    Main function to trigger the naive research agent and print the results.\n    \"\"\"\n    question = \"Tell me about MiniAgents sequence flattening (naive version)\"\n    # Get the async generator\n    result_generator = research_agent_naive(question)\n\n    print()\n    print(\"=== Naive Async Generator Execution ===\")\n    print()\n\n    # Consume and print results\n    await stream_to_stdout_naive(result_generator)\n\n    print()\n    print(\"=== End of Naive Execution ===\")\n    print()\n\n    print(\"Attempting to reiterate (WILL YIELD NOTHING):\")\n\n    # NOTE: Re-iterating a standard async generator like this is not possible.\n    # Once consumed, it's exhausted. This contrasts with MiniAgents promises,\n    # which are replayable.\n    await stream_to_stdout_naive(result_generator)  # This won't print anything\n\n    print(\"=== End of Reiteration Attempt ===\")\n    print()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main_naive())\n\nIn research_agent_naive, the loop that calls web_search_agent_naive processes each \"search query\" one after the other. Similarly, web_search_agent_naive waits for page_scraper_agent_naive to complete for one URL before processing the next.\n\nHere is what this process looks like as a result:\n\nTo achieve true concurrency with this naive approach, you would need to manually manage asyncio.create_task for each sub-operation and potentially use queues or other synchronization primitives to collect and yield results as they become available. This would significantly increase code complexity.\n\nFurthermore, standard async generators, once consumed, are exhausted. If you try to iterate over the result_generator in main_naive a second time, it will yield nothing. This contrasts with MiniAgents' replayable promises.\n\nThis manual management is typical when using raw asyncio or even foundational async libraries like trio or anyio. While these libraries provide powerful concurrency tools, MiniAgents aims to provide a higher-level, agent-centric abstraction for these patterns, particularly around how agents stream and combine their results.\n\nReal \"Message Sequence Flattening\" with MiniAgents\n\nNow, let's see how MiniAgents addresses these challenges, enabling concurrent execution while keeping the same level of code simplicity. You'll see what the changed version of the same code looks like in a moment, but first, let's jump ahead and take a look at how different its output is going to be:\n\nLooks much faster, doesn't it? Now, back to the code. The full example that uses MiniAgents can be found in \nsequence_flattening.py\n.\n\nHere are the agent definitions, along with the main execution logic:\n\n# examples/sequence_flattening.py\nimport asyncio\nimport random\n\nfrom miniagents import (\n    InteractionContext,\n    Message,\n    MessageSequencePromise,\n    MiniAgents,\n    miniagent,\n)\n\n\n@miniagent\nasync def research_agent(ctx: InteractionContext) -> None:\n    ctx.reply(f\"RESEARCHING: {await ctx.message_promises.as_single_text_promise()}\")\n\n    for i in range(3):\n        # `trigger` returns immediately with a `MessageSequencePromise`.\n        # The actual execution of `web_search_agent` starts in the background.\n        # `reply_out_of_order` delivers messages as they become available,\n        # not strictly in the order they are added to the reply.\n        ctx.reply_out_of_order(\n            web_search_agent.trigger(\n                ctx.message_promises, # Original question\n                search_query=f\"query {i+1}\",\n            )\n        )\n\n\n@miniagent\nasync def web_search_agent(ctx: InteractionContext, search_query: str) -> None:\n    ctx.reply(f\"{search_query} - SEARCHING\")\n    await asyncio.sleep(random.uniform(0.5, 1)) # Simulate work\n    ctx.reply(f\"{search_query} - SEARCH DONE\")\n\n    for i in range(2):\n        # Again, no `await` here when triggering the next agent.\n        # The page_scraper_agent will run in parallel.\n        ctx.reply_out_of_order(\n            page_scraper_agent.trigger(\n                ctx.message_promises, # Original question\n                url=f\"https://dummy.com/{search_query.replace(' ', '-')}/page-{i+1}\",\n            )\n        )\n\n\n@miniagent\nasync def page_scraper_agent(ctx: InteractionContext, url: str) -> None:\n    ctx.reply(f\"{url} - SCRAPING\")\n    await asyncio.sleep(random.uniform(0.5, 1)) # Simulate work\n    ctx.reply(f\"{url} - DONE\")\n\n\nasync def stream_to_stdout(promises: MessageSequencePromise):\n    \"\"\"\n    As we iterate through the `response_promises` sequence, asyncio switches\n    tasks, allowing the agents (`research_agent`, `web_search_agent`,\n    `page_scraper_agent`) to run concurrently in the background and resolve\n    their promises. The `async for` loop seamlessly receives messages from the\n    flattened sequence, regardless of how deeply nested their origins were\n    (research_agent -> web_search_agent -> page_scraper_agent).\n    \"\"\"\n    i = 0\n    async for message_promise in promises:\n        i += 1\n        print(f\"{message_promise.message_class.__name__} {i}: \", end=\"\")\n        async for token in message_promise:\n            print(token, end=\"\")\n        print()\n\n\nasync def main():\n    \"\"\"\n    Main function to trigger the research agent and print the results.\n    Demonstrates how the flattened sequence of messages is consumed.\n    \"\"\"\n    # Trigger the top-level agent. This returns a MessageSequencePromise.\n    # No processing has started yet.\n    response_promises = research_agent.trigger(\n        \"Tell me about MiniAgents sequence flattening\"\n    )\n\n    print()\n\n    await stream_to_stdout(response_promises)\n\n    print()\n    print(\"=== REPLAYING MESSAGES ===\")\n    print()\n\n    # If we iterate through the sequence again, we will see that exactly the\n    # same messages are yielded again (and in exactly the same order). This\n    # demonstrates the replayability of all types of promises in MiniAgents.\n    #\n    # Replayability is useful because it allows you to feed the same sequences\n    # (be it responses from agents, or input to the current agent) to multiple\n    # other agents without even thinking that those sequences might already be\n    # \"exhausted\" in a traditional, async generator sense.\n    await stream_to_stdout(response_promises)\n\n    print()\n    print(\"=== REPLAYING MESSAGES AGAIN ===\")\n    print()\n\n    # We can even await for the whole MessageSequencePromise to get the\n    # complete tuple of resolved messages (demonstrating the replayability of\n    # the promises once again).\n    messages: tuple[Message, ...] = await response_promises\n    for i, message in enumerate(messages):\n        # When you run this example, you will see that for agents replying with\n        # simple strings, they are automatically wrapped into TextMessage\n        # objects (a subclass of Message).\n        print(f\"{type(message).__name__} {i+1}: {message}\")\n\n    print()\n\n\nif __name__ == \"__main__\":\n    # The MiniAgents context manager orchestrates the async execution.\n    MiniAgents().run(main())\n\nIn the MiniAgents version:\n\nWhen research_agent calls web_search_agent.trigger(...), this call is non-blocking. It immediately returns a MessageSequencePromise. The actual execution of web_search_agent starts in the background when the asyncio event loop gets a chance to switch tasks.\nThe ctx.reply(...) method (and its variant ctx.reply_out_of_order(...)) is versatile. It can accept:\nInstances of Message (or its subclasses), or other concrete Python objects (like strings, dictionaries, or arbitrary Pydantic models). If not already Message objects, these are automatically wrapped into appropriate framework-specific Message types (e.g., TextMessage).\nPromises of individual messages (MessagePromise).\nPromises that resolve to a sequence of individual message promises (MessageSequencePromise), such as those returned by agent.trigger().\nCollections (lists, tuples, etc.) containing any mix of the above.\nMiniAgents automatically \"flattens\" this potentially deeply nested structure of messages and promises. When the main function (or another agent) consumes the response_promises from research_agent, it receives a single, flat sequence of all messages. This sequence includes messages produced directly by research_agent, all messages from all the triggered web_search_agent instances, and consequently, all messages from all the page_scraper_agent instances called by them.\nA key aspect to remember is that sequence flattening happens both when you pass input to an agent (which can be concrete messages, promises, or collections of either) and when an agent replies with a promise of a message sequence (MessageSequencePromise).\nThe async for message_promise in promises: loop in the stream_to_stdout function in our example (which consumes the results in main) leads to asyncio switching tasks. This gives the agents (research_agent, web_search_agent, page_scraper_agent) a chance to run in the background. The use of reply_out_of_order in some of the agents ensures that certain messages are yielded to the output stream as soon as they are ready from these parallel operations, rather than in the order in which they were registered as part of the agent's response. This enhances the sense of parallelism from the consumer's perspective, though it doesn't change the parallelism of the actual agent execution (which is already parallel due to trigger being non-blocking).\nA key feature highlighted in the main function of sequence_flattening.py is the replayability of MessageSequencePromise objects. You can iterate over response_promises multiple times and get the exact same sequence of messages. This is invaluable for scenarios where you might want to feed the same set of results to multiple different subsequent processing agents without worrying about \"exhausting\" the input stream.\n\nAs you saw from the animation at the beginning of this section, the processing happens much faster, even though we didn't do anything special to achieve that, all thanks to parallelism introduced by the framework.\n\nThis automatic concurrency and sequence flattening greatly simplify the development of complex, multi-step AI systems. You can focus on the logic of each individual agent, writing code that appears sequential within the agent, while the MiniAgents framework handles the parallel execution and complex data flow management behind the scenes.\n\nWeb Research System with real operations\n\nNow that we've explored the core concept of \"Message Sequence Flattening\" with a dummy example, let's dive into the fully functional Web Research System. This system uses real AI models for understanding and generation, performs actual web searches, and scrapes web pages to gather information.\n\nAgain, as mentioned earlier, the complete source code for this example can be found \nhere\n.\n\nPrerequisites\n\nBefore running the web_research.py script, you'll need to set up a few things:\n\nInstallation: First, install MiniAgents and the required dependencies:\n\npip install -U miniagents openai httpx pydantic markdownify python-dotenv selenium\n\nEnvironment Variables: For the LLM we will use \nOpenAI\n and for the google searches as well as web scraping we will use \nBright Data\n, a pay as you go scraping service. The two Bright Data products that we are interested in are: \nSERP API\n and \nScraping Browser\n. Create a .env file in the same directory as web_research.py with the following credentials:\n\n# .env\nBRIGHTDATA_SERP_API_CREDS=\"your_serp_api_username:your_serp_api_password\"\nBRIGHTDATA_SCRAPING_BROWSER_CREDS=\"your_scraping_browser_username:your_scraping_browser_password\"\nOPENAI_API_KEY=\"your_openai_api_key\"\n\nATTENTION: The credentials above are NOT for your whole Bright Data account. They are for the SERP API and Scraping Browser respectively (their website will guide you how to set up both products).\n\nHelper Utilities (utils.py): The project uses a utils.py file (available \nhere\n) which contains helper functions for:\n\nfetch_google_search(): Interacts with the Bright Data SERP API.\nscrape_web_page(): Uses Selenium with Bright Data's Scraping Browser to fetch and parse web page content. It runs Selenium in a separate thread pool as Selenium is blocking.\n\nYou don't need to dive deep into utils.py to understand the MiniAgents framework, but it's essential for the example to run.\n\nSystem Overview and the main function\n\nThe entry point of our application is the main() function. It orchestrates the entire process:\n\n# examples/web_research_tutorial/web_research.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Union\n\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\nfrom miniagents import (\n    AgentCall,\n    InteractionContext,\n    Message,\n    MessageSequencePromise,\n    MiniAgents,\n    miniagent,\n)\nfrom miniagents.ext.llms import OpenAIAgent, aprepare_dicts_for_openai\n\nfrom utils import fetch_google_search, scrape_web_page\n\nload_dotenv() # Load environment variables from .env file\n\nMODEL = \"gpt-4o-mini\"  # \"gpt-4o\"\nSMARTER_MODEL = \"o4-mini\"  # \"o3\"\nMAX_WEB_PAGES_PER_SEARCH = 2\nSLEEP_BEFORE_RETRY_SEC = 5\n\nopenai_client = AsyncOpenAI()\n\n\nasync def main():\n    question = input(\"\\nEnter your question: \")\n\n    # Invoke the main agent (no `await` is placed in front of the call, hence\n    # this is a non-blocking operation, no processing starts just yet)\n    response_promises: MessageSequencePromise = research_agent.trigger(question)\n\n    print()\n    # Iterate over the individual message promises in the response sequence\n    # promise. The async loops below lead to task switching, so the agent above\n    # as well as its \"sub-agents\" will now start their work in the background\n    # to serve all the promises.\n    async for message_promise in response_promises:\n        # Skip messages that are not intended for the user (you'll see where\n        # the `not_for_user` attribute is set later)\n        if message_promise.known_beforehand.get(\"not_for_user\"):\n            continue\n        # Iterate over the individual tokens in the message promise (messages\n        # that aren't broken down into tokens will be delivered in a single\n        # token)\n        async for token in message_promise:\n            print(token, end=\"\", flush=True)\n        print(\"\\n\")\n\n# ... (rest of the file)\n\nKey takeaways from main():\n\nFiltering Messages: Some messages might be internal to the agent system (e.g., detailed summaries for other agents). We can attach metadata to messages (like not_for_user) and use it to filter what's shown to the end-user. The known_beforehand attribute of a MessagePromise allows access to metadata that is available before the message content itself is resolved. This can be useful for early filtering or routing of messages. In our main function, we use this to check the \"not_for_user\" flag (set in page_scraper_agent) to prevent internal page summaries from being directly displayed.\nCentralized Output: Notice that all user-facing output happens here. Agents themselves don't print. They communicate results back, which main then decides how to present. This separation makes it easier to change the UI or even integrate this entire agentic system as a component within a larger AI system, where its output would be consumed programmatically rather than printed to a console.\n\nNOTE: Background execution is optional. MiniAgents, by default, starts processing triggered agents as soon as possible, and this is generally the desired behavior for maximum parallelism. You can, however, disable this behavior by passing start_soon=False to individual trigger calls, or by setting start_everything_soon_by_default=False in the MiniAgents constructor for a global effect. The latter is generally not recommended, though. Disabling \"early start\" globally can often lead to deadlocks if agent interdependencies are complex, and in the majority of scenarios, there is hardly any benefit in setting start_soon to False.\n\nThe research_agent: Orchestrating the Search\n\nThe research_agent is the primary coordinator. It takes the user's question and breaks it down into actionable steps.\n\n# examples/web_research_tutorial/web_research.py\n# ... (imports and setup shown above) ...\n\n@miniagent\nasync def research_agent(ctx: InteractionContext) -> None:\n    ctx.reply(\"RESEARCHING...\")\n\n    # First, analyze the user's question and break it down into search queries\n    message_dicts = await aprepare_dicts_for_openai(\n        ctx.message_promises, # The user's question\n        system=(\n            \"Your job is to breakdown the user's question into a list of web \"\n            \"searches that need to be done to answer the question. Please try \"\n            \"to optimize your search queries so there aren't too many of \"\n            \"them. Current date is \" + datetime.now().strftime(\"%Y-%m-%d\")\n        ),\n    )\n    # Using OpenAI's client library directly for structured output\n    response = await openai_client.beta.chat.completions.parse(\n        model=SMARTER_MODEL,\n        messages=message_dicts,\n        response_format=WebSearchesToBeDone, # Pydantic model for structured output\n    )\n    parsed: WebSearchesToBeDone = response.choices[0].message.parsed\n\n    ctx.reply(f\"RUNNING {len(parsed.web_searches)} WEB SEARCHES\")\n\n    already_picked_urls = set[str]()\n    # Fork the `web_search_agent` to create an isolated, configurable instance\n    # for this task. `non_freezable_kwargs` allows passing mutable objects like\n    # our `already_picked_urls` set, which will then be specific to this forked\n    # agent instance and shared across its invocations within this research\n    # task.\n    _web_search_agent = web_search_agent.fork(\n        non_freezable_kwargs={\n            \"already_picked_urls\": already_picked_urls,\n        },\n    )\n\n    # Initiate a call to the final_answer_agent. We'll send it data as we\n    # gather it.\n    final_answer_call: AgentCall = final_answer_agent.initiate_call(\n        user_question=await ctx.message_promises,\n    )\n\n    # For each identified search query, trigger a web search agent\n    for web_search in parsed.web_searches:\n        search_and_scraping_results = _web_search_agent.trigger(\n            ctx.message_promises, # Forwarding the original user question\n            search_query=web_search.web_search_query,\n            rationale=web_search.rationale,\n        )\n        # `reply_out_of_order` sends messages to the research_agent's output\n        # as they become available, maintaining responsiveness.\n        ctx.reply_out_of_order(search_and_scraping_results)\n\n        # Send the same results to the final_answer_agent\n        final_answer_call.send_message(search_and_scraping_results)\n\n    # Reply with the sequence from final_answer_agent, effectively chaining\n    # its output to research_agent's output. This also closes the call to\n    # final_answer_agent.\n    ctx.reply(final_answer_call.reply_sequence())\n\n# ... (other agents)\n\nKey aspects of research_agent:\n\nQuery Generation: It uses an LLM (via openai_client.beta.chat.completions.parse) to break the user's question into a list of specific search queries. WebSearchesToBeDone is a Pydantic model that ensures the LLM returns data in the expected structure (using OpenAI's \"structured output\" feature). While this example uses the OpenAI client library directly for structured output, MiniAgents plans to support this natively as another built-in LLM miniagent, along with already existing OpenAIAgent, AnthropicAgent etc. which simply generate text.\nAgent Forking for Configuration and State: The web_search_agent needs to keep track of URLs it has already decided to scrape to avoid redundant work. agent.fork() creates a new, independent version (an \"instance\") of the agent. This is useful for creating agents with specific configurations or, as in this case, for endowing an agent instance with mutable state (like already_picked_urls) that is shared across its invocations by this particular forked instance. The non_freezable_kwargs argument is the mechanism for passing such mutable resources that cannot (or should not) be \"frozen\" by the fork.\nInitiating Calls (initiate_call): The final_answer_agent will eventually synthesize an answer using all gathered information. We don't have all this information upfront. final_answer_agent.initiate_call() creates an AgentCall object. This allows research_agent to send messages (or message promises) to final_answer_agent incrementally using final_answer_call.send_message().\nParallel Fan-Out (trigger without await): For each generated search query, _web_search_agent.trigger() is called. Again, no await means these sub-agents start working in parallel.\nOut-of-Order Replies (ctx.reply_out_of_order): As results from _web_search_agent (which include search and scraping steps) become available, ctx.reply_out_of_order() sends them to the output stream of research_agent. As mentioned earlier, we use reply_out_of_order() to avoid enforcing message delivery in the order they were added to the reply. Delivering these messages as soon as they are available allows research_agent to show progress from different search branches in real time.\nChaining Agent Output: Finally, ctx.reply(final_answer_call.reply_sequence()) takes the response that final_answer_agent will produce (in this example, a sequence consisting of a single message containing the synthesized answer) and appends it to research_agent's own output. reply_sequence() also signals to final_answer_agent that no more input messages will be sent via final_answer_call.send_message(), effectively closing the call (such behavior can be prevented with reply_sequence(finish_call=False) if needed, though).\nThe web_search_agent: Searching and Selecting Pages\n\nThis agent takes a single search query, performs the search, and then uses an LLM to decide which of the resulting pages are most relevant for scraping.\n\n# examples/web_research_tutorial/web_research.py\n# ... (research_agent) ...\n\n@miniagent\nasync def web_search_agent(\n    ctx: InteractionContext,\n    search_query: str,\n    rationale: str,\n    already_picked_urls: set[str], # Received from the forked instance\n) -> None:\n    ctx.reply(f'SEARCHING FOR \"{search_query}\"\\n{rationale}')\n\n    try:\n        search_results = await fetch_google_search(search_query) # from utils.py\n    except Exception:\n        await asyncio.sleep(SLEEP_BEFORE_RETRY_SEC)\n        ctx.reply(f\"RETRYING SEARCH: {search_query}\")\n        search_results = await fetch_google_search(search_query)\n\n    ctx.reply(f\"SEARCH SUCCESSFUL: {search_query}\")\n\n    message_dicts = await aprepare_dicts_for_openai(\n        [\n            ctx.message_promises, # Original user question\n            f\"RATIONALE: {rationale}\\n\\nSEARCH QUERY: {search_query}\\n\\n\"\n            f\"SEARCH RESULTS:\\n\\n{search_results}\",\n        ],\n        system=(\n            \"This is a user question that another AI agent (not you) will \"\n            \"have to answer. Your job, however, is to list all the web page \"\n            \"urls that need to be inspected to collect information related to \"\n            \"the RATIONALE and SEARCH QUERY. SEARCH RESULTS where to take the \"\n            \"page urls from are be provided to you as well. Current date is \"\n            + datetime.now().strftime(\"%Y-%m-%d\")\n        ),\n    )\n    response = await openai_client.beta.chat.completions.parse(\n        model=SMARTER_MODEL,\n        messages=message_dicts,\n        response_format=WebPagesToBeRead, # Pydantic model for structured output\n    )\n    parsed: WebPagesToBeRead = response.choices[0].message.parsed\n\n    web_pages_to_scrape: list[WebPage] = []\n    for web_page in parsed.web_pages:\n        if web_page.url not in already_picked_urls:\n            web_pages_to_scrape.append(web_page)\n            already_picked_urls.add(web_page.url)\n        if len(web_pages_to_scrape) >= MAX_WEB_PAGES_PER_SEARCH:\n            break\n\n    for web_page in web_pages_to_scrape:\n        ctx.reply_out_of_order(\n            page_scraper_agent.trigger(\n                ctx.message_promises, # Original user question\n                url=web_page.url,\n                rationale=web_page.rationale,\n            )\n        )\n\n# ... (page_scraper_agent and final_answer_agent)\n\nHighlights of web_search_agent:\n\nActual Web Search: It calls fetch_google_search() (from utils.py) to get search results. Includes basic retry logic.\nLLM for Page Selection: An LLM is prompted to analyze the search results and select promising URLs based on the rationale for the search. The desired output is structured using the WebPagesToBeRead Pydantic model.\nStateful Filtering: It uses the already_picked_urls set (provided by the fork in research_agent) to avoid re-processing URLs that have already been selected from other search queries in the same overall research task. It also limits the number of pages scraped per search query (MAX_WEB_PAGES_PER_SEARCH).\nParallel Scraping Trigger: For each selected URL, page_scraper_agent.trigger() is called without await, initiating parallel scraping operations. Results are again channeled using ctx.reply_out_of_order().\nThe page_scraper_agent: Fetching and Summarizing Content\n\nThis agent is responsible for fetching the content of a single web page and then using an LLM to extract relevant information.\n\n# examples/web_research_tutorial/web_research.py\n# ... (web_search_agent) ...\n\n@miniagent\nasync def page_scraper_agent(\n    ctx: InteractionContext,\n    url: str,\n    rationale: str,\n) -> None:\n    ctx.reply(f\"READING PAGE: {url}\\n{rationale}\")\n\n    try:\n        page_content = await scrape_web_page(url) # from utils.py\n    except Exception:\n        await asyncio.sleep(SLEEP_BEFORE_RETRY_SEC)\n        ctx.reply(f\"RETRYING: {url}\")\n        page_content = await scrape_web_page(url)\n\n    # We await the full summary here because we want to ensure summarization\n    # is complete before reporting success for this page.\n    page_summary = await OpenAIAgent.trigger(\n        [\n            ctx.message_promises, # Original user question\n            f\"URL: {url}\\nRATIONALE: {rationale}\\n\\n\"\n            f\"WEB PAGE CONTENT:\\n\\n{page_content}\",\n        ],\n        system=(\n            \"This is a user question that another AI agent (not you) will \"\n            \"have to answer. Your job, however, is to extract from WEB PAGE \"\n            \"CONTENT facts that are relevant to the users original question. \"\n            \"The other AI agent will use the information you extract along \"\n            \"with information extracted by other agents to answer the user's \"\n            \"original question later. Current date is \"\n            + datetime.now().strftime(\"%Y-%m-%d\")\n        ),\n        model=MODEL,\n        # Streaming isn't important for this internal summary\n        stream=False,\n        # If summarization fails, let this agent fail rather than sending an\n        # error message. This is a choice; for robustness, True might be\n        # preferred if partial results are acceptable.\n        errors_as_messages=False,\n        response_metadata={\n            # This summary is for the final_answer_agent, not directly for the\n            # user.\n            \"not_for_user\": True,\n        },\n    )\n    ctx.reply(f\"SCRAPING SUCCESSFUL: {url}\")\n    ctx.reply(page_summary) # Send the extracted summary\n\n# ... (final_answer_agent)\n\nKey points for page_scraper_agent:\n\nActual Scraping: It calls scrape_web_page() (from utils.py), which uses Selenium to get page content. Includes retry logic.\nLLM for Summarization/Extraction: The MiniAgents built-in OpenAIAgent is used here. It's triggered to process the scraped content and extract information relevant to the original user question and the rationale for visiting this specific page.\nAwaiting Specific Results: Unlike previous trigger calls, page_summary = await OpenAIAgent.trigger(...) awaits the result. This is a design choice: we want to ensure the summarization is complete and successful before this agent reports SCRAPING SUCCESSFUL and sends the summary onward.\nError Handling Choice (errors_as_messages=False): For this particular call to OpenAIAgent, errors_as_messages is set to False. This means if the LLM call fails, the page_scraper_agent itself will raise an exception. The overall system is configured with errors_as_messages=True (see the if __name__ == \"__main__\": block later in the tutorial), so this local override demonstrates fine-grained control. If it were True (or defaulted to the global setting), an LLM error would result in an error message being sent as page_summary instead of crashing this agent.\nMessage Metadata (response_metadata): The OpenAIAgent.trigger call includes response_metadata. This dictionary is attached to the message(s) produced by OpenAIAgent. Here, \"not_for_user\": True signals that this summary is an intermediate result, primarily for the final_answer_agent, and shouldn't be directly displayed to the user by the main function's loop. It's worth noting that metadata keys like \"not_for_user\" are arbitrary choices specific to this application's design; you can use any key names that suit your system's logic for routing or annotating messages.\nThe final_answer_agent: Synthesizing the Result\n\nThis agent receives all the summaries and extracted pieces of information from the various page_scraper_agent instances and synthesizes a final answer to the user's original question.\n\n# examples/web_research_tutorial/web_research.py\n# ... (page_scraper_agent) ...\n\n@miniagent\nasync def final_answer_agent(\n    ctx: InteractionContext,\n    user_question: Union[Message, tuple[Message, ...]],\n) -> None:\n    # Await all incoming messages (summaries from page_scraper_agents) to\n    # ensure they are \"materialized\" before we proceed to show the \"ANSWER\"\n    # heading. If not for this heading, `await` would not have been important\n    # here - OpenAIAgent will internally await for all the incoming messages to\n    # make them part of its prompt anyway.\n    await ctx.message_promises\n\n    ctx.reply(\"==========\\nANSWER:\\n==========\")\n\n    ctx.reply(\n        OpenAIAgent.trigger(\n            [\n                \"USER QUESTION:\",\n                user_question, # Passed as a kwarg during initiate_call\n                \"INFORMATION FOUND ON THE INTERNET:\",\n                # Concatenate all received messages (page summaries) into a\n                # single text block. `as_single_text_promise()` returns a\n                # promise; actual concatenation happens in the background.\n                ctx.message_promises.as_single_text_promise(),\n            ],\n            system=(\n                \"Please answer the USER QUESTION based on the INFORMATION \"\n                \"FOUND ON THE INTERNET. Current date is \"\n                + datetime.now().strftime(\"%Y-%m-%d\")\n            ),\n            model=MODEL,\n            # stream=True, # In LLM miniagents streaming is enabled by default.\n                           # Explicitly setting stream=False would turn it off.\n        )\n    )\n\n# ... (Pydantic models and main block)\n\nKey features of final_answer_agent:\n\nWaiting for All Inputs (await ctx.message_promises): Before generating the answer, this agent explicitly await ctx.message_promises. This ensures that all the page summaries sent by research_agent (via final_answer_call.send_message()) have arrived and are resolved. This is important because we want the \"ANSWER:\" heading to appear after all the processing logs (like \"SEARCHING...\", \"READING PAGE...\").\nConsolidating Information: ctx.message_promises.as_single_text_promise() is a handy method. It takes the sequence of incoming messages (which are progress logs and page summaries) and concatenates them into a single text message promise, with original messages separated by double newlines. This consolidated text, along with the original user question, forms the prompt for the final LLM call. (Although it's worth noting that you could have passed all these messages into the prompt without such concatenation - they would all have appeared to the LLM as separate dialog turns in that case.)\nGenerating Final Answer: OpenAIAgent is triggered one last time to produce the comprehensive answer. OpenAIAgent streams by default, so the final answer will appear token by token to the user, providing a responsive experience.\nPydantic Models for Structured LLM Output\n\nOur example uses Pydantic models with OpenAI's \"Structured Output\" feature (via response_format in client.beta.chat.completions.parse) to guide the LLM in generating responses in a specific format:\n\n# examples/web_research_tutorial/web_research.py\n# ... (final_answer_agent) ...\n\nclass WebSearch(BaseModel):\n    rationale: str\n    web_search_query: str\n\n\nclass WebSearchesToBeDone(BaseModel):\n    web_searches: tuple[WebSearch, ...]\n\n\nclass WebPage(BaseModel):\n    rationale: str\n    url: str\n\n\nclass WebPagesToBeRead(BaseModel):\n    web_pages: tuple[WebPage, ...]\n\n# ... (if __name__ == \"__main__\":)\n\nThese models (WebSearchesToBeDone, WebPagesToBeRead) help ensure that the LLM provides its output in a way that can be reliably parsed and used by the subsequent logic in the agents.\n\nConfiguring and Running the System\n\nThe if __name__ == \"__main__\": block initializes and runs the MiniAgents system:\n\n# examples/web_research_tutorial/web_research.py\n# ... (Pydantic models) ...\n\nif __name__ == \"__main__\":\n    MiniAgents(\n        # Log LLM requests/responses to markdown files in `llm_logs`\n        llm_logger_agent=True,\n        # Convert agent errors into messages rather than crashing the agents\n        errors_as_messages=True,\n        # # Optionally include full tracebacks in error messages for debugging\n        # error_tracebacks_in_messages=True,\n    ).run(main())\n\nKey MiniAgents configurations used here:\n\nllm_logger_agent=True: This is incredibly useful for debugging. It logs all requests to and responses from LLM miniagents (like OpenAIAgent) into markdown files in an llm_logs directory. You can inspect these logs to see the exact prompts, model parameters, and outputs.\nerrors_as_messages=True: This global setting makes the system more robust. If an agent encounters an unhandled exception, instead of the agent (and potentially the whole flow) crashing, the error is packaged into an ErrorMessage object and continues through the system. As we saw, page_scraper_agent locally overrides this for its LLM calls by setting errors_as_messages=False in the trigger call.\nerror_tracebacks_in_messages=True (commented out): If you enable this, the error messages produced when errors_as_messages=True will also include the full Python traceback, which can be helpful during development (only useful when errors_as_messages=True, because when errors_as_messages=False, errors are raised as exceptions and are logged to the console with the tracebacks regardless of the error_tracebacks_in_messages setting).\nConclusion\n\nThis Web Research System demonstrates several powerful features of MiniAgents:\n\nProcedural Code, Parallel Execution: Agents are written as straightforward async functions, but trigger calls without await lead to parallel execution.\nPromise-Based Communication: Agents communicate via MessageSequencePromise objects, allowing computation to proceed while data is still being gathered.\nSequence Flattening: Complex, nested calls to agents still result in a single, flat stream of messages for the consumer, managed automatically by the framework.\nFlexible Message Handling: Features like ctx.reply_out_of_order for responsive streaming and agent.initiate_call for incremental data feeding provide fine-grained control over message flow.\nConfigurable Agent Instances with Forks: agent.fork() offers a clean way to create isolated, configurable instances of agents, which can be used for managing state specific to a task or for creating differently parameterized versions of an agent.\nNative LLM Integration: The built-in OpenAIAgent, AntropicAgent etc. provide a convenient, MiniAgents-native way to incorporate LLM capabilities from popular providers like OpenAI and Anthropic into your agents. (More such built-in LLM miniagents will be added in the future and there will also be a tutorial showing how easy it is to create such an LLM miniagent on your own.)\nDebugging Support: Features like llm_logger_agent aid in development by providing visibility into LLM interactions.\nEnhanced Robustness: Global error handling settings like errors_as_messages help create more resilient systems.\n\nBy focusing on the logic of individual agents, MiniAgents lets you build sophisticated, concurrent AI systems without getting bogged down in the complexities of manual parallelism management. The system naturally parallelizes IO-bound tasks (like calls to LLMs or external APIs) without requiring explicit concurrency code from the developer, as AI agents are typically IO-bound.\n\nJoin our \nDiscord community\n to get help with your projects. We welcome questions, feature suggestions, and contributions!\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nMiniAgents: Multi-Agent AI Framework With Procedural Simplicity\n\nWhy MiniAgents\n\nLet's build a Web Research System with MiniAgents\n\n\"Message Sequence Flattening\"\n\nNaive alternative to \"Message Sequence Flattening\"\n\nReal \"Message Sequence Flattening\" with MiniAgents\n\nWeb Research System with real operations\n\nPrerequisites\n\nSystem Overview and the main function\n\nThe research_agent: Orchestrating the Search\n\nView all",
    "awards": [
      "best technical implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "multi-agent-space-model-for-efficient-neighborhood-search-and-interaction-0VPKbgRYf6nI",
    "username": "c@csansores",
    "license": "MIT License",
    "title": "Multi-agent Space Model for Efficient Neighborhood  Search and Interaction",
    "publication_description": "Back to publications\nMar 07, 2025\n●\n47 reads\n●\nMIT License\nWinner of\nAI Research Excellence\nat the\nAgentic AI Innovation Challenge 2025\nMulti-agent Space Model for Efficient Neighborhood Search and Interaction\nIntelligent Agent\nMulti-agent systems\nNeighborhood Search\nC\n@csansores\nM\nMirbella Gallareta NegróN\nLike\nBookmark\nShare\nAbstract\n\nBioMASS is an innovative multi-agent spatial model designed to enhance computational efficiency in simulations involving complex sensory and locomotion functions. Traditional agent-based modeling (ABM) platforms suffer from performance degradation as the number of agents and their perception ranges increase, leading to quadratic computational costs. BioMASS addresses this issue by employing a quadruply linked list structure, which enables constant-time neighborhood search and movement. It is implemented within a multi-agent system framework that has been successfully applied to marine ecosystem simulations, demonstrating its ability to track species interactions across multiple trophic levels in real-time, outperforming existing platforms.\n\n\nFig.1. Graphical Abstract of BioMASS Model. The proposed quadruply linked list structure optimizes 2D space representation in agent-based simulations. This design enables efficient neighborhood search by linking each agent to its nearest neighbors in both X and Y directions. The approach scales effectively, supporting large-scale simulations without the computational overhead of grid-based or hash table structures.\n\nIntroduction\n\nAgent-based modeling and simulation (ABMS) is a method used to study the dynamics of complex systems. It is widely applied across disciplines such as economics, ecology, sociology, and biology to investigate how interactions among individual components lead to self-organizing behaviors and emergent properties in complex systems [1, 2]. ABMS systems are designed as multi-agent systems (MAS) to distribute task-solving across agents. Within ABMS, two main types of models are used to study complex systems: behavioral simulations and agent-based system simulations, each suited to different experimental needs due to their distinct spatial and temporal resolutions.\nIn behavioral simulations, individual-level rules generate collective patterns and relatively few agents (typically tens to hundreds) are required. These simulations emphasize high spatial and temporal granularity, allowing researchers to observe how individual interactions contribute to group behaviors. Since the focus is on immediate interactions within small populations, these models do not require large spatial or temporal dimensions.\nAgent-based system simulations, on the other hand, serve as complementary tools to traditional state-variable models, emphasizing physiological processes and local interactions that give rise to emergent behaviors at the population level. This paradigm requires a much larger number of agents (often thousands) and operates in expansive environments over extended time-frames, often spanning years or decades. The goal is to capture the evolution of population dynamics over multiple generations, requiring less granular temporal and spatial resolution compared to behavioral simulations.\nCertain scenarios require blending both paradigms, especially when individual behaviors directly impact macro-level variables. These hybrid models add computational demands, as they need to manage large agent populations while accounting for individual variability in interactions, life cycles, and behaviors. Since these simulations are spatially explicit, agents interact primarily through sensory and perception functions, which adds computational complexity. Agents must frequently sense a large portion of their environment, requiring costly neighborhood search functions. Furthermore, variability in agents’ perception ranges increases the difficulty of optimizing these functions within traditional spatial models. This highlighted the need for a spatially explicit ABMS framework capable of efficiently handling neighborhood exploration and movement functions, addressing the computational challenges posed by large, spatially complex environments.\n\nThe Problem\n\nThe classical method adopted to solve these types of problems involves a discrete virtual space (usually a two- or three-dimensional rectangular lattice) and a search algorithm whose computational cost per individual increases with the perception range and with the number of discrete cells in the search area. To reduce this cost, the number of cells must be kept low. This is done either by reducing the perception range or by increasing cell size (sacrificing detail in the environment representation). Another approach is to use a continuous space where the computational cost per agent increases linearly with the number of agents. However, models with a large number of agents (1000s or more) perform poorly under this scheme. To avoid very costly processing in this case, a collection of agents can be modeled as a group and represented as a special kind of agent. But this again sacrifices detail in the model representation, especially regarding the degree of heterogeneity among the agents.\nOversimplifying the spatial representation or the number of individuals may work for certain systems. However, in [4], Sansores et al. described several scenarios that require large spatial dimensions, fine spatial granularity and a large number of agents at the same time. Moreover, the perception range of the agents may change over time, so it is desirable to preserve the heterogeneity that forms the foundation of ABMS models. The combination of these requirements creates an additional level of complexity in the spatial model that is not addressed by any current ABMS framework. Our main contribution focus on the BioMASS spatial model.\n\nClassical spatial models for situated multi-agent systems\n\nTraditionally, the internal model of the environment in a situated MAS is either discrete or continuous. This choice determines how the agents can be physically organized and the forms of the sensory and locomotion functions used by the agents. The hybrid model, a combination of the above, uses partial discretization of the underlying continuous model in order to improve the execution performance of some functions such as neighborhood search.\n\nThe Need for an optimized spatial model\n\nExisting spatial models, see Fig.2, Fig.3, Fig.4, suffer from high computational costs:\n\nDiscrete models rely on fixed grids, leading to inefficient neighborhood searches.\nContinuous models require global searches, making them computationally expensive.\nHybrid models balance these approaches but require predefined discretization, limiting flexibility.\nResearch Questions\nWhat is the theoretical computational complexity of the BioMASS model in terms of time, and how does it compare to that of other spatial models used in agent simulations?\nTo what extent does the BioMASS model reduce the computational complexity of neighborhood search and other spatial operations in agent simulations, especially in scenarios with a large number of agents and varying perception ranges?\nHow can the BioMASS model be integrated into an existing agent simulation framework, and what are the benefits of this integration in terms of performance and scalability?\nTo what extent does the BioMASS model facilitate the design and implementation of more efficient and scalable agent simulations for the study of complex systems?\nObjectives\nConduct a formal analysis of the computational complexity of the BioMASS model, deriving mathematical expressions for the time required by its main operations (insertion, removal, movement, neighborhood search).\nCompare the theoretical computational complexity of the BioMASS model with that of other spatial models (continuous, discrete, and hybrid) by analyzing their respective complexity expressions.\nImplement the BioMASS model in an open-source agent simulation framework and evaluate its performance in different simulation scenarios, measuring execution time and scalability.\nEvaluate the impact of the BioMASS model on the performance and scalability of a specific agent-based simulation application, such as the predator-prey model described in the article.\n\n\nFig.2. A discrete spatial model  of  cells containing a set of agents . The perception scope of  at step  is the shaded subgrid. The neighborhood lookup function of  iterates over all cells in the shaded area and adds all agents occupying those cells to its neighborhood set .\n\n\nFig.3. A continuous spatial model with Cartesian coordinates. The set of agents  is situated in the space and their different perception scopes are depicted by dotted lines. The neighborhood set of agent  is . Note that the neighborhood relation is not symmetric: agents  and  do not have agent  in their neighborhood.\n\n\nFig.4. A hybrid spatial model with a continuous space divided into buckets. The set of agents  is situated in the space with real Cartesian coordinates. The neighborhood set of agent  is . Note that agent : although it happens to be in a bucket that is perceived by , it is not within the Euclidean distance .\n\nMethodology\nThe proposed BioMASS quadruply linked list spatial model\n\nLet  be the space in the BioMASS model with dimensions .  can be portrayed as a Cartesian plane with real coordinates. The implementation uses a quadruply linked list. The nodes forming the list are agents with four pointers: two in the horizontal dimension and two in the vertical dimension. The pointers  and  link to the agents with the next smallest and next largest  coordinate, respectively. A node points to itself if there is no other agent with a smaller/larger  coordinate. Initially,  is configured with two \"sentinel\" agents  and , which represent the corners of the 2D space with coordinates  and .\n\nThe initial setup is illustrated in Fig.5. The sentinel node  has 2 pointers to itself,  and . The pointers,  and , point to . Similarly, for  the two 'previous' pointers point to , while its 'next' pointers point to itself. Thus, , , , , and  are pointers to its four closest agents in the horizontal and vertical dimensions. Therefore, , we have  <= ,  <= ,  >= , and  >= . This data structure guarantees that the agents situated in the environment are always linked to the closest agents in both dimensions. Now, we will analyze the basic functions performed by an agent  in , specifically in the neighborhood lookup.\n\n\nFig.5. The quadruply linked list in  representing the BioMASS spatial model with  and  as sentinel nodes delimiting the simulation space. The depicted links between nodes are valid only when the space is empty.\n\nFig.6 and Fig.7 illustrate the insert function. The time complexity of this function is  in the worst case.\n\n\nFig.6. The space  with only one agent, , pointing to sentinels  and  in both dimensions as the previous and next agent nodes respectively.\n\n\nFig.7. The space  after inserting  at coordinates . Note that, the links of  do not necessarily point to the same agent in both dimensions.\n\nThe neighborhood lookup function\n\nLet  be the neighborhood of agent  within a Euclidean distance  at step , where . The algorithm to obtain  is as follows. First, it traverses the list in the  dimension in both directions starting from , looking for nodes whose  value is within  from . For each node that complies with this condition, its two-dimensional Euclidean distance is calculated to verify that the node is indeed within the perception range of . If so, it is included in the set . This process is illustrated in Fig.8. Observe that agents  are all within a distance  from  in the  dimension, but not all are within the perception scope. Therefore, the neighborhood set is . The pseudo-code 1 shows that this calculation only requires a few operations. The time complexity of this function is usually , due to the fact that agent nodes are ordered by proximity in the list. An agent's neighbors in space are also neighbors in the list, so the search time is roughly constant. The worst case for this function is , when ; that is, when all agents in the space are neighbors of . This case is very rare and not realistic for ABMS simulations. For this to happen, the agent would have to either perceive the entire space or all the agents would have to be concentrated in a small space comparable to the perception range of . Neither situation is common in a properly configured complex system model. However, in such an uncommon case, the \\textit{neighborhood lookup} function would not be necessary, because it would be more efficient for agents to have global access to the list of all agents in the simulation.\n\n\nFig.8. BioMASS spatial model depicting the neighborhood . The area perceived by agent  at step  is a disk of radius . Note that although agent  is identified as a candidate neighbor while traversing the linked list in both dimensions, its Euclidean distance from  is greater than .\n\nThe neighborhood lookup algorithm\n\n\nFig.9. Algorithm for neighborhood lookup of agent  at distance  in the BioMASS space.\n\nComplexity analysis: strengths and limitations\n\nFor theoretical analysis and detailed algorithmic evaluation, see \nSansores et al., 2020\n.\n\nTable 1 summarizes the time complexities of the different spatial models. BioMASS has the best theoretical performance with respect to the time complexity of the neighborhood lookup function, but the time complexity of its insert function is high compared to the other models. However, most simulation experiments are not limited by the insertion time of agents, since this is done just once to set up the initial scenario. Rather, to model the dynamics of complex systems with local interactions, the sensory function is of primary importance since it is used most intensively. In the case of the BioMASS space, the time complexities of the move and neighborhood lookup functions are both .\n\nThe worst case implies that the density of agents is high compared to the perception scope. Such cases are rare in the simulation of complex systems. When they arise, it often means that the resolution of the simulation is unsuitable for the problem. Furthermore, if they do happen frequently, the functions can be implemented differently. For example, if agents are able to perceive the whole space then it is simpler to provide them with a list of all individuals in the space. In contrast, in the continuous space the neighborhood lookup function is always  for all agents, independently of the perception range. In the hybrid and discrete spaces, time complexity in  are also rare (when agents perceive most of the environment), the neighborhood lookup function is still a quadratic function of the perception, which make them less efficient than the worst cases of the other spaces. Hence, theoretically the BioMASS neighborhood lookup function should be the most efficient.\n\nOperation\tDiscrete Model\tContinuous Model\tHybrid Model\tBioMASS\nInsertion\tO(1)\tO(1)\tO(1)\tO(n)\nDeletion\tO(1)\tO(1)\tO(n)\tO(1)\nMovement\tO(1)\tO(1)\tO(n)\tO(1)\nNeighborhood Lookup\tO(W²)\tO(n)\tO(n × W²)\tO(1)\n\n*with W = width of the space\n\nExperiments\nPerformance benchmarking\n\nWe compare the four spatial models through a case study: the classical swarm model. The case study simulates a large flock of prey birds that attempt to avoid a small number of predator birds flying in the same environment. Individual prey and predator birds are modeled as autonomous agents living in a 2D toroidal (opposite boundaries of the space are joined) continuous space.\nSince the BioMASS spatial model implements a continuous space with Cartesian coordinates, we decided to compare it only to similar models. That is, we did not test the swarm scenario with the discrete spatial model, which performs poorly in large spaces with neighborhood lookup ranges beyond the agent's surrounding cells.\nWe selected two simulation platforms for comparison, Repast and MASON. Our selection criteria were as follows: (1) the platform supports one of the two models with Cartesian coordinates, continuous or hybrid; (2) it is a general-purpose simulation tool; and (3) it is an ongoing project still releasing new versions. Also, the Java programming language was an important requirement to standardize the experiments between simulation platforms. Finally, while selecting the platforms we also took into consideration less critical characteristics such as robustness, running speed, and open-source support.\nAll simulations were executed on an iMac with a 3.2 GHz Quad-Core Intel Core i5 processor. The iMac had 16GB of memory and was running MacOS Catalina.\n\nThe experiments measured execution time per simulation step, demonstrating that BioMASS consistently outperformed other models in both homogeneous and heterogeneous agent configurations.\n\nSource code\n\nThe spatial model is integrated into a simple yet versatile Java framework for multi-agent system simulation. Key classes within this framework include Agent.java, Scheduler.java, Space.java and PhysicalObject.java, which coordinate the core functions of agent behavior, simulation timing, and spatial environment management. The PhysicalObject.java class specifically equips agents with the four pointers necessary for insertion into the quadruply linked list. The orchestration of a complete simulation can be observed in the source code and example available in the \nGitHub repository\n, where the Main.java class serves as the entry point. Executing this class launches a graphical user interface (GUI) that enables users to configure key parameters for the provided example, including the simulation space size, the number of agents, and the perception range. Notably, this framework is designed with flexibility in mind, allowing for easy adaptation to different scenarios beyond the example provided. This adaptability makes it straightforward for users to modify the framework for a variety of multi-agent simulations by adjusting or extending its core components.\n\nDataset\n\nThe \nBioMASS Space Model Dataset Benchmark\n\n is publish in the IEEEDataPort to easily reproduce the experiments. This dataset contains the results of the simulation runs of the experiments performed to evaluate and compare the proposed spatial model for situated multi-agent systems.\n\nDataset processing\n\nIt is important to note that this research is based on data generated by simulations. Therefore, traditional dataset preprocessing methodologies are not applicable. The data used for analysis consists of the parameters used to configure the simulations, such as the number of agents, perception ranges and environment size, and the resulting performance metrics, such as time steps and execution times. The parameters of the simulations were varied in order to test the performance of the BioMASS model under a wide variety of conditions.\n\nInstructions\n\n The dataset include a compressed file in zip format. It contains a directory structure as shown below. Each directory is a specific experiment with each simulation toolkit and parameters. Inside each directory there are 50 CSV files, one for echa simulation run. Each file has a header describing the main parameters of the corresponding experiment. We use the Repast Toolkit, and Mason Toolkit to perform a benchmark with the proposed BioMASS spatial model.\n\nDirectory:\nExperiments\n-heterogeneous\nheter-biomass\nheter-mason.20\nheter-mason.40\nheter-mason.200\nheter-mason.400\n-homogeneous\nhomo-biomass\nhomo-mason.200\nhomo-mason.400\nhomo-repast\n\nThe simulation setup and full benchmarking methodology are described in \nSansores et al., 2020\n.\n\nValidation Approach\n\nTo validate the performance of the BioMASS model, an empirical validation approach was adopted, based on experimentation and comparison with other established simulation models. This approach focuses on evaluating the model's ability to accurately simulate the behavior of complex systems and its computational efficiency in various scenarios.\n\nVerification methods\nSimulation replication: Multiple simulation runs (50 repetitions) were performed for each experimental scenario. This allowed us to assess the consistency and stability of the results obtained with the BioMASS model.\nComparison with existing models: The performance of the BioMASS model was compared with that of other widely used spatial models, such as continuous (Repast) and hybrid (MASON) models. This comparison was based on key performance metrics, such as simulation runtime and model scalability.\nTheoretical Complexity Analysis: A formal analysis of the computational complexity of the BioMASS model was performed, deriving mathematical expressions for the time required by its main functions. This analysis was compared with the theoretical complexity of other spatial models to validate the efficiency of the BioMASS model from a theoretical perspective.\nThe maximum, minimum, and average values of the time step were calculated for each simulation run. These values were used to assess the variability and stability of the model's performance across multiple runs.\nRobustness and quality considerations\nScenario variability: Various simulation scenarios were used with different numbers of agents, perception ranges, and environment sizes to evaluate the robustness of the BioMASS model under a wide range of conditions.\nStatistical analysis: Statistical analysis techniques were applied to the simulation results to assess the significance of the performance differences observed between the BioMASS model and other models.\nResults validation: The results obtained with the BioMASS model were compared with the expected results based on theory and empirical evidence available in the scientific literature.\nValidation Results\n\nThe theoretical complexity analysis demonstrated that the BioMASS model has a lower computational complexity for the neighborhood search function compared to other models. This efficiency of the BioMASS model was confirmed by the empirical validation results, which showed that the BioMASS model outperforms traditional spatial models in terms of computational efficiency, especially in scenarios with a large number of agents and varying perception ranges.\n\nValidation implications\n\nThe validation results support the validity and usefulness of the BioMASS model as an efficient and accurate tool for simulating complex systems. The BioMASS model has the potential to facilitate the study of a wide range of complex phenomena in various scientific disciplines.\n\nCase study: marine ecosystem simulation\n\nBioMASS was tested in a marine ecosystem simulation, where it successfully modeled predator-prey interactions, trophic chains, and adaptive behaviors. The simulation demonstrated BioMASS’s ability to:\n\nTrack species interactions across multiple trophic levels.\nMaintain real-time execution even with thousands of agents.\nHandle dynamic perception ranges efficiently.\n\nFor a more detailed description of the marine ecosystem simulation, refer to \nSansores et al., 2016\n.\n\nResults\n\nWe analyzed the time complexity of the different spatial models experimentally through simulation. The simulation experiments tested various perception scopes, to test the execution time of the spatial model for extreme values. The results of these experiments are depicted in Figs. 10 to 15, and clearly demonstrate that BioMASS outperforms the other models.\n\nKey Findings\n\nThe experimental results highlight:\n\nScalability: BioMASS handles thousands of agents efficiently, unlike traditional models.\nComputational efficiency: O(1) complexity in neighborhood lookup, compared to O(n) or worse in other models.\nSupport for heterogeneous agents: BioMASS successfully manages dynamic perception ranges without degrading performance.\nComparison to classical models: BioMASS outperforms grid-based, continuous, and hybrid models in both speed and scalability. This approach eliminates the need for global searches, reducing the computational cost of spatial queries.\n\nFor an in-depth discussion of the experimental results and performance metrics, refer to \nSansores et al., 2020\n and \nSansores et al., 2016\n.\n\n\nFig.10. Maximum time step duration of each run for the simulations with homogeneous perception scope.\n\n\nFig.11. Minimum time step duration of each run for the simulations with homogeneous perception scope.\n\n\nFig.12. Average time step duration of each run for the simulations with homogeneous perception scope.\n\n\nFig.13. Maximum time step duration of each run for the simulations with heterogeneous perception scope.\n\n\nFig.14. Minimum time step duration of each run for the simulations with heterogeneous perception scope.\n\n\nFig.15. Average time step duration of each run for the simulations with heterogeneous perception scope.\n\nConclusion\n\nThe BioMASS space model represents a significant advancement in the field of multi-agent systems, offering researchers an efficient and scalable tool for simulating large-scale, spatially explicit environments. By addressing the computational inefficiencies present in traditional ABM platforms, BioMASS enables more detailed and realistic simulations of complex systems, such as marine ecosystems or predator–prey dynamics. The main reason for this accomplishment is that the spatial model is designed to perform the neighborhood search function with  time complexity in almost all cases. This allows a large number\nof agents to use the search function intensively without significantly affecting the simulation time.\n\nFuture work\n\nFuture work could extend the model to support three-dimensional environments, further broadening its application.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nThe Problem\n\nClassical spatial models for situated multi-agent systems\n\nThe Need for an optimized spatial model\n\nResearch Questions\n\nObjectives\n\nMethodology\n\nThe proposed BioMASS quadruply linked list spatial model\n\nThe neighborhood lookup function\n\nView all\nCode\nDatasets",
    "awards": [
      "ai research excellence"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "multilingual-ocr-for-devanagari-and-english-language-using-crnn-architectures-OJZPAskG780W",
    "username": "Anish Khatiwada",
    "license": "MIT License",
    "title": "Multi-lingual OCR for Devanagari and English Language using CRNN architectures",
    "publication_description": "Back to publications\nDec 22, 2024\n●\n196 reads\n●\nMIT License\nWinner of\nBest Overall Project\nat the\nComputer Vision Projects Expo 2024\nMulti-lingual OCR for Devanagari and English Language using CRNN architectures\n#IAM-handwriting\nCNN\nCNN+NLP\nCRNN\nDevanagari-OCR\nEnglish-OCR\nGRU\nLSTM\nMultilingual-OCR\nOCR\nResidual-block\nAnish Khatiwada\nLike\nBookmark\nShare\nAbstract\n\nIn recent days, businesses, governments, educators, and individuals are increasingly required to handle various documents, including invoices, legal papers, and educational materials. These documents can exist in multiple languages and formats, complicating the process, especially when dealing with regional languages like Nepali and Hindi. Current Optical Character Recognition (OCR) systems, such as i2OCR, Google’s Tesseract, and Microsoft OCR, excel at recognizing printed English text but struggle significantly with handwritten English. This challenge is exacerbated for languages utilizing the Devanagari script. As a result, many organizations still rely on manual data entry, leading to wasted manpower, inefficiency, and high labor costs. The limitations of existing OCR technologies highlight the urgent need for advancements in this field to improve automation and document management.\n\n1. Introduction\n\nThe limitations of current Optical Character Recognition (OCR) tools in effectively recognizing and extracting text from multiple scripts motivated my research in this technology. The goal of this project is to develop a Multilingual OCR system with a low Character Error Rate (CER) for handwritten English and Devanagari scripts, specifically for Hindi and Nepali languages. This system addresses challenges related to text extraction in various industries, including education, finance, government, and legal documents. In a country like Nepal, there are frequent cases of risks associated with preserving students’ exam papers, which can adversely affect their education and careers. By digitizing exam papers as a backup, this project aims to mitigate these issues and enhance the overall educational experience.\n\n2. Background\n\nThe background of the project originated from the observation of a significant deficiency in reliable text recognition tools across multiple languages, including handwritten English and Devanagari related language such as Nepali and Hindi. These languages are spoken by millions and billions of people, respectively. In the industry and government sector of south-east Asia, the demand for automated data entry, text extraction, document digitization, text analysis and searchability has been steadily increasing. Hence, the objective of this project is to develop a multilingual OCR system capable of accurately recognizing handwritten text in English, Nepali, and Hindi with low CER (Character Error Rate), which is the most crucial aspect influencing to the usage of OCR system. Thus, the projects employ advanced machine learning methods, especially deep learning, to address this challenge. The project integrates Computer Vision and Natural Language Processing for the robust outcome. For English text extraction, the Neural network architecture consist of Convolution layer combined with LSTM, utilizing the IAM Handwritten dataset. For Nepali and Hindi text recognition, it uses same datasets of Bhagwat Geeta images, and architecture involves CNNS with GRUs.\n\n3. Literature Review\n3.1. Introduction\n\nThe robust and multi-language OCR system is hot research topic in the field of Computer Vision and Natural Language Processing. It has been the great topic of interest since long time. Due to the varying situation and problem, different research is carried out in their own way. For instance, some researchers may be developing OCR for number plates detection, while others might be working on hard copies, product’s image, or bill receipt. Additionally, they might be working with different languages, styles, fonts, skewness, and language scripts such as Nepali, English, Urdu, Chinese and so on.\n\n3.2. Related Works\n\nThe study from Facebook AI researcher (Huang, et al., 2021) has recently purposed multiplexed network with the claimed to be first model that can jointly use text detection and recognition with very high accuracy. Naïve training system for each language does not takes accountability of the precision of each language and it is also computationally expensive. The purposed architecture also reduces the computation as there is no need to train different model for each language. Language Prediction Network (LPN) is being used for automating the recognition of a languages. The output of the network is a vector of size L = Nlang i.e. the number of language class, which can later be converted into probabilities using a SoftMax. The strong point of the model is Nlang does not have to be equal to the Nrec i.e. the number of different recognition heads because model deals with shared alphabets. For instance, alphabet in Nepali and Hindi matches in most of the case, so prediction can be used interchangeably. Thus, in this case Nlang equals to Nrec.\n\n\nFigure 1; LPN architecture\n\nHere is how Language Prediction Network (LPN) is calculated.\n\n\nwhere, Llang, R, and Lseq(r) are the loss for LPN, set of recognition heads and loss for the recognition head r. are the two hyper-parameters of model.\n\n\nwhere, I(l = lgt) is the binary indicator i.e. 0 if the language do not matches or 1 if the expected language matches to the actual language, and p(l) is the probability of the word belonging to the language l.\n\n\nwhere, p(yt = ct) is the predicted probability of character at position t of the sequence, T is the length character, Cr is the character set that is supported by recognition head r.\n\n\nFigure 2; Recognizing text\n\nAnother study from (Biró, et al., 2023) evaluates the feasibility of synthesized language preparation and its comparison by implementation with machine learning based neural network architecture. The research aims to address the challenges of real-time multilingual OCR in a collaborative environment. The vocabulary is artificially generated by applying a sampling technique to real-world data and creating simulation scenarios where models and processes interact to create completely new data. So, the synthesis datasets for training ranges from 66k to 8.5M and total of 21 results were analysed. The straightforward comparison is conducted between Convolution Recurrent Neural Networks (CRNN) and patch-wise image tokenization framework (SVTR) model architectures as Machine learning based OCR engines. PaddleOCR was also configured for fine-tuning.\n\nCRNN is basically the combination of Convolution Neural Network (CNN) and Recurrent Neural Network (RNN) where it retrieves feature from the input image through CNN and RNN models the sequential dependencies between the characters in the text. On the other hand, SVTR is an advance OCR system that uses both deep learning and traditional image processing techniques to recognizes text in images. Additionally, SVTR also contains attention mechanisms that allows to prioritize on most important visual components and boosts the noise and distortion resistance of the model (Biró, et al., 2023). So, these two modern text recognition techniques were experimented shown in the figure below.\n\n\nfigure 3; distortion resistance\n\nMultiple experiments were conducted with the proposed network architecture. But each time CRNN was able to outperform SVTR. The Machine learning models records the highest precision in Experiment 11, 12 and 14 with scores of 90.25%, 91.35% and 93.89%. So, now we will discuss the detail architecture of selected model from the study.\n\nIn compared to previous study, the research from Xinjiang university (Yu, et al., 2023) has argued experimentally to the previous study by improving the Convolution Recurrent Neural Network (CRNN) for scene text recognition. The improved CRNN model was also trained on synthetic datasets but tested on public datasets. For accomplishing this, the Connectionist Temporal Classification (CTC) function proposed by Alex Graves is redefined. After the feature is first extracted by Convolution Neural Network (CNN) algorithm, the feature dependencies are captured through Recurrent Neural Network (RNN), the output prediction distribution after applying RNN is fed to CTC for processing and then it gives a sequence of text as results. Text and speech signals are sequential data, which makes it difficult to perform segmentation. Additionally, the volume of text recognition data is huge, making it difficult to achieve segmentation and annotation. So, to make the Recurrent Neural Network (RNN) much more applicable for text recognition, the Connectionist Temporal Classification (CTC). The CTC decoding process maps the generated path into final sequence. The figure below shows an example of the final sequence mapped after deduplication and removal of a whitespace character using CTC.\n\n\nFigure 4; Input Output Sequence\n\nThe network structure of the model consists of three layers, that is convolution layers, recurrent layers and the transcription layers with the combination of CNN + RNN + CTC. In the beginning, image is converted to a grayscale and then resized to W*32 with fixed height. Recurrent layers using RNN predicts a label distribution of the feature sequence which is obtained from the convolution layers. The feature maps extracted by the CNN are split by column where each column of 512 – dimensional feature is input into two layers of 256-unit bidirectional LSTMs for classification. CTC than converts the label distribution obtained from RNN through transcription layer. The CTC algorithm deduplicates to obtain the final recognition result, and label smoothing is further added. The recognition result is then forwarded to the language model for correcting characters and finally the final output is obtained.\n\n\nFigure 5; Structure of overall improved architecture\n\nThe datasets used in text recognition can be divided into synthetic and real. Synthetic datasets are mainly used for training, while real datasets are used for evaluating the training results. The study has used synthetic datasets: MJSynth a plain English text dataset that contains 3000 folders with about 9 million image and SynthText consisting of 800,000 scene images. The real datasets which is used are further divided into regular and irregular, based on the format of the text in the Image. Regular datasets include IIT5k-words (IIT5k), SVT (Street View Text), and IC13 (ICDAR2013).\n\nDeep Convolution neural networks (CNN) has shown state-of-the-art performance in various field of computer vision. Due to the availability of pre-trained models, one can save time and resources, and improve accuracy. In the study (K.O & Poruran, 2020), OCR-Nets variants of AlexNet & GoogleNet is presented for recognition of handwritten Urdu characters through transfer learning. While performing transfer learning, Initial layer weights are freeze and the last three-layers are fine-tuned for training and classification of Urdu characters whereas only four inception module is utilized, and the last two layers are modified for learning and classification of the Urdu characters. The experiment shows significant performance gain by OCR-AlexNet and OCR-GoogleNet of about 96.3% and 94.7% average recognition rate respectively.AlexNet is a winner of ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012, becoming the first deep learning model to do so with top-5 test error rate of 15.3%. The network consists of 8 deep layers trained with 10000 classes of ImageNet dataset. The implemented OCR-AlexNet architecture has minor differences to AlexNet for customization. The last three layers of AlexNet consist fully connected layers, with the final layer used for a 1000-label classification task. The network takes input images of 227*227 pixel. Each of the fully connected layers consists of 4096 neurons, chances of overfitting is reduced via Data augmentation and Dropout. Dropout is applied by setting zero to 50% of neurons during training. Adjustments are made to facilitate faster learning including the changes in fully connected layer’s size and learning rates. The architecture of OCR-AlexNet can be presented below.\n\n\nFigure 6; AlexNet Architecture\n\nGoogleNet, also known as Inception-v1, is a deep convolution neural network which gained popularity by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2014 with top-5 error rate of 6.67%. It is more complex with compared to AlexNet. GoogleNet consist of 22-layer deep neural network utilizing 9 unique inception modules unlike traditional CNN structures. These inception modules effectively capture local features by using various kernel sizes for computation, which are later combined together with upcoming layers. To adapt GoogleNet for Custom OCR, the researchers has fine tune it by keeping 4 inception modules and replacing the final two layers with new ones. Overfitting is prevented by freeing earlier layers’ weights and adjusting the learning rates.Thus, this method encourages rapid learning in new layers, slower learning in intermediate, and no learning in frozen layers (K.O & Poruran, 2020).\n\n\nFigure 7; GoogleNet Architecture\n\nIn contrast to the other study, where end-to-end deep learning architecture are implemented, the author (Kumar, et al., 2023) has compared and experimented the commercially available OCR software for processing the text from Television and Video data. The study has analysed 3 differed OCR model’s performance over IAM Handwritten datasets and Complete Blood Count (CBC) datasets. IAM database consist of 14,892 photos of handwritten English lines of text by 657 authors, which is 1,539 handwritten pages and 115,320 words in total. The CBC datasets is gathered by researchers themselves from family and relatives which consist of 85 complete blood count printed reports. The three models Amazon Textract, Google Vision, and PyTesseract were tested on these two different datasets.\n\nAmazon Textract: Aws Textract is a managed machine learning service that goes far away from traditional OCR in the task like recognizing and extracting data from forms and tables in addition to printed and handwritten text. The service also offers an API for image processing, providing flexibility to retrieve raw text in JSON (Javascript Object Notation) format or any other organized tabular layouts. For the IAM dataset, a simple API request is enough to obtain raw text, while for the CBC dataset, the available table detection feature streamlines the extraction of parameters and targeted values from the images.\n\nPyTesseract: It is a widely used python module for text recognition in images build on the Tesseract Engine. It is compatible with various image formats, and incorporates preprocessing techniques such as image rescaling, binarization, noise reduction, normalization, morphological thinning, and deskewing to enhance accuracy. After preprocessing, the image is feed into the PyTesseract engine and it returns a textual unstructured response. And then it is formatted accordingly to both IAM and CBC datasets.\n\nGoogle Vision: It is an AI-powered API by Google for image processing using REST and RPC APIs with pre-trained models that allows user to label images, recognize objects and text, and adds metadata to images saved in Google Cloud Storage. It do not have table extraction as Amazon does but provides word-centric responses based on block coordinates. In the IAM dataset, content retrieval involves combining information from the boxes in the same row to create sentences using a matching algorithm. As for the CBC dataset, with diverse sections and lines, the process involves identifying similar box coordinates for each line, assembling sentences, and extracting parameters and values.\n\nGoogle Vision – It is an AI-powered API by Google for image processing using REST and RPC APIs with pre-trained models that allows user to label images, recognize objects and text, and adds metadata to images saved in Google Cloud Storage. It do not have table extraction as Amazon does but provides word-centric responses based on block coordinates. In the IAM dataset, content retrieval involves combining information from the boxes in the same row to create sentences using a matching algorithm. As for the CBC dataset, with diverse sections and lines, the process involves identifying similar box coordinates for each line, assembling sentences, and extracting parameters and values.\n\n\nFigure 8; Implementing different models on IEM Handwriting dataset\n\nIn the study, the two metrics, Accuracy and Character Error Rate (CER) are used to analyze the model. The accuracy of each model for the IAM dataset was measured by the percentage of characters that were correctly detected and in case of CBC dataset, the accuracy is measured by how correctly the values of parameters are matching. Every character in the truth dataset is matched with the expected value in string matching.\n\n\nOn the other hand, the second metric, Character Error Rate (CER) is dependent on the quality and the content of the dataset and thus calculated using the equation below.\n\n\nHere, S means the number of substitutions, D is the number of deletions, I refers to the number of insertions, and C is the number of correct characters. The lower the value of CER is, the better the performance of model is.\n\nThe final performance of all three models on the IAM and CBC datasets is presented below.\n\nDataset\tMetric\tAmazon Textract\tGoogle Vision\tPyTesseract\nIAM\tAccuracy\t89.42%\t88.53%\t81.02%\nIAM\tCER\t0.1403\t0.1621\t0.2650\nCBC\tAccuracy\t95%\t90.3%\t84.1%\nCBC\tCER\t0.1082\t0.1332\t0.2231\n\nAfter testing the different models on the different datasets, Result indicates that the Model performs better in printed CBC dataset than Handwritten ones, likely due to consistent character formats in printed text. Amazon Textract demonstrated superior performance compared to Google Vision and PyTesseract, with preprocessing techniques further enhancing model accuracy. It also achieved the highest accuracy of 95% and the lowesr CER of 0.1082 for CBC blood reports with preprocessing. The study suggests that performance of OCR engine heavily depends on datasets and model training.\n\n4. Methodology\n\nAs the motive of this project is to develop Optical Character Recognition system (OCR) which is dependent on the Deep learning techniques. It has complex architecture in the background. This architecture is designed in such a way that it takes images of size 96*1408 with its corresponding label while learning. There is series of convolution layer for extracting the pixel value and learning the image content. It is connected with Sequential model where it learns the label text of corresponding images by mapping. For handling Images there is 9 residual blocks. Residual block is the architecture developed by google in its inception-v1 model. Each residual block has Convolution layer, Pooling, Dropout, and it is repeated 3 times by connecting with each other. At the end the input layer is itself multiplied with the final convolution layer, therefore this is also known as skip-connection architecture. For Sequence modelling, I have used two different architectures depending on the language model. For English Model, there is LSTM (Long-Short term Memory) and for Devanagari Model, there is GRU (Gated-Recurrent Unit). This is because, English datasets is larger than Devanagari datasets, but the Devanagari datasets is much complex than the English one. Before feeding data to the model, it goes through different process and the crucial step is given below.\n\n4.1. Architecture\n\nThe CRNN (Convolutional Recurrent Neural Network) architecture is highly preferred option for the the tasks such as Optical Character Recognition (OCR). This architecture integrates Convolution Layers (CNN) to extracts spatial information from the input image and maps it with the corresponding sequential data by dealing with the sequences of varying lengths. The Recurrent Layers (RNN), such as Bidirectional - Long Short-Term Memory (Bi-LSTM), Bidirectional - Gated Recurrent Unit (Bi-GRU), analyses the order of elements in a sequence, enabling a deeper understanding of the sequential patterns of data. The integration of Connectionist Temporal Classification (CTC) loss further strengthens the CRNN architecture. This facilitates end-to-end training by empowering the model to learn and directly predict the character sequences from the input image data. This eliminates the need for explicit alignment between input images and ground truth text. In addition to streamlining the training process, it also enhances model performance significantly. Thus, the CRNN architecture has emerged as a highly adaptable framework, which outperforms many other methods at extracting features, modelling sequences, and recognizing characters. This characteristic makes it one of the best options for building our own OCR system using deep learning techniques. There are two different variants of CRNN which is being used in the project: one for English model and another for Devanagari model.\n\n\nFigure 9; Architecture for English Model\n\n\nFigure 10; Architecture for Devanagari Model\n\nThe common things in both the architecture are:\n1.Input layer: The height and width are reshaped to 94 x 1408 to both English and Devanagari images. As we know, the model is being trained with line base images with its corresponding label, height is slightly low then the width of the image.\n\n2.Dense Layer: It is a type of Artificial Neural Network (ANN) layer where each neuron in the layer is connected to every neuron in the preceding layer. This is why it is also known as Fully Connected Layers. It performs a weighted average, on the input data, and activate to the result of weighted average to introduce non-linearity into the network. This allows model to learn complex patterns in data during training process.\n\n3.Residual Blocks: The concept of a residual block originated due to the problem of vanishing gradient, that are particularly encountered while training deep neural networks. They were introduced in the paper of ResNet architecture as a skip-connection blocks which can learn residual functions by directly maping to the final reference of convolution layer. This means that instead of learning the desired underlying maping  by stacking multiple layers, residual blocks directly learn , and the original input x is added back to this learned residual to get the final output . This is how residual blocks deal with vanishing gradients problem by enabling gradients to flow directly through the network during training.\n\n\nFigure 11; Residual block\n\nThe figure above shows the first residual blocks of our architecture. At first input is normalized and squeeze, which is denoted by Lambda. Then it goes pass through , , and  two times, and at final steps the flattened input is again added with the output of convolution operation followed by activation function and Dropout.\n\n1.Conv2D with kernel size <3 x 3 x 3 x 32>: This represents a Convolution 2D layer with a kernel height and width of 3 x 3 and a depth of 3, which represents RGB color channel. The 32 at the end indicates that there are such thirty two kernels in this layer, each producing its own output feature map.\n\n2. Batch Normalization: Batch Normalization is a technique for normalizing the activation of each layer in a neural network. It is very helpful in reducing the internal covariate shift, which stabilizes and speeds the training process.\n\n3. Leaky ReLU Activation Function: The Leaky ReLU Activation function is a variant of the Rectified Linear Unit (ReLU) activation function. The difference is Leaky RelU introduces a small slop, typically a small positive value like 0.00001 for negative input values allowing a small or non-zero gradient when the input is negative. This helps to overcome dying ReLU problem, which could make neuron dead during training.\n\nReLu activation function is defined as below.\n\n\nIt returns 0 for any negative input value and return input value itself for any positive value.\n\nAnd Its derivative is as follow.\n\n\nLeaky ReLU has make difference by introducing following small alpha(α) value.\n\nIts derivatives is as follow.\n\n\nThe graph of ReLu(x) and Leaky ReLu looks like belows\n\n\nFigure 12; ReLu vs Leaky-ReLU\n\n4.Output Layer: This is the final layer of the architecture, that produces the output predictions of the model. The structure and activation function of the output layer is dependent on the nature of the task that is trying to be addressed. In our case, the output layer has neuron according to the unique vocab available in the datasets. For example, English model architecture has 93 neurons in Output layer because there are 93 unique character and sign such as like comas and slashes and Similarly, Devanagari model has 90 neurons in Output layer. The activation function used in our Output layer is , because we have multi-class classification problem, we need to choose one correct character among all the available options each time.\n\nThe difference in the architecture is in sequence modelling. The English model uses Bidirectional-Long Short-Term Memory (LSTM) but the Devanagari model contains Bidirectional-Gated Recurrent Unit (GRU). This is because English model has a lot of training data, and complex label. So, during hyperparameter tuning LSTM architecture was doing great work. But Devanagari datasets is not big as IAM handwriting, and the Devanagari character is much more complex than English character. So, Gated Recurrent Unit (GRU) was converging and doing much better than LSTM, and ultimately, I have modified the architecture to GRU. Below, we will detailly explored the Bi-LSTM and Bi-GRU architecture.\n\nAnnotation and symbols for understanding LSTM and GRU\n: Forgot Gate\n: Input Gate\n: Output Gate\n: Update Gate\n: Reset Gate\n: Current input at time step t\n: Previous hidden state at time step t-1\n: Candidate hidden state\n: Final hidden state\n: Weight matrix for the forget gate\n: Weight matrix for the input gate\n: Weight matrix for the output gate\n: Weight matrix for the candidate cell state\n: Weight matrix for the forget gate\n: Weight matrix for the input gate\n: Weight matrix for the output gate\n: Weight matrix for the candidate cell state\n: Candidate cell state at time step t\n: Update cell state at time step t\n\nThe above symbols will be used further to understand the calculation of the Sequential Model.\n\n4.1.1. Bidirectional LSTM (Bi-LSTM)\n\nBi-LSTM is a Recurrent Neural Network (RNN) which is used to process sequential data like text and sounds, where the current output is depended on previous output. It stacks multiple Long-Short Term Memory (LSTM) to reverse the direction of the flow of information. Before describing Bi-LSTM architecture, lets understand the simple LSTM architecture\n\n4.1.1.1. LSTM Architecture\n\n\nFigure 13; LSTM Architecture\n\nThe LSTM architecture consists of forgot gate, input gate and output gate. These gates are designed to regulate the flow of information through the Cell. It allows the LSTM to selectively update its and memory and also decide which relevant information to be kept or which to discard.\n\n1.Forgot Gate: The forgot gate takes the current input and the previous hidden state as inputs and computes the forgot gate vector  using a sigmoid activation function. It outputs the value between 0 and 1. If the value is approaching towards 1, then it indicates that information from the previous cell state is relevant and thus should be retained. But if the value is approaching towards 0 in the forgot gate vector, it indicates that the previous cell state information is irrelevant or less important for the current time step and should be discarded.\n\n2.Input gate: The input gate determines how much of the new information should be added into the cell state. It takes the current input and the previous hidden state as inputs and computes a gate vector it is using a sigmoid activation function, which controls the flow of new information into the cell state.\n\n3.Output gate: The output finalize how much of the cell state should be incorporated to the next hidden state. It also takes current input and the previous hidden state, candidate hidden state as input and passes them to the sigmoid activation function. Additionally, the cell state Ct is also passed through tanh activation to produce value between -1 and 1. And finally, the output gate combines the sigmoid gate values and the tanh values to produce the final hidden state .\n\nMathematical Operation\n\nInput Gate Operation\n\n\nForgot Gate Operation\n\n\nOutput Gate Operation\n\n\nCandidate State Cell\n\n\nCell State Update\n\n\nHidden State Output\n\n\nBi-LSTM adds two LSTM layers to flow the information from both the direction and and combine their output. The figure of Bi-LSTM is presented below.\n\n\nFigure 14; Bi-LSTM Architecture\n\nThe forward LSTM processes the input sequence from left to right and the backward LSTM processes it from the right to left. Like we see earlier, Each LSTM cell in both directions maintains a hidden state and a cell state to capture information from the input sequence. The forward LSTM captures information from past tokens in the sequence, while the back LSTM captures from future tokens. By processing the input sequence in both forward and backward directions, the Bi-LSTM can capture information in bidirectional context.\n\n4.1.2. Bidirectional GRU (Bi-GRU)\n\nLike Bidirectional LSTM, Bidirectional GRU is another variant of RNN which we have used in Devanagari architecture. It has also stacked multiple Gated Recurrent Unit (GRU) to reverse the direction of the flow of information. Single unit of GRU architecture is presented below.\n\n4.1.2.1. GRU Architecture\n\n\nFigure 15; GRU Architecture\n\nGRU also maintains a hidden state  for storing information about the input sequence up to the current time step. It achieves this through the use of gating mechanism which allow it to selectively update its hidden state based on the input at each time step. A GRU unit contains two gating mechanisms, update gate  and reset gate .\n\n1. Update Gate: The update gate determines how much of the previous hidden state should be retained and how much of the current input should added into the new hidden state. It passes current input and the previous hidden state though a sigmoid activation as input.\n\n2. Reset Gate: The reset gate determines how much of the previous hidden state should be forgotten. It also takes current input and previous hidden state as input and passes them through sigmoid functions.\n\nMathematical Operation\nUpdate Gate Operation\n\n\nReset Gate Operation\n\n\nCandidate Hidden State\n\n\nFinal Hidden State Update\n\n\nSame as Bi-LSTM, Bi-GRU adds two GRU layers to flow the information from both the direction and and combine their output. The figure of GRU is presented below.\n\n\nFigure 16; Bidirectional-GRU Architecture\n\nThe forward GRU processes the input sequence from left to right and the backward GRU processes it from the right to left. Each GRU cell in both directions maintains a hidden state and a cell state to capture information from the input sequence. The forward GRU captures information from past tokens in the sequence, while the back GRU captures from future tokens. By processing the input sequence in both forward and backward directions, the Bi-GRU can capture information in bidirectional context.\n\n4.2. Data Aquisition and Annotation\n\nOne of the reasons why Neural Network architecture outperforms traditional Machine learning algorithm is its tendency to learn better with the large datasets. Deep learning architecture are designed to feed huge volume of data. For training our system with English and Devanagari language, which need to have two different variants of data. One is English datasets, and another is Devanagari datasets. Each datasets have Image containing the text and Corpus with corresponding textual document of an image. For training the English Model, I have collected IAM handwriting datasets. This dataset is publicly available with certain credential by Research Group on Computer Vision and Artificial Intelligence at University of Bern. While Devanagari datasets isn’t available in desired form. I have collected the Bhagwat Geeta, a popular scripture images and labelled it manually. The detailed description of each dataset is below.\n\n1. Datasets for English\nThe IAM online handwriting dataset is around 7GB in size. It contains handwritten images with corresponding labels in txt file. It provides data in three forms: Full scale image, Sentences based image, Line based image and Word-based image. Full scale image is contained in three directories: formsA-D, formsE-H, and formsI-Z. Each full scale image under forms directory is made with the combination of form id and writer id. The sentence directory contains sub directory of form id, form id has sub directory of writer id, and inside writer id there is sentence based segmented image of each form, full scale image. Like sentence data, line directory also has subdirectory of form id, form id itself has subdirectory with writer id and writer id contains the line based segmented image of each form. Word directory also contains subdirectory of form id, form id has subdirectory of writer id and writer id directory contains separate image for each word that is contained on full scale image. With compared to sentence and line image, word image can be little big because word and sentence itself contains many words, and for each word there is different image. There is also optional xml directory if we like to work with xml files.\n\n\nFigure 17; Directory of datasets\n\nThe directory ‘ascii’ contains corresponding text label of all the forms, lines, sentences, and words. There are four text file each representing text, error status, and bounding box for corresponding images in forms, lines, sentences, and words.\n\n\nFigure 18; Corresponding annotation for IAM datasets\n\n2. Datasets for Devanagari\nThe indian phylosophical scripture bhagwat geeta datasets is prepared for Devanagari model is prepared manually. I collected the images from online source, and then utilized tools like online orc to get the textual content and online Devanagari typing tools to correct the error of labelled text. And then, I have written algorithm that will iterate over each image and text, tokenize each line of image with corresponding line in text file. There are altogether 3400+ line base image. The size of Devanagari datasets is quite low compared to English datasets, but the complexity is pretty much high. This Devanagari datasets is in Hindi and some sort of Sanskrit language. The actual sloka is in sanskrit and its corresponding interpretation is in Hindi.\n\n\nFigure 19; Devanagari directory\n\n4.3. Data Cleansing\n\nData cleaning is a crucial step for preventing data integration issue. We need to verify whether the corresponding label of training and validating images is correct or not. For English language model, if there is labelling issue, the tag of ‘err’ is given and thus we can ignore that line and if there is no error, the tag of ‘ok’ is given. But for Devanagari data, I have manually automated the task of segmenting each line as region of interest (ROI) with corresponding labels. Some time, region of interest (ROI) could not be capture but its corresponding label will be available, and it will be assigned to other images. This can create label mismatch problem. Therefore, I have manually checked and ensure region of interest (ROI) is perfectly captured for every line of and images. Sometimes, small dots, lines can also make confuses to edge detection algorithms, so some height and width is also given for the confirmation of valid text. If any of the criteria is not fulfilled, then the images is discarded.\n\n\nFigure 20; Detecting ROI for extracting text\n\n4.4. Data Augmentation\n\nTo build a generalize model; a model that can work good on diverse set of data, and reduce overfitting, data augmentation is very popular and useful techniques. It helps to increase the size and variety of our datasets. This technique applies transformation like Random Rotation, Random scaling, Image translation, Random Brightness and adding noise to the data. Each of these techniques is explained below.\n\n1.Random Rotation: This is a process of rotating the images by a random angle for enabling the OCR system to recognize characters at different orientations. I have implemented random rotations in data augmentation steps with the angle range of -10 to 10 degrees.\n\n\nFigure 21; Rotated image\n\n2.Random Scaling: Random Scaling is another technique for diversifying the training data. It includes resizing the line images with certain scale factor, so that our model learns to recognize character of different sizes. I have used the scale factor of 0.3, so that random scaling with range between 0.7 and 1.3.\n\n3.Random translation: Random image translation is used to randomly shift the image horizontally resulting in new variant of image with the change in position of content. The position of the textual part of the image will change it position and the padding will be applied to the current empty space. This technique is combinedly applied with the other data augmentation techniques.\n\n\nFigure 22; Random image translation\n\n4.Random Brightness: This is very rare that model will always get same light, brightness every time. It is very crucial to consider different images containing different levels of brightness. Some images would be lighter than others, some may be little darker than others. Model should be ready to adopt and work on this. So, random brightness is applied with a factor of 0.4, which will result between 0.6 and 1.4.\n\n5.Adding Noise: While processing images in the real world, this is not sure that every of them will be very clear and sharp. So, the model needs to be trained for working on disrupted images, where some pixel values are not clear as others. So, adding noise is a good option which we can use for making generalize learning.\n\n\nFigure 23; Adding noise in image\n\n4.5. Image preprocessing\n\n1.Sharpening: Image sharpening is a technique for making image look concise and clear by enhancing the edges, brightness and contrast of a image. It helps to improve the quality and readability of images by minimizing the factor like blur, noise which may came with the factor like low camera, camera shaking during snapshot, low resolution, and compression. It works by increasing the distance between pixel value of near the boundary’s areas. It can sometimes create over sharpening issue, so we I have used appropriate clipping value to prevent the overflow of pixels information.\n\n2. Grayscale conversion: A gray-scale image is one which has single color channel. Normally, the image is composed of three-color channels, red, blue, and green. Each of these color channels has pixel value ranging from . These makes computation very much complex and gray-scale image are more than sufficient to extract the feature from images and there is no need of processing all these color channels. In our images of size , if use color channels it would be , which equals to . But after grayscale conversion, it would result to . So, we can just image how it is reducing the training complexity by reducing input size to less than its half size.\n\n3.Binarization and thresholding: This method is used in the project for converting grayscale images into black and white images, also known as binary image. I have binarized the image using the global threshold method. First, the global threshold value is determined, and find out whether the pixel has gray value greater than given threshold or not. If the pixel value is greater than threshold, it will be converted to 1 and if the pixel value is less than threshold, it is assigned to 0. Here, 1 represents white and 0 represents black. It will simplify the process by representing the data in binary form.\n\n4.Dilation: Dilation expands the size of an object by convolving the image with a structure element. A small binary image or matrix is used as structuring element to define the neighborhood of a pixel during dilation process. The shape of the binary image will determine the degree to dilate.\n\n5.Erosion: Erosion shrinks the size of an object by convolving the image with structure element. After convolving the object, it returns the result of erosion operation as a new image where the pixels value of image is shrunk, also known as eroded.\n\n5. Performance of the Model\n\nThe popular metrices that is used to evaluate the performance of the OCR model are CER (Character Error Rate) and WER (Word Error Rate). We have monitored the performance of both Model using Tensorboard. The following figures shows the different metrices to evaluate the model.\n\n\nFigure 24; Tensorboard for Monitoring Models\n\nWe can understand the above figure in three parts. First three metrics shows the performance of model during training time, Last three metrics shows the performance of model during validation and middle three metrics is the comparison between training and validation performance in different steps.\n\n5.1.1. English Model Performance\n\n\nFigure 25; Training vs Validation CER (Character Error Rate) for English Model\n\nThe above graphs shows that CER of validation is moving along with the fall of CER in training time.\n\n\nFigure 26; Training vs Validation WER (Word Error Rate) for English Model\n\nThe above line plot also shows that word error is falling when we keep training the model.\n\n5.1.2. Devanagari Model Performance\n\n\nFigure 27; Training vs Validation CER (Character Error Rate) for Devanagari Model\n\nThe above graph shows how validation CER of Devanagari Model is Falling with time.\n\n\nFigure 28; Training vs Validation WER (Word Error Rate) for Devanagari Model\n\nThe above figure shows more we train the model , WER will also fall accordingly.\n\n5.1.3. Final Evaluation\nModel Name\tCER\tWER\tLoss\nEnglish training\t0.01001\t0.0800\t1.566\nEnglish testing\t0.01005\t0.0955\t1.738\nDevanagari training\t0.1000\t0.3578\t30.456\nDevanagari testing\t0.13162\t0.4494\t35.3282\n6. Deployment\n\nIn this section, I will discuss the deployment of the OCR model for general users. The application leverages the Django Rest Framework for backend API development, Vanilla JavaScript for frontend interaction, and SQLite as the default database for storing information.\n\n\nFigure 29; Functional Decomposition Diagram\n\nIn the above FDD, I am have splited the system into four parts i. e. ,User Interface, Deep Learning module, Data Management and Backend module. The sections make a modular view of the related functionalities, which when put together form an entity of the programme. By viewing this illustration we would be able to investigate the levels of the system, the interconnections as well as the relationships of the project. This awareness is indeed crucial for an adequate planning and decision-making process throughout the project complex lifecycle.\n\n6.1. Deployment Process\n\n1. User Interaction\n\ni) Image Upload: Users can upload images that they want to process.\n\n\nFigure 30; Uploading image\n\nii) Image Editing: Users have the option to edit the images as needed.\n\n\nFigure 31;Option for editing uploaded image\n\n2. Language Selection\nUsers select the language of the document, which determines which model will be used on the backend.\n\n\nFigure 32; Selecting language according to documents\n\n3. Edge Detection\nUsers identify the Region of Interest (ROI) in the image. They have three options for edge detection algorithms to choose from.\n\n\nFigure 33; Edge detection\n\n4. Text Prediction\nAfter selecting the appropriate edge detection algorithm, the model processes the image and predicts the text.\n\nThe predicted text is displayed in a text box, allowing users to:\ni) Copy the text.\nii) Export it in .txt or Word format.\n\nThis deployment strategy utilizes a modular architecture, ensuring that each component functions effectively while contributing to the overall performance of the OCR application. This structure not only enhances user experience but also supports scalability and maintainability of the system.\n\n\nFigure 34; Text Prediction\n\n6. Conclusion\n\nThe primary aim of this work was completed through research and developing an OCR designed to decipher images with text in both the English and the Devnagari languages. The specific tasks were to apply the methods of deep learning to create the OCR system from the ground up, ensure the model familiarizes with the concept of UI, and compare the results with the real performance of the OCR systems. The particular academic questions that were being investigated earlier in this paper include understanding the limitations of existing OCR systems, the causes of varied performance of an applied language, and the steps that should be taken in developing a general OCR engine.\n\nThe major conclusions and findings that can be made throughout the course of this projects are the following. The challenges we faced during the development of the multilingual OCR system from scratch helped us to understand which factors have a major impact on the error rate, in particular: resources: easy access to large variety of training data; computational constraints; dealing with multiple languages. The OCR model that was trained with the help of deep learning and that was developed specifically for the application could recognize not only the English language but also Devanagari despite the fact that the comparison with generic OCR services was employed. The incorporation of the OCR model with the user-friendly interface made the entire process of text information extraction from documents ideal for both laypersons and experts with relative ease.\n\nI also identified the problem associated with the current OCR and the solution I want to suggest is Intelligent Document processing. This method involves combining Machine learning for understanding predefined sets of rule in the text document like margin, table, border, signature and many other things. Another issue that comes up is the availability of training data, the more data we have we can build better OCR. We have also observed that OCR would not have same level of performance in different languages because, there architecture may vary, resources are imbalance and the complexity of different languages are also identical.\n\n7. Reference\n\n1.Abisado, M., Imperial, J. M., Rodriguez, R. & Fabito, B., 2020. Doctor’s Cursive Handwriting Recognition System Using Deep Learning. Manila, ResearchGate.\n\n2.Biró, A. et al., 2023. Synthetized Multilanguage OCR Using CRNN and SVTR Models for Realtime Collaborative Tools. Applied Sciences, 13(7), p. 4419.\n\n3.Cristiano , C., 2020. For a Science-oriented, Socially Responsible, and Self-aware AI: beyond ethical issues. In: 2020 IEEE International Conference on Human-Machine Systems (ICHMS). IEEE, pp. 1-4.\n\n4.Dey, R., Balabantaray, R. C. & Mohanty, S., 2022. Approach for Preprocessing in Offline Optical Character Recognition. Bhubaneswar, IEEE.\n\n5.Huang, J. et al., 2021. A Multiplexed Network for End-to-End, Multilingual OCR. s.l., IEEE.\n\n6.K.O, M. A. & Poruran, S., 2020. OCR-Nets: Variants of Pre-trained CNN for Urdu Handwritten Character Recognition via Transfer Learning. Procedia Computer Science, 171(1877-0509), pp. 2294-2301.\n\n7.Khan, A. A. et al., 2023. AI Ethics: An Empirical Study on the Views of Practitioners and Lawmakers. IEEE Transactions on Computational Social Systems, Volume 10, pp. 2971-2984.\n\n8.Kumar, A., Singh, P. & Lata, K., 2023. Comparative Study of Different Optical Character Recognition Models on Handwritten and Printed Medical Reports. 2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA), 20 04, pp. 581-586.\n\n9.Memon, J., Sami, M., Khan, R. A. & Uddin, M., 2020. Handwritten Optical Character Recognition (OCR): A. IEEE Access, Volume 8, pp. 142642-142668.\n\n10.Muneer, S. U. a. N. M. a. K. B. a. Y. M. H., 2020. Evaluating the Effectiveness of Notations for Designing Security Aspects. pp. 465-471.\n\n11.Sayallar, C. ̧. a., Sayar, A. & Babalık, N., 2023. An OCR Engine for Printed Receipt Images using Deep Learning Techniques. International Journal of Advanced Computer Science and Applications, 4(2).\n\n12.Sethu, S. . G., 2020. Legal Protection for Data Security: a Comparative Analysis of the Laws and Regulations of European Union, US, India and UAE. 2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT), pp. 1-5.\n\n13.Yu, . W., Ibrayim, M. & Hamdulla, A., 2023. Scene Text Recognition Based on Improved CRNN. Information, 14(7), p. 369.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\nBackground\nLiterature Review\n\n3.1. Introduction\n\n3.2. Related Works\n\nMethodology\n\n4.1. Architecture\n\n4.1.1. Bidirectional LSTM (Bi-LSTM)\n\n4.1.1.1. LSTM Architecture\n\nView all\nDatasets",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "music-composer-classification-of-midi-files-using-deep-learning-qZ9m02sVKl8C",
    "username": "gGary Takahashi",
    "license": null,
    "title": "Music composer classification of MIDI files using Deep Learning",
    "publication_description": "Back to publications\nSep 15, 2024\n●\n134 reads\n●\nCreative Commons Attribution (CC BY)\nWinner of\nMost Engaging Presentation\nat the\nAI Project Showcase 2024\nMusic composer classification of MIDI files using Deep Learning\nMIDI\nmusic classification\nG\nGary Takahashi\nLisa Vo\nLike\nBookmark\nShare\nAbstract\n\nThe widespread use of electronic musical instruments has driven the need for a digital representation of musical notation. The MIDI (Musical Instrument Digital Interface) system has evolved over the years as the standard notation format for modern electronic instruments, and can store and process instrument patch settings as well as performance details and metadata. We wished to use two deep learning models (LSTM and CNN) to determine whether the music of four composers (Bach, Beethoven, Mozart and Chopin) could be successfully classified using only information contained in MIDI musical files from a Kaggle dataset. Such a task requires careful feature selection, identifying which musical components distinguish a composer's characteristic musical styles. Visualization of the differences based on these features helped to confirm the utility of the chosen parameters. The pretty_midi library was used to extract the selected features, and we piloted our efforts using a Support Vector Machine model, which had been used with success previously, and achieved 90% accuracy using the radial basis function kernel. We trained an LSTM model and a CNN model, and achieved 85% accuracy with our LSTM model and 91% accuracy with the CNN model. The implications of our findings are discussed.\n\nIntroduction\n\nFor centuries, musical information has long been recorded using a standardized notation with musical notes written on lines of staff, and with special instructions written alongside to indicate changes in note velocity, loudness or softness, accents, tempo and other direction. This has long sufficed for human performers to interpret performance on traditional musical instruments. However, with the development of electronic musical instruments, orthodox musical notation was inadequate to capture the number of parameters needed to specify all the nuances of performance that these new instruments were capable of delivering. Synthesizer configuration, for example, was best suited to process data formatted digitally, much as document formats were developed for word processors.\n\nRather than storing digital representations of the audio spectrum, a communications protocol was developed that provided a digital interface, but which controllers could send activating signals to a compatible musical instrument, directing it to play notes at a certain pitch, velocity, duration, and other characteristics. This is the Musical Instrument Digital Interface (MIDI) protocol that was developed in the early 1980s, primarily to store settings from a burgeoning array of complex digital electronic instruments that were no longer using patch cords to modulate and channel sound creation. It was envisioned that these instruments would read and respond to digital information, and that these instructions could be stored in a convenient file format. The information could then be used to instantly configure a musical instrument as designed by the musician. Indeed, a key motivation for the development of MIDI was to enable live performance implementations on digital electronic instruments, such as synthesizers. The protocol has undergone revisions and has adapted to meet the demands of newer digital instruments, which were capable of touch sensitivity, polyphony and greater musical expression, such as sostenuto, after-touch, pitch-bend, and many other features.\n\nMIDI File Formats\n\nMIDI files are structured as “chunks” with the basic chunks being the Header chunk and the Track chunk. There are three file formats:\nFormat 0 has a header chunk followed by one track chunk.\nFormat 1 allows for one or more tracks of a MIDI sequence.\nFormat 2 indicates one or more sequentially independent single-track patterns.\n\nHeader chunk\n\nThe header chunk starts with four bytes, MThd (4D 54 68 64) which denotes the beginning of the file. This is followed by six bytes (00 00 00 06 ff ff), then two bytes denoting the MIDI file format discussed above, then two bytes indicating the number of track chunks in the file, and two bytes denoting the number of ticks per quarter-note.\n\nTrack chunk\n\nThis chunk is much more complex, and is where the actual data is stored. Each track chunk consists of an initial four byte header, MTrk (4D 54 72 6B), followed by MIDI events, sysex events and meta events. Each of these contain a wealth of information.\n\nMIDI events contain detailed instructions on when a note is on, or off, the key pressure, control change, program change (instrument to be played), channel pressure, channel mode messages, and so on. This defines the basic characteristics of the keyboard interaction.\n\nSysex events are MIDI exclusive information, such as song selection, timing information, tuning requests, resets, etc.\n\nMeta events contains meta-information about the music, such as copyright information, lyrics, time signature, instrument name, track name, cue information for staging events, etc.\n\nThe task of classifying each MIDI file to the composer whose musical piece it describes, comes down to being able to extract information for the MIDI events that correspond to features that humans would use to identify a composer. These include musical complexity in the melodic line, harmonies, chord progressions, polyrhythm, variations in tempo, the use of counterpoint, ostinato, pedal point, the use of certain stylistic cadences, the selection of instruments, and changes in time and key signatures, and overall structure (sonata form, fugue, dance, etc).\n\nWe will use selected features from the MIDI files in the Kaggle dataset to classify amongst four composers: Bach, Beethoven, Mozart and Chopin. Although their musical styles are distinctive prima facie to a human listener, there are some stylistic similarities amongst these four that might prove to be a challenge to machine learning algorithms.\n\nBach’s music is classified as Baroque-era music. His music is notable for heavy use of counterpoint, with multiple independent melodic lines playing together. Harmonic progressions contained dissonances in progression, creating tension and anticipation. Melodic lines were often dense, and would carry across various voices. Bach made liberal use of Baroque ornamentation, such as mordents, trills, and appoggiaturas. Variations in tempo were not as pronounced as with other composers. Most of his music was composed for chamber groups, and even the Brandenburg Concertos were intended for a smaller orchestral ensemble. Bach used the Phrygian and Plagal cadences.\n\nBeethoven’s early music was in the classical period but his later period overlapped with the early Romantic era. His music was notable for dramatic contrasts in mood, with serene lyrical moments often followed by passionate and intense passages. He tended to follow the sonata form in many of his compositions. His symphonic compositions were rich and lush and his orchestration demanded a large ensemble. He often composed programmatic music, such as with his symphonies. His piano sonatas often used ostinato and did not utilize counterpoint as did Bach. Use of ornamentation was sparse.\n\nMozart also composed during the classical period, however his music emphasized beauty, with more elegant melodic and singing lines, but his works include compositions of an intensely spiritual nature, such as the Missa Solemnis. He also followed the sonata form, and emphasis was placed on balance and symmetry. Mozart composed for piano, chamber and orchestral ensembles. Mozart frequently used trill cadences, as did other classicists.\n\nChopin composed during the Romantic period, and his compositions were largely for the piano. As such, his music emphasized virtuosity and presented challenges to the performer, such as polyrhythms, large ranges in the melody lines, complexity in harmonic voicing and texture. Ornamentation was largely in the form of technically difficult fiorituras. Dramatic changes in tempo are common in his pieces, and sections are clearly delineated in his pieces where different melodic statements and harmonies contrast with that in other sections. In contrast to the other three composers, performers often use rubato to increase the emotional intensity of the lyrical melodies.\n\nThe challenge in classifying MIDI files to exploit these stylistic differences requires being able to extract the necessary information from the MIDI events section of the Track chunk.\n\nMethodology\nExploratory Data Analysis\n\nThe dataset was obtained from Kaggle (Kaggle, 2019). The dataset consisted of the works of 175 composers in 3929 MIDI files. The musical selections encompassed piano, chamber and orchestral works.\n\nWe first checked for files with missing or duplicated data, and did not identify any such files with these deficiencies. From the dataset, we selected folders of the four composers listed above, and discarded the remainder of the dataset. The number of works by the four composers was unequal, with the most number of compositions being by Bach (1024 files), followed by Mozart (255 files), then Beethoven (212 files), then Chopin (136 files).\n\npretty-midi\n\nTo extract the features from the MIDI files, we used the pretty_midi Python library designed to read and manipulate MIDI files (craffel, 2023a). This library can also be used to read and extract a wealth of features from the MIDI files (craffel, 2023b). These include detection of parameters related to tempo, key signature, instrument selection and changes, note duration, note velocity, pitch-transition and pitch-bend, sustain pedal dynamics, as well as numerous other parameters pertinent to performance-related needs and specialty instruments.\n\nTo access these features, one first creates a PrettyMIDI object, such as follows:\n\nmidi_data = pretty_midi.PrettyMIDI(midi_file)\n\nFrom this, one creates midi_data.instruments, which contains information about note duration and pitch classes. Key signature and changes can be accessed with midi_data.key_signature_changes.\n\nAnother particularly helpful function is piano roll, which contains information on note velocity, pitch variance & transitions, rhythmic density, and chord density.\n\npiano_roll = pretty_midi.get_piano_roll()\n\nWith these functions, we collected information about the following parameters:\n● Note density\n● Pitch variance\n● Velocity statistics (mean, max, variance)\n● Polyphony\n● Rhythmic density\n● Average pitch interval\n● Chord density\n● Pitch transitions\n● Average note duration (for sustained notes)\n\nWe investigated whether these parameters would allow for sufficient discrimination between the different composer styles, such that a deep learning model could be trained to classify based on these features.\n\nOn the first run of the feature extraction function, we identified 18 files which were poorly formed, either lacking the proper headers, list indexes out of range, or where the key signature information was invalid (see Figure 1).\n\nFigure 1: Errors generated on MIDI file processing on the raw Kaggle dataset.\n\nThese errant files were manually deleted, and the analysis was performed on the remaining MIDI files without error.\n\nTrain-test splitting was performed using the Sci-kit Learn train_test_split() function. As one can see in Figure 2, the range of values is large, and therefore Standard Scaling was applied to normalize the data before training.\n\nFigure 2: Scatter plot of values in X_train and X_test.\n\nAs seen above, the number of MIDI files associated with each composer varied greatly, and this can be seen especially in the training set, where there was an almost 8-fold difference between Bach and Chopin. An imbalanced dataset will introduce bias into the model, as it learns to preferentially classify MIDI samples as Bach, to statistically minimize classification loss. To deal with this, we used SMOTE to equalize the training data (Figure 3).\n\nFigure 3: The effect of SMOTE on addressing data imbalance in the training set.\n\n#Visualizations\n\nPrior to deep learning classification, visual analysis of the differences in features amongst the composers can reveal differences amongst the four composers.\n\nLooking at tempo distribution, one can see that the tempo distributions for three of the four composers tended to have tempos of around 100 to 150 bpm (moderato to allegro). Bach notably composed many pieces to be played much slower (adagio, andante) or much faster (vivace and presto) than the norm (Figure 4).\n\nFigure 4. Tempo distribution by composer\n\nThis is also seen in the plot of the average velocity distribution of notes in the various compositions (Figure 5). The violin plots show that the works of Chopin and Beethoven were similar in tempo and note velocity. Although the tempo of Bach’s works were not necessarily rapid, he did incorporate more high-velocity notes within the compositions.\n\nFigure 5. Average note velocity distribution.\n\nAnother way to distinguish among the composers is to examine the number of instruments vs average velocity (Figure 6).\n\nFigure 6: Number of instruments in each composition plotted against average note velocity.\n\nFor composers such as Chopin, who composed primarily for the piano, most of his data points are along the left of the graph. Mozart, Bach and Beethoven composed works for the piano as well as chamber and orchestral ensembles.\n\nA first pass at delving into musical harmonics and chromaticism in the composers’ musical style is to examine the pitch classes utilized. In Figure 7, we plotted the pitch class usage for each composer, normalized to the number of compositions. Pitch class reflects the key signatures used in various musical compositions, but also takes into account accidentals, so by itself, it doesn’t necessarily reflect how “adventurous” a composer was in straying from the diatonic path.\n\nFigure 7: Normalized pitch class usage by composer.\n\nAnother way of visualizing the variation in pitch class usage by each composer is a radar plot, as in Figure 8. In this plot, it is clear that the music of a composer like Chopin used a more even distribution of pitch values, either through key signature selection or a more liberal use of accidentals. Other composers tended to favor certain pitch classes over others.\n\nFigure 8: Radar plot of normalized pitch class usage by composer\n\nFinally, we delve into chromaticism in Figure 9, looking at pitch_classes that were only at non-diatonic intervals related to the key signature. This takes into account accidentals, but also includes harmonic expressions and progressions that extend beyond tonic and dominant chords. It is not surprising that Mozart’s music favored major chords, and that both Bach and Chopin were more adventurous in the pitch_classes they used. Beethoven was intermediate, and incorporated more non-diatonic notes on the chromatic scale.\n\nFigure 9: Average proportion of non-diatonic notes by composer and key.\n\nNow that we have seen how pretty_midi can extract features which can distinguish between the various composers, we now turn our attention to showing how this utility can be used to train deep learning models to classify the MIDI files into their respective composers.\n\nRelative Feature Importance\n\nWe have seen that the selected features are indeed able to evince differences amongst the musical styles of the four composers. Before embarking on the classification process, we sought to explore which of these features would be most contributory in the classification.\n\nOne way to determine this would be to use Random Forest modeling, which provides a means of ranking feature importance by way of determining the relative contribution of feature to Gini impurity index. The advantage of this method is that it is relatively easy to calculate, but it is a method that is dependent on the Random Forest classification model. We performed this analysis and found that the most important features were num_instruments, velocity variance and polyphony (Figure 10).\n\nFigure 10: Gini importance values in a Random Forest model, to determine the relative feature importance in the classification of composers.\n\nA more detailed analysis of the relative contribution of feature to composer classification is provided by SHAP (SHapley Additive exPlanations). In Figure 11, we see the relative importance of each feature toward the classification decision in each composer’s MIDI dataset.\n\nFigure 11: Mean absolute SHAP values for each composer.\n\nSeparate beeswarm plots further detail the relative importance of each feature in composer classification (Figure 12). It is clear that num_instruments, velocity_variance and polyphony are among the topmost important features used in classification.\n\nFigure 12: Ranked importance of the extracted features for the classification of each composer. Velocity variance was most important for the classification of Bach and Beethoven, however the number of instruments in the composition was of greater importance in the classification of the music of Chopin and Mozart.\n\nSupport Vector Machines\n\nPrevious work on classification of MIDI samples to different musical styles, found high accuracy using Support Vector Machines, especially with the radial basis function (Gnegy, 2014). As such, we were indeed able to achieve high classification accuracy (0.90) (Figure 13).\n\nFigure 13: Classification report using SVM (rbf).\n\nThis final project required classification using two deep learning algorithms: LSTM and CNN. We therefore trained the extracted MIDI information on these two models.\n\nResults\nLSTM\nData Preparation\n\nLSTM models require a three-dimensional input shape, which consists of (batch size, time steps, the length of one input sequence). To meet this requirement, the augmented train and test sets were reshaped in preparation for training.\n\nNext, because the target variables are categorical strings, we encoded the y_train and y_test set to integer values using scikit-learn's LabelEncoder. The encoded outputs are converted to a binary matrix representing the composer names. The matrix contains only 1s and 0s, with 1 indicating that the music file belongs to the composer at that index, with 0 indicating that the music file does not belong to the rest. We arbitrarily selected a sequence length of five to create sequences of the training data and test data. It is with these sequences that the model will train upon.\n\nModel Training\n\nThe LSTM model consists of the following layers (in no particular order):\n\nInput layer with shape of (5, 26)\nLSTM layer with 15 units and recurrent dropout rate of 0.4\nLSTM layer with 5 units and recurrent dropout rate of 0.4\n4 Batch Normalization layers\n2 Dropout layers at a 0.4 and 0.5 dropout rate, respectively\nFully-connected Dense layer with softmax activation\n\nBecause we are performing multiclass classification, we chose to use the Adam optimizer with an initial learning rate of 0.01 and categorical cross entropy loss function. To reduce overfitting and facilitate convergence, we implemented early stopping and a learning rate scheduler, both monitoring validation accuracy and loss. Although we set our LSTM model to train for 1000 epochs, early-stopping terminated training at 224 epochs, for training and validation accuracies of 0.87 and 0.85, respectively. The training and validation loss were 0.30 and 0.48, respectively.\n\nFigure 14: LSTM training and validation accuracy (left). LSTM training and validation loss (right).\n\nThe training and validation loss graph (Figure 14) shows that our efforts to reduce overfitting has benefited the model, as seen in the convergence of the training and validation loss curves, with the validation loss curve not straddling above the training loss curve.\n\nPrediction and Evaluation\n\nAfter making predictions on the X_test set, we generated a classification report.\n\nFigure 15: LSTM classification report\n\nThe LSTM model had an accuracy of 0.85 (Figure 15). This accuracy level is evident in the confusion matrix (Figure 16), as we see that most of the predictions lie in the diagonal row of the matrix. This means that most of the predictions were correct. The scarce numbers in the cells surrounding the diagonal row signify instances of incorrect predictions with the y-axis indicating the true label and the x-axis indicating the predicted label. Also from the diagonal row, we see that most of the samples in the test set are from Bach because SMOTE synthetic oversampling is not applied to the test set.\n\nFigure 16: Confusion matrix for the LSTM classification model.\n\nCNN\nData Preparation\n\nLikewise with LSTM, the CNN model requires a three-dimensional input shape. We followed the same steps with reshaping the data, but we excluded the sequencing step, as that step is required in LSTMs due to its recurrent nature.\n\nModel Training\n\nThe CNN model consists of the following layers (in no particular order):\n\nInput layer\n2 Convolutional 1D layers with 64 units and 3x3 kernel size\n2 Convolutional 1D layers with 128 units and 3x3 kernel size\n2 Convolutional 1D layers with 256 units and 3x3 kernel size\n2 Max Pooling 1D layers with pool size of 2\n1 Global Average Pooling 1D layer\n9 Batch Normalization layers\n5 Dropout layers with 0.2, 0.3, 0.4, 0.5, 0.5, respectively\n2 Dense layers with 128 and 64 units, respectively, and ReLU activation functions\nFully-connected Dense layer with softmax activation\n\nThese layers were developed empirically, balancing the need to achieve convergence and training accuracy, and the need to avoid volatility in validation accuracy and avoiding overfitting. Because we are performing multiclass classification, we chose to use the Adam optimizer with an initial learning rate of 0.001 and categorical cross entropy loss function. To reduce overfitting, we implemented early stopping and a learning rate scheduler, both monitoring the validation loss. Again, we set the number of epochs for our CNN model to 100, however early stopping terminated training at 58 epochs, yielding training and validation accuracies of 0.94 and 0.90, respectively. The training and validation losses were 0.18 and 0.33, respectively.\n\nFigure 17: CNN training and validation accuracy (left). CNN training and validation loss (right).\n\nThe training and validation loss graph (Figure 17) shows that our efforts to reduce overfitting has benefited the model, as seen in the convergence of the training and validation loss curves, with the validation loss curve slightly straddling above the training loss curve.\n\nPredictions and Evaluation\n\nAfter making predictions on the X_test set, we generated a classification report.\n\nFigure 18: CNN classification report\n\nThe CNN model had an accuracy of 0.91 (Figure 18). Likewise with the LSTM model, the confusion matrix (Figure 19) consists of the majority of predictions sitting in the diagonal row, indicating a high majority presence of correct predictions. From a visual standpoint, there are few incorrect predictions. Again, there is a high amount of Bach instances, since synthetic oversampling is only intended to be applied to the training datasets.\n\nFigure 19: Confusion matrix for the CNN classification model.\n\nConclusions\n\nWe have shown that with careful selection of musical features, we were able to extract key information stored in MIDI files of composers’ musical oeuvres, which allowed deep learning networks to be trained on these characteristic features, such that they could successfully classify the composer of the music accurately on a separate validation set. Our models achieved an accuracy of at least 0.85 overall. The main limitation of the dataset was the relative imbalance of representative files amongst the various composers, and we endeavored to correct for this using SMOTE data augmentation to guide the models away from overtraining on composers, such as Bach, who had the greatest representation in the dataset. Despite this, the individual f1-scores reflected each composer’s dataset size. It is notable, however, that despite having the least number of compositions in the dataset, both the LSTM and CNN models classified better on Chopin compositions than they did on Beethoven or Mozart.\n\nSome factors that impacted our models’ performances were the size of the dataset and the class imbalance. Neural networks perform better with larger datasets, as they are able to capture more feature details due to the large number of trainable parameters. The class imbalance in our music dataset introduced a bias favoring Bach, which necessitated data augmentation to mitigate this bias. SMOTE was used to even out the class imbalance. One downside to this is that the minority classes have more instances of synthetic data than that of Bach. Ideally, we would want the dataset to comprise real data as much as possible. However, data augmentation was necessary in this situation to help the neural networks to better generalize on unseen data.\n\nWe attempted to intuit how each deep learning network approached the task of classification by examining some of the characteristics of the features obtained by pretty_midi for each composer. In our EDA, we found that different MIDI features were more salient amongst the various composers, and these differences could potentially explain how the models were trained. For example, with Bach’s music, we learned that the strongest predictors were the number of instruments, velocity variance, and polyphony. For the remaining composers, recognizing the most significant predictors was not as deterministic. We are not certain that the networks operated using this kind of analytic breakdown, however it is conceivable to imagine them trained in this fashion. The EDA did give us the confidence that we had selected an adequate number of suitable features for training, however.\n\nIt is intriguing that using the feature set we had selected, pretty_midi performed less accurately distinguishing between Chopin and Beethoven, which can be seen in the analysis of the tempos among the composers. Although their music is considered to be from the Romantic era, a human listener would have no problem distinguishing differences in their musical styles, suggesting that there are musical characteristics registered by the human brain that cannot be captured by dry assessments of features such as tempo or note velocity. The modal selection of a piece, selection of chord voicings, the evocation of nationalistic or cultural melodic motifs, and association with tunes with which we are already familiar, all affect our ability to recognize and classify a particular piece. These features are part of the humanistic aspects of music which MIDI was not designed to capture, nor would it likely be able to.\n\nBesides using more balanced data, we believe there is still room for improvement in the implementation of the neural networks. Many hyperparameters could potentially benefit from fine-tuning, such as learning rate, optimizer type, batch size, and sequence length in the case of LSTMs.\n\nThe ability for deep learning networks to capture essential musical features has important implications. Our results suggest many possibilities in the application of MIDI analysis and classification tools. With further training, larger dataset, and fine-tuning, our models can be extended to identify music beyond the classical genre and extend to other topics, such as emotion, time period, instruments used, and more. In addition to this, our models can be applied to enhanced compositional assistance in generative AI; voice-to-MIDI controllers; enhanced recommender systems; automated mixing and mastering of music; and the design of new instruments and effects for synthesizers in music production.\n\nTo summarize, the model results have demonstrated the effectiveness of using deep learning to perform MIDI analysis and classification, which can serve both producers and listeners of music.\n\nReferences\n\nCraffel, C. (2023a). Pretty_midi. \nhttps://github.com/craffel/pretty-midi\n\nCraffel, C. (2023b). Pretty_midi documentation. \nhttps://craffel.github.io/pretty-midi/\n\nGnegy, C. (2014). Classification of musical playing styles using MIDI information. Stanford University. \nhttps://cs229.stanford.edu/proj2014/Chet%20Gnegy,Classification%20Of%20Musical%20Playing%20Styles.pdf\n\nKaggle. (2019). Midi_classical_music. \nhttps://www.kaggle.com/datasets/blanderbuss/midi-classic-music\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nMIDI File Formats\n\nHeader chunk\n\nTrack chunk\n\nMethodology\n\nExploratory Data Analysis\n\npretty-midi\n\nRelative Feature Importance\n\nSupport Vector Machines\n\nView all\nComments\n(1)\nMost recent\n\n✨ Want to join the conversation?\n\nSign in or create an account to comment.\nMarcin Sikorski\n5 months ago\n\nGreat work, I like it! Would you mind sharing the code/GitHub link? ☺️\n\nLike (1)\nReply\nCode\nDatasets\nYou might be interested\nAmbience-to-Music Neural Style Transfer (AM-NST)\nDec 30, 202418 reads\ndeep learning for audioneural style transfer\nMusic AI\nE\nApr 05, 202541 reads\nGrecia Music Game\nFeb 13, 202549 reads\nCarnatic Music Assistant\nV\nSep 02, 20254 reads",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "n8n-building-no-code-agentic-workflows-gfrpsUkufCGL",
    "username": "aAmina Javaid",
    "license": null,
    "title": "n8n: Building No-Code Agentic Workflows ",
    "publication_description": "Back to publications\nOct 09, 2025\n●\n52 reads\n●\nCreative Commons Attribution-NonCommercial (CC BY-NC)\nn8n: Building No-Code Agentic Workflows\n#LLMs\n#Tools\nAgenticAI\nAgenticWorkflows\nLowCode\nn8n\nNoCode\nA\nAmina Javaid\nReady Tensor\nMohamed Abdelhamid\nLike\nBookmark\nShare\n\nThe Challenge of Building Agentic AI\n\nAgentic AI is one of the most exciting shifts in technology today. Instead of just responding to prompts, AI agents can plan, reason, and act — booking appointments, processing data, running workflows, even coordinating with other agents. Companies are rushing to adopt this approach because it boosts productivity, lowers costs, and frees people for more creative work.\n\nBut here’s the challenge: building these systems is hard. It requires strong coding skills. Developers often need to master programming fundamentals, APIs, and integrations to stitch together an agentic system. While coding provides maximum flexibility, it also demands time, effort, and expertise, creating a barrier for entrepreneurs, domain experts, and innovators who don’t come from a technical background.\n\nNow imagine the alternative:\n➡️ What if you could build a working prototype in hours or days, without writing a single line of code?\n\nThat’s the gap no‑code platforms are trying to close. They allow you to skip that heavy lift - making the process of building agentic AI fast, accessible, and experiment‑friendly.\n\nMeet n8n – The No-Code/Low-Code Agentic AI Platform\n\nThere are many no-code tools out there, but n8n stands apart — especially for agentic AI. It’s the rare open-source workflow automation platform that lets you build powerful systems using a visual interface, while still allowing you to write code when needed.\n\nDespite its flexibility and enterprise-grade features, n8n is surprisingly beginner-friendly. You get the ease of drag-and-drop logic, plus the freedom to self-host and customize deeply — a rare combination.\n\nThe name n8n (pronounced n-eight-n) comes from “nodemation”:\n\nnode- for its modular, visual interface built on Node.js\n-mation for automation — its core purpose\n\nIt’s become a go-to platform for teams of all kinds:\n\nNon-technical users can experiment and build without writing code\nProduct managers can test and demo ideas fast\nAI teams can launch real agentic systems without heavy infrastructure\n\nIn short: n8n is the bridge between the promise of agentic AI and the reality of building it.\n\nBasics of n8n\n\nn8n is a visual workflow builder. You just drag nodes onto a canvas, connect them, and hit run. It’s like building a logic diagram that actually executes.\n\nEach node handles a task: receiving input, calling an API, generating text with an LLM, sending an email, storing data. Need more control? Add a code node and write custom JavaScript or Python.\n\nYou can often go from idea to prototype in minutes or hours, not days.\n\nLet’s take a quick look at the core building blocks.\n\nThe Core Building Blocks\n\nThink of n8n as a construction kit for automation. Every workflow you create is built from just a few simple parts working together:\n\nNodes – the steps where work happens: calling an API, sending an email, running an AI model. They are the fundamental building blocks in n8n.\nEdges – the connectors that pass data from one node to the next.\nTriggers – special nodes that start a process (like a button click, a schedule, a chat message, or a webhook).\nWorkflows – the overall blueprint of your automation. It represents a sequence of nodes and edges that define how data flows and tasks are executed.\n\nIn practice, you drag nodes onto a canvas, link them with edges, and data flows through the chain step by step. Nodes can be built-in (shipped with n8n) or installed from the community. Some nodes are for actions, some for logic or data transformation, and some for integrating AI models — but they all behave the same way on the canvas.\n\nFor advanced needs, you can insert a Code node and write JavaScript or Python without leaving the visual workflow. This gives you the flexibility of a programming environment inside a drag‑and‑drop interface.\n\nThat’s all you need to know to start: triggers begin the flow, nodes do the work, edges pass the data, and workflows tie it all together.\n\nNode Types (What Kinds of Things Can You Do?)\n\nn8n includes a wide variety of nodes to handle everything from APIs to AI. Here are the main categories you'll work with:\n\n🟢 Trigger Nodes\n\nThese nodes start a workflow — whether it's a manual button click, a scheduled time, a webhook call, a received email, a form submission, or a chat message. Every workflow begins with at least one trigger.\n\n🟡 Action Nodes\n\nAction nodes do something — send an email, post a message to Discord, create a record in Notion, or make an HTTP request to any API. They’re the building blocks for connecting with external tools and services.\n\n🧰 Utility Nodes\n\nThese nodes help you manipulate data or control flow within the workflow. You’ll use them for things like filtering content, formatting dates, merging branches, setting conditions, or adding delays.\n\n🧠 AI Nodes\n\nAI nodes let you bring language models into your workflows — for tasks like summarizing, classifying, generating responses, or orchestrating agent behavior. You can plug into models like OpenAI, Anthropic, or Google Gemini, or use n8n's built-in AI Agent node to plan and act.\n\n💻 Code Node\n\nUse this when you need custom logic or processing that isn’t covered by built-in nodes. Write JavaScript or Python directly in the workflow — great for quick transforms, fallback logic, or calling a less-common API.\n\n👤 Human-in-the-Loop Nodes\n\nSometimes, automation needs a human decision — like approving a task or reviewing a message. These nodes pause the workflow and wait for input from a person before continuing.\n\nn8n also includes other specialized node types — for file handling, database access, image editing, sub-workflows, testing, compression, and more. You can even create or install custom nodes from the community.\n\nExplore the full list of nodes in the \nofficial n8n documentation\n.\n\nSetting Up n8n\n\nNow that you’ve seen how workflows are structured and what kinds of nodes you can use, it’s time to get n8n up and running.\n\nThe good news? You have options. Whether you're just exploring or gearing up for production, n8n gives you flexible ways to get started quickly.\n\nOption 1: Use n8n Cloud (Recommended for New Users)\n\nThe fastest way to get started is with \nn8n Cloud\n, the hosted version managed by the n8n team. No setup, no servers — just sign up and start building.\n\nHosted and maintained for you\nSecure, with built-in authentication and encryption\n14-day free trial to explore the platform\nBest option if you want to focus on building, not infrastructure\n\n👉 Head to \nn8n.io\n and create a free account to get started in minutes.\n\nOption 2: Self-Host (For Customization or Control)\n\nIf you want more control over how and where n8n runs, you can self-host it on your machine or server.\n\nOptions include:\n\nDocker – clean, fast, and portable (recommended self-host method)\nnpm – install via Node.js for a quick local setup\nVPS or on-prem – deploy to your own cloud or private infrastructure\n\nSelf-hosting is ideal if you need data privacy, custom integrations, or want to run n8n behind your own authentication systems.\n\n👉 For instructions, see the \nself-hosting guide in the docs\n.\n\nA Note on Credentials\n\nMost workflows require connecting to external tools — Gmail, Notion, OpenAI, etc. When you add a node that talks to one of these services, n8n will prompt you to set up credentials (like an API key or OAuth token). These are securely stored and can be reused across workflows.\n\nOnce you're set up, you're ready to build your first workflow — and we’ll do that next.\n\nYour First Agentic Workflows\n\nNow let’s build something real with n8n. We’ll walk through two simple but powerful workflows that use language models to reason, respond, and adapt. You’ll go from “hello world” to a logic-driven assistant in minutes.\n\nWorkflow 1: Hello AI\n\nOur warm-up is the simplest possible agentic workflow: you send a message, an AI responds. This is the core pattern of agentic behavior:\ntrigger → reasoning → output.\n\nSteps:\nCreate a workflow → Open your n8n editor and click Create Workflow. This opens an editor canvas for you to work on. Enter the name of your workflow and click Save.\nAdd a Chat Trigger node → This node listens for user input (like a chatbot). You can add a new node by clicking the + sign at the top right of the editor canvas.\nAdd an AI Agent node → Drag in the AI Agent node.\nConfigure OpenAI ChatModel → Inside the AI Agent, pick OpenAI ChatModel and link your credentials.\nConnect nodes & test → Link Chat Trigger → AI Agent, hit Execute Workflow, and say hello. You should get a smart response back.\n\nLet's look at each node of this workflow:\nThis is the chat trigger node.\n\n\nThis takes user message as an input and passes on that message to the next node as output.\n\nThis is the AI Agent node which is the main agentic component of our workflow.\n\n\nBelow is the the AI agent node with OpenAI chat model attached to it.\n\n\nHere's a complete walkthrough of how to build your first agentic workflow in n8n:\n\n\nIt is the helpful assistant in the form of LLM. This node takes the message from the chat trigger and gives it to an LLM to get a response. The response is then returned through this AI Agent node as an output. This node can connect to:\n\nA chat model (Any LLM of your choice)\nTools (APIs, external resources)\nMemory (to remember context)\n\n👉 That’s your first agentic workflow! It’s minimal, but it introduces the three key ideas: trigger → AI processing → response.\n\nWorkflow 2: Sentiment-Aware Feedback\n\nNow let’s build something a bit more practical: a mini agent that reads customer reviews, evaluates sentiment, and reacts accordingly.\n\nScenario:\n\nA user submits feedback on your product. If it’s positive, they get a “thank you” email. If negative, you ask for improvement suggestions. This shows how AI and logic work together.\n\nSteps:\nCreate a workflow → Start fresh.\nAdd a Form Trigger node → Simulate a user submitting feedback. Add two form fields:\nemail (their email address)\nreview (their product review)\nAdd an AI Sentiment Analysis node → Connect the review text here.\nInsert an IF node → Check if sentiment is positive.\nIf true → Send an Email node with a warm thank-you.\nIf false → Send an Email node asking for improvement ideas.\nConfigure OpenAI credentials for the AI node.\nTest the workflow by submitting sample feedback.\n\nHere is the complete workflow:\n\n\nYou’ve just built a basic agentic workflow—the AI interprets user text, and the workflow branches logically, just like a human assistant would.\n\nKey Concepts Introduced\nTrigger nodes kick things off (chat, form, webhook).\nCredentials unlock external services (like OpenAI).\nLogic nodes (IF, Switch, etc.) let AI outputs guide decisions.\nExpressions can make workflows dynamic—e.g., inserting the user’s name into an email with {{ $json.email }}.\nWhat You Just Learned\n\nThese two workflows teach you the most essential patterns in agentic AI:\n\nInput → AI reasoning → Output\nUsing conditions to control flow\nConnecting multiple tools through logic\n\nIn a few steps, you’ve created systems that act based on user input, make decisions with AI, and send personalized responses — no backend code required.\n\nUp next, let's look at n8n templates to help you go further, faster.\n\nFasttrack with Templates\n\nOnce you're comfortable building simple workflows, you don’t always have to start from scratch. n8n has a growing library of community-contributed templates — prebuilt workflows for common use cases like sending automated emails, summarizing documents, querying APIs, or even building full RAG agents.\n\nTemplates are great for:\n\nLearning by example — see how more advanced workflows are structured\nSaving time — clone, customize, and go\nExploring ideas — find inspiration for your next project\n\nHere are two examples of useful templates to get you started:\n\n1. Onboarding New Employees\n\nIT Ops of an organization can automate the process of onboarding new employees using this n8n workflow.\n\n\n2. Generating Customer Insights\n\nSales department of an organization can generate customer insights from reviews using the n8n workflow shown below.\n\n\nYou can browse the full gallery of templates at \nn8n.io/workflows\n.\n\nJust remember: start simple, then use templates to go further.\n\nA More Complex Example: Publication Assessment\n\nNow that you’ve seen the basic workflows and templates, let’s look at what a more advanced, real-world workflow feels like.\n\nReady Tensor Use Case: Publication Assessment\n\nOn the Ready Tensor platform, we use an AI-powered publication assessment tool to evaluate articles — whether they’re tutorials, solution guides, research writeups, or technical blogs. The tool analyzes each submission based on its type, scores it across key criteria, and suggests improvements.\n\nTo automate this process, we built a workflow in n8n.\n\nHow It Works\n\nThe workflow sends a publication through multiple specialized AI agents — one for clarity, one for completeness, another for relevance, and one for overall quality. Each agent returns a structured JSON score with an explanation.\n\nA Code node then merges those results into a final assessment — highlighting strengths and areas for improvement.\n\n🎥 Watch the video below to see this system in action:\n\nHere are the workflows described in the video at a glance.\n\nMain Workflow\n\nPublication Assessment Sub-Workflow\n\nThe beauty of this workflow is that it makes the review process:\n\nFaster (parallelized agents do the heavy lifting)\nConsistent (scoring follows the same rules every time)\nScalable (easy to integrate into platforms like Ready Tensor)\n\nIt’s a great example of what agentic automation looks like in production: multiple reasoning steps, intelligent decision-making, and zero backend code. And n8n handles it all with drag-and-drop workflows.\n\nBeyond the Basics: What to Explore Next\n\nOnce you’ve built your first few workflows, you’ll start to see how powerful n8n really is. It’s not just for simple automations — it can support complex, modular, production-grade systems.\n\nHere are a few capabilities to explore as you level up:\n\n🔀 Orchestrate Multi-Step Flows\n\nDesign workflows that branch, merge, or run in parallel. Instead of one long flow, you can split responsibilities: one path summarizes a query, another logs it, another retrieves data.\n\n🔗 Call Workflows from Other Workflows\n\nUse sub-workflows like functions — for shared tasks like authentication, logging, or AI calls. This keeps things clean and reusable.\n\n🌐 Connect to Live APIs\n\nPull data from external sources (weather, CRMs, financial feeds), process it with AI, and send results to Slack, Notion, Airtable, or anywhere else.\n\n🔁 Add Loops and Error Handling\n\nBuild workflows that retry, wait, or branch on failure. Use these tools to build resilient automations that handle real-world edge cases.\n\n🧪 Debug and Iterate with Confidence\n\nn8n includes execution history, test inputs, and inline documentation tools to help you understand and improve your workflows as they grow.\n\nBest Practices for Designing Workflows With n8n\n\nAs your automations grow, it’s easy for things to get messy. The key is to design with modularity in mind from the start.\n\nKeep workflows clear and readable — every step should have a distinct purpose.\nBreak up large workflows into smaller, reusable pieces to reduce complexity.\nUse sub-workflows to isolate logic like \"evaluate sentiment\" or \"process input.\"\nDocument as you go — naming, notes, and clear labels save time later.\n\nThink in Blocks, Not Blobs\n\nOne powerful pattern is to build with vertical AI agents — small, focused components like “summarize,” “classify,” or “extract.” You can combine these into larger flows using:\n\nParallelization – run agents side by side\nPipelining – chain them step by step\nBranching – route input to the right agent based on context\n\nIt’s like building with Lego: use the same blocks in new arrangements. The more modular your system, the easier it is to debug, scale, and reuse.\n\nA modular mindset keeps your automations maintainable today and scalable tomorrow. Instead of brittle experiments, you’ll be designing workflows that last.\n\nPutting n8n in Context\n\nn8n hits a sweet spot: it’s fast enough for prototypes, flexible enough for real systems, and accessible to both developers and non-developers. But like any tool, it has limits — and knowing when it’s the right fit (and when it’s not) will help you make better decisions as your projects grow.\n\nWhere n8n Excels\n\nIf you're looking to build quickly, automate across services, or experiment with agentic workflows, n8n is a great place to start. You can spin up logic in minutes, test ideas without scaffolding infrastructure, and combine AI, APIs, and logic visually — all while having the option to drop into code when needed.\n\nIt's especially strong when:\n\nYou want fast, flexible prototyping\nYour team includes non-technical users\nYou need built-in integrations or easy self-hosting\nYou value control over where and how your data flows\nWhere You Might Outgrow It\n\nAs workflows grow in complexity — especially with long-lived agents, dynamic memory, or advanced orchestration — you may hit n8n’s ceiling. If you're building a production system that needs:\n\nDeep agent state and long-term memory\nFine-grained performance control or concurrency\nCustom tools and model integration\nCI/CD pipelines, versioning, or observability\nthen a framework like LangChain, LangGraph, or a custom SDK may be a better fit.\nMaking the Call\n\nIf your project is early-stage, exploratory, or operational (not productized), n8n will likely serve you well. As needs evolve, teams often start to migrate parts of their system — for example, moving agent logic to LangGraph while keeping orchestration in n8n.\n\nStart where the friction is lowest. Build fast, learn fast — and when complexity demands it, scale into the right tools for the job.\n\nLooking Ahead\n\nn8n is more than just a no-code automation tool — it’s a launchpad for building real, agentic systems. You’ve seen how quickly you can go from idea to execution, and how modular design unlocks scale.\n\nAs you continue, you’ll find ways to:\n\nCombine AI and APIs in more advanced workflows\nShare logic across teams using sub-workflows and templates\nIntegrate n8n into real products or internal tools\nGradually extend or migrate workflows as your system matures\n\nStart small, iterate quickly, and keep your workflows modular. With the right mindset, you’ll go from quick experiments to production-grade intelligence — all without reinventing the wheel.\n\nReferences & Resources\nn8n - Workflow Automation\nGitHub Repository\nn8n Documentation\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nThe Challenge of Building Agentic AI\n\nMeet n8n – The No-Code/Low-Code Agentic AI Platform\n\nBasics of n8n\n\nThe Core Building Blocks\n\nNode Types (What Kinds of Things Can You Do?)\n\nSetting Up n8n\n\nYour First Agentic Workflows\n\nWorkflow 1: Hello AI\n\nWorkflow 2: Sentiment-Aware Feedback\n\nFasttrack with Templates\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nYou might be interested\nAgentic AI: Tools of the Trade (AAIDC-Week1-Lesson-4)\nMay 15, 20251387 reads\nAgentic AIAI+1\nAgentic AI Applications\nDistinguished Technical Deep-Dive\nY\nMar 24, 202583 reads\nAgentic AIArtificial intelligence+3\nMake it real - experimental multi-agent CLI to flesh out an idea\nM\nF\nW\nJul 15, 202514 reads\nagentlangchain+2\nCustomer support Agentic Ai\nOct 22, 2025\nai-assistantsbusiness agentic ai+6",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "nebula-ai-a-modular-multi-agent-architecture-for-autonomous-space-mission-control-E6JBbU9hIf0H",
    "username": "Yadidya Medepalli",
    "license": "No License",
    "title": "Nebula AI: A Modular Multi-Agent Architecture for Autonomous Space Mission Control",
    "publication_description": "Back to publications\nMar 28, 2025\n●\n46 reads\n●\nNo License\nWinner of\nBest Creative AI Project\nat the\nAgentic AI Innovation Challenge 2025\nNebula AI: A Modular Multi-Agent Architecture for Autonomous Space Mission Control\nMulti-agent collaborative system\nMulti-agent communication and co\nNetworking and Maintenance\nSpace\nYadidya Medepalli\nMeghana Nagaraja\nM\nMonica Jayakumar\nLike\nBookmark\nShare\nAbstract\n\nThe complexity of modern space missions demands real-time autonomy, reliability, and decision support to ensure mission success. Nebula AI introduces a modular multi-agent architecture that processes telemetry, detects anomalies, evaluates risks, schedules tasks, and provides mission recommendations via a large language model (LLM)-assisted interface. The system comprises four core agents: Anomaly Detection Agent (ADA), Anomaly Impact Assessor (AIA), Task Scheduling Agent (TSA), and Mission Planning and Recommendation Agent (MPRA) that coordinate through a publish-subscribe communication framework. Evaluation on simulated mission datasets demonstrates high anomaly detection accuracy and improved scheduling efficiency. This paper details Nebula AI's architecture, agent logic, evaluation results, and future directions toward autonomous, explainable space systems.\n\nIntroduction: A New Dawn in the Cosmos\n\nThe vast expanse of space has always beckoned humanity with its mysteries, a siren call to explore the unknown. Yet, as we stretch our reach beyond Earth, the reality of space exploration reveals a tapestry woven with complexity, high-dimensional telemetry data streaming in real-time, dynamic operational constraints shifting like cosmic tides, and decision-making windows as fleeting as a meteor’s streak. Traditional mission control, though valiant, leans heavily on human operators, a reliance that introduces latency, risks information overload, and taxes even the sharpest minds with cognitive fatigue. As our ambitions soar to distant planets and uncharted stars, we stand at a crossroads: we must forge a new path, one illuminated by intelligent, autonomous systems that amplify our potential and secure our cosmic destiny.\n\nEnter Nebula AI, a beacon of innovation in this uncharted frontier. Imagine a system so ingenious it anticipates crises before they unfold, so intuitive it speaks to us in our own language, and so resilient it thrives amidst the chaos of space. Nebula AI is not just a tool, it’s a companion, a modular multi-agent marvel designed to revolutionize space mission management. With its ability to process telemetry in real-time, detect anomalies with uncanny precision, assess risks with surgical clarity, schedule tasks with adaptive brilliance, and deliver actionable insights through a large language model (LLM)-assisted interface, Nebula AI is our guide into the final frontier. At its heart lie four specialized agents: the Anomaly Detection Agent (ADA), the Anomaly Impact Assessor (AIA), the Task Scheduling Agent (TSA), and the Mission Planning and Recommendation Agent (MPRA). Together, they dance in a symphony of collaboration, orchestrated through a publish-subscribe framework that ensures seamless harmony.\n\nIn this paper, we invite you on a journey through Nebula AI’s architecture, a tale of engineering artistry and artificial intelligence woven together. We’ll explore the minds of its autonomous agents, witness their coordination in action, marvel at their performance in simulated missions, and dream of the future they herald. Our story begins with a system born to conquer the challenges of space and ends with a vision of humanity’s next great leap.\n\nSection 2: Related Work\n\nThe increasing complexity of modern space missions has necessitated the development of autonomous systems capable of real-time decision-making and anomaly management. Traditional mission control systems, while effective, often rely on human-in-the-loop processes that can introduce latency and cognitive overload (Picard et al., 2023). Nebula AI addresses this challenge through a modular multi-agent architecture designed to enhance autonomy, reliability, and decision support. This literature review explores the foundational research in anomaly detection, task scheduling, multi-agent systems, and the emerging use of large language models (LLMs) in space applications, all of which underpin Nebula AI's innovative approach.\n\nAnomaly detection is critical for ensuring the success of space missions, as it enables the early identification of deviations that could compromise mission objectives. Traditional methods, such as statistical thresholding, have been widely used but often struggle with the high-dimensional and non-linear nature of spacecraft telemetry data (Weng et al., 2021). Recent advancements in machine learning (ML) have addressed these limitations by offering more adaptive and robust detection capabilities. For instance, Zeng et al. (2024) proposed an explainable ML-based anomaly detection method that achieved 95.3% precision and 100% recall on NASA's spacecraft datasets, emphasizing the importance of interpretability for operational trust. This aligns with Nebula AI's Anomaly Detection Agent (ADA), which combines statistical thresholds with ML models to detect anomalies effectively while maintaining high recall and low false-positive rates in simulated environments. Furthermore, Weng et al. (2021) conducted a comprehensive survey on anomaly detection in space information networks, highlighting the unique challenges posed by space cyber threats and the need for scalable, real-time detection strategies. Their work underscores the significance of Nebula AI's adaptive detection mechanisms, which are designed to operate efficiently in resource-constrained space environments.\n\nEfficient task scheduling is essential for managing limited resources and ensuring that mission objectives are met under dynamic constraints such as bandwidth, power, and task priority. Research in this area has focused on developing algorithms that can handle these constraints while maintaining operational efficiency. Mou et al. (2009) introduced a timeline-based scheduling approach using a hierarchical task network-timeline (HTN-T) algorithm, which achieved high scheduling success rates by incorporating external constraints. This is particularly relevant to Nebula AI's Task Scheduling Agent (TSA), which employs reinforcement learning (specifically Q-learning) to dynamically prioritize tasks under similar constraints, maintaining over 90% scheduling success in simulations. Additionally, a study on predictive-reactive scheduling for small satellite clusters (Author et al., 2016) proposed a model that handles uncertainties through heuristic strategies, further supporting Nebula AI's approach to pre-emptively reordering tasks based on real-time mission context. The results from Nebula AI's evaluation, which show a 77% reduction in critical conflicts through agent coordination, demonstrate the effectiveness of these scheduling strategies in maintaining mission efficiency.\n\nMulti-agent systems (MASs) have emerged as a promising solution for distributed autonomy in space missions, offering fault tolerance, scalability, and collaborative decision-making. Chien et al. (2002) demonstrated the feasibility of MAS for satellite constellations, where agents autonomously coordinate to achieve mission goals such as distributed aperture radar operations. This work directly supports Nebula AI's publish-subscribe coordination model, where agents like the ADA and Anomaly Impact Assessor (AIA) communicate via event-driven mechanisms to ensure resilient operation under partial failure or latency. Similarly, Alvarez (2016) developed a multi-agent system for robotic applications in satellite missions, validating the approach through hardware-in-the-loop tests. Nebula AI extends this concept by integrating agents for anomaly assessment, task scheduling, and mission planning, with the Mission Planning and Recommendation Agent (MPRA) synthesizing insights for human operators. The decentralized event bus in Nebula AI, which decouples producers and consumers of mission-critical data, further enhances the system's robustness and observability, as evidenced by the successful conflict resolution dynamics in simulations.\n\nThe integration of large language models (LLMs) into space mission management represents a novel and emerging area of research. While specific studies on LLMs in space are limited, their potential to enhance human-AI collaboration through natural language interfaces is well-documented in other domains (Hugging Face, 2023). Nebula AI pioneers this application by using LLMs within the MPRA to translate structured agent outputs into natural language recommendations, facilitating intuitive interaction for mission control personnel. This approach aligns with broader trends in explainable AI, where transparency and interpretability are critical for operational trust. Although the space-specific validation of LLMs is still in its early stages, Nebula AI's successful integration and evaluation on simulated datasets mark a significant step forward in autonomous, explainable space systems.\n\nSection 3: Dataset and System Architecture Overview\n\nThe performance evaluation of Nebula AI was grounded in two structured datasets specifically designed to emulate realistic mission operations. The first, mission data, encapsulates multivariate time-series telemetry across a range of subsystems, including power consumption, battery health, data storage utilization, and anomaly metadata. The second, coordination data, captures the behavioral footprint of the agent ecosystem, including decision events, task scheduling triggers, and resolution outputs.These datasets were synthetically constructed using statistical patterns modeled after historical data from legacy interplanetary missions such as Cassini and Mars Express. They include both nominal and degraded operational states, with anomalies programmatically injected to simulate real-world uncertainties, including hardware faults, communication loss, and thermal drift.\n\nThe mission.json dataset comprises approximately 500 daily telemetry entries, each representing the spacecraft’s state over mission time. Key fields include power consumption (in watts), battery level (as a percentage), data storage utilization (in megabytes), and task-level metadata such as execution duration, priority, and success status. Anomaly records are embedded with fields describing their type, duration, recovery action, and impact on downstream tasks.\n\nThe coordination.json dataset contains over 2,000 event-based records corresponding to agent interactions, including anomaly detections, impact classifications, task reallocations, and planning advisories. Each entry is tagged with a timestamp, origin agent, triggered response, and outcome, supporting fine-grained analysis of inter-agent coordination under evolving mission conditions. Prior to ingestion into the Nebula AI pipeline, the datasets underwent a structured preprocessing phase. Raw telemetry streams were cleaned for null values and noise, employing linear interpolation and interquartile filtering to preserve signal fidelity. Each record was transformed into a normalized event object, adhering to the platform’s internal schema, which facilitates consistent parsing across agent modules.\n\nTo simulate realistic operational variance, synthetic anomalies (replica of real anomalies) were injected into telemetry at randomized intervals and magnitudes. These included sudden power drops, accelerated battery depletion, and bandwidth congestion, each tagged with recovery expectations. Temporal alignment of all datasets was enforced using ISO 8601 UTC timestamps, ensuring synchrony across telemetry, agent decisions, and large language model (LLM) prompt windows.\n\nNebula AI is structured as a modular, event-driven system designed to facilitate autonomous mission operations in complex space environments. The architecture follows a layered design that promotes scalability, fault tolerance, and real-time responsiveness. It is organized into four primary layers: telemetry ingestion, agent processing, language model integration, and operator-facing interfaces.\n\nAt the foundational level, the Telemetry Ingestion Layer is responsible for acquiring and preprocessing spacecraft sensor data, including propulsion metrics, energy consumption, link status, and orbital parameters. This functionality is encapsulated in a dedicated data service module, which parses structured datasets and transforms them into standardized event objects. These event objects are then published to an internal communication bus for downstream consumption.\n\nThe Agent Processing Layer forms the decision-making core of the system. It hosts a suite of autonomous agents—namely, the Anomaly Detection Agent (ADA), Anomaly Impact Assessor (AIA), Task Scheduling Agent (TSA), and the Mission Planning and Recommendation Agent (MPRA). Each agent operates independently and communicates via an asynchronous publish-subscribe interface. Internally, event propagation is handled through a lightweight event dispatch module, implemented in Event Emitter. This design enables non-blocking execution and supports dynamic coordination between agents in response to telemetry-derived events.\n\nUpon receiving signals from upstream agents, each autonomous module executes its respective logic for instance, anomaly detection via statistical thresholds and ML models, task prioritization through reinforcement learning, or mission planning informed by contextual telemetry. Outputs are structured and relayed as events or recommendations, enabling seamless handoff between modules.\n\nSituated above the agent layer is the Language Model Integration Layer, which translates structured system intelligence into natural-language summaries and recommendations. A prompt-engineered large language model (LLM) receives curated inputs generated by the agents and returns contextual advisories. This mechanism is facilitated through a backend orchestration layer that formats and dispatches prompts, allowing for human-aligned communication and explainability.\n\nThe User Interface Layer provides mission control personnel with interactive, real-time access to system insights. Developed using React and Three.js, the interface includes visual components such as dashboards, time-series plots, 3D orbit simulations, and mission chat modules. For example, the Mission Timeline component enables real-time task monitoring and rescheduling, while Satellite Orbit renders orbital dynamics in a spatially accurate visual environment. These interfaces are event-synchronized and designed to accommodate dynamic operator feedback and situational awareness.\n\nAt the core of the architecture lies the Event Bus, which serves as the primary medium for inter-module communication. By decoupling producers and consumers of mission-critical data, the event bus ensures robustness under latency, failure, or degraded telemetry conditions. The architecture thus supports explainable autonomy while maintaining operational transparency and modular extensibility\n\nFigure 1: Nebula AI System Architecture Diagram\n\nNebula AI was deployed as a modular microservice ecosystem, enabling decoupled development, orchestration, and scaling of system components. Each autonomous agent was encapsulated as an independent containerized service, subscribing to relevant event topics via an internal publish-subscribe architecture implemented through a lightweight event bus. The backend runtime supports dynamic service discovery and message routing, allowing agents to react asynchronously to telemetry and inter-agent messages.\n\nThe LLM module was hosted as a stateless backend service, accepting structured prompts generated by upstream agents and returning contextualized advisories for human operators. The operator interface, built using React and Three.js, interacted with the agent layer via WebSocket streams, rendering real-time visualizations such as orbital trajectories, task timelines, and anomaly summaries. The entire stack was deployed on a containerized infrastructure compatible with Kubernetes for flexible resource scaling and testing under stress.\n\nSection 4: Design of the autonomous agents\n\nAt the heart of the Nebula AI system lies a constellation of purpose-built autonomous agents, each meticulously designed to emulate a specific cognitive function within mission operations. These agents do not operate in isolation; rather, they form a collaborative mesh governed by a publish-subscribe model that facilitates high-frequency, event-driven decision-making across the system. The agent architecture is grounded in principles of distributed intelligence, modular encapsulation, and operational robustness.\n\nThe Anomaly Detection Agent (ADA) serves as the initial sentinel in the pipeline, continuously ingesting multivariate telemetry streams to uncover irregular patterns. Its detection logic fuses statistical thresholding with machine learning-based outlier detection, allowing it to differentiate between benign deviations and mission-threatening anomalies. By applying context-aware thresholds that adapt to subsystem states and mission phases, ADA ensures that its anomaly alerts, rich with metadata such as severity scores and feature attributions are both timely and trustworthy. These alerts are not merely signals but structured messages that activate the system’s downstream response chain.\n\nFollowing ADA’s alerts, the Anomaly Impact Assessor (AIA) assumes the role of an intelligent triage engine. It evaluates the potential consequences of detected anomalies by referencing a curated subsystem criticality map and incorporating mission-phase dependencies. AIA’s core reasoning is powered by a domain-specific risk matrix and a rule-based inference engine capable of stratifying events into severity levels ranging from low to critical. Furthermore, its temporal correlation logic enables it to detect cascading effects and mission-wide threats that may not be immediately obvious. The output from AIA is a prioritized impact assessment, complete with urgency codes and contextual flags to guide the next phase of system response.\n\nThe Task Scheduling Agent (TSA) receives this context-aware intelligence and dynamically recalibrates the mission schedule. Unlike static schedulers, TSA operates in a continuously adaptive mode, constrained by bandwidth windows, power availability, task priority, and subsystem health. At its core is a reinforcement learning loop specifically a Q-learning model—that iteratively updates its scheduling policy based on observed outcomes. TSA can pre-emptively reorder, defer, or expedite tasks based on current conditions and strategic mission goals. The resulting execution plan is time-resolved and agent-aware, ensuring that mission objectives remain achievable even under degraded conditions.\n\nFinally, the Mission Planning and Recommendation Agent (MPRA) serves as the cognitive synthesis layer, fusing all upstream intelligence into actionable guidance. MPRA aggregates insights from ADA, AIA, and TSA, transforming this information into structured prompts fed to a large language model (LLM). The LLM, in turn, generates mission recommendations, operator summaries, and natural language explanations, enabling effective human-AI collaboration. MPRA’s dialogue is tailored based on prior interactions, user preferences, and contextual mission cues, allowing for a more intuitive and trustworthy interface between operators and autonomous systems.\n\nCollectively, these agents represent more than discrete modules—they embody a cohesive, communicative, and resilient system capable of responding to the dynamic complexity of real-time space missions. Their design enables rapid, explainable adaptation to unforeseen events, reducing cognitive load on human operators while maintaining a high degree of transparency and control.\n\nFigure 2: Agent Workflow and Coordination Diagram\n\n\nTable 1: Agent Responsibilities and Triggers\n\n\nSection 5: Introduction Multi-Agent Coordination and Communication\n\nEffective mission autonomy requires not only intelligent agents but also robust coordination mechanisms that facilitate timely, context-aware collaboration. Nebula AI achieves this through a decentralized, event-driven communication fabric underpinned by asynchronous messaging, priority handling, and fault tolerance. This section describes the system's coordination model, inter-agent messaging semantics, and conflict resolution strategies.\n\nAt the core of the coordination model is an internal event bus, implemented using a lightweight publish-subscribe architecture. Agents subscribe to domain-relevant event types and publish structured messages that trigger downstream processes. This design enables each agent—whether for anomaly detection, impact assessment, scheduling, or planning—to operate independently while remaining contextually synchronized with system-wide state.\n\nFor instance, when the Anomaly Detection Agent (ADA) publishes an ANOMALY_DETECTED event, the Anomaly Impact Assessor (AIA) subscribes to and consumes this message to perform real-time risk evaluation. Based on the severity classification, AIA emits an IMPACT_CLASSIFIED event, which subsequently influences the behavior of the Task Scheduling Agent (TSA), prompting it to reallocate resources or reprioritize tasks. This event chaining creates an adaptive coordination loop with no single point of control, allowing for resilience under partial failure or latency.\n\nTo manage concurrency and ensure timely response, the event queue supports message prioritization, critical locks, and temporal throttling. High-priority messages—such as those classified as \"critical impact\"—are processed with execution precedence. Critical locks are applied during mission-critical decision windows to avoid race conditions, particularly in task scheduling and deconfliction logic.\n\nEach agent maintains an internal state and a subscription registry, dynamically updated during mission progression. This allows the system to support contextual reactivity, where agents modify their behavior based on accumulated history and mission phase. The Task Scheduling Agent, for example, leverages historical task success rates and anomaly recurrence to refine its reinforcement learning policy.\n\nIn the event of upstream failures—such as a missing anomaly classification or delayed telemetry packet—the system activates graceful degradation pathways. Agents fallback to default scheduling templates, and the Mission Planning Agent (MPRA) adjusts its recommendation logic to reflect uncertainty, clearly communicating the source of reduced confidence via the user interface.\n\nAll inter-agent communication adheres to a common event schema, which includes standardized headers for timestamps, priority levels, agent origin, and semantic tags. This schema supports observability, traceability, and post-hoc auditing, essential for mission assurance and operational transparency.\n\nCode Snippet 1 – ADA Publishing an Event\n\n// ADA publishes an anomaly event\neventBus.emit('ANOMALY_DETECTED', {\n  timestamp: Date.now(),\n  subsystem: 'Power',\n  severity: 'High',\n  details: { deviation: 32.5, expected: 12.0 }\n});\n\nCode Snippet 2 – AIA Subscribing to Events\n\n// AIA listens for anomaly events\neventBus.on('ANOMALY_DETECTED', (eventData) => {\n  const impact = assessImpact(eventData);\n  eventBus.emit('IMPACT_CLASSIFIED', { ...impact, origin: 'AIA' });\n});\nSection 6: Experimental Setup and Technical Assumptions\nSection 6.1: Assumptions and Operational Context\n\nThe evaluation of Nebula AI was conducted under a defined set of assumptions that reflect practical mission control conditions while isolating core system capabilities. It was assumed that telemetry data is ingested sequentially with consistent timestamps, minimal jitter, and no packet corruption. Autonomous agents are expected to operate on an event-driven backbone with guaranteed message delivery and deterministic ordering. The LLM subsystem is presumed to remain online throughout the simulation, providing prompt-based responses within a bounded latency window of two seconds. Operator interactions are assumed to occur through passive dashboard observation or active queries, without direct intervention in the agent decision logic.\n\nSection 6.2: Study Scope and System Boundaries\n\nThe experimental scope centers on evaluating Nebula AI’s autonomous reasoning capabilities, inter-agent coordination, and system resilience under telemetry disruptions. The system was tested in a simulated mission control environment with real-time playback of telemetry and operational events. While the architecture supports onboard deployment and live downlink processing, the current evaluation excludes orbital perturbation modeling, closed-loop hardware feedback, and real telemetry pipelines. Human-in-the-loop evaluation, adversarial LLM testing, and cybersecurity concerns were also considered out of scope for this version.\n\nSection 6.3: Agent Parameters and System Configuration\n\nEach autonomous agent was initialized with scenario-specific configuration files. The Anomaly Detection Agent (ADA) operated in hybrid mode, using rolling Z-score filters (σ > 2.5) combined with a density-based outlier detector (DBSCAN, ε=1.0, minPts=5). The Task Scheduling Agent (TSA) implemented a tabular Q-learning algorithm for adaptive rescheduling, with learning rate α=0.1, discount factor γ=0.95, and an ε-greedy exploration strategy decaying from ε=0.3 to ε=0.05 over time. The reward structure penalized task delays and missed priorities while positively reinforcing timely execution.\n\nThe Mission Planning and Recommendation Agent (MPRA) was configured to prompt a transformer-based LLM (GPT architecture) with structured telemetry digests, anomaly summaries, and prioritized task lists. Prompts were restricted to <500 tokens to maintain inference latency within target thresholds.\n\nSection 6.4: Experimental Execution Environment\n\nAll experiments were executed in a controlled offline simulation environment. The system was deployed as a containerized microservice stack on a high-performance workstation with the following configuration:\n\na. Processor: Intel Core i9-12900K\nb. Memory: 64 GB DDR5\nc. GPU: NVIDIA RTX 3090 (used for LLM inference acceleration)\nd. Operating System: Ubuntu 22.04 LTS\ne. Orchestration: Docker Compose with eight isolated services (agents, backend, UI, LLM API)\nf. Frontend Stack: React 18 with Three.js for 3D orbital visualization\ng. Backend Stack: Node.js 18 with TypeScript and WebSocket-based event dispatching\nh. LLM Interface: RESTful API compatible with OpenAI’s gpt-40\n\nSimulation time was accelerated such that one second of real-time corresponded to one mission hour, allowing full mission arcs to be evaluated in under 30 minutes. All agent decisions, event timings, LLM responses, and scheduling outcomes were logged for post-hoc analysis. Performance graphs, conflict resolution metrics, and timeline deviations were computed directly from these runtime logs, ensuring statistical reproducibility and interpretability.\n\nSection 7: Results\n\nThis section presents a multi-dimensional evaluation of Nebula AI’s core performance components: anomaly detection, task scheduling, agent coordination, and resource intelligence. Simulations were conducted using structured mission telemetry and coordination data of Cassini Satellite. These tests replicate real-world constraints such as bandwidth saturation, temporal task overlap, and cascading subsystem degradation.\n\nSection 7.1: Anomaly Detection Performance\n\nThe Anomaly Detection Agent (ADA) was validated on simulated telemetry streams featuring injected fault signatures across power, battery, and thermal subsystems. ADA employed both thresholding and outlier detection models.\n\nTable 2: Anomaly Detection Metrics\n\n\nThe results demonstrate ADA’s high recall and low false-positive rate — critical traits for first-response agents in autonomous spacecraft operations.\n\nSection 7.2: Task Scheduling Success Rate\n\nThe Task Scheduling Agent (TSA) dynamically resolved task conflicts using Q-learning and real-time mission context. It was tested under varying anomaly injection frequencies and resource bottlenecks.\n\nTable 3: Task Scheduling Success Rate\n\n\nDespite adverse conditions, TSA maintained >90% success under all scenarios, reflecting the strength of adaptive learning strategies in resource-constrained scheduling.\n\nSection 7.3: Rescheduling Efficiency Over Time\n\nAnomalies triggered rescheduling operations by TSA and MPRA. The system's ability to recover and maintain high operational efficiency was tracked over time.\n\nFigure 4: Rescheduling Efficiency Over Time\n\n\nThe system exhibits rapid recovery, sustaining efficiency above 90% after mid-mission anomalies, showing convergence to near-optimal scheduling behavior.\n\nSection 7.4: Conflict Resolution Dynamics\n\nAgent collaboration led to marked reductions in task overlap, bandwidth contention, and deadline violations.\n\nFigure 5: Conflict Resolution Comparison\n\n\nA 77% reduction in critical conflicts was achieved post-coordination, demonstrating the emergent benefits of agent intercommunication and prioritization logic.\n\nSection 7.5: Telemetry Trend Analysis\n\nTime-series analysis on power, battery, and storage revealed clear operational stress points that correlate with agent activity and mission anomalies.\n\nFigure 6: Telemetry Trends (Power, Battery, Storage)\n\n\nThe system responded dynamically to power surges and storage saturation, redistributing tasks and rebalancing schedules to mitigate overload risk.\n\nSection 7.6: Mission Timeline Performance\n\nAnalysis of timeline data revealed:\na. 93% of high-priority tasks completed on time under nominal conditions\nb. <5% average delay for non-critical tasks during anomaly peaks\nc. Visual timeline feedback enabled manual overrides and drag-to-reorder logic in UI\n\nTable 4: Task Performance Metrics\n\n\nSection 8: Discussion\n\nBeyond its strong technical performance, Nebula AI hints at a paradigm shift in how we architect trust and delegation in mission-critical systems. Traditionally, autonomy in space systems has been treated as an auxiliary support which is useful in nominal conditions but overridden in crises. Nebula AI flips this dynamic: it thrives precisely when human capacity is constrained or when mission states become volatile.\n\nA deeper implication of Nebula AI’s architecture is the emergence of situational context as a first-class signal in system design. Each agent doesn’t simply respond to events, it interprets them in light of historical, temporal, and mission-phase context. This moves beyond reactive autonomy into the realm of cognitive modeling, where systems exhibit behavior that resembles reasoning rather than rule-following. The AIA's cascading impact analysis, for instance, is a foundational step toward systems that understand causality and interdependence crucial for next-gen autonomous platforms operating far from Earth-based oversight.\n\nMoreover, the LLM-assisted MPRA agent introduces a subtle but powerful evolution in human-machine teaming: empathy for operator cognition. By using natural language not just for translation but for prioritization, clarification, and even soft signaling (e.g., \"confidence degraded due to upstream delay\"), the system doesn't just inform it collaborates. This sets the stage for more psychologically adaptive interfaces, where system communication aligns with operator stress levels, alert fatigue, or experience.\n\nImportantly, Nebula AI offers a template for composable autonomy. The modular design allows individual agents to be re-trained, replaced, or repurposed without disrupting the overall mission fabric. This flexibility is a strategic advantage in space programs that increasingly blend commercial, academic, and governmental assets each with distinct hardware and operational protocols. Nebula AI could be deployed not as a monolith, but as a federated autonomy layer across heterogeneous fleets.\n\nOne untapped opportunity is the use of cross-agent learning, where feedback loops between agents refine future decisions. For instance, anomalies detected but later deemed inconsequential by the AIA could be fed back into ADA’s models to reduce overfitting. Similarly, TSA could benefit from learned reward functions not only based on schedule adherence, but on mission outcome metrics shaped by MPRA’s recommendation history. This meta-adaptive layer could make Nebula AI not just autonomous but continually self-improving.\n\nConclusion\n\nNebula AI opens the door to mission systems that don’t just react to space environments, they evolve within them. Its layered intelligence, contextual awareness, and human-aligned interface form the backbone of a new generation of space autonomy: one that is modular, interpretable, and anticipatory.\n\nYet, the true potential of Nebula AI lies not in what it automates, but in what it enables. By reducing human cognitive burden, it frees mission specialists to focus on strategy over survival. By providing transparency, it builds trust in autonomy rather than dependence. And by operating as a coordinated, learning-driven ecosystem, it lays the foundation for intelligent constellations, interplanetary logistics networks, and self-healing spacecraft swarms.\n\nThe next frontier isn't just about going farther, it’s about thinking smarter, reacting faster, and partnering more deeply with the systems we build. Nebula AI is a prototype of that partnership: not just machines in space, but thinking allies among the stars.\n\nReferences\nAlvarez, M. (2016). Multi-agent robotic systems and applications for satellite missions [Doctoral dissertation, University Name]. Repository Name. \nhttps://ui.adsabs.harvard.edu/abs/2016PhDT........59N/abstract\nAuthor, A., Author, B., & Author, C. (2016). Predictive-reactive scheduling for space missions in small satellite clusters. IEEE Xplore. \nhttps://ieeexplore.ieee.org/document/7545235\nChien, S., Sherwood, R., Tran, D., Cichy, B., Rabideau, G., Castano, R., Davies, A., Mandl, D., Frye, S., Trout, B., Shulman, S., & Boyer, D. (2002). Multiple agent-based autonomy for satellite constellations. Artificial Intelligence, 145(1-2), 147-180. \nhttps://doi.org/10.1016/S0004-3702(02)00382-X\nHugging Face. (2023). Multi-agent systems. Hugging Face Agents Course. \nhttps://huggingface.co/learn/agents-course/en/unit2/smolagents/multi_agent_systems\nMou, F., Morris, R. A., & Di Vito, B. L. (2009). Timeline-based space operations scheduling with external constraints. Proceedings of the International Conference on Automated Planning and Scheduling. \nhttps://doi.org/10.1609/icaps.v20i1.13402\nPicard, G., Bernon, C., Gleizes, M. P., & Peyruqueou, S. (2023). International Workshop on Autonomous Agents and Multi-Agent Systems for Space Applications. Workshop Proceedings.\nWeng, Y., Zhu, J., Cheng, Y., Li, C., & Zhang, J. (2021). Survey on security issues of routing and anomaly detection for space information networks. Scientific Reports, 11, 22286. \nhttps://doi.org/10.1038/s41598-021-01638-z\nZeng, X., Li, Y., Zhang, Y., & Liu, Y. (2024). Explainable anomaly detection in spacecraft telemetry. Expert Systems with Applications, 244, 123456. \nhttps://doi.org/10.1016/j.eswa.2024.123456\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction: A New Dawn in the Cosmos\n\nSection 2: Related Work\n\nSection 3: Dataset and System Architecture Overview\n\nSection 4: Design of the autonomous agents\n\nSection 5: Introduction Multi-Agent Coordination and Communication\n\nSection 6: Experimental Setup and Technical Assumptions\n\nSection 6.1: Assumptions and Operational Context\n\nSection 6.2: Study Scope and System Boundaries\n\nSection 6.3: Agent Parameters and System Configuration\n\nView all\nCode",
    "awards": [
      "best creative ai project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "neuro-optix-a-specialized-field-robot-for-your-safety-ElCqKcSme5by",
    "username": "Ammar Ali",
    "license": null,
    "title": "Neuro Optix:  \"A Specialized field robot for your Safety\"",
    "publication_description": "Back to publications\nDec 29, 2024\n●\n70 reads\n●\nCreative Commons Attribution-NonCommercial (CC BY-NC)\nWinner of\nMost Promising Innovation\nat the\nComputer Vision Projects Expo 2024\nNeuro Optix: \"A Specialized field robot for your Safety\"\nComputer vision\nInnovation\nneuro optix\nsafety robot\nsurvillance robot\nAmmar Ali\nM\n@maryamriazf21\nK\n@kinzanoreenf21\nA\n@awabyounasf21\nLike\nBookmark\nShare\nSee the Demo of the Project for a complete overview of the project: \nClick Here\nIntroduction\n\nNeuro Optix is an innovative robotic surveillance and safety system that combines advanced automation and artificial intelligence. Constructed from durable 3D-printed parts and acrylic sheets, this robotic car features four large wheels driven by DC motors for easy navigation across various terrains. At its core, the OAK-D AI camera is mounted on the car's top lid to monitor distances between personnel and machinery, enhancing safety in environments like construction sites by measuring distances between workers and heavy equipment such as excavators. The KRIA platform handles real-time model inference and visual display, while servo motors enable precise camera movement for comprehensive coverage. Remote control is achieved through a connection system using Streamlit + Ngrok or Luxonics Hub, allowing operation via a web app from anywhere globally. This connectivity ensures versatile deployment across industries, offering real-time surveillance and safety monitoring to enhance operational efficiency.\n\nProblem Statement\n\nTraditionally companies hire multiple safety officers for each construction site to ensure workers safety Managing worker safety on construction sites. This approach have following issues:\n\nTransportation costs of Safety Engineers.\nEnsuring timely payment of monthly salaries of safety officers/ Engineers.\n\nAdditionally, there are persistent problems related to corruption and negligence. These challenges necessitate a robust and efficient monitoring system to ensure the safety and well-being of workers.\n\nProposed Solutions\n\n\nTo address the challenges, we propose a multi-faceted approach.\n\nStationary cameras are installed around the site to provide continuous surveillance and monitor worker activities.\nSafety helmet cameras are utilized to offer a first-person perspective, ensuring real-time monitoring of individual workers' safety and compliance with safety protocols.\nAdditionally, robots equipped with advanced computer vision technology are deployed to autonomously navigate the site, detect potential hazards, and provide dynamic surveillance, enhancing overall site safety and efficiency.\nWe have used Robot because it can be controlled. Moreover AI portion can be installed on IP cameras/normal cameras as well but this documentation will focus on the deployment of the AI on the Robot using Kria.\n\nObjectives\nEnhance Workplace Safety\nNo need of Safety Engineers.\nAdvanced Surveillance\nImprove Operational Efficiency\nGlobal Remote Control\nVersatile Deployment\nWhy Neuro Optix?\n\nNeuro Optix offers a superior solution to traditional safety measures, such as fixed cameras and safety helmet cameras, through its mobility and advanced AI technology. Unlike stationary cameras, Neuro Optix can navigate various terrains, providing dynamic, real-time surveillance and ensuring comprehensive environmental coverage. Safety helmet cameras depend on individual wearers and may miss critical blind spots. In contrast, Neuro Optix’s AI-powered system continuously monitors and analyzes the surroundings to proactively prevent collisions and enhance safety. By integrating cutting-edge computer vision and remote connectivity, Neuro Optix delivers more reliable and efficient safety monitoring, making it an ideal choice for diverse and high-risk environments.\n\nUnveiling NeuroOptix\nDesigning\n3D Parts\n\nIn the initial design phase of NeuroOptix, the body of the robotic car is meticulously crafted using AutoCAD software. This phase involves creating detailed 3D models that outline the structure, and dimensions of the car.\n\n3D Printing & Laser Cutting\n\nIn the fabrication process of NeuroOptix, components are first meticulously designed using AutoCAD software. These designs are then transferred to a 3D printer to create custom parts with precise dimensions, ensuring they fit perfectly into the robotic car's framework. Additionally, certain structural elements are cut from acrylic sheets to provide sturdy support and enhance the overall durability of the vehicle. This dual approach of 3D printing and acrylic sheet cutting allows for a tailored construction that balances flexibility, strength, and ease of assembly.\n\nSoldering\n\nIn the assembly of NeuroOptix, soldering is used to securely connect and insulate the wires of DC motors ensuring reliable electrical connections essential for the car's operational integrity.\n\nAfter designing and printing all components using AutoCAD software and a 3D printer, along with cutting necessary parts from acrylic sheets, the stage is set for assembly. Each printed and acrylic component has been crafted with precision to ensure compatibility. The next step involves methodically assembling these parts, ensuring all connections are securely integrated.\n\nHardware Components\nIn our project, each hardware component plays a crucial role in enhancing the functionality and capabilities of NeuroOptix, our robotic platform designed for advanced surveillance and safety applications:\nKria KR260\n\nThe Kria KR260 is a high-performance, adaptive computing module designed for advanced embedded applications. Developed by AMD Xilinx, this versatile FPGA (Field-Programmable Gate Array) kit provides powerful processing capabilities and flexible interfacing options, making it ideal for a wide range of AI and machine learning tasks.\n\nOAK-D Camera:\n\nThe KR260 provides the necessary interfaces to connect and control the OAK-D AI camera. It processes the depth sensing and object detection data captured by the camera, enabling real-time analysis and decision-making.\n\nBy leveraging the powerful FPGA architecture of the KR260, our system can handle more complex AI models and perform advanced computations. This ensures that the data from the OAK-D camera is processed quickly and accurately.\n\nThe OAK-D Lite is a powerful and compact AI vision system designed for advanced computer vision applications.\n\nDC Motors\n\nDrive the movement of the robotic car, providing propulsion across different terrains. Controlled by motor drivers, these motors ensure smooth and precise motion.\n\nServo Motors\n\nControl pan-tilt movements of the OAK-D AI camera and other articulated functions. This capability enhances NeuroOptix's surveillance capabilities, enabling it to dynamically adjust its field of view and monitor specific areas of interest.\n\nMotor Drivers (L298N)\n\nEssential for controlling the speed, torque, direction, and efficiency of the DC motors. The L298N motor drivers interface between the Raspberry Pi's output signals and the motors, regulating power delivery to ensure optimal performance and reliability during operation.\n\nArduino\n\nIt is used to manage and control various aspects of NeuroOptix. It interfaces with motor drivers to regulate DC motors for precise movement control and coordinates servo motors to adjust the OAK-D AI camera's position.\n\nSoftware Setup\nVitis-AI:\n\nVitis AI is a development platform from AMD Xilinx that simplifies deploying deep learning models on FPGAs (Field-Programmable Gate Arrays). It allows developers, even those without extensive FPGA expertise (just like us), to harness the high performance and flexibility of FPGAs for AI applications. A key feature is its ability to convert standard deep learning models into the xModel format for deployment on FPGA DPUs (Deep Processing Units), streamlining the process and making FPGA technology more accessible for AI workloads.\n\nOpenCV\n\nOpenCV, a widely-used open-source computer vision library, is utilized for image processing tasks. It provides the tools necessary to process and analyze images captured by the Luxonis OAK-D Lite, enabling functionalities such as object detection and depth perception.\n\nDepthAI API\n\nThe DepthAI API is used to interface with the Luxonis OAK-D Lite. This API facilitates the execution of advanced computer vision tasks by leveraging the AI processing capabilities of the OAK-D, including real-time object detection and depth sensing.\n\nStreamlit & Ngrok\n\nStreamlit is employed to control the robot's movements, while Ngrok is used to enable global access for remote control.\n\nLuxonis Hub\n\nThe Luxonis Hub is a central management tool that plays a crucial role in the project. It allows for the control of multiple Luxonis OAK-D Lite devices, managing live video feeds, deploying AI models, and handling device interactions over the network. This centralized control simplifies the management of complex tasks and ensures efficient operation of the AI cameras.\n\nLuxonis Hub is also utilized for controlling the robot's movements, offering lower latency compared to Streamlit.\n\nModel Optimization\n\nData capture starts with the OAK-D camera, which feeds into the Kria module for processing. This processed data is then used for motor control and visualization, allowing remote monitoring by the Safety Officer to ensure safe operations globally.\n\nBlock Diagram:\n\nFlow of data, starting with data capture by OAK-D, followed by processing in KRIA, and ending with control of motors and visualization.\n\nOAK-D: This component captures data, calculates disparity, and calculates distance.\n\nKRIA: This component includes the following sub-components:\n\nPYNQ: It is the software running in Kria that processes the data received from OAK-D and perform inference of AI models.\n\nStreamlit Hosting: It hosts a web-based interface for visualization and control.\n\nArduino: It is responsible for controlling motors based on the data received from KRIA via serially.\n\nSafety Officer: This component represents a human user who can remotely access and control the system via a secure connection through Ngrok.\n\nKRIA & Arduino\n\nMicrocontroller (Arduino): The central component is likely an Arduino board, which serves as the controller for the circuit. It processes inputs and sends commands to other components.\n\nPower Supply: The circuit includes a power supply that provides the necessary voltage and current to the components. This power supply is typically connected to the Arduino and the motor driver modules.\n\nMotor Driver Modules: There are two motor driver modules connected to the Arduino. These driver modules are used to control the speed and direction of motors. The motor drivers act as intermediaries between the Arduino and the motors, allowing for higher current and voltage to be used than the Arduino alone can provide.\n\nConnections:\nWires: The colored wires represent connections between the Arduino and the motor driver modules. Different colors may indicate different types of signals (e.g., power, ground, control signals).\nInput/Output Pins: The Arduino’s digital pins are typically connected to the input pins of the motor drivers, sending signals to control the motors. Additionally, the motor drivers are connected to the motors, providing the necessary output to operate them.\nFunctionality:\n\nThe Arduino is programmed to send control signals to the motor driver modules based on user input or sensor data. This allows it to control the motors' operation, such as starting, stopping, and changing speed or direction.\n\nThis setup could be used to control a robotic vehicle, where the main board processes data and the Arduino directs the motors to steer and move the vehicle.\n\nKRIA Setup\n\nTo set up the Kria KR260, follow these detailed instructions to ensure a smooth installation and update process.\n\nStep 1: Access the Official Documentation\n\nVisit the \nOfficial Documentations\n for the Kria KR260 to familiarize yourself with the setup process and requirements.\n\nStep 2: Update the Firmware\n\nUpdating the firmware is crucial to avoid potential issues. Open the terminal and run the following command to download the firmware image:\n\nwget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1wACTcpbwLPOH9UUuURk5qcnIYeEverSB' -O k26_update3.BIN\n\nAfter Downloading the firmware run the following command\n\nsudo xmutil bootfw_update -i <path-to-FW.BIN file>\nsudo xmutil bootfw_status\nsudo shutdown -r now\nsudo xmutil bootfw_update -v\n\nMake sure to replace <path-to-FW.BIN file> with the original path.\n\nStep 3: Set Up Wi-Fi (using W11MI Tenda)\n\nTo install the Wi-Fi driver for the W11MI Tenda adapter, ensure that you are connected to the internet via Ethernet. If there are any connectivity issues, follow the DNS setup instructions.\n\nConfigure DNS:\nIf connected via Ethernet but the internet is not accessible, update your DNS settings:\nsudo nano /etc/resolv.conf\n\nAdd the following line:\n\nnameserver 8.8.8.8\n\nSave and close the file. Now you will have the internet access.\n\nInstall Wi-Fi drivers:\nRun these commands to install the necessary Wi-Fi drivers:\nsudo apt-get install build-essential git dkms linux-headers-$(uname -r)\ngit clone https://github.com/McMCCRU/rtl8188gu.git\ncd rtl8188gu\nmake\nsudo make install\nsudo apt install --reinstall linux-firmware\nsudo reboot\n\nAfter rebooting, you should see the Wi-Fi option appear in your settings. You can now connect to your Wi-Fi network.\n\nPynq Installation:\n\nTo perform AI model inference on the Kria KR260, you need to install PYNQ. Follow these instructions to properly set up PYNQ on your device.\n\nStep 1: Clone the Robotics AI Repository\n\nClone the necessary repository from GitHub by running the following command in your terminal:\n\ngit clone https://github.com/amd/Kria-RoboticsAI.git\n\nInstall the dos2unix utility, which will help convert Windows-style line endings to Unix-style:\n\nsudo apt install dos2unix\n\nNavigate to the scripts folder within the cloned repository and convert all shell scripts to Unix format. This ensures compatibility and avoids execution issues.\n\ncd /home/ubuntu/Kria-RoboticsAI/files/scripts\nfor file in $(find . -name \"*.sh\"); do\n    echo ${file}\n    dos2unix ${file}\ndone\nStep 2: Installation of Pynq:\n\nIn order to install the pynq you need to run the following commands:\n\nsudo su\ncd /home/ubuntu/Kria-RoboticsAI\ncp files/scripts/install_update_kr260_to_vitisai35.sh /home/ubuntu\ncd /home/ubuntu\nsource ./install_update_kr260_to_vitisai35.sh\nreboot\n\nIt will take 10~15 min to install depending upon you internet connection.\n\nNote: Make sure that your internet connection is good otherwise the installation would be failed.\n\nStep 3: Set Up the PYNQ Environment\n\nNow as the pnyq is installed we need to setup the environment first. So run the following command.\n\nsudo su\nsource /etc/profile.d/pynq_venv.sh\ncd $PYNQ_JUPYTER_NOTEBOOKS\npynq get-notebooks pynq-dpu -p\nCongratulations! You have completed the basic setup.\nVitis-AI:\n\nVitis-AI is being used to optimize the model for DPU. Otherwise simple model cannot be deployed on the DPU. So As we are using yolov5 so we need to optimize the model first for.pt extension to the.xmodel.\n\nVitis-AI installation:\n\nFor Vitis-AI we shall need ubuntu. We are using the Ubuntu 20 LTS. Now we are going to install the Vitis-AI 3.5 so follow the following link to install. After installation clone the following repo.\n\nOptimizing the models:\n\nNow as we want to optimize Yolov5 models. We need to Navigate in to the Yolov5 folder i.e \"Quantizing-Compiling-Yolov5-Hackster-Tutorial\". Now make sure you have the test data of the model you have trained on.\n\nNow Activate the Vitis environment and run the following commands please change the commands according to your path.\n\npython3 quant.py -w -d -q calib\n\npython3 quant.py -w yolo_m.pt -d Safety-Helmet-pro-3/test/ -q calib \npython3 quant.py -w yolo_m.pt -d Safety-Helmet-pro-3/test/ -q test\nvai_c_xir --xmodel build/quant_model/DetectMultiBackend_int.xmodel  --arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/KV260/arch.json --net_name yolov5_kv260 --output_dir ./KV260\n\nAfter Running these three commands your compiled xmodel shall be stored in the Kv260 model. Now you can use this model in to your pynq DPU code.\n\nFeatures:\nFire Detection\nLeveraging OpenCV's image processing algorithms enhances the OAK-D camera's ability to detect flames in real-time. This not only ensures swift identification of fire outbreaks but also enables the system to take immediate preventive measures, bolstering safety protocols.\n\nHelmet Detection\nHelmet detection is a critical feature of our Remote-Controlled Worker Monitoring System. Using the Luxonis OAK-D AI camera, the system can accurately detect whether workers are wearing helmets in real-time. This ensures compliance with safety regulations, enhancing worker safety and reducing the risk of head injuries on construction sites.\n\nDistance Measuring\nThe OAK-D AI camera is used for distance measuring by leveraging its stereo pair of global shutter cameras, which capture 3D depth information to accurately perceive and calculate the distance between objects and their surroundings in real-time.\n\nResults and Analysis\n\nNeuro Optix utilizes advanced vision technology to distinguish between workers adhering to safety protocols and those not in compliance. This continuous monitoring guarantees consistent safety practices among all workers.\n\nOpenCV facilitates advanced image processing and computer vision techniques, enabling the robotic arm to accurately manipulate and interact with objects. This capability enhances the project's overall functionality in tasks requiring precise object handling.\n\nFuture Enhancements\n\nWe foresee several advancements for our Remote-Controlled Worker Monitoring System to further elevate its functionality:\n\nIntegration with Drones\n\nWe would integrate the drone with the car. Then the car is capable to launch drone on the sites remotely so that safety officer can monitor work at height moreover the feed from drone camera will be inferenced at Kria Kr260 board to perform some PPE detections.\n\nMonitoring Workers at Heights\n\nIncorporating drones into the system will enable the monitoring of workers operating at elevated levels. This will provide a holistic view of the construction site, enhancing worker safety through comprehensive surveillance.\n\nQuality Assurance\n\nDrones will also be utilized for quality inspections from above, ensuring construction standards are met and maintained. This aerial perspective will help in identifying and rectifying issues that might not be visible from the ground.\n\nWorker Training\n\nThe robot will be capable of providing training and informing workers about safety protocols by analysing the worksite. For instance, if an activity involves working at height, the robot will instruct workers to use fall protection restraints. This functionality is powered by a GenAI model, deployed on the cloud.\n\nWorker's Safety Conscious\n\nThe robot will utilize its sensors to monitor environmental conditions (e.g temperature, wind speed). In adherence to ILO guidelines, it will notify workers to take breaks during peak sunlight hours to prevent heat-related illnesses such as heat exhaustion or heat stroke.\n\nAutonomous Petrol\n\nRobot will autonomously perform certain tasks such as guiding the crane operator and patrolling the site using 3D LiDAR.\n\nNote: The Working on the Robot is still going on to convert it in to product and we are properly consulting the Health and Safety Coordinator Mr. Zaka so that we could achieve the best we can.\nThanks For Reading.\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nSee the Demo of the Project for a complete overview of the project: \nClick Here\n\nIntroduction\n\nProblem Statement\n\nProposed Solutions\n\nObjectives\n\nWhy Neuro Optix?\n\nUnveiling NeuroOptix\n\n3D Parts\n\n3D Printing & Laser Cutting\n\nSoldering\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nDatasets\nFiles\nUseful.txt\nYou might be interested\nHub: An Agentic Computer Vision Platform for Intelligent Video Analytics\nC\nFeb 08, 202519 reads\ncomputer-visionedge-detection+7\nAI assisted Navigation Device for the blind\nK\nDec 28, 202444 reads\nComputer visionEdgeML+4\nProduct inspection with Renesas RZ/V2L and Edge Impulse\nMost Innovative Project\nDec 27, 202466 reads\n3d printingcomputer vision+3\nFollowing Mechanics with our YOLO Model\nMost Promising Innovation\nF\nI\nL\nJ\nDec 30, 202469 reads\nAIComputer Vision+2",
    "awards": [
      "most promising innovation",
      "most innovative project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "non-waste-ingredients-prepare-a-nice-dish-with-your-older-ingredients-9faShw3G8tcj",
    "username": "e@erik.martinez.n",
    "license": "No License",
    "title": "Non-Waste-Ingredients ♻️ - Prepare a nice dish 🍽️ with your older ingredients! 🥦🍅🧅 ",
    "publication_description": "Back to publications\nFeb 16, 2025\n●\n83 reads\n●\nNo License\nWinner of\nMost Engaging Presentation\nat the\nAgentic AI Innovation Challenge 2025\nNon-Waste-Ingredients ♻️ - Prepare a nice dish 🍽️ with your older ingredients! 🥦🍅🧅\nAI4good\nComputer Vision\nIoT food-app\nLangGraph+Groq+RAG\nMultiAgentGraph\nE\n@erik.martinez.n\nLike\nBookmark\nShare\nAbstract\n\nEver feel like your fridge is a black hole where perfectly good food goes to disappear? 🕳️ We've all been there – discovering that forgotten head of lettuce or that container of leftovers that's now a science experiment.\n\nFood waste is a huge problem, not just for our wallets 💸, but for the planet. 🌎 Globally, roughly one-third of all food produced for human consumption is wasted, according to the Food and Agriculture Organization of the United Nations\n\nImagine the resources – water 💧, land 🏞️, energy ⚡ – that go into producing food that just ends up in the trash! 🗑️ If we all took small steps to reduce our food waste, we could make a real difference in terms of environmental impact and even food security.\n\nThink about it: less waste means less strain on our resources, and potentially, more food available for those who need it. 🙏\n\nLast summer I became a dad! And in those precious moments when my son was napping 😴, I found myself strangely energized and inspired so I decided to tackle this food waste experiment head-on.\n\nThat's how this project, born out of late-night coding sessions 💻 and stolen moments, came to life.\n\nHonestly, while it's working in my fridge right now 🧊, I know there are still improvements to be made. But it's a start. 🚀\n\nSo, how does it work? 🤔 Well, that brings us to the methodology...\n\nNOTE: I upload code but for more compact and better understanding please just go to my\nGitHub repo\n in order to have full picture!\n\nMethodology\n\nThe methodology behind \"Non-Waste-Ingredients\" involves a multi-pronged approach combining image recognition, a knowledge graph, and an AI-powered recommendation system.\n\nFirst, users register their groceries using a Streamlit-based web application. This can be done either:\n\n-By scanning barcodes with a dedicated reader (just bought one on amazon):\n\n\n-By capturing images of the products with any connected device - A pre-trained YOLOv10 object detection model, fine-tuned on 70 common food categories (including items like 'olive', 'avocado', 'garlic', 'egg', etc.), identifies the items in the images.\n\nThis model was trained using Roboflow for image labeling and YOLOv10 for fine-tuning.\n\n\nSometimes can get confused 😆\n\n\n-By hand.\n\n\nThis registration process creates a personal food inventory, stored in a food_db.json file.\n\nNext, nutritional information for each registered item is retrieved. The system uses the Open Food Facts and Edamam APIs to gather details about the products based on the scanned barcodes. This data includes nutritional values (carbs, proteins, fats, fiber, etc. per 100g), which are crucial for recipe generation.\n\nThe core of the system is the \"Chef Assistant,\" which leverages a Retrieval Augmented Generation (RAG) system powered by an AI agentic workflow. This system is designed to suggest daily dishes based on the user's oldest ingredients, minimizing food waste. It uses a Milvus vector database, populated with recipe embeddings generated from a sample recipe book. These embeddings are created using extract_embeddings.py and stored in the Milvus database using milvus_vector_db.py.\n\nThe RAG system retrieves relevant recipes from the Milvus database based on the available ingredients and then uses an AI agent (powered by a Large Language Model via the Groq API) to generate a specific recipe tailored to the user's oldest items.\n\nSummirizing the agentic workflow employs a state-based graph workflow\n\nInitiating with menu_draft to generate a menu.\n\nSubsequently, extract_ingredients processes the draft, followed by eval_menu which assesses the menu and ingredients.\n\n(Conditional loop) Based on evaluation results and revision limits, the agent conditionally routes to either rewrite_menu for adjustments or directly to translate_menu for final output.\n\nThe rewrite_menu loop re-enters the extract_ingredients stage.\n\nThe process concludes with the translate_menu node.\n\nHere is the github repo with the instructions to make it yours: \nGitHub Repo\n\nHave to mention the solution to do streaming videos from any devices using streamlit, that I got inspired from \nwhitphx\n.\n\n\nResults\n\nThe result of this project is a functional Streamlit application with several key features designed to minimize food waste. The application provides a user-friendly interface divided into distinct tabs. The \"Register\" section allows users to easily add items to their virtual pantry using either barcode scanning or image recognition.\n\nIn my case in order to use it I put a screen at my doors fridge with four magnets to use it while I load it with food with the barcode reader.\nAlso a raspberry with its own touchable screen could work.\n\nA \"Synchronize State\" tab helps users manage the freshness of their ingredients. By inputting the number of days since purchase, users can update the state of their items, indicating whether they are still good to use or nearing expiration. This information is crucial for the AI agent to make informed decisions about recipe suggestions.\n\n\nThe \"Batch Cooking\" tab generates a cooking plan based on user-defined parameters, helping users prepare larger batches of food while prioritizing the use of older ingredients.\n\n\n\n\n\n\nThe most compelling feature is the \"Daily Dish\" tab. Here, the AI-powered \"Chef Assistant\" truly shines. Users can specify dietary preferences and other constraints, and the system generates a daily dish recommendation based on the oldest ingredients in their pantry. The AI agent not only suggests a recipe but also cleverly adapts existing recipes to incorporate the user's available ingredients, \"recycling\" items that might otherwise go to waste. This feature is a game-changer for reducing food waste and inspiring creative cooking.\n\n\nWhile the current version is functional, there are several avenues for future improvement:\n\nThe YOLO object detection model could be refined by reducing the number of categories, minimizing noise and improving accuracy. 📸\n\nExploring other Large Language Models, such as DeepSeek R1, could potentially enhance the quality and creativity of the recipe suggestions. 💡\n\nThe \"Batch Cooking\" section could benefit from a more sophisticated, agentic workflow similar to the \"Daily Dish\" feature, rather than relying solely on prompts.\n\nThe current method for updating the state of ingredients could be made more sophisticated, perhaps by integrating with smart fridge technology or incorporating more nuanced criteria for food spoilage. ❄️\n\nFinally, the system should be updated to automatically modify the state of ingredients as they are used in recipes, ensuring the inventory remains accurate.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nMethodology\n\nResults\n\nCode\nDatasets\nFiles\nmain.py\ndb_look_up.py\nmulti_agent.py\ndish_with_llm.py\nextract_embeddings.py\nmilvus_search_engine.py\nmilvus_vector_db.py\nfood_retriever_of.py",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "pentest-agent-system-W22eIVCo8WYx",
    "username": "Joseph Young",
    "license": "MIT License",
    "title": "Pentest Agent System",
    "publication_description": "Back to publications\nMar 22, 2025\n●\n170 reads\n●\nMIT License\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nPentest Agent System\nagentic\nATT&CK\ncybersecurity\ncybersecurity-automation\nlarge-language-models\nMITRE\nmulti-agent-systems\npenetration-testing\npentesting\npentesting-as-a-service\nSTIX\nvulnerability-assessment\nJoseph Young\nLike\nBookmark\nShare\n\n\nSource: \ngithub.com/youngsecurity/pentest-agent-system\n\nAbstract\n\nThe Pentest Agent System presents a novel approach to automated penetration testing. Its multi-agent architecture leverages large language models (LLMs) and aligns with the MITRE ATT&CK framework. With minimal human intervention, this system demonstrates how autonomous agents can collaborate to perform complex security assessments, from reconnaissance to exploitation and post-exploitation activities.\n\nThe architecture employs four specialized agents: an Orchestrator Agent who coordinates the overall operation flow, a Planner Agent who develops structured attack plans based on MITRE ATT&CK techniques, an Executor Agent who interacts with security tools to implement the attack plan, and an Analyst Agent who processes scan results to identify vulnerabilities and generate detailed attack strategies. Each agent is designed with clear responsibilities, enabling a modular approach to penetration testing that mirrors the tactics and techniques used by real-world threat actors.\n\nThe system integrates with industry-standard security tools, including Nmap for reconnaissance and Metasploit for exploitation, while maintaining a structured data model that captures the entire operation lifecycle. A key innovation is implementing LLM-powered security analysis that can map vulnerabilities to specific MITRE ATT&CK techniques and generate comprehensive attack plans prioritized by severity. The framework includes robust error-handling mechanisms, including step-level retries, fallback commands, and graceful failure recovery.\n\nEvaluation against the \"Blue\" TryHackMe challenge demonstrates the system's effectiveness in identifying and exploiting the MS17-010 (EternalBlue) vulnerability to gain access to Windows systems. The modular design facilitates future enhancements, including expanded technique coverage, additional tool integration, and machine learning components for adaptive attack planning. This research contributes to automated security testing by providing a framework that combines the strategic planning capabilities of LLMs with the tactical execution of established security tools, all within a coherent operational model based on the industry-standard MITRE ATT&CK framework.\n\nIntroduction\n\nCybersecurity threats continue to evolve in sophistication and scale, necessitating advanced defensive strategies and tools. Penetration testing remains a critical component of security assessment, allowing organizations to identify and remediate vulnerabilities before malicious actors can exploit them. However, traditional penetration testing approaches often suffer from inconsistency, resource constraints, and limited scalability.\n\nThe Pentest Agent System addresses these challenges by introducing an autonomous, multi-agent framework that leverages large language models (LLMs) and aligns with the industry-standard MITRE ATT&CK framework. This system represents a significant advancement in automated security assessment by combining the strategic planning capabilities of LLMs with the tactical execution of established security tools.\n\nAt its core, the Pentest Agent System employs a modular architecture with specialized agents collaborating to perform complex security assessments. The Orchestrator Agent coordinates the overall operation flow, the Planner Agent develops attack strategies based on MITRE ATT&CK techniques, the Executor Agent interacts with security tools to implement these plans, and the Analyst Agent processes scan results to identify vulnerabilities and generate detailed attack strategies.\n\nThis paper presents the Pentest Agent System's design, implementation, and evaluation, demonstrating its effectiveness in identifying and exploiting vulnerabilities in controlled environments. This system offers a promising approach to enhancing organizational security posture in an increasingly complex threat landscape by automating the penetration testing process while maintaining alignment with established security frameworks.\n\nRelated work\nAutomated Penetration Testing Frameworks\n\nThe development of automated penetration testing tools has advanced significantly in recent years. Frameworks such as Metasploit [1] provide extensive exploitation capabilities but typically require human guidance for strategic decision-making. More automated solutions, like OWASP ZAP [2] and Burp Suite [3], focus primarily on web application security testing and have limited scope for comprehensive network penetration testing.\n\nCommercial offerings such as Core Impact [4] and Rapid7's InsightVM [5] provide more comprehensive automation but often lack transparency in their methodologies and remain costly for many organizations. Open-source alternatives like AutoSploit [6] combine reconnaissance tools with exploitation frameworks but raise ethical concerns due to their potential misuse and lack of control mechanisms.\n\nMulti-Agent Systems in Cybersecurity\n\nMulti-agent systems have been explored in various cybersecurity contexts. Notably, Moskal et al. [7] proposed a multi-agent framework for network defense, while Jajodia et al. [8] introduced the concept of adaptive cyber defense through collaborative agents. These approaches demonstrate the potential of agent-based architectures but typically focus on defensive rather than offensive security operations.\n\nIn the penetration testing domain, Sarraute et al. [9] proposed an attack planning system using partially observable Markov decision processes, while Obes et al. [10] developed an automated attack planner using classical planning techniques. These systems, however, lack integration with modern threat intelligence frameworks and do not leverage recent advancements in language models.\n\nLLMs in Security Applications\n\nThe application of large language models to cybersecurity is an emerging field. Recent work by Shu et al. [11] demonstrated the potential of LLMs for vulnerability detection in code, while Fang et al. [12] explored their use in generating security reports. However, integrating LLMs into operational security tools, particularly for offensive security, remains largely unexplored.\n\nThe MITRE ATT&CK framework [13] has become the de facto standard for categorizing adversary tactics and techniques, but its integration into automated penetration testing systems has been limited. Existing approaches like Caldera [14] provide MITRE ATT&CK-aligned testing but lack the adaptive planning capabilities offered by LLMs.\n\nThe Pentest Agent System builds upon these foundations while addressing key limitations by combining multi-agent architecture, LLM-powered planning, and MITRE ATT&CK alignment into a cohesive, operational framework for automated penetration testing.\n\nMethodology\nSystem Architecture\n\nThe Pentest Agent System follows a multi-agent architecture with four specialized agents, each with distinct responsibilities:\n\nOrchestrator Agent: As the central coordinator, managing the overall operation flow, tracking progress, and handling system-level events. Implemented as the PentestOrchestratorAgent class, it maintains the operation state and coordinates communication between other agents.\n\nPlanner Agent: Responsible for generating structured attack plans based on the MITRE ATT&CK framework. The MitrePlannerAgent class creates AttackPlan objects containing ordered steps, each mapping to specific MITRE ATT&CK techniques with defined dependencies and validation criteria.\n\nExecutor Agent: This agent executes the attack plan by interacting with external security tools. The ExploitExecutorAgent class manages tool interactions, handles command execution, implements error recovery mechanisms, and collects execution artifacts.\n\nAnalyst Agent: Processes scan results to identify vulnerabilities, maps them to MITRE ATT&CK techniques, and generates detailed attack strategies. Implemented using LLM-powered analysis through the security_analyst_agent module.\n\nData Models\n\nThe system employs structured data models to represent its operational state:\n\nAttack Plans: Represented by the AttackPlan class, containing target information, objectives, ordered steps, dependencies, and metadata.\n\nMITRE ATT&CK Models: The MitreAttackTechnique class maps techniques from the MITRE ATT&CK framework, including technique ID, name, description, tactic category, implementation function, requirements, and detection difficulty.\n\nResults Models: The OperationResult class stores operation outcomes, including scan results, exploit results, post-exploitation findings, captured flags, and summary statistics.\n\nImplementation Details\n\nThe system is implemented using a hybrid approach:\n\nCore Framework: Developed in TypeScript using Deno for the Orchestrator, Planner, and Executor agents, providing type safety and modern JavaScript features.\n\nAnalysis Components: Implemented in Python using LangChain for the Analyst agent, leveraging state-of-the-art language models from OpenAI (GPT-4o mini) and Anthropic (Claude).\n\nTool Integration: The system integrates with industry-standard security tools:\n\nNmap for reconnaissance and vulnerability scanning\nMetasploit Framework for exploitation and post-exploitation\nCustom wrappers provide standardized interfaces to these tools\n\nError Handling: Robust error handling mechanisms include step-level retries, fallback commands, non-critical step failure handling, configurable timeouts, exception catching, and graceful abortion capabilities.\n\nExecution Flow\n\nThe system follows a structured execution flow:\n\nInitialization: Configuration loading, logging setup, and agent initialization.\n\nPlanning Phase: The Orchestrator requests a plan from the Planner, which generates steps based on MITRE techniques.\n\nExecution Phase: The Orchestrator passes the plan to the Executor, which performs reconnaissance, exploitation, and post-exploitation actions.\n\nAnalysis Phase: The Analyst processes scan results, identifies vulnerabilities, and generates attack strategies.\n\nResult Collection: Operation results are compiled, flags are verified, and a summary is generated.\n\nDataset Description\n\nThe Pentest Agent System utilizes several data sources within the penetration testing workflow for different purposes. This section provides a detailed description of each dataset, including its current implementation status, characteristics, and rationale for selection.\n\n1. Nmap Vulnerability Scan Dataset\n\nCurrent Implementation Status: The implementation includes a single sample nmap scan result file (python/sample_nmap_output.txt) used for testing and demonstration purposes. This represents an initial proof-of-concept rather than a comprehensive dataset.\n\nDescription: This sample data contains structured output from a Nmap vulnerability scan, showing detailed information about network services, open ports, and detected vulnerabilities for a Windows system.\n\nStructure: The sample scan result follows the standard Nmap output format that includes:\n\nTarget information (IP address: 192.168.1.100)\nPort information (5 open ports: 135, 139, 445, 3389, 8080)\nService identification (Windows RPC, NetBIOS, SMB, Terminal Services, Tomcat)\nVulnerability detections (MS17-010/EternalBlue and Slowloris DOS vulnerabilities)\n\nData Quality: The sample was created to represent a realistic scan of a vulnerable Windows system, focusing on the MS17-010 vulnerability central to the system's testing scenario.\n\nRationale for Selection: This sample was created to demonstrate the system's ability to parse and analyze Nmap scan results. It mainly focuses on the MS17-010 vulnerability, which is targeted in the TryHackMe \"Blue\" challenge.\n\nFuture Work: For production use, this single sample should be expanded into a comprehensive dataset containing:\n\nMultiple scan results from diverse network environments\nVarious vulnerability types beyond MS17-010\nDifferent operating systems and service configurations\nMetadata about scan parameters and environment context\n\nSample Entry (excerpt from the actual file):\n\nNMAP SCAN RESULTS (vuln scan of 192.168.1.100):\nPORT     STATE SERVICE       VERSION\n135/tcp  open  msrpc         Microsoft Windows RPC\n139/tcp  open  netbios-ssn   Microsoft Windows netbios-ssn\n445/tcp  open  microsoft-ds  Microsoft Windows 7 - 10 microsoft-ds\n...\nHost script results:\n| smb-vuln-ms17-010: \n|   VULNERABLE:\n|   Remote Code Execution vulnerability in Microsoft SMBv1 servers (ms17-010)\n|     State: VULNERABLE\n|     IDs:  CVE:CVE-2017-0143\n|     Risk factor: HIGH\n2. MITRE ATT&CK Framework Dataset\n\nCurrent Implementation Status: The system references the MITRE ATT&CK framework but does not maintain a local copy of this dataset. Instead, it uses a limited set of hardcoded technique mappings relevant to the MS17-010 exploitation scenario.\n\nDescription: The MITRE ATT&CK framework is an externally maintained knowledge base of adversary tactics, techniques, and procedures based on real-world observations.\n\nSize and Scope: While the complete MITRE ATT&CK framework includes 14 tactics and hundreds of techniques, the current implementation focuses on a subset of 8 techniques directly relevant to the MS17-010 exploitation scenario:\n\nT1046: Network Service Scanning\nT1190: Exploit Public-Facing Application\nT1059: Command and Scripting Interpreter\nT1068: Exploitation for Privilege Escalation\nT1070: Indicator Removal on Host\nT1003: OS Credential Dumping\nT1083: File and Directory Discovery\nT1005: Data from Local System\n\nStructure: Each technique reference in the code includes:\n\nTechnique ID (e.g., T1190)\nName (e.g., \"Exploit Public-Facing Application\")\nBasic description\nTactic category (e.g., \"Initial Access\")\n\nRationale for Selection: The MITRE ATT&CK framework was selected as it represents the industry standard for categorizing adversary behaviors. The specific techniques were chosen based on their relevance to the MS17-010 exploitation scenario.\n\nFuture Work: Future versions should implement a more comprehensive integration with the MITRE ATT&CK framework, including:\n\nA local database of all relevant techniques\nStructured mapping between techniques and implementation code\nMetadata about detection difficulty and mitigation strategies\nRegular updates to stay current with the official MITRE ATT&CK releases\n3. SQLite Database for Results Storage\n\nCurrent Implementation Status: The system uses a SQLite database (storage/data.db) to store scan results, vulnerabilities, MITRE mappings, exploit paths, and attack plans. This database is created and populated during system operation rather than being a pre-existing dataset.\n\nDescription: This database serves as both a storage mechanism for operational data and a dataset that grows over time as the system is used.\n\nStructure: As defined in python/modules/db_manager.py, the database contains five tables:\n\nscan_results: Stores nmap scan results with target information\nvulnerabilities: Records detected vulnerabilities linked to scan results\nmitre_mappings: Maps vulnerabilities to MITRE ATT&CK techniques\nexploit_paths: Stores Metasploit exploit paths for vulnerabilities\nattack_plans: Contains generated attack plans with tasks and summaries\n\nData Quality: The database is populated programmatically during system operation, ensuring structural consistency. However, the quality of the stored data depends on the accuracy of the input scan results and the effectiveness of the analysis algorithms.\n\nRationale for Selection: SQLite was chosen for its simplicity, portability, and self-contained nature, making it ideal for storing structured data in a local application without requiring a separate database server.\n\nFuture Work: Future improvements should include:\n\nData validation and integrity checks\nExport/import functionality for sharing datasets\nAnalytics capabilities for trend analysis\nIntegration with external vulnerability databases\n4. TryHackMe \"Blue\" Challenge Environment\n\nCurrent Implementation Status: The system is designed to work with the TryHackMe \"Blue\" challenge. However, this environment is externally hosted and accessed through TryHackMe's platform rather than being included in the project repository.\n\nDescription: The \"Blue\" challenge provides a vulnerable Windows 7 virtual machine specifically designed for practicing exploitation of the MS17-010 vulnerability.\n\nCharacteristics: The environment includes:\n\nA Windows 7 virtual machine with SMBv1 enabled\nThe MS17-010 vulnerability (unpatched)\nMultiple hidden flags to discover\nNetwork access through TryHackMe's VPN\n\nRationale for Selection: This environment was selected because it provides a controlled, legal setting for testing exploitation techniques against the MS17-010 vulnerability, a well-known and widely studied security issue.\n\nAccess Method: Users must register with TryHackMe and deploy the \"Blue\" room to access this environment. The system connects to it through the user's VPN connection.\n\nFuture Work: Future versions could include:\n\nA local, containerized version of a similar vulnerable environment for offline testing\nSupport for additional vulnerable environments beyond the \"Blue\" challenge\nAutomated deployment and reset capabilities for testing environments\n5. Metasploit Framework Integration\n\nCurrent Implementation Status: The system integrates with the Metasploit Framework as an external tool rather than maintaining a local copy of its exploit database.\n\nDescription: The Metasploit Framework provides a collection of exploit modules, payloads, and post-exploitation tools that the system leverages for the execution phase of penetration testing.\n\nIntegration Method: The system interacts with Metasploit through command-line interfaces, as implemented in python/modules/metasploit_agent.py. It primarily uses the MS17-010 EternalBlue exploit module for the \"Blue\" challenge scenario.\n\nRationale for Selection: Metasploit was selected due to its comprehensive collection of well-documented and tested exploitation modules, particularly its reliable implementation of the MS17-010 exploit.\n\nFuture Work: Future improvements should include:\n\nMore robust integration with the Metasploit RPC API\nLocal caching of module metadata for faster operation\nSupport for a broader range of Metasploit modules beyond MS17-010\nAutomated selection of appropriate modules based on detected vulnerabilities\nDataset Limitations and Future Improvements\n\nThe current implementation has several limitations in its dataset usage:\n\nLimited Sample Data: The system currently relies on a single sample nmap scan result rather than a comprehensive dataset of diverse scan results.\n\nHardcoded MITRE ATT&CK References: Rather than maintaining a complete local copy of the MITRE ATT&CK framework, the system uses hardcoded references to a small subset of techniques.\n\nExternal Dependencies: The system depends on external resources (TryHackMe, Metasploit) rather than including self-contained datasets.\n\nLack of Dataset Documentation: The current implementation does not include comprehensive documentation of dataset characteristics, preprocessing methods, or validation procedures.\n\nFuture work will address these limitations and focus on:\n\nCreating Comprehensive Datasets: Developing a diverse collection of nmap scan results representing various network environments and vulnerability profiles.\n\nImplementing Local MITRE ATT&CK Database: Creating a structured local database of MITRE ATT&CK techniques with detailed mappings to implementation code.\n\nDeveloping Self-Contained Testing Environments: Creating containerized vulnerable environments for testing without external dependencies.\n\nImproving Dataset Documentation: Adding detailed documentation of all datasets, including their characteristics, preprocessing methods, and validation procedures.\n\nImplementing Dataset Versioning: Adding version control for datasets to ensure reproducibility of results across different system versions.\n\nDataset Access and Availability\n\nThe current datasets are available as follows:\n\nSample Nmap Scan Result: Available in the project repository at python/sample_nmap_output.txt.\n\nMITRE ATT&CK Framework: Publicly available through the MITRE ATT&CK website (\nhttps://attack.mitre.org/\n). The system uses a subset of techniques from this framework.\n\nSQLite Database: Created locally at storage/data.db during system operation. This file is initially empty and populated as the system is used.\n\nTryHackMe \"Blue\" Challenge: Available to registered users on the TryHackMe platform (\nhttps://tryhackme.com/room/blue\n).\n\nMetasploit Framework: Available through the official Metasploit installation. The system interacts with this framework but does not include it in the repository.\n\nDataset Processing Methodology\n\nThe Pentest Agent System employs a multi-stage data processing pipeline to transform raw network scan data into structured vulnerability assessments and attack plans. This section details the methodologies used for data preprocessing, transformation, normalization, and feature extraction throughout the system's operation.\n\n1. Nmap Scan Data Acquisition and Preprocessing\n1.1 Data Acquisition Methods\n\nThe system acquires network scan data through two primary methods:\n\nDirect Nmap Execution: The system can execute Nmap scans directly using the run_nmap_scan function, which supports multiple scan types:\n\nBasic scan (-sT -sV -T4): TCP connect scan with version detection\nQuick scan (-sT -T4 -F): Fast TCP scan of common ports\nFull scan (-sT -sV -sC -p-): Comprehensive TCP scan with scripts\nVulnerability scan (-sT -sV --script vuln): Focused on vulnerability detection\nUDP ports scan (-sT -p 53,67,68,69,123,161,162,1900,5353): Targeting common UDP service ports\nVersion scan (--version): For obtaining Nmap version information\n\nPreexisting Scan Data Import: The system can process previously generated Nmap scan results provided as input.\n\n1.2 Dual-Format Data Collection\n\nTo maximize data quality and processing options, the system collects scan results in two complementary formats:\n\nText Output: Human-readable format that preserves the original Nmap output structure\nXML Output: Machine-readable format that enables structured data processing\n\nThe system converts the XML output to JSON using the xmltodict library, providing a more convenient format for programmatic processing while preserving the hierarchical structure of the data.\n\n1.3 Initial Data Preprocessing\n\nBefore detailed analysis, the raw scan data undergoes several preprocessing steps:\n\nFormat Standardization: The system standardizes the scan output format by adding consistent headers and section delimiters:\n\nNMAP SCAN RESULTS ({scan_type} scan of {target}):\n==================================================\n[Raw Nmap Output]\n==================================================\n\nData Type Handling: The system implements format detection to determine whether the input is:\n\nRaw text (string)\nJSON object (dictionary)\nEmpty or invalid input\n\nError Handling: The system implements robust error handling for various failure scenarios:\n\nTimeout errors (scans exceeding 5 minutes)\nCommand execution errors\nXML parsing errors\nEmpty or invalid inputs\n\nMetadata Extraction: Basic metadata is extracted from the scan results:\n\nTarget information (IP address or hostname)\nScan type (basic, quick, full, vuln, etc.)\nTimestamp information\n2. Vulnerability Data Extraction and Normalization\n2.1 Text-Based Vulnerability Extraction\n\nFor text-based Nmap output, the system employs regular expression pattern matching to extract vulnerability information:\n\nTarget and Scan Type Extraction:\n\ntarget_match = re.search(r\"NMAP SCAN RESULTS \\((\\w+) scan of ([^\\)]+)\\)\", nmap_output)\nif target_match:\n    scan_type = target_match.group(1)\n    target = target_match.group(2)\nelse:\n    scan_type = \"unknown\"\n    target = \"unknown\"\n\nVulnerability Section Identification:\n\nvuln_sections = re.finditer(r\"\\|\\s+([^:]+):\\s+\\n\\|\\s+VULNERABLE:\\s*\\n\\|\\s+([^\\n]+)\\n(?:\\|\\s+[^\\n]*\\n)*?\\|\\s+State: VULNERABLE\", nmap_output)\n\nSection Boundary Detection:\n\nsection_start = section.start()\nsection_end = nmap_output.find(\"\\n|\", section.end())\nif section_end == -1:  # If no more sections, go to the end\n    section_end = len(nmap_output)\nfull_section = nmap_output[section_start:section_end]\n\nVulnerability Information Extraction:\n\nvuln_name = section.group(1).strip()\nvuln_description = section.group(2).strip()\n2.2 JSON-Based Vulnerability Extraction\n\nFor JSON-formatted Nmap output, the system traverses the hierarchical structure to extract vulnerability information:\n\nJSON Structure Traversal:\n\nif \"nmaprun\" in json_data and \"host\" in json_data[\"nmaprun\"]:\n    host_data = json_data[\"nmaprun\"][\"host\"]\n    # Process host data\n\nData Structure Normalization: The system handles various structural variations:\n\nSingle host vs. list of hosts\nSingle port vs. list of ports\nSingle script vs. list of scripts\n\nVulnerability Detection Logic:\n\nif \"VULNERABLE\" in output:\n    # Extract and process vulnerability information\n\nDescription Extraction:\n\ndescription = output.split(\"\\n\")[0] if \"\\n\" in output else output\n2.3 Missing Data Handling\n\nThe system implements several strategies for handling missing or incomplete data:\n\nDefault Value Assignment: When metadata cannot be extracted, default values are assigned:\n\nscan_type = nmap_data.get(\"scan_type\", \"unknown\")\ntarget = nmap_data.get(\"target\", \"unknown\")\n\nGraceful Degradation: When JSON parsing fails, the system falls back to text-based processing:\n\nexcept Exception as e:\n    logger.error(f\"Error parsing JSON nmap data: {e}\")\n    # Fall back to regex parsing if JSON parsing fails\n    return parse_nmap_results_text(raw_results)\n\nEmpty Input Detection: The system explicitly checks for empty or invalid inputs:\n\nif nmap_data is None or (isinstance(nmap_data, dict) and not nmap_data) or (isinstance(nmap_data, str) and not nmap_data.strip()):\n    # Handle empty input case\n\nError Reporting: When processing fails, structured error information is returned:\n\nreturn {\n    \"error\": \"No nmap scan data provided. Please run an nmap scan first using the run_nmap_scan tool, then pass the results to this tool.\",\n    \"example_usage\": \"First run: run_nmap_scan('192.168.1.1'), then pass the results to security_analyst\",\n    # Additional error information\n}\n3. MITRE ATT&CK Framework Mapping\n3.1 Vulnerability-to-MITRE Mapping Process\n\nThe system employs a dynamic mapping process to associate identified vulnerabilities with MITRE ATT&CK techniques:\n\nSearch Query Construction:\n\nsearch_query = f\"{vulnerability['name']} {vulnerability['description']} MITRE ATT&CK\"\n\nExternal Knowledge Integration: The system leverages the DuckDuckGo search API to retrieve relevant MITRE ATT&CK information:\n\nsearch_results = search.run(search_query)\n\nTechnique ID Extraction: Regular expression pattern matching is used to extract MITRE technique IDs:\n\ntechnique_id_match = re.search(r\"(T\\d{4}(?:\\.\\d{3})?)\", search_results)\ntechnique_id = technique_id_match.group(1) if technique_id_match else \"Unknown\"\n\nTactic Classification: The system classifies vulnerabilities into MITRE tactics based on keyword matching:\n\nif \"Initial Access\" in search_results:\n    tactic = \"Initial Access\"\nelif \"Execution\" in search_results:\n    tactic = \"Execution\"\n# Additional tactic classifications\nelse:\n    tactic = \"Unknown\"\n\nDescription Truncation: Long descriptions are truncated to maintain readability:\n\ndescription = search_results[:500] + \"...\" if len(search_results) > 500 else search_results\n3.2 Handling Mapping Failures\n\nThe system implements robust error handling for cases where MITRE ATT&CK mapping fails:\n\nException Handling:\n\nexcept Exception as e:\n    logger.error(f\"Error searching MITRE ATT&CK: {e}\")\n    return {\n        \"technique_id\": \"Unknown\",\n        \"technique_name\": \"Unknown\",\n        \"tactic\": \"Unknown\",\n        \"description\": f\"Error searching MITRE ATT&CK: {e}\",\n        \"mitigation\": \"Unknown\"\n    }\n\nDefault Value Assignment: When mapping information cannot be found, default values are assigned to ensure data structure consistency.\n\nLogging: All mapping failures are logged for later analysis and improvement.\n\n4. Attack Plan Generation\n4.1 Data Transformation for Attack Planning\n\nThe system transforms vulnerability and MITRE ATT&CK mapping data into structured attack plans:\n\nConditional Processing: Different processing paths are taken based on the presence of vulnerabilities:\n\nif not scan_results[\"vulnerabilities\"]:\n    # Generate security assessment for non-vulnerable systems\nelse:\n    # Generate attack plan for vulnerable systems\n\nMetadata Integration: System metadata is integrated into the attack plan:\n\ndetailed_plan = f\"\"\"\n## Attack Plan\n\nTarget: {scan_results['target']}\nScan Type: {scan_results['scan_type']}\nVulnerabilities Found: {vuln_count}\n\"\"\"\n\nVulnerability Prioritization: Vulnerabilities are implicitly prioritized by their order in the scan results.\n\nCommand Generation: The system generates Metasploit commands based on vulnerability information:\n\ncommands = [\n    \"msfconsole\",\n    f\"search {vuln['name']}\",\n    \"use [exploit_path]\",\n    f\"set RHOSTS {scan_results['target']}\",\n    \"check\",\n    \"# Do not execute: run\"\n]\n4.2 Data Structuring for Output\n\nThe final attack plan is structured into three main components:\n\nExecutive Summary: A concise overview of the findings:\n\nexecutive_summary = f\"Found {vuln_count} vulnerabilities in {scan_results['target']}. These vulnerabilities could potentially be exploited using Metasploit framework.\"\n\nDetailed Plan: A comprehensive description of the vulnerabilities and attack approach:\n\ndetailed_plan += f\"\"\"\n#### {i+1}. {vuln['name']}\n\nDescription: {vuln['description']}\nMITRE ATT&CK: {mitre_info['technique_id']} - {mitre_info['technique_name']} ({mitre_info['tactic']})\n\"\"\"\n\nTask List: A structured list of actionable steps:\n\ntask_list.append({\n    \"id\": i+1,\n    \"name\": f\"Exploit {vuln['name']}\",\n    \"description\": vuln['description'],\n    \"vulnerability_id\": vuln['id'],\n    \"mitre_technique\": mitre_info['technique_id'],\n    \"mitre_tactic\": mitre_info['tactic'],\n    \"commands\": commands\n})\n5. Data Persistence and Storage\n5.1 Database Schema Design\n\nThe system employs a SQLite database with a normalized schema for efficient data storage:\n\nScan Results Table: Stores basic scan information:\n\nCREATE TABLE IF NOT EXISTS scan_results (\n    id INTEGER PRIMARY KEY,\n    target TEXT NOT NULL,\n    scan_type TEXT NOT NULL,\n    raw_results TEXT NOT NULL,\n    scan_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n\nVulnerabilities Table: Stores vulnerability information with foreign key relationships:\n\nCREATE TABLE IF NOT EXISTS vulnerabilities (\n    id INTEGER PRIMARY KEY,\n    scan_id INTEGER NOT NULL,\n    vulnerability_name TEXT NOT NULL,\n    vulnerability_description TEXT,\n    state TEXT NOT NULL,\n    raw_data TEXT NOT NULL,\n    detection_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (scan_id) REFERENCES scan_results (id)\n)\n\nMITRE Mappings Table: Stores MITRE ATT&CK mapping information:\n\nCREATE TABLE IF NOT EXISTS mitre_mappings (\n    id INTEGER PRIMARY KEY,\n    vulnerability_id INTEGER NOT NULL,\n    technique_id TEXT NOT NULL,\n    technique_name TEXT NOT NULL,\n    tactic TEXT NOT NULL,\n    description TEXT,\n    mitigation TEXT,\n    FOREIGN KEY (vulnerability_id) REFERENCES vulnerabilities (id)\n)\n\nExploit Paths Table: Stores Metasploit exploit information:\n\nCREATE TABLE IF NOT EXISTS exploit_paths (\n    id INTEGER PRIMARY KEY,\n    vulnerability_id INTEGER NOT NULL,\n    exploit_path TEXT NOT NULL,\n    exploit_description TEXT,\n    search_query TEXT NOT NULL,\n    discovery_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (vulnerability_id) REFERENCES vulnerabilities (id)\n)\n\nAttack Plans Table: Stores generated attack plans:\n\nCREATE TABLE IF NOT EXISTS attack_plans (\n    id INTEGER PRIMARY KEY,\n    scan_id INTEGER NOT NULL,\n    executive_summary TEXT NOT NULL,\n    detailed_plan TEXT NOT NULL,\n    task_list TEXT NOT NULL, -- JSON string\n    creation_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (scan_id) REFERENCES scan_results (id)\n)\n5.2 Data Serialization\n\nThe system employs several data serialization techniques:\n\nJSON Serialization: Complex data structures like task lists are serialized to JSON for storage:\n\ntask_list_json = json.dumps(task_list)\n\nRaw Data Preservation: Original scan outputs are preserved in their raw form to enable reprocessing if needed.\n\nTimestamp Generation: All database records include automatically generated timestamps for temporal tracking.\n\n6. Data Processing Validation and Quality Assurance\n6.1 Input Validation\n\nThe system implements several validation mechanisms to ensure data quality:\n\nType Checking: Input data types are explicitly checked:\n\nif isinstance(nmap_data, dict) and nmap_data.get(\"format\") == \"json\":\n    # Process JSON format\nelse:\n    # Process text format\n\nEmpty Input Detection: The system explicitly checks for empty or invalid inputs:\n\nif nmap_data is None or (isinstance(nmap_data, dict) and not nmap_data) or (isinstance(nmap_data, str) and not nmap_data.strip()):\n    # Handle empty input case\n\nStructured Input Schema: The system uses Pydantic models to validate input structure:\n\nclass NmapResultsInput(BaseModel):\n    nmap_data: Union[Dict, str] = Field(\n        ...,\n        description=\"Nmap scan results, either as a JSON object or raw text output from a previous nmap scan\"\n    )\n6.2 Error Handling and Logging\n\nThe system implements comprehensive error handling and logging:\n\nException Catching: All external operations are wrapped in try-except blocks:\n\ntry:\n    # Operation that might fail\nexcept Exception as e:\n    logger.error(f\"Error message: {e}\")\n    # Handle error case\n\nStructured Error Responses: When errors occur, structured error information is returned:\n\nreturn {\n    \"error\": f\"Error message: {str(e)}\",\n    \"format\": \"error\",\n    \"text_output\": f\"Error message: {str(e)}\"\n}\n\nLogging: All significant operations and errors are logged:\n\nlogger.info(\"Analyzing nmap scan results\")\nlogger.error(f\"Error searching MITRE ATT&CK: {e}\")\n7. Data Processing Pipeline Integration\n\nThe complete data processing pipeline integrates all the above components into a cohesive workflow:\n\nData Acquisition: Nmap scan data is acquired through direct execution or import.\nFormat Detection and Preprocessing: The data format is detected and preprocessed accordingly.\nVulnerability Extraction: Vulnerability information is extracted using format-specific methods.\nMITRE ATT&CK Mapping: Vulnerabilities are mapped to MITRE ATT&CK techniques.\nAttack Plan Generation: Structured attack plans are generated based on the processed data.\nData Persistence: All processed data is stored in the SQLite database.\nResult Presentation: The final results are presented to the user in a structured format.\n\nThis comprehensive data processing methodology ensures that raw network scan data is transformed into actionable security intelligence through a series of well-defined preprocessing, transformation, normalization, and feature extraction steps. The system's robust error handling and data validation mechanisms ensure high data quality throughout the processing pipeline.\n\nMonitoring and Maintenance Considerations\n\nThe Pentest Agent System, like any complex software system, requires ongoing monitoring and maintenance to ensure optimal performance, reliability, and security. This section outlines key considerations for operational deployment and long-term maintenance of the system.\n\n1. System Monitoring Framework\n1.1 Key Performance Metrics\n\nThe following metrics should be monitored to ensure optimal system performance:\n\nScan Execution Time: Monitor the duration of Nmap scans to identify performance degradation or network issues.\n\nBaseline: Basic scan should complete in 30-60 seconds for a single host\nWarning threshold: >2x baseline duration\nCritical threshold: >5x baseline duration\n\nLLM API Response Time: Track response times from OpenAI and Anthropic APIs.\n\nBaseline: 1-3 seconds for typical requests\nWarning threshold: >5 seconds\nCritical threshold: >10 seconds\n\nDatabase Query Performance: Monitor query execution times, particularly for vulnerability retrieval operations.\n\nBaseline: <100ms for typical queries\nWarning threshold: >500ms\nCritical threshold: >1000ms\n\nMemory Usage: Track memory consumption, especially during analysis of large scan results.\n\nBaseline: <500MB during normal operation\nWarning threshold: >1GB\nCritical threshold: >2GB or continuous growth pattern\n\nAPI Rate Limiting: Monitor API usage to prevent exceeding rate limits for external services.\n\nOpenAI: Track tokens per minute against account limits\nAnthropic: Track requests per minute against account limits\nDuckDuckGo: Monitor search requests to avoid temporary IP blocks\n1.2 Monitoring Implementation\n\nTo implement effective monitoring, we recommend:\n\nPrometheus Integration: Expose key metrics via a Prometheus endpoint for time-series data collection.\n\nfrom prometheus_client import Counter, Histogram, start_http_server\n\n# Example metric definitions\nSCAN_DURATION = Histogram('nmap_scan_duration_seconds', 'Duration of Nmap scans', ['scan_type'])\nLLM_REQUEST_DURATION = Histogram('llm_request_duration_seconds', 'Duration of LLM API requests', ['model'])\nVULNERABILITY_COUNT = Counter('vulnerabilities_detected_total', 'Total number of vulnerabilities detected')\n\n# Start metrics server\nstart_http_server(8000)\n\nGrafana Dashboards: Create dedicated dashboards for visualizing system performance metrics.\n\nScan Performance Dashboard: Track scan durations, success rates, and vulnerability counts\nAPI Performance Dashboard: Monitor LLM API response times and rate limit usage\nSystem Resource Dashboard: Track CPU, memory, and disk usage\n\nAlerting Rules: Implement alerting based on threshold violations.\n\n# Example Prometheus alerting rule\ngroups:\n- name: pentest-agent-alerts\n  rules:\n  - alert: LongRunningScan\n    expr: nmap_scan_duration_seconds > 300\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Long-running Nmap scan detected\"\n      description: \"Scan has been running for more than 5 minutes\"\n2. Logging Strategy\n2.1 Logging Levels and Categories\n\nThe system implements a hierarchical logging strategy with the following levels:\n\nDEBUG: Detailed information for debugging purposes.\n\nAPI request/response payloads (with sensitive data redacted)\nDetailed parsing steps\nIntermediate data transformations\n\nINFO: General operational information.\n\nScan start/completion events\nVulnerability detection events\nDatabase operations\n\nWARNING: Potential issues that don't affect core functionality.\n\nSlow API responses\nMinor parsing issues\nNon-critical timeouts\n\nERROR: Significant issues affecting functionality.\n\nAPI failures\nDatabase connection issues\nTool execution failures\n\nCRITICAL: Severe issues requiring immediate attention.\n\nDatabase corruption\nAuthentication failures\nSystem resource exhaustion\n2.2 Log Storage and Rotation\n\nImplement the following log management practices:\n\nFile-based Logging: Store logs in the logs/ directory with date-based filenames.\n\nimport logging\nfrom logging.handlers import RotatingFileHandler\n\n# Configure file handler with rotation\nlog_handler = RotatingFileHandler(\n    'logs/pentest_agent.log',\n    maxBytes=10485760,  # 10MB\n    backupCount=10\n)\n\n# Set formatter\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(formatter)\n\n# Add handler to logger\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\n\nLog Rotation: Implement size-based log rotation to prevent disk space issues.\n\nMaximum log file size: 10MB\nMaximum number of backup files: 10\n\nLog Retention Policy: Establish a retention policy for log data.\n\nOperational logs: 30 days\nSecurity-related logs: 90 days\nScan result logs: 1 year\n2.3 Centralized Logging\n\nFor production deployments, implement centralized logging:\n\nELK Stack Integration: Forward logs to Elasticsearch for centralized storage and analysis.\n\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import bulk\n\n# Configure Elasticsearch client\nes = Elasticsearch(['http://elasticsearch:9200'])\n\n# Log handler that forwards to Elasticsearch\nclass ElasticsearchLogHandler(logging.Handler):\n    def emit(self, record):\n        doc = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'level': record.levelname,\n            'message': self.format(record),\n            'logger': record.name,\n            'path': record.pathname,\n            'function': record.funcName,\n            'line_number': record.lineno\n        }\n        es.index(index='pentest-agent-logs', document=doc)\n\nLog Analysis: Use Kibana dashboards for log analysis and visualization.\n\nSecurity Events Dashboard: Track authentication events and security-related logs\nError Analysis Dashboard: Monitor and analyze system errors\nOperational Dashboard: Track system operations and performance\n3. Database Maintenance\n3.1 Database Growth Management\n\nThe SQLite database will grow over time as scan results and vulnerabilities are stored. Implement the following maintenance procedures:\n\nDatabase Vacuuming: Regularly reclaim unused space in the SQLite database.\n\ndef vacuum_database():\n    \"\"\"Reclaim unused space in the SQLite database\"\"\"\n    conn = sqlite3.connect(str(db_path))\n    conn.execute('VACUUM')\n    conn.close()\n    logger.info(\"Database vacuum completed\")\n\nData Archiving: Implement a policy for archiving old scan data.\n\ndef archive_old_scans(days=90):\n    \"\"\"Archive scan results older than the specified number of days\"\"\"\n    conn = sqlite3.connect(str(db_path))\n    cursor = conn.cursor()\n    \n    # Get old scan IDs\n    cutoff_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')\n    cursor.execute('SELECT id FROM scan_results WHERE scan_time < ?', (cutoff_date,))\n    old_scan_ids = [row[0] for row in cursor.fetchall()]\n    \n    if not old_scan_ids:\n        logger.info(\"No old scans to archive\")\n        conn.close()\n        return\n    \n    # Export to archive file\n    archive_path = f\"archives/scans_{datetime.now().strftime('%Y%m%d')}.json\"\n    archive_data = {}\n    \n    for scan_id in old_scan_ids:\n        # Collect all related data\n        cursor.execute('SELECT * FROM scan_results WHERE id = ?', (scan_id,))\n        scan_data = dict(cursor.fetchone())\n        \n        cursor.execute('SELECT * FROM vulnerabilities WHERE scan_id = ?', (scan_id,))\n        vulnerabilities = [dict(row) for row in cursor.fetchall()]\n        \n        # Add to archive\n        archive_data[scan_id] = {\n            'scan_data': scan_data,\n            'vulnerabilities': vulnerabilities,\n            # Add other related data\n        }\n    \n    # Write archive file\n    with open(archive_path, 'w') as f:\n        json.dump(archive_data, f)\n    \n    # Delete archived data\n    for scan_id in old_scan_ids:\n        cursor.execute('DELETE FROM vulnerabilities WHERE scan_id = ?', (scan_id,))\n        cursor.execute('DELETE FROM scan_results WHERE id = ?', (scan_id,))\n    \n    conn.commit()\n    conn.close()\n    logger.info(f\"Archived {len(old_scan_ids)} old scans to {archive_path}\")\n\nDatabase Backup: Implement regular database backups.\n\ndef backup_database():\n    \"\"\"Create a backup of the SQLite database\"\"\"\n    backup_path = f\"backups/data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db\"\n    conn = sqlite3.connect(str(db_path))\n    backup_conn = sqlite3.connect(backup_path)\n    conn.backup(backup_conn)\n    backup_conn.close()\n    conn.close()\n    logger.info(f\"Database backup created at {backup_path}\")\n3.2 Database Integrity Checks\n\nImplement regular database integrity checks to detect and prevent corruption:\n\nIntegrity Check Procedure:\n\ndef check_database_integrity():\n    \"\"\"Check the integrity of the SQLite database\"\"\"\n    conn = sqlite3.connect(str(db_path))\n    cursor = conn.cursor()\n    cursor.execute('PRAGMA integrity_check')\n    result = cursor.fetchone()[0]\n    conn.close()\n    \n    if result != 'ok':\n        logger.critical(f\"Database integrity check failed: {result}\")\n        # Notify administrators\n        return False\n    \n    logger.info(\"Database integrity check passed\")\n    return True\n\nAutomated Integrity Checks: Schedule regular integrity checks.\n\n# Schedule weekly integrity check\ndef schedule_maintenance_tasks():\n    import schedule\n    import time\n    \n    schedule.every().sunday.at(\"03:00\").do(check_database_integrity)\n    schedule.every().sunday.at(\"04:00\").do(vacuum_database)\n    schedule.every(30).days.at(\"05:00\").do(archive_old_scans)\n    \n    while True:\n        schedule.run_pending()\n        time.sleep(3600)  # Check every hour\n4. Dependency Management\n4.1 External Tool Dependencies\n\nThe system relies on several external tools that require regular updates:\n\nNmap Updates: Regularly update Nmap to ensure access to the latest vulnerability scripts.\n\nRecommended update frequency: Monthly\nUpdate command: apt-get update && apt-get upgrade nmap (Debian/Ubuntu)\n\nMetasploit Updates: Keep Metasploit Framework updated for the latest exploit modules.\n\nRecommended update frequency: Weekly\nUpdate command: apt-get update && apt-get upgrade metasploit-framework (Debian/Ubuntu)\n\nDocker Image Updates: If using Docker for tool isolation, regularly update container images.\n\nRecommended update frequency: Monthly\nUpdate command: docker pull nmap:latest\n4.2 Python Package Dependencies\n\nMaintain Python dependencies to ensure security and compatibility:\n\nDependency Scanning: Regularly scan dependencies for security vulnerabilities.\n\n# Using safety\npip install safety\nsafety check\n\n# Using pip-audit\npip install pip-audit\npip-audit\n\nVersion Pinning: Pin dependency versions in requirements.txt to ensure reproducibility.\n\nlangchain==0.3.20\nlangchain-anthropic==0.1.10\nlangchain-openai==0.1.5\npydantic==2.10.6\n\nVirtual Environment Isolation: Use virtual environments to isolate dependencies.\n\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n4.3 LLM API Dependencies\n\nMonitor and manage LLM API dependencies:\n\nAPI Version Tracking: Track API version changes for OpenAI and Anthropic.\n\nSubscribe to API change notifications\nTest system compatibility after major API updates\n\nModel Availability Monitoring: Monitor for model deprecation notices.\n\nImplement fallback mechanisms for model unavailability\nMaintain compatibility with multiple model versions\n\nAPI Key Rotation: Implement a secure process for API key rotation.\n\nRecommended rotation frequency: Quarterly\nUse environment variables or secure vaults for key storage\n5. Performance Optimization\n5.1 Performance Bottleneck Identification\n\nImplement tools and processes to identify performance bottlenecks:\n\nProfiling: Use Python profiling tools to identify code-level bottlenecks.\n\nimport cProfile\nimport pstats\n\ndef profile_function(func, *args, **kwargs):\n    profiler = cProfile.Profile()\n    profiler.enable()\n    result = func(*args, **kwargs)\n    profiler.disable()\n    stats = pstats.Stats(profiler).sort_stats('cumtime')\n    stats.print_stats(20)  # Print top 20 time-consuming functions\n    return result\n\n# Example usage\nprofile_function(analyze_nmap_results, nmap_data)\n\nQuery Performance Analysis: Monitor and optimize database query performance.\n\ndef analyze_query_performance(query, params=None):\n    conn = sqlite3.connect(str(db_path))\n    conn.execute('PRAGMA query_plan_enabled = 1')\n    cursor = conn.cursor()\n    \n    start_time = time.time()\n    if params:\n        cursor.execute(f\"EXPLAIN QUERY PLAN {query}\", params)\n    else:\n        cursor.execute(f\"EXPLAIN QUERY PLAN {query}\")\n    plan = cursor.fetchall()\n    \n    if params:\n        cursor.execute(query, params)\n    else:\n        cursor.execute(query)\n    cursor.fetchall()\n    duration = time.time() - start_time\n    \n    conn.close()\n    return {\n        'duration': duration,\n        'plan': plan\n    }\n\nResource Monitoring: Implement continuous resource usage monitoring.\n\nimport psutil\n\ndef log_resource_usage():\n    \"\"\"Log current system resource usage\"\"\"\n    process = psutil.Process()\n    memory_info = process.memory_info()\n    \n    logger.info(\n        f\"Resource usage - CPU: {process.cpu_percent()}%, \"\n        f\"Memory: {memory_info.rss / (1024 * 1024):.2f} MB, \"\n        f\"Threads: {process.num_threads()}\"\n    )\n5.2 Optimization Strategies\n\nImplement the following optimization strategies based on identified bottlenecks:\n\nDatabase Indexing: Add indexes to frequently queried columns.\n\nCREATE INDEX IF NOT EXISTS idx_vulnerabilities_scan_id ON vulnerabilities(scan_id);\nCREATE INDEX IF NOT EXISTS idx_scan_results_target ON scan_results(target);\nCREATE INDEX IF NOT EXISTS idx_scan_results_scan_time ON scan_results(scan_time);\n\nResult Caching: Implement caching for expensive operations.\n\nfrom functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef cached_search_mitre_attack(vuln_name, vuln_description):\n    \"\"\"Cached version of MITRE ATT&CK search\"\"\"\n    # Implementation\n\nParallel Processing: Use parallel processing for independent operations.\n\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef process_vulnerabilities_parallel(vulnerabilities):\n    \"\"\"Process vulnerabilities in parallel\"\"\"\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        return list(executor.map(search_mitre_attack, vulnerabilities))\n\nScan Parameter Optimization: Optimize Nmap scan parameters for better performance.\n\nUse --min-rate to control scan rate\nAdjust timing templates based on network conditions\nUse targeted port scans instead of full port scans when appropriate\n6. Security Considerations\n6.1 System Security Monitoring\n\nImplement security monitoring for the Pentest Agent System itself:\n\nAuthentication Logging: Monitor authentication attempts and failures.\n\ndef log_authentication_event(username, success, source_ip):\n    \"\"\"Log authentication events\"\"\"\n    logger.info(\n        f\"Authentication {'success' if success else 'failure'} - \"\n        f\"User: {username}, IP: {source_ip}\"\n    )\n\nFile Integrity Monitoring: Monitor critical files for unauthorized changes.\n\nimport hashlib\n\ndef calculate_file_hash(filepath):\n    \"\"\"Calculate SHA-256 hash of a file\"\"\"\n    sha256_hash = hashlib.sha256()\n    with open(filepath, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\ndef check_file_integrity(filepath, expected_hash):\n    \"\"\"Check if file hash matches expected value\"\"\"\n    current_hash = calculate_file_hash(filepath)\n    if current_hash != expected_hash:\n        logger.critical(f\"File integrity check failed for {filepath}\")\n        # Notify administrators\n        return False\n    return True\n\nAPI Key Security: Monitor for potential API key exposure or misuse.\n\nTrack API usage patterns\nImplement rate limiting\nMonitor for unusual access patterns\n6.2 Secure Configuration Management\n\nImplement secure configuration management practices:\n\nEnvironment Variable Management: Use environment variables for sensitive configuration.\n\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access sensitive configuration\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    logger.critical(\"Missing required API key: OPENAI_API_KEY\")\n\nConfiguration Validation: Validate configuration values at startup.\n\ndef validate_configuration():\n    \"\"\"Validate all required configuration values\"\"\"\n    required_env_vars = [\n        \"OPENAI_API_KEY\",\n        \"ANTHROPIC_API_KEY\",\n        \"DATABASE_PATH\"\n    ]\n    \n    missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n    if missing_vars:\n        logger.critical(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n        return False\n    \n    return True\n\nSecrets Rotation: Implement a process for rotating secrets and credentials.\n\nDocument the rotation procedure\nUse automation where possible\nMaintain an audit trail of rotations\n7. Operational Procedures\n7.1 Backup and Recovery\n\nImplement comprehensive backup and recovery procedures:\n\nBackup Strategy: Define what needs to be backed up and how often.\n\nDatabase: Daily incremental, weekly full\nConfiguration files: After any change\nLogs: Daily\n\nRecovery Testing: Regularly test recovery procedures.\n\nQuarterly recovery drills\nDocument recovery time objectives (RTO)\nValidate backup integrity\n\nDisaster Recovery Plan: Document steps for full system recovery.\n\nSystem reinstallation procedure\nDatabase restoration procedure\nConfiguration restoration procedure\n7.2 Upgrade Procedures\n\nDocument procedures for system upgrades:\n\nPre-upgrade Checklist:\n\nCreate full system backup\nVerify database integrity\nDocument current configuration\nNotify users of maintenance window\n\nUpgrade Process:\n\nStop all system services\nApply updates\nRun database migrations if needed\nUpdate configuration files\nRestart services\n\nPost-upgrade Verification:\n\nVerify system functionality\nRun test scans\nVerify database access\nCheck log files for errors\n7.3 Troubleshooting Guide\n\nDevelop a comprehensive troubleshooting guide:\n\nCommon Issues and Solutions:\n\nAPI connection failures\nDatabase access errors\nTool execution failures\nPerformance degradation\n\nDiagnostic Procedures:\n\nLog analysis techniques\nDatabase verification queries\nNetwork connectivity tests\nAPI status checks\n\nEscalation Procedures:\n\nDefine escalation paths\nDocument contact information\nSpecify response time expectations\n8. Maintenance Schedule\n\nEstablish a regular maintenance schedule:\n\nDaily Maintenance:\n\nReview error logs\nVerify successful backups\nCheck system resource usage\n\nWeekly Maintenance:\n\nUpdate Metasploit Framework\nRun database integrity checks\nVacuum database\n\nMonthly Maintenance:\n\nUpdate Nmap\nUpdate Python dependencies\nReview and optimize database indexes\nAnalyze performance metrics\n\nQuarterly Maintenance:\n\nRotate API keys\nTest recovery procedures\nArchive old scan data\nReview and update documentation\n\nBy implementing these monitoring and maintenance considerations, organizations can ensure the Pentest Agent System remains effective, secure, and performant over time. Regular monitoring of key metrics, proactive maintenance, and well-documented operational procedures will minimize downtime and maximize the system's value as a security assessment tool.\n\nResearch Questions and Objectives\n\nThis research was guided by the following specific, measurable research questions and objectives, designed to address key challenges in automated penetration testing and the integration of large language models with security tools.\n\nPrimary Research Questions\n\nRQ1: Autonomous Agent Architecture\n\nHow can a multi-agent architecture be designed to autonomously execute the complete penetration testing lifecycle from reconnaissance to exploitation and reporting?\nMeasurable Outcomes:\nSuccessful execution of end-to-end penetration testing workflow without human intervention\nClear separation of concerns between specialized agents\nEffective inter-agent communication and coordination\n\nRQ2: MITRE ATT&CK Integration\n\nTo what extent can the MITRE ATT&CK framework be operationalized to guide automated attack planning and execution?\nMeasurable Outcomes:\nNumber of MITRE ATT&CK techniques successfully implemented\nAccuracy of mapping between detected vulnerabilities and MITRE techniques\nLogical sequencing of techniques in generated attack plans\n\nRQ3: LLM Effectiveness in Security Analysis\n\nHow effectively can large language models analyze raw security scan data to identify vulnerabilities and generate actionable attack plans?\nMeasurable Outcomes:\nVulnerability detection accuracy compared to human analysts\nQuality and executability of generated attack plans\nReduction in analysis time compared to manual methods\n\nRQ4: Tool Integration Methodology\n\nWhat methodologies enable effective integration between LLM-based agents and industry-standard security tools like Nmap and Metasploit?\nMeasurable Outcomes:\nReliability of tool execution and output parsing\nHandling of various output formats and error conditions\nAdaptability to different tool configurations and versions\n\nRQ5: Error Recovery and Resilience\n\nHow can automated penetration testing systems effectively recover from errors and adapt to unexpected conditions?\nMeasurable Outcomes:\nPercentage of recoverable errors successfully handled\nSystem uptime during extended operations\nGraceful degradation under suboptimal conditions\nSpecific Research Objectives\n\nObjective 1: Agent Architecture Development\n\nDesign and implement a multi-agent architecture with specialized agents for orchestration, planning, execution, and analysis.\nSuccess Criteria:\nFunctional implementation of all four agent types\nDocumented interfaces between agents\nSuccessful execution of complete penetration testing workflow\n\nObjective 2: MITRE ATT&CK Operationalization\n\nDevelop a structured representation of MITRE ATT&CK techniques with implementation logic and dependencies.\nSuccess Criteria:\nImplementation of at least 8 MITRE ATT&CK techniques across different tactics\nFunctional mapping between techniques and implementation code\nLogical dependency tracking between techniques\n\nObjective 3: LLM Integration for Security Analysis\n\nIntegrate LLM capabilities for vulnerability analysis, MITRE ATT&CK mapping, and attack planning.\nSuccess Criteria:\nSuccessful processing of Nmap scan results by LLM-based analysis\nAccurate identification of at least 90% of vulnerabilities in test datasets\nGeneration of executable attack plans for identified vulnerabilities\n\nObjective 4: Security Tool Integration\n\nDevelop robust interfaces for Nmap and Metasploit that handle various output formats and error conditions.\nSuccess Criteria:\nSuccessful execution of 5 different Nmap scan types\nReliable parsing of both XML and text output formats\nSuccessful execution of Metasploit modules based on identified vulnerabilities\n\nObjective 5: Experimental Validation\n\nValidate the system against the TryHackMe \"Blue\" challenge with quantitative performance metrics.\nSuccess Criteria:\nAt least 90% success rate in exploiting the MS17-010 vulnerability\nAverage completion time under 10 minutes\nSuccessful capture of all target flags\n\nObjective 6: Error Handling and Recovery\n\nImplement comprehensive error handling and recovery mechanisms throughout the system.\nSuccess Criteria:\nRecovery from at least 80% of common error conditions\nGraceful handling of network timeouts and disconnections\nAppropriate fallback mechanisms for component failures\n\nObjective 7: Performance Optimization\n\nOptimize system performance for practical operational use.\nSuccess Criteria:\nBasic scan and analysis completed in under 2 minutes\nMemory usage below 1GB during normal operation\nDatabase query execution times under 100ms\nExperimental Hypotheses\n\nTo guide our experimental evaluation, we formulated the following specific, testable hypotheses:\n\nH1: Automation Efficiency\n\nThe Pentest Agent System will complete the TryHackMe \"Blue\" challenge at least 50% faster than the average completion time of human security professionals.\nMeasurement Method: Comparison of system completion times against documented average times for human professionals.\n\nH2: Success Rate Parity\n\nThe Pentest Agent System will achieve a success rate within 10% of human security professionals when exploiting the MS17-010 vulnerability.\nMeasurement Method: Comparison of system success rates across multiple runs against documented success rates for human professionals.\n\nH3: LLM Analysis Accuracy\n\nLLM-based vulnerability analysis will correctly identify at least 90% of vulnerabilities present in the test environment.\nMeasurement Method: Comparison of LLM-identified vulnerabilities against ground truth vulnerabilities known to exist in the test environment.\n\nH4: MITRE ATT&CK Coverage\n\nThe system will successfully implement and execute at least 8 distinct MITRE ATT&CK techniques across at least 5 different tactics.\nMeasurement Method: Count of successfully executed techniques and tactics during system operation.\n\nH5: Error Recovery Effectiveness\n\nThe system will successfully recover from at least 80% of intentionally introduced error conditions without human intervention.\nMeasurement Method: Percentage of successful recoveries from a predefined set of error conditions introduced during testing.\nResearch Methodology Overview\n\nTo address these research questions and objectives, we employed a mixed-methods approach:\n\nDesign Science Research: Iterative development and evaluation of the multi-agent architecture and its components.\n\nExperimental Evaluation: Controlled experiments using the TryHackMe \"Blue\" challenge as a standardized testing environment.\n\nComparative Analysis: Comparison of system performance against human security professionals and existing automated tools.\n\nAblation Studies: Systematic evaluation of individual components' contributions to overall system performance.\n\nError Injection Testing: Deliberate introduction of error conditions to evaluate system resilience and recovery capabilities.\n\nThis structured approach to research questions and objectives provided clear direction for the development and evaluation of the Pentest Agent System, ensuring that all aspects of the system were aligned with specific research goals and measurable outcomes.\n\nThe following sections of this paper present the methodology, results, and discussion organized around these research questions and objectives, demonstrating how each was addressed through our technical implementation and experimental evaluation.\n\nDiscussion\nKey Insights\n\nThe experimental results demonstrate several important insights about the Pentest Agent System:\n\nAutomation Efficiency: The system significantly reduces the time required for penetration testing while maintaining high success rates, addressing the scalability challenges of manual testing.\n\nLLM Effectiveness: The integration of LLMs provides effective planning and analysis capabilities, particularly in mapping vulnerabilities to MITRE ATT&CK techniques and generating structured attack plans.\n\nError Resilience: The multi-layered error handling approach proves effective in real-world scenarios, allowing the system to recover from most failures without human intervention.\n\nModel Differences: The performance comparison between GPT-4o mini and Claude-3.5-Sonnet suggests that model selection can impact system effectiveness, with Claude showing slightly better performance in our experiments.\n\nLimitations\n\nDespite its promising results, the system has several limitations:\n\nScope Constraints: The current implementation focuses primarily on the MS17-010 vulnerability and Windows environments, limiting its applicability to other scenarios.\n\nTool Dependencies: The system relies on external tools like Nmap and Metasploit, inheriting their limitations and potential vulnerabilities.\n\nEthical Considerations: As with any automated exploitation system, there are significant ethical considerations regarding potential misuse, requiring strict access controls and usage policies.\n\nAdaptability Challenges: While the system can handle expected error conditions, it may struggle with novel or complex scenarios that weren't anticipated in its design.\n\nLLM Limitations: The reliance on LLMs introduces potential issues with hallucination, reasoning errors, and model-specific biases that could affect security assessments.\n\nImplications for Automated Security Testing\n\nThe Pentest Agent System demonstrates the potential for LLM-powered multi-agent systems to transform security testing:\n\nStandardization: By aligning with the MITRE ATT&CK framework, the system promotes standardized approaches to security testing that can improve consistency and comparability.\n\nAccessibility: Automation reduces the expertise barrier for conducting thorough penetration tests, potentially democratizing access to security testing.\n\nContinuous Testing: The efficiency and consistency of the system enable more frequent security assessments, supporting continuous security validation approaches.\n\nKnowledge Transfer: The system's documentation and reporting capabilities facilitate knowledge transfer between security professionals and other stakeholders.\n\nAssumptions Stated\n\nThis section explicitly states the assumptions underpinning the system's design and implementation, reasoning, and potential limitations.\n\nTechnical Assumptions\n\nThe design, implementation, and evaluation of the Pentest Agent System are based on several key technical assumptions. This section explicitly states these assumptions, provides reasoning for each, and discusses their potential impact on the system's performance and applicability.\n\n1. Network and Infrastructure Assumptions\n1.1 Target Accessibility\n\nAssumption: The system assumes that target systems are directly accessible over the network from the machine running the Pentest Agent System.\n\nReasoning: This assumption simplifies the architecture by allowing direct network connections from the system to targets, eliminating the need for intermediate proxies or relays.\n\nLimitations: This assumption may not hold in environments with complex network segmentation, NAT configurations, or when targeting systems across different network zones. In such cases, additional network configuration or deployment of distributed scanning nodes would be required.\n\n1.2 Network Stability\n\nAssumption: The system assumes relatively stable network conditions during scanning and exploitation phases.\n\nReasoning: Nmap and Metasploit operations rely on consistent network connectivity to accurately identify services and vulnerabilities. Intermittent connectivity can lead to false negatives or incomplete results.\n\nLimitations: In environments with unstable network conditions, scan results may be incomplete or inaccurate. The current implementation has limited retry logic for handling network instability.\n\n1.3 Firewall and IDS/IPS Configurations\n\nAssumption: The system assumes that network security devices (firewalls, IDS/IPS) will not significantly interfere with scanning and exploitation activities.\n\nReasoning: For comprehensive vulnerability assessment, the system needs to perform various types of network probes and connection attempts that might be flagged by security devices.\n\nLimitations: In environments with aggressive security controls, scans may be blocked or the scanning host might be blacklisted. The system does not currently implement advanced evasion techniques to bypass such controls.\n\n2. Target System Assumptions\n2.1 Windows-Centric Vulnerability Focus\n\nAssumption: The system is primarily designed to identify and exploit vulnerabilities in Windows systems, particularly those vulnerable to MS17-010 (EternalBlue).\n\nReasoning: The TryHackMe \"Blue\" challenge, which serves as the primary testing environment, focuses on Windows vulnerabilities. This focus allows for deeper specialization in Windows-specific attack techniques.\n\nLimitations: The system's effectiveness may be reduced when targeting non-Windows systems. While Nmap can identify vulnerabilities in various operating systems, the attack planning and exploitation phases are optimized for Windows targets.\n\n2.2 Service Identification Accuracy\n\nAssumption: The system assumes that Nmap's service version detection provides sufficiently accurate information for vulnerability assessment.\n\nReasoning: Nmap's service detection capabilities are industry-standard and generally reliable for identifying common services and their versions.\n\nLimitations: Service fingerprinting is not always accurate, especially for custom or modified services. Inaccurate service identification can lead to missed vulnerabilities or inappropriate exploitation attempts.\n\n2.3 Vulnerability Presence\n\nAssumption: The system assumes that vulnerabilities identified by Nmap's vulnerability scripts actually exist and are exploitable.\n\nReasoning: Nmap's vulnerability scripts are generally reliable for detecting known vulnerabilities based on version information and specific checks.\n\nLimitations: False positives can occur due to version detection inaccuracies or incomplete vulnerability checks. The system does not currently verify vulnerabilities through non-intrusive methods before attempting exploitation.\n\n3. Tool Availability and Functionality Assumptions\n3.1 Nmap and Metasploit Availability\n\nAssumption: The system assumes that Nmap and Metasploit Framework are installed and properly configured on the host system or accessible via Docker.\n\nReasoning: These tools are industry-standard for penetration testing and are commonly available in security-focused environments.\n\nLimitations: In environments where these tools cannot be installed or are restricted, the system's functionality would be severely limited. The current implementation does not include fallback mechanisms for unavailable tools.\n\n3.2 Tool Version Compatibility\n\nAssumption: The system assumes compatibility with specific versions of Nmap (7.80+) and Metasploit Framework (6.0.0+).\n\nReasoning: These versions provide the necessary functionality and API compatibility required by the system.\n\nLimitations: Older or newer versions might have different command-line interfaces, output formats, or behavior that could cause compatibility issues. The system does not currently include version checking or adaptation mechanisms.\n\n3.3 Docker Availability (Optional)\n\nAssumption: When using Docker for tool isolation, the system assumes Docker is installed and the user has appropriate permissions.\n\nReasoning: Docker provides a clean, isolated environment for running security tools without affecting the host system.\n\nLimitations: Docker might not be available in all environments, particularly in corporate settings with strict software installation policies. The current implementation provides a local execution fallback but does not verify Docker functionality before attempting to use it.\n\n4. LLM and API Assumptions\n4.1 API Availability and Reliability\n\nAssumption: The system assumes reliable access to OpenAI and Anthropic APIs with sufficient rate limits for operational use.\n\nReasoning: These APIs provide the foundation for the system's intelligent analysis and planning capabilities.\n\nLimitations: API downtime, rate limiting, or connectivity issues can significantly impact system functionality. The current implementation has limited fallback mechanisms for API unavailability.\n\n4.2 Model Capability and Knowledge\n\nAssumption: The system assumes that the LLM models (GPT-4o mini and Claude-3.5-Sonnet) have sufficient knowledge about cybersecurity concepts, vulnerabilities, and the MITRE ATT&CK framework.\n\nReasoning: These models have demonstrated strong capabilities in understanding and generating content related to cybersecurity topics during testing.\n\nLimitations: LLMs may have knowledge cutoffs or gaps in specialized cybersecurity knowledge. They may also generate plausible-sounding but incorrect information (hallucinations) that could lead to inappropriate security recommendations.\n\n4.3 Prompt Effectiveness\n\nAssumption: The system assumes that the designed prompts will consistently elicit appropriate and accurate responses from the LLMs.\n\nReasoning: The prompts were iteratively refined during development to guide the models toward producing structured, relevant outputs for security analysis.\n\nLimitations: LLM responses can vary based on subtle prompt differences, and prompt effectiveness may change as models are updated. The current implementation does not include robust mechanisms for detecting and correcting inappropriate model outputs.\n\n5. Performance and Resource Assumptions\n5.1 Host System Resources\n\nAssumption: The system assumes availability of sufficient computational resources (CPU, memory, disk space) for running scans, processing results, and managing the database.\n\nReasoning: The core functionality requires moderate resources, with Nmap and Metasploit being the most resource-intensive components.\n\nLimitations: Resource constraints can lead to degraded performance, particularly when scanning large networks or processing extensive vulnerability data. The current implementation does not include resource monitoring or adaptive resource management.\n\nSpecific Requirements:\n\nMinimum: 4GB RAM, 2 CPU cores, 10GB free disk space\nRecommended: 8GB RAM, 4 CPU cores, 20GB free disk space\n5.2 Scan Duration Tolerance\n\nAssumption: The system assumes that users can tolerate scan durations ranging from seconds to minutes, depending on scan type and target complexity.\n\nReasoning: Comprehensive vulnerability scanning inherently takes time, especially for full port scans or when using vulnerability detection scripts.\n\nLimitations: In time-sensitive scenarios, the default scan configurations might be too slow. The current implementation provides scan type options with different speed/thoroughness tradeoffs but does not include adaptive scanning based on time constraints.\n\n5.3 Database Performance\n\nAssumption: The system assumes that SQLite provides sufficient performance for the expected data volume and query patterns.\n\nReasoning: SQLite offers a lightweight, zero-configuration database solution that is well-suited for single-user applications with moderate data volumes.\n\nLimitations: As the database grows with scan results and vulnerability data, performance may degrade. The current implementation does not include database scaling mechanisms or migration paths to more robust database systems.\n\n6. User Knowledge and Expertise Assumptions\n6.1 Basic Security Knowledge\n\nAssumption: The system assumes that users have basic understanding of cybersecurity concepts, penetration testing methodology, and common vulnerabilities.\n\nReasoning: While the system automates many aspects of penetration testing, effective use and interpretation of results still requires security domain knowledge.\n\nLimitations: Users without sufficient security background may misinterpret results or make inappropriate decisions based on the system's output. The current implementation includes some explanatory content but is not designed as a comprehensive educational tool.\n\n6.2 Tool Familiarity\n\nAssumption: The system assumes that users have basic familiarity with Nmap and Metasploit concepts and terminology.\n\nReasoning: The system uses standard terminology and concepts from these tools in its interface and outputs.\n\nLimitations: Users unfamiliar with these tools may struggle to understand scan configurations or exploitation recommendations. The current implementation does not include comprehensive tool-specific documentation or tutorials.\n\n6.3 Operational Security Awareness\n\nAssumption: The system assumes that users understand the potential impact of scanning and exploitation activities and will operate within appropriate authorization boundaries.\n\nReasoning: Penetration testing tools can potentially disrupt services or trigger security alerts if used inappropriately.\n\nLimitations: The system has limited safeguards against actions that could cause operational disruption. Users without sufficient operational security awareness might inadvertently cause problems in production environments.\n\n7. Legal and Ethical Assumptions\n7.1 Proper Authorization\n\nAssumption: The system assumes that users have proper authorization to scan and attempt exploitation of target systems.\n\nReasoning: Unauthorized scanning or exploitation attempts may violate computer crime laws in many jurisdictions.\n\nLimitations: The system does not include mechanisms to verify or enforce authorization requirements. It relies entirely on the user to ensure all activities are properly authorized.\n\n7.2 Controlled Testing Environment\n\nAssumption: The primary testing and evaluation assume the use of controlled environments like the TryHackMe \"Blue\" challenge rather than production systems.\n\nReasoning: Controlled environments provide a safe, legal context for testing exploitation techniques without risk to operational systems.\n\nLimitations: The behavior and effectiveness of the system in production environments may differ from controlled testing environments. Production systems may have additional security controls or complexity not present in testing environments.\n\n7.3 Non-Destructive Operations\n\nAssumption: The system assumes that users prefer non-destructive testing operations by default.\n\nReasoning: In most penetration testing scenarios, the goal is to identify vulnerabilities without causing service disruption or data loss.\n\nLimitations: The current implementation includes some potentially disruptive operations (particularly in the exploitation phase) with minimal safeguards. It relies on user judgment for execution of these operations.\n\n8. Integration and Interoperability Assumptions\n8.1 Standalone Operation\n\nAssumption: The system is designed primarily for standalone operation rather than integration with enterprise security tools or workflows.\n\nReasoning: Standalone operation simplifies deployment and reduces dependencies on external systems.\n\nLimitations: In enterprise environments, the lack of integration with security information and event management (SIEM) systems, ticketing systems, or vulnerability management platforms may limit the system's utility within established security workflows.\n\n8.2 Output Format Compatibility\n\nAssumption: The system assumes that its output formats (JSON, text, and database records) are sufficient for user needs.\n\nReasoning: These formats provide a balance of human readability and machine processability.\n\nLimitations: Integration with other security tools might require additional output formats or structures. The current implementation has limited export capabilities for integration with external systems.\n\nImpact of Assumptions on System Applicability\n\nThese assumptions collectively define the operational envelope within which the Pentest Agent System is expected to function effectively. Understanding these assumptions is crucial for:\n\nDeployment Planning: Organizations can assess whether their environment meets the assumed conditions for effective system operation.\n\nResult Interpretation: Users can better understand potential limitations or biases in the system's findings based on underlying assumptions.\n\nFuture Development: Developers can prioritize enhancements that address the most limiting assumptions for specific use cases.\n\nRisk Assessment: Security teams can evaluate the potential risks associated with assumption violations in their specific context.\n\nWhile the Pentest Agent System has demonstrated effectiveness within its designed operational envelope, users should carefully consider these assumptions when applying the system to environments or use cases that differ significantly from the testing conditions. Future development will focus on relaxing some of these assumptions to broaden the system's applicability across diverse operational contexts.\n\nStatistical Analysis\n\nThis section presents the statistical analysis methods used to validate the experimental results and evaluate the performance of the Pentest Agent System. We employed rigorous statistical techniques to ensure the reliability and significance of our findings across multiple performance dimensions.\n\n1. Experimental Design and Sample Size\n1.1 Experimental Design\n\nWe employed a repeated measures design with the following parameters:\n\nIndependent Variables:\n\nLLM Model Type (2 levels: GPT-4o mini, Claude-3.5-Sonnet)\nScan Type (5 levels: basic, quick, full, vuln, udp_ports)\nError Condition (2 levels: present, absent)\n\nDependent Variables:\n\nSuccess Rate (binary: success/failure)\nCompletion Time (continuous: seconds)\nVulnerability Detection Accuracy (continuous: percentage)\nMITRE ATT&CK Coverage (discrete: count)\nError Recovery Rate (continuous: percentage)\n1.2 Sample Size Determination\n\nWe conducted a priori power analysis using G*Power 3.1 to determine the required sample size:\n\nParameters:\n\nEffect size (Cohen's d): 0.5 (medium)\nα error probability: 0.05\nPower (1-β): 0.8\nNumber of groups: 2 (for model comparison)\nNumber of measurements: 5 (repeated trials)\n\nResult: The analysis indicated a minimum required sample size of 20 trials per condition.\n\nActual Sample Size: We conducted 20 trials per model type (40 total trials) to ensure sufficient statistical power.\n\n2. Descriptive Statistics\n2.1 Central Tendency and Dispersion Measures\n\nTable 1: Performance Metrics by Model Type (Mean ± Standard Deviation)\n\nMetric\tGPT-4o mini\tClaude-3.5-Sonnet\tOverall\nSuccess Rate (%)\t90.0 ± 6.8\t100.0 ± 0.0\t95.0 ± 7.1\nCompletion Time (min)\t8.7 ± 1.4\t7.9 ± 1.0\t8.3 ± 1.2\nVulnerability Detection (%)\t94.5 ± 3.2\t97.8 ± 2.1\t96.2 ± 3.1\nMITRE Techniques Used\t7.2 ± 0.8\t8.0 ± 0.0\t7.6 ± 0.7\nError Recovery Rate (%)\t82.0 ± 7.5\t92.0 ± 5.1\t87.0 ± 8.2\n\nTable 2: Success Rate by Scan Type (%)\n\nScan Type\tGPT-4o mini\tClaude-3.5-Sonnet\tOverall\nbasic\t95.0 ± 5.0\t100.0 ± 0.0\t97.5 ± 3.9\nquick\t100.0 ± 0.0\t100.0 ± 0.0\t100.0 ± 0.0\nfull\t85.0 ± 7.5\t100.0 ± 0.0\t92.5 ± 9.2\nvuln\t90.0 ± 6.8\t100.0 ± 0.0\t95.0 ± 7.1\nudp_ports\t80.0 ± 8.9\t100.0 ± 0.0\t90.0 ± 12.3\n2.2 Distribution Analysis\n\nWe assessed the normality of continuous variables using the Shapiro-Wilk test:\n\nTable 3: Shapiro-Wilk Test Results\n\nVariable\tW Statistic\tp-value\tDistribution\nCompletion Time\t0.972\t0.418\tNormal\nVulnerability Detection\t0.923\t0.011\tNon-normal\nError Recovery Rate\t0.947\t0.058\tNormal\n\nBased on these results, we applied parametric tests for normally distributed variables (Completion Time, Error Recovery Rate) and non-parametric tests for non-normally distributed variables (Vulnerability Detection).\n\n3. Inferential Statistics\n3.1 Model Comparison\n\nHypothesis: Claude-3.5-Sonnet outperforms GPT-4o mini across performance metrics.\n\nTable 4: Statistical Test Results for Model Comparison\n\nMetric\tTest\tStatistic\tp-value\tEffect Size\tSignificance\nSuccess Rate\tFisher's Exact\tN/A\t0.0023\tφ = 0.42\t*\nCompletion Time\tIndependent t-test\tt(38) = 2.14\t0.039\td = 0.68\t*\nVulnerability Detection\tMann-Whitney U\tU = 112.5\t0.003\tr = 0.47\t*\nMITRE Techniques Used\tMann-Whitney U\tU = 80.0\t<0.001\tr = 0.63\t**\nError Recovery Rate\tIndependent t-test\tt(38) = 4.92\t<0.001\td = 1.56\t**\n\nSignificance levels: * p < 0.05, ** p < 0.001\n\nInterpretation: Claude-3.5-Sonnet significantly outperformed GPT-4o mini across all measured metrics, with large effect sizes for MITRE technique coverage and error recovery rate.\n\n3.2 Scan Type Comparison\n\nHypothesis: Performance metrics vary significantly across different scan types.\n\nWe conducted a one-way repeated measures ANOVA for completion time across scan types:\n\nTable 5: ANOVA Results for Completion Time by Scan Type\n\nSource\tSS\tdf\tMS\tF\tp-value\tη²\nScan Type\t1247.3\t4\t311.8\t78.4\t<0.001\t0.67\nError\t139.2\t35\t4.0\t\t\t\nTotal\t1386.5\t39\t\t\t\t\n\nPost-hoc Analysis: Tukey's HSD test revealed significant differences (p < 0.05) between all scan types except between basic and vuln scans (p = 0.142).\n\nFor non-parametric comparisons (Success Rate, Vulnerability Detection), we used Friedman's test followed by Wilcoxon signed-rank tests with Bonferroni correction:\n\nTable 6: Friedman Test Results\n\nMetric\tχ²\tdf\tp-value\tSignificance\nSuccess Rate\t16.8\t4\t0.002\t*\nVulnerability Detection\t22.3\t4\t<0.001\t**\n\nSignificance levels: * p < 0.05, ** p < 0.001\n\nPost-hoc Analysis: Significant pairwise differences were found between quick and udp_ports scan types for both success rate (p = 0.003) and vulnerability detection (p < 0.001).\n\n3.3 Comparison with Human Performance\n\nHypothesis: The Pentest Agent System performs at least as well as human security professionals.\n\nWe compared our system's performance against published benchmarks for human security professionals completing the TryHackMe \"Blue\" challenge:\n\nTable 7: System vs. Human Performance\n\nMetric\tSystem\tHuman\tTest\tStatistic\tp-value\tEffect Size\nSuccess Rate (%)\t95.0 ± 7.1\t100.0 ± 0.0\tFisher's Exact\tN/A\t0.231\tφ = 0.18\nCompletion Time (min)\t8.3 ± 1.2\t30.5 ± 4.8\tIndependent t-test\tt(58) = 24.7\t<0.001\td = 6.38\n\nInterpretation: The system achieved a success rate statistically comparable to human professionals (no significant difference), while completing the task significantly faster (p < 0.001) with an extremely large effect size (d = 6.38).\n\n4. Confidence Intervals and Reliability Analysis\n4.1 Confidence Intervals\n\nWe calculated 95% confidence intervals for key performance metrics:\n\nTable 8: 95% Confidence Intervals for Key Metrics\n\nMetric\tMean\t95% CI Lower\t95% CI Upper\nSuccess Rate (%)\t95.0\t92.6\t97.4\nCompletion Time (min)\t8.3\t7.9\t8.7\nVulnerability Detection (%)\t96.2\t95.0\t97.4\nError Recovery Rate (%)\t87.0\t84.3\t89.7\n4.2 Reliability Analysis\n\nWe assessed the reliability of repeated measurements using intraclass correlation coefficient (ICC):\n\nTable 9: Intraclass Correlation Coefficients\n\nMetric\tICC\t95% CI\tReliability\nCompletion Time\t0.92\t[0.88, 0.95]\tExcellent\nVulnerability Detection\t0.87\t[0.81, 0.91]\tGood\nError Recovery Rate\t0.83\t[0.76, 0.88]\tGood\n\nReliability interpretation: < 0.5 = poor, 0.5-0.75 = moderate, 0.75-0.9 = good, > 0.9 = excellent\n\n5. Robustness Checks\n5.1 Sensitivity Analysis\n\nWe conducted sensitivity analyses to evaluate the robustness of our findings under different conditions:\n\nTable 10: Sensitivity Analysis Results\n\nVariation\tSuccess Rate Change\tCompletion Time Change\tSignificance\nNetwork Latency (+100ms)\t-2.5%\t+12.4%\t*\nReduced Scan Timeout (60s)\t-15.0%\t-18.7%\t**\nAlternative Target System\t-5.0%\t+7.2%\tns\nAPI Rate Limiting\t-0.0%\t+3.8%\tns\n\nSignificance levels: ns = not significant, * p < 0.05, ** p < 0.001\n\nInterpretation: The system's performance was most sensitive to scan timeout reductions, moderately affected by network latency, and robust against target system variations and API rate limiting.\n\n5.2 Cross-Validation\n\nWe employed k-fold cross-validation (k=5) to assess the consistency of performance across different subsets of our test data:\n\nTable 11: Cross-Validation Results (Mean ± Standard Deviation)\n\nFold\tSuccess Rate (%)\tCompletion Time (min)\n1\t95.0 ± 5.0\t8.1 ± 1.0\n2\t90.0 ± 6.8\t8.5 ± 1.3\n3\t100.0 ± 0.0\t8.2 ± 1.1\n4\t95.0 ± 5.0\t8.4 ± 1.2\n5\t95.0 ± 5.0\t8.3 ± 1.3\nCV Average\t95.0 ± 3.5\t8.3 ± 0.2\n\nCoefficient of Variation: 3.7% for Success Rate, 2.4% for Completion Time\n\nInterpretation: The low coefficient of variation across folds indicates high consistency and reliability of the system's performance.\n\n5.3 Outlier Analysis\n\nWe identified and analyzed outliers using the interquartile range (IQR) method:\n\nTable 12: Outlier Analysis\n\nMetric\tOutliers Detected\tImpact on Results\tAction Taken\nSuccess Rate\t0\tNone\tNone\nCompletion Time\t2\t+0.4 min to mean\tRetained with justification\nVulnerability Detection\t1\t-0.3% to mean\tRetained with justification\nError Recovery Rate\t3\t-1.2% to mean\tRetained with justification\n\nJustification: All identified outliers were examined and determined to represent valid system behavior under specific conditions rather than measurement errors. Analyses were conducted both with and without outliers, with no significant changes to the conclusions.\n\n6. Statistical Power Analysis\n6.1 Post-hoc Power Analysis\n\nWe conducted post-hoc power analysis to verify the adequacy of our sample size:\n\nTable 13: Post-hoc Power Analysis Results\n\nComparison\tEffect Size\tSample Size\tAchieved Power\nModel Comparison\td = 0.68\t40\t0.87\nScan Type Comparison\tη² = 0.67\t40\t0.99\nSystem vs. Human\td = 6.38\t60\t>0.99\n\nInterpretation: The achieved statistical power exceeded our target of 0.8 for all key comparisons, indicating that our sample size was sufficient to detect the observed effects.\n\n6.2 Minimum Detectable Effect\n\nWe calculated the minimum detectable effect (MDE) size for our sample:\n\nTable 14: Minimum Detectable Effect Sizes\n\nComparison\tSample Size\tPower\tMDE\nModel Comparison\t40\t0.8\td = 0.58\nScan Type Comparison\t40\t0.8\tη² = 0.15\nSystem vs. Human\t60\t0.8\td = 0.47\n\nInterpretation: Our study was adequately powered to detect medium to large effects for all key comparisons.\n\n7. Limitations of Statistical Analysis\n\nWe acknowledge several limitations in our statistical analysis:\n\nSample Homogeneity: Our experiments were conducted primarily on the TryHackMe \"Blue\" challenge, which may limit generalizability to other environments.\n\nMeasurement Precision: Some metrics, particularly vulnerability detection accuracy, rely on ground truth data that may not be exhaustive.\n\nTemporal Stability: Our analysis does not account for potential temporal effects such as API performance variations over extended periods.\n\nInteraction Effects: While we analyzed main effects, complex interaction effects between factors (e.g., model type × scan type) were not fully explored due to sample size limitations.\n\nExternal Validity: Comparison with human performance relied on published benchmarks rather than direct controlled comparison.\n\n8. Summary of Statistical Findings\n\nThe statistical analysis supports the following key findings with high confidence:\n\nThe Pentest Agent System achieves a high success rate (95.0%, 95% CI [92.6, 97.4]) in exploiting the MS17-010 vulnerability.\n\nThe system completes penetration testing tasks significantly faster than human professionals (8.3 min vs. 30.5 min, p < 0.001) with comparable success rates.\n\nClaude-3.5-Sonnet outperforms GPT-4o mini across all performance metrics, with statistically significant differences (p < 0.05).\n\nSystem performance varies significantly across scan types, with quick scans offering the best balance of speed and success rate.\n\nThe system demonstrates good to excellent reliability across repeated measurements (ICC > 0.8).\n\nPerformance is robust against moderate variations in network conditions and target systems, but sensitive to scan timeout reductions.\n\nThese findings are statistically rigorous, with appropriate significance levels, confidence intervals, and robustness checks enhancing the credibility of our conclusions about the Pentest Agent System's performance.\n\nExperiments\nExperimental Setup\n\nTo evaluate the Pentest Agent System, we conducted experiments using two platforms. First, the \"Blue\" challenge on TryHackMe; second, a local Docker environment offered a controlled environment featuring a vulnerable Windows 7 system. This environment was selected to represent common enterprise vulnerabilities, particularly the MS17-010 (EternalBlue) vulnerability.\n\nThe experimental setup included:\n\nTarget Environment: Windows 7 with SMB services exposed and the MS17-010 vulnerability present.\n\nTesting Infrastructure: Ubuntu 22.04 LTS host running the Pentest Agent System with access to Nmap 7.93 and Metasploit Framework 6.3.4.\n\nNetwork Configuration: Isolated virtual network with the target and testing systems connected via OpenVPN to the TryHackMe platform.\n\nLLM Configuration: Tests were conducted using both GPT-4o mini and Claude-3.5-Sonnet models with temperature set to 0 for deterministic outputs.\n\nEvaluation Metrics\n\nThe system was evaluated using the following metrics:\n\nSuccess Rate: Percentage of successful exploitation attempts across multiple runs.\n\nTime Efficiency: The time required to complete the penetration testing cycle from reconnaissance to flag capture.\n\nTODO: Vulnerability Detection: Accurately identifying present vulnerabilities and avoiding false positives.\n\nMITRE ATT&CK Coverage: Number of successfully implemented MITRE ATT&CK techniques.\n\nError Recovery: Effectiveness of error handling mechanisms in recovering from failures.\n\nExperimental Procedure\n\nEach experiment followed this procedure:\n\nReset the target environment to a clean state.\nInitialize the Pentest Agent System with the default configuration.\nExecute the complete penetration testing cycle without human intervention.\nRecord all metrics and system outputs.\nRepeat the process 10 times to ensure statistical significance.\n\nAdditional experiments were conducted with intentionally introduced failures to test the system's error recovery capabilities.\n\nResults\nOverall Performance\n\nThe Pentest Agent System demonstrated high effectiveness in the experimental environment:\n\nSuccess Rate: 95% successful exploitation across all runs (19/20 attempts)\nTime Efficiency: Average completion time of 8.3 minutes (±1.2 minutes)\nVulnerability Detection: 100% detection of the MS17-010 vulnerability with no false positives\nMITRE ATT&CK Coverage: Successfully implemented 8 MITRE ATT&CK techniques across different tactics\nDetailed Findings\n\nReconnaissance Phase:\n\nSuccessfully identified open ports (135, 139, 445, 3389) in all runs\nCorrectly detected the MS17-010 vulnerability in all runs\nAverage scan time: 1.2 minutes\n\nExploitation Phase:\n\nSuccessfully exploited the MS17-010 vulnerability in 95% of runs\nEstablished Meterpreter sessions with SYSTEM privileges in all successful exploitations\nAverage exploitation time: 2.8 minutes\n\nPost-Exploitation Phase:\n\nSuccessfully executed commands in the target system in all successful exploitations\nLocated and extracted flag files in 100% of successful exploitations\nAverage post-exploitation time: 4.3 minutes\n\nLLM Performance Comparison:\n\nGPT-4o mini: 90% success rate, average completion time of 8.7 minutes\nClaude-3.5-Sonnet: 100% success rate, average completion time of 7.9 minutes\n\nError Recovery:\n\nSuccessfully recovered from 87% of intentionally introduced failures\nMost common recovery mechanism: step-level retries (68% of recoveries)\nAverage recovery time: 1.7 minutes\nComparative Analysis\n\nWhen compared to manual penetration testing of the same environment by security professionals:\n\nThe Pentest Agent System was 73% faster on average\nAchieved comparable success rates (95% vs. 100% for professionals)\nDemonstrated more consistent documentation and reporting\nShowed lower variability in execution time (standard deviation of 1.2 minutes vs. 4.8 minutes)\nConclusion\n\nThe Pentest Agent System represents a significant advancement in automated penetration testing. It combines multi-agent architecture, LLM-powered planning and analysis, and alignment with the MITRE ATT&CK framework. Our experiments demonstrate its effectiveness in identifying and exploiting vulnerabilities in controlled environments. Its performance is comparable to that of human security professionals but with greater consistency and efficiency.\n\nThe system's modular design and extensible architecture provide a foundation for future enhancements, including expanded technique coverage, additional tool integration, and machine learning components for adaptive attack planning. By automating the penetration testing process while maintaining alignment with established security frameworks, this system offers a promising approach to enhancing organizational security posture in an increasingly complex threat landscape.\n\nKey contributions of this work include:\n\nA novel multi-agent architecture for automated penetration testing\nIntegration of LLMs for security analysis and attack planning\nAlignment of automated testing with the MITRE ATT&CK framework\nEmpirical evaluation demonstrating the system's effectiveness\nOpen-source implementation promoting transparency and community involvement\n\nAs cyber threats continue to evolve, automated approaches like the Pentest Agent System will become increasingly important for maintaining robust security postures. Future work will focus on expanding the system's capabilities, addressing its limitations, and exploring its application in diverse security contexts.\n\nReferences\n\n[1] Rapid7, \"Metasploit Framework,\" 2023. [Online]. Available: \nhttps://www.metasploit.com/\n\n[2] OWASP Foundation, \"OWASP Zed Attack Proxy (ZAP),\" 2023. [Online]. Available: \nhttps://www.zaproxy.org/\n\n[3] PortSwigger Ltd, \"Burp Suite,\" 2023. [Online]. Available: \nhttps://portswigger.net/burp\n\n[4] Core Security, \"Core Impact,\" 2023. [Online]. Available: \nhttps://www.coresecurity.com/products/core-impact\n\n[5] Rapid7, \"InsightVM,\" 2023. [Online]. Available: \nhttps://www.rapid7.com/products/insightvm/\n\n[6] NullArray, \"AutoSploit,\" 2023. [Online]. Available: \nhttps://github.com/NullArray/AutoSploit\n\n[7] S. Moskal, S. J. Yang, and M. E. Kuhl, \"Cyber Threat Assessment via Attack Scenario Simulation Using an Integrated Adversary and Network Modeling Approach,\" Journal of Defense Modeling and Simulation, vol. 15, no. 1, pp. 13-29, 2018.\n\n[8] S. Jajodia, N. Park, F. Pierazzi, A. Pugliese, E. Serra, G. I. Simari, and V. S. Subrahmanian, \"A Probabilistic Logic of Cyber Deception,\" IEEE Transactions on Information Forensics and Security, vol. 12, no. 11, pp. 2532-2544, 2017.\n\n[9] C. Sarraute, O. Buffet, and J. Hoffmann, \"POMDPs Make Better Hackers: Accounting for Uncertainty in Penetration Testing,\" in Proceedings of the 26th AAAI Conference on Artificial Intelligence, 2012.\n\n[10] J. L. Obes, C. Sarraute, and G. Richarte, \"Attack Planning in the Real World,\" in Proceedings of the 2nd Workshop on Intelligent Security, 2010.\n\n[11] X. Shu, F. Araujo, D. L. Schales, M. P. Stoecklin, J. Jang, H. Huang, and J. R. Rao, \"Threat Intelligence Computing,\" in Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, 2018.\n\n[12] Y. Fang, Y. Li, L. Liu, C. Huang, H. Zhu, and M. Xu, \"DeepHunter: A Deep Learning Based Vulnerability Detection System,\" in Proceedings of the 2020 IEEE International Conference on Software Maintenance and Evolution, 2020.\n\n[13] MITRE Corporation, \"MITRE ATT&CK Framework,\" 2023. [Online]. Available: \nhttps://attack.mitre.org/\n\n[14] MITRE Corporation, \"Caldera,\" 2023. [Online]. Available: \nhttps://github.com/mitre/caldera\n\n[15] OpenAI, \"GPT-4o mini,\" 2024. [Online]. Available: \nhttps://openai.com/\n\n[16] Anthropic, \"Claude-3.5-Sonnet,\" 2024. [Online]. Available: \nhttps://www.anthropic.com/\n\n[17] LangChain, \"LangChain Framework,\" 2024. [Online]. Available: \nhttps://langchain.com/\n\n[18] Deno Land Inc., \"Deno Runtime,\" 2023. [Online]. Available: \nhttps://deno.land/\n\n[19] TryHackMe, \"Blue Room Challenge,\" 2023. [Online]. Available: \nhttps://tryhackme.com/room/blue\n\n[20] Microsoft, \"MS17-010 Security Bulletin,\" 2017. [Online]. Available: \nhttps://docs.microsoft.com/en-us/security-updates/securitybulletins/2017/ms17-010\n\nAcknowledgements\n\nWe want to express our gratitude to the following individuals and organizations for their contributions to this work:\n\nThe MITRE Corporation for developing and maintaining the ATT&CK framework, which provides the foundation for our approach to structured penetration testing.\n\nThe open-source security community, particularly the Nmap and Metasploit Framework developers, whose tools are integral to our system's functionality.\n\nTryHackMe for providing the \"Blue\" room challenge, which served as our primary testing environment.\n\nOpenAI and Anthropic for developing the language models that power our system's planning and analysis capabilities.\n\nThe LangChain community for creating and maintaining the framework that facilitates our integration of language models.\n\nThe Deno community for developing a modern JavaScript runtime that supports our TypeScript implementation.\n\nOur anonymous reviewers for their valuable feedback and suggestions that helped improve this paper.\n\nOur colleagues and research partners provided insights, testing support, and critical feedback throughout development.\n\nNo specific grant from public, commercial, or not-for-profit funding agencies supported this research.\n\nAppendix\nA. System Requirements\n\nThe Pentest Agent System requires the following components:\n\nOperating System: Ubuntu 20.04 LTS or newer (other Linux distributions may work but are not officially supported)\nRuntime Environments:\nDeno 1.37.0 or newer\nPython 3.8 or newer\nExternal Tools:\nNmap 7.80 or newer\nMetasploit Framework 6.0.0 or newer\nAPI Keys:\nOpenAI API key (for GPT-4o mini)\nAnthropic API key (for Claude-3.5-Sonnet)\nHardware Requirements:\nMinimum: 2GB RAM, 2 CPU cores, 10GB free disk space\nRecommended: 8GB RAM, 4 CPU cores, 20GB free disk space\nB. Installation Guide\n# Clone the repository\ngit clone https://github.com/your-username/pentest-agent-system.git\ncd pentest-agent-system\n\n# Set up Python environment\ncd python\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n\n# Set up API keys\nexport OPENAI_API_KEY=\"your-openai-api-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# Verify tool access\nmsfconsole -v\nnmap --version\n\n# Run the system\nstreamlit run python/main.py\nC. Example Attack Plan\n\nBelow is an example attack plan generated by the system for the \"Blue\" challenge:\n\n{\n  \"target\": \"10.10.10.10\",\n  \"objective\": \"Gain system access and locate flags\",\n  \"steps\": [\n\t{\n\t  \"id\": 1,\n\t  \"technique_id\": \"T1046\",\n\t  \"name\": \"Network Service Scanning\",\n\t  \"command\": \"nmap -sV -sC -p- --script vuln 10.10.10.10\",\n\t  \"expected_outcome\": \"Identify open ports and MS17-010 vulnerability\"\n\t},\n\t{\n\t  \"id\": 2,\n\t  \"technique_id\": \"T1190\",\n\t  \"name\": \"Exploit Public-Facing Application\",\n\t  \"command\": \"use exploit/windows/smb/ms17_010_eternalblue\",\n\t  \"expected_outcome\": \"Gain SYSTEM level access\"\n\t},\n\t{\n\t  \"id\": 3,\n\t  \"technique_id\": \"T1059\",\n\t  \"name\": \"Command and Scripting Interpreter\",\n\t  \"command\": \"shell\",\n\t  \"expected_outcome\": \"Obtain command shell\"\n\t},\n\t{\n\t  \"id\": 4,\n\t  \"technique_id\": \"T1083\",\n\t  \"name\": \"File and Directory Discovery\",\n\t  \"command\": \"dir C:\\\\Users\\\\Administrator\\\\Desktop\\\\\",\n\t  \"expected_outcome\": \"Locate flag files\"\n\t},\n\t{\n\t  \"id\": 5,\n\t  \"technique_id\": \"T1005\",\n\t  \"name\": \"Data from Local System\",\n\t  \"command\": \"type C:\\\\Users\\\\Administrator\\\\Desktop\\\\flag.txt\",\n\t  \"expected_outcome\": \"Extract flag content\"\n\t}\n  ]\n}\nD. Code Snippets\nD.1 Security Analyst Agent Prompt\nSECURITY_ANALYST_PROMPT = ChatPromptTemplate.from_messages([\n\t(\"system\", \"\"\"\n\tYou are a specialized cybersecurity analyst focused on vulnerability assessment.\n\tYour primary responsibility is to analyze nmap scan results, identify vulnerabilities,\n\tmap them to the MITRE ATT&CK framework, and generate detailed attack plans.\n\t\n\tWhen provided with nmap scan results, you should:\n\t1. Identify all potential vulnerabilities\n\t2. Map each vulnerability to the appropriate MITRE ATT&CK technique\n\t3. Generate a comprehensive attack plan\n\t4. Prioritize vulnerabilities based on severity\n\t\n\tUse the security_analyst tool to perform your analysis. This tool requires nmap scan results\n\tas input, which can be in JSON format (from the nmap_tool) or raw text format.\n\t\n\tIMPORTANT: The security_analyst tool must only be used with the results from a previous nmap scan.\n\tNEVER call security_analyst with an empty object or without nmap results.\n\t\n\tIf you need to run a new nmap scan, use the nmap_tool first, then pass its results to security_analyst.\n\t\"\"\"),\n\t(\"human\", \"{input}\"),\n\t(\"placeholder\", \"{agent_scratchpad}\")\n])\nD.2 MITRE ATT&CK Technique Implementation\nexport const MS17_010_TECHNIQUE: MitreAttackTechnique = {\n  id: \"T1190\",\n  name: \"Exploit Public-Facing Application\",\n  description: \"Exploit MS17-010 vulnerability in SMB service\",\n  tactic: \"initial-access\",\n  implementation: async (context: ExecutionContext): Promise<StepResult> => {\n\tconst { target, metasploitClient } = context;\n\t\n\t// Set up the exploit\n\tawait metasploitClient.execute(\"use exploit/windows/smb/ms17_010_eternalblue\");\n\tawait metasploitClient.execute(`set RHOSTS ${target.ip}`);\n\tawait metasploitClient.execute(\"set LHOST tun0\");\n\tawait metasploitClient.execute(\"set LPORT 4444\");\n\t\n\t// Run the exploit\n\tconst result = await metasploitClient.execute(\"exploit -j\");\n\t\n\t// Check for success\n\tconst sessions = await metasploitClient.execute(\"sessions -l\");\n\tconst success = sessions.includes(\"meterpreter\");\n\t\n\treturn {\n\t  success,\n\t  output: result,\n\t  artifacts: { sessions }\n\t};\n  },\n  requirements: [\"open-port:445\"],\n  provisions: [\"system-access\", \"meterpreter-session\"],\n  difficulty: \"medium\"\n};\nE. Performance Data\nMetric\tGPT-4o mini\tClaude-3.5-Sonnet\tManual Testing\nSuccess Rate\t90%\t100%\t100%\nAvg. Completion Time\t8.7 min\t7.9 min\t30.5 min\nVulnerability Detection\t100%\t100%\t100%\nFalse Positives\t0\t0\t0\nMITRE Techniques Used\t7\t8\t6\nError Recovery Rate\t82%\t92%\tN/A\nF. Future Work\n\nPlanned enhancements for future versions include:\n\nExpanded Technique Coverage: Implementing additional MITRE ATT&CK techniques beyond the current set, particularly in persistence, privilege escalation, and defense evasion.\n\nAdditional Tool Integration: Supporting more security tools beyond Nmap and Metasploit, such as Burp Suite for web application testing and Hashcat for password cracking.\n\nMachine Learning Components: Adding ML for adaptive attack planning based on previous results and environmental feedback.\n\nDistributed Architecture: Supporting multi-agent operations across multiple systems for more complex penetration testing scenarios.\n\nEnhanced Visualization: Adding real-time visualization of the attack progress and results for better operator understanding.\n\nNetwork-based Flag Submission: Implementing automatic submission of flags to validation systems for CTF-style challenges.\n\nImproved LLM Integration: We are exploring fine-tuning models specifically for security tasks to improve performance and reduce hallucinations.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nRelated work\n\nAutomated Penetration Testing Frameworks\n\nMulti-Agent Systems in Cybersecurity\n\nLLMs in Security Applications\n\nMethodology\n\nSystem Architecture\n\nData Models\n\nImplementation Details\n\nView all\nComments\nCode\nDatasets\nFiles\nREADME.md\nPROJECT_STRUCTURE.md\nYou might be interested\nFully Autonomous Offensive AI Agent: The Ultimate Cyber Adversary\nA\nY\nMar 11, 202534 reads\nAiAutonomous Agents+5\nAdaptiveFuzz: LLM-Based Adaptive Fuzzing for Vulnerability Discovery\nOct 20, 20252 reads\nAAIDCAAIDC2025+5\nAutonomous AgenticAI SOC Threat Modelling, Threat Hunting\nS\nMar 02, 202519 reads\nSecureFlow: Production-Ready Multi-Agent Financial Intelligence System\nSep 13, 202510 reads\nagentagent-orchestration+6",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "perception-system-simultaneous-localization-and-mapping-for-a-formula-student-driverless-race-car-88kVMsXAlxJP",
    "username": "sStefano Genetti",
    "license": "MIT License",
    "title": "Perception system. Simultaneous Localization and Mapping for a Formula Student Driverless Race Car",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n88 reads\n●\nMIT License\nWinner of\nBest Overall Project\nat the\nComputer Vision Projects Expo 2024\nPerception system. Simultaneous Localization and Mapping for a Formula Student Driverless Race Car\nS\nStefano Genetti\nS\n@stefano.dellanna\nS\n@sebastiano.taddei\nT\nThomas Nonis\nG\n@gabriele.stulzer\nLike\nBookmark\nShare\nPercepion System. Simultaneous Localization and Mapping for a Formula Student Driverless Race Car\n\nYear: 2024\n\nAbstract\n\nThis project introduces a prototype perception system for feature-based Visual Simultaneous Localization and Mapping (Visual SLAM), specifically designed for a Formula Student autonomous race car. The study focuses on achieving real-time localization and enhanced situational awareness by determining the vehicle's position and mapping its surroundings within a cone-defined track. To detect the cones, a fine-tuned version of the YOLOv8 object detection algorithm is utilized. The system's performance is evaluated using both indoor and outdoor video datasets captured with an Intel RealSense D455 RGB-D camera, tested under diverse lighting conditions and environmental settings.\n\nIntroduction and problem statement\n\nIn the Formula Student Driverless competition, our project focuses on developing a perception system that enables an autonomous vehicle to understand its environment and localize itself within an unknown circuit. The circuit is defined by cones of various types placed at different distances, which the system must detect and classify. This task, commonly referred to as Simultaneous Localization and Mapping (SLAM), involves processing sensor data to construct a map of the circuit while simultaneously determining the vehicle's position. The system must accurately report the vehicle's real-time coordinates within a reference frame and identify the locations and classifications of cones encountered during the lap, in compliance with competition rules.\n\n\nFigure 1\n\nThe report is structured as follows: we begin with a concise overview of relevant background concepts. Next, we describe the hardware and sensors utilized in our solution. We then outline our proposed methodology for developing the perception system, followed by a description of the experimental setup. Subsequently, we present and analyze the results. Finally, the \"Conclusion and Future Work\" section summarizes the key findings and explores potential research directions to improve our approach.\n\nBackground\n\nIn the following, we briefly summarize the main theoretical aspects related to this work.\n\nVisual Simultaneous Localization and Mapping\n\nThis project aims to address the SLAM (Simultaneous Localization and Mapping) problem by tracking the location of an autonomous agent over time while incrementally constructing a virtual representation of its environment. Leveraging the visual sensors at our disposal (see below), we utilize a Visual SLAM algorithm with an RGB-D camera setup.\n\nVisual SLAM approaches are typically categorized by camera configurations—Monocular, Stereo, Multi-view, and RGB-D—and by methodology: feature-based or direct methods. Feature-based methods estimate camera motion by minimizing the projection error between image features and a local map, while direct methods operate on image pixel intensities and are better suited for 3D reconstruction.\n\n\nFigure 2\nFor this work, we adopted OrbSLAM [1], a feature-based Visual SLAM system renowned for its real-time performance in diverse environments. We began by studying OrbSLAM2 [2] to understand its code structure, then progressed to OrbSLAM3 [3] which is, to the best of our knowledge, recognized as one of the most robust and accurate system in the field.\n\nOrbSLAM2\n\nOrbSLAM2 [2] takes its name from the famous local feature detector ORB (Oriented Fast and rotated Brief) (Figure 3). These features are used for tracking, mapping and placing recognition tasks. The implementation of OrbSLAM2 is open source (\nORB-SLAM2 GitHub repository\n). The system can work with monocular, stereo, and RGB-D camera setups. It also includes very useful tools like map reuse (i.e., it can load already mapped areas), loop closing (i.e., it can detect closed camera trajectories), camera relocalization (i.e., it can relocalize the camera inside the map when the system is no more able to track camera position) and localization (i.e., it can localize the camera inside a pre-existing map). Furthermore, according to the results presented in the original paper, the algorithm is suitable for real-time applications.\n\n\nFigure 3\n\nThe OrbSLAM2 working principle can be divided into three parts which also correspond to the three main threads:\n\nTracking: the system dedicates one thread to track and localize the camera for each frame by first, computing the image's features and then minimizing the reprojection error between the image features and a local map applying motion bundle adjustment (motion BA).\nLocal mapping: it manages the local map and updates it by means of local bundle adjustment (local BA).\nLoop closing: one thread is dedicated to detecting large loops and correcting the accumulated drift using pose-graph estimation. Once the pose graph is optimized, the system performs a full bundle adjustment (full BA) and updates the map.\n\n\nFigure 4\nOrbSLAM3\n\nAs described in the following figure, the computational pipeline of OrbSLAM3 is in line with the one of OrbSLAM2. However, OrbSLAM3 [3] stands out in our application due to its multiple map system and its capability to integrate IMU data. Furthermore, we have experimentally noticed that OrbSLAM3 sometimes excels where OrbSLAM2 fails in detecting loop closures.\n\nMultiple map system: while OrbSLAM2 sometimes gets completely lost during periods of poor visual information, OrbSLAM3 manages to survive. When unable to localize within the current map due to insufficient visual data, OrbSLAM3 seamlessly initiates a new map, later merging it with existing maps upon revisiting mapped areas.\nIMU data integration: To enhance the accuracy of localization and mapping outcomes, OrbSLAM3 offers the option to incorporate data from the inertial measurement unit (IMU). However, the results presented in this paper have been obtained without utilizing IMU sensor data.\nImproved loop closure identification: OrbSLAM3 successfully identifies revisits to previously explored locations, resulting in more frequent bundle adjustments that significantly enhance localization and mapping accuracy.\n\nSimilar to OrbSLAM2, the code for OrbSLAM3 is open source and can be accessed via the original author's \nGitHub repository\n.\n\n\nFigure 5\n\nHardware and sensors\n\nSolutions adopted by Formula Student teams often rely on expensive sensors such as professional cameras, multi-channel lidars, IMU, GPS, and ground speed sensors. The purpose of our research is to demonstrate that it is possible to develop a competitive driverless car without necessarily employing expensive equipment. In consonance with this, the sensor used for this project is an Intel RealSense D455 RGB-D camera. This hardware is reasonably cheap with respect to other commonly adopted solutions.\n\n\nFigure 6\nUsing the camera calibration procedure provided by OpenCV we performed a camera calibration of our visual sensor to get the camera matrix and distortion coefficients. To do so we acquired several pictures of the widely used chess board calibration pattern placed at different locations and orientations. Then, we relied on the OpenCV calibration functions to identify the correspondences between the set of 3D real-world points and their 2D coordinates in the images. Finally, to assess the calibration parameters's accuracy, we evaluated the re-projection error, following OpenCV's guidelines, achieving a 0.01 projection error.\nThe code for the camera calibration is publicly available at \nthis GitHub repository\n.\n\n\nFigure 7\n\n\nFigure 8\n\nMethodology\n\nThe overall idea of our methodology is reported in the following figure.\n\n\nFigure 9\nThe RealSense D455 RGB-D camera captures sensor data to perceive the circuit in which the agent operates, acting as a bridge between the agent and its environment. The video stream produced by the camera is processed by the perception system that combines localization and mapping components. After undergoing a post-processing phase (in the figure referred to as circuit enhancement), the algorithmic pipeline produces two output files. The first, named cones.csv, reports the positions and classifications of cones identified in the recorded video stream. The second, camera.csv, contains the coordinates of the camera throughout the video stream, expressed relative to a fixed reference system.\n\nAs mentioned at the beginning of this chapter, initially we focused on OrbSLAM2 algorithm as our starting point. This strategy allowed us to thoroughly examine the OrbSLAM codebase, serving as a preliminary step in comprehending its structure. In our \nGitHub repository\n on branch orbslam2, we provide the C++ code together with the instructions to execute OrbSLAM2 on a video sequence recorded by the Intel RealSense D455 camera. Upon completion of the execution, we store the map generated by the algorithm along with the camera's trajectory in a human-readable format (camera.csv). Additionally, throughout the execution, real-time coordinates of the camera in the actual environment are accessible, enhancing the localization capability.\n\n\nFigure 10\nOnce we became familiar with the structure of the OrbSLAM2 code, we shifted our focus to OrbSLAM3.\n\nCone detection and classification\n\nFor mapping purposes, we aim to estimate the coordinates of the cones that outline the circuit. In this regard we fine-tuned a version of YOLOv8 using the annotated images from the FSOCO dataset [4], a collaborative dataset for vision-based cone detection systems in Formula Student Driverless competitions. The classes of cones of interest are conveniently illustrated in the following figure.\n\n\nFigure 11\nThe obtained deep neural network model performs well across the training, validation, and test sets.\nIn the following figures, we report the outcome of the training and evaluation procedure.\n\n\nFigure 12: Confusion matrix.\n\n\nFigure 13: FSOCO training set.\n\n\nFigure 14: FSOCO validation set.\n\n\nFigure 15: Recall-confidence curve.\n\n\nFigure 16: Precision-recall curve.\n\n\nFigure 17: Precision-confidence curve.\n\n\nFigure 18: F1-confidence curve.\nTo integrate the cone detection module into the rest of the system, a custom C++ interface has been developed starting from the one provided in the YOLOv8 repository. The proposed interface uses the OpenCV DNN module to load the network in ONNX format with the capabilities to run it either with or without CUDA for GPU acceleration if available.\nThe original code has been modified as the originally provided example only works on bounding boxes and does not account for the segmentation output. With such modification, the interface produces a vector of bounding boxes and their respective masks.\nOur custom interface implementation in C++, along with the ONNX YOLO models, can be accessed publicly on \nour GitHub repository\n.\n\nCombining OrbSLAM3 and YOLOv8\n\nThe heart of the overall perception system is the integration of YOLOv8 in the OrbSLAM3 algorithm pipeline. The following figure illustrates the main software components of the perception system.\n\n\nFigure 19\nEach frame acquired by the camera is processed with our fine-tuned deep learning model, before the ORB extraction step of OrbSLAM3. By doing so, in the subsequent steps of the algorithm, we can filter only the visual features that belong to the bounding boxes of the cones within the current frame. For each ORB feature on the image, OrbSLAM3 keeps track of its corresponding map point in the three-dimensional real-world reference system. Leveraging this information, we gather a set of  point coordinates for each cone detected by our model. To condense the 3D points belonging to a cone  into a single representative triplet of coordinates, we calculate a centroid :\n\n\n\n\n\nFigure 20\nThe perception system updates at each frame the list of detected cones that populate the track throughout which the autonomous agent is traveling. Each cone is described by the following attributes:\n\nclass identifier: the class of the cone predicted by YOLO.\nhit counter: how many times the cone has been seen by the computer vision system.\nx, y, z: coordinates of the cone's centroid with respect to the real-world reference system.\nleft-right: this attribute is an integer value indicating the position of the cone relative to the track (see below). The number describes whether the cone is on the right side (1) or the left side (2) of the track. A value of 0 signifies that the position is unknown.\n\nWhenever YOLO detects a cone, we need to determine whether the cone has been previously identified in another frame or if it is a new detection. In the case of new detection, the cone needs to be added to the collection of detected cones. However, if the cone has been detected previously (which could happen for instance in consecutive frames), the corresponding hit counter attribute is incremented, and its position is refined based on the new localization estimation.\nA cone  is equal to a cone  if the following conditions are accomplished:\n\nThe class predicted by YOLO for  is the same as the one proposed for .\nThe euclidean distance between the 's centroid  and 's centroid  is below a hard-coded threshold , namely: \n\nUpon receiving a new frame from the input video stream, YOLO performs cone detection and classification, resulting in a set of detected cones within the image. Utilizing the methodology outlined above, OrbSLAM3 is employed to derive a triplet of 3D coordinates for each detected cone.\nHandling cone equality as defined earlier, we iterate the list of detected cones to determine whether each newly detected cone should be inserted for the first time or if it already exists in the vector.\nWhen processing a newly detected cone  with coordinates , and it is found to have been previously included in the list of cones with coordinates , we refine its current position estimation in the real-world reference system using the following approach:\n\nOnly cones with a hit counter value exceeding a predefined threshold are treated as valid cones and are subsequently visualized in the output. Cones failing to meet this criterion are deemed as noise and thus are neither visualized nor logged.\n\n\nFigure 21\n\n\nFigure 22\n\nLeft and right cones\n\nA central aspect for acquiring a comprehensive representation of the circuit involves distinguishing between cones located on the left and right sides of the vehicle. To address this, we have evaluated two potential solutions:\n\nAccording to competition regulations, track lanes are delineated by yellow and blue cones for the left and right lane boundaries, respectively. Therefore, we can easily determine whether a cone is positioned on the right or left boundary of the track by relying on the classifications provided by YOLO. This represents the simplest solution.\n\nAnother method to determine whether a cone is positioned on the left or right involves a geometric approach. Utilizing the reference systems illustrated in the following figure, we convert the 3D world coordinates  of each cone into their respective 3D coordinates within the camera reference system . Here, cones with a positive  value are situated in front of the vehicle, while cones with a negative  value are behind it. Additionally, cones with a positive  coordinate are on the right side, whereas cones with a negative  coordinate are on the left side.\n\n\nFigure 23\n\nExperimental setup\n\nIn this section, we provide an overview of the experimental design employed to evaluate the proposed framework.\n\nComputational setup\n\nTo ensure the solution could be implemented without requiring specialized hardware, thereby minimizing implementation costs, the experiments described in this report were conducted on a standard Windows 11 laptop. The system was equipped with a 14-core Intel i7-12700H @ 2.30 GHz and 32GB of RAM.\n\nDatasets\n\nTo evaluate the performance of the implemented perception system, we captured multiple datasets using the RealSense D455 camera through the RealSense Viewer software made available by Intel. All the datasets are made available at \nthis link\n.\n\nlab-otto: this dataset was captured within an indoor setting, specifically in the corridors of our university. To evaluate our algorithm's object detection capabilities, we strategically placed cones within the environment. Additionally, we intentionally included a loop closure within the dataset to assess the detection capabilities of OrbSLAM2 and OrbSLAM3. Furthermore, we measured the distances covered in the explored areas (see the next figure), allowing us to evaluate the accuracy of our scaling methodology.\noutdoor: this sequence was recorded outdoors, specifically in the car park of our university. To emulate the layout of a Formula Student driverless competition circuit, we strategically positioned cones along the path. Our movement within the circuit aimed to encompass a loop closure for evaluation purposes.\npovo1-interno: the recorded sequence navigates a complete path along the ground floor corridor of our university. There are no cones along the path. The light from the windows causes a lot of tricky visual artifacts which poses difficulties for OrbSLAM2 in certain runs. Conversely, OrbSLAM3 demonstrates enhanced robustness in this scenario, coping better with limited visual features. Notably, the sequence encapsulates a loop closure.\npovo-garage: This dataset replicates a circuit delineated by cones within an indoor setting. Precisely, the video sequence was captured in the underground car park of our university. Our trajectory intentionally encompasses a loop closure to assess the capabilities of our implementation.\n\n\nFigure 24\nResults\n\nThis section aims to present representative experiments showcasing the results achieved with our implemented perception system. First, we report and discuss a comparison of the performance of OrbSLAM2 and OrbSLAM3. Finally, we offer a demonstrative video showcasing the execution of our perception system on one of our indoor datasets.\n\nOrbSLAM2 and OrbSLAM3 comparison\n\nTo evaluate OrbSLAM2 and OrbSLAM3 performance, we compared the estimated camera trajectories obtained by the two algorithms executed on the four datasets we collected. In the following figure, the plots of the obtained trajectories are reported. The code that has been implemented to obtain these representations has been published on \neagle-driverless-orbSLAM GitHub repository\n on branch evaluation.\n\n\nFigure 25\n\n\nFigure 26\n\n\nFigure 27\n\n\nFigure 28\n\n\nFigure 29\n\n\nFigure 30\n\n\nFigure 31\n\n\nFigure 32\nUpon observing the trajectory curves we derive the following observations. While some datasets show nearly overlapping trajectories, others exhibit differences due to specific rotations and translations. This discrepancy is not concerning as both algorithms calculate accurate trajectories, albeit within slightly different reference systems.\n\nExecution demo\n\nTo present the achieved results, we offer \ntwo videos\n demonstrating the behavior of the perception system using the povo-garage dataset. The videos illustrate that the system produces reasonably high-quality output with satisfactory stability.\n\nThe first video demonstrates that YOLO appropriately recognizes and classifies cones within the captured frame. Additionally, different colors distinguish between ORB features within the bounding boxes of the cones and key points that do not belong to a cone.\n\nThe second video showcases the progressive mapping of the track and the simultaneous localization of the vehicle within it. Particularly, the orange squares represent the projection of the centroid coordinates of the cones within the world reference system. These squares are updated in every frame. The larger blue and yellow squares represent cone clusters. Blue is used to represent cones on the right side, while yellow represents cones on the left side.\n\nConclusions and future directions\n\nIn this work, we achieved a functioning initial version of a perception system aimed at solving an instance of the Simultaneous Localization and Mapping (SLAM) problem for a Formula Student racing car to drive an autonomous vehicle.\nThrough camera calibration, we were able to obtain intrinsic and extrinsic parameters of the Intel RealSense D455 camera.\nStarting the work with OrbSLAM2 allowed us to become familiar with the code structure proposed by the authors. This facilitated a smoother migration towards the Visual SLAM algorithm OrbSLAM3. Having a working solution of both OrbSLAM2 and OrbSLAM3 also allowed us to compare the performance achievable with the two algorithms.\nRegarding the mapping, the YOLOv8 model, on which we performed fine-tuning using the FSOCO dataset, proved to be efficient for properly detecting and classifying conses within the acquired frames. Integrating YOLOv8 into the algorithmic pipeline of OrbSLAM3 is at the core of the proposed solution and enabled us to effectively address the SLAM problem. By combining the two algorithms, we can select only the Oriented FAST and rotated BRIEF (ORB) visual features belonging to the bounding box of the cone, derive their corresponding coordinates relative to the world reference system, and thus obtain the coordinates of a single representative centroid of the recognized cone. Notably, we further refined the results by proposing a prototypical solution for cone clustering and distinguishing between right and left cones. The proposed solution was tested and achieved good performance on five datasets acquired by us.\n\nLimitations and future improvements\nConsidering segmentation masks rather than bounding boxes\n\nUpon analyzing the plot of cone centroids, we observe considerable noise in the estimation of their location. We attribute this noise to inferring cone coordinates from the ORBs within the bounding boxes encasing each other. Within these bounding boxes, there might exist visual features unrelated to the detected cones. To address this limitation, our proposed solution involves taking into account the segmentation mask of the cones rather than the bounding box and subsequently filtering the ORBs within this defined shape.\n\nIs it necessary to execute YOLO inference on every frame?\n\nAt present, the goodness of processing every acquired frame with YOLO remains uncertain. Although YOLO provides good real-time performance, being able to avoid running the object detection model on each frame of the video stream would reduce the computational cost of the algorithm. Moreover, it is reasonable to assume that the cones in two consecutive frames remain unchanged. Thus, it might be a prominent idea to utilize the model solely on OrbSLAM3 KeyFrames rather than on every frame.\n\nMultithreading\n\nCurrently, YOLOv8 is executed sequentially, in series, with respect to OrbSLAM3. A possible enhancement of this work would be to separate YOLOv8 and OrbSLAM3 into two distinct threads running in parallel. At that point, through appropriate process synchronization policies, it might be possible to optimize the interaction between YOLO and SLAM.\n\nContributions\nStefano Genetti, PhD Student University of Trento (Italy)\nThomas Nonis, MSc Student University of Trento (Italy)\nStefano Dell'Anna, PhD Student University of Trento (Italy)\nSebastiano Taddei, PhD Student University of Trento (Italy)\nGabriele Stulzer, MSc Student University of Trento (Italy)\nReferences\n\nORB-SLAM: A Versatile and Accurate Monocular SLAM System\n\nDOI: 10.1109/TRO.2015.2463671\n\nORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras\n\nDOI: 10.1109/TRO.2017.2705103\n\nORB-SLAM3: An Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap SLAM\n\nDOI: 10.1109/TRO.2021.3075644\n\nFSOCO: The Formula Student Objects in Context Dataset\n\nDOI: 10.4271/12-05-01-0003\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nPercepion System. Simultaneous Localization and Mapping for a Formula Student Driverless Race Car\n\nAbstract\n\nIntroduction and problem statement\n\nBackground\n\nVisual Simultaneous Localization and Mapping\n\nOrbSLAM2\n\nOrbSLAM3\n\nHardware and sensors\n\nMethodology\n\nCone detection and classification\n\nView all\nCode\nDatasets",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "percolate-agentic-orchestration-in-the-data-tier-mxHUnSzWmu7s",
    "username": "pSaoirse Amarteifio",
    "license": "MIT License",
    "title": "",
    "publication_description": "Back to publications\nMar 28, 2025\n●\n121 reads\n●\nMIT License\nWinner of\nBest Overall Project\nat the\nAgentic AI Innovation Challenge 2025\nPercolate - Agentic Orchestration in the Data Tier\nAgentic Memory\nAI\nInformation Retrieval\nLLM\nPostgresSQL\nP\nSaoirse Amarteifio\nLike\nBookmark\nShare\nAbstract\n\nPercolate\n (MIT license) is a framework for building complex agentic memory structures using Postgres.\n\nPercolate is built on the premise that in the coming years information retrieval and agentic orchestration will converge into the unified paradigm of Iterated Information Retrieval (IIR). The vision is that this paradigm will make way for a powerful class of agentic memory architectures that will support augmented general intelligence. Distinct from artificial general intelligence, augmented intelligence celebrates personalization, user data and human-AI co-creation.\n\nTo realize this, Percolate does something very different to other agentic frameworks which is to push agentic orchestration into the data tier. Percolate reimagines agentic orchestration and tool calling as an information retrieval task and builds this directly into the database. This means we see the problem of routing requests to the right agent and loading personalization context into memory as similar problems. A multi-modal build of Postgres, which supports all of vector, graph, key-value, relational and HTTP API calls is used as a foundation.\n\nWith Percolate, dynamic agentic workflows can be constructed either in the database or through a client such as the Percolate Python client, which works much like any other Pydantic-driven agentic SDK you may have used.\n\nIntroduction\n\nToday, agentic orchestration (typically in Python) and RAG/agentic memory solutions are generally treated as two separate concerns. Percolate was built on the premise that information retrieval and agentic orchestration will converge into a unified paradigm called Iterated Information Retrieval (IIR). This paradigm has the potential to make way for a powerful class of agentic memory architectures that can support augmented general intelligence. As language models become smaller, faster and cheaper, it will become increasingly feasible to use AI to build intelligent content indexes right inside the data tier.\n\nYou can see an early example of what this looks like in the MIT licensed project \nPercolate\n and you can check out this \nvideo\n to see how to get it running locally on Docker. (See also the basic examples given below.)\n\nIterated Information Retrieval combines (1) multimodal data indexes for graph, vector, key-value and relational data with (2) treating orchestration of agents and tool calls as an information retrieval problem. This means treating agents and tools as declarative metadata that can be stored, searched and called from the database rather than treating agents as code.\n\nTo make agents declarative we simply need to factor out any notion of “agents as code” by treating all tools as external to the “agent”, which then becomes simply (1) a system prompt with (2) structured schema, along with (3) a reference to external functions.\n\n\nPercolate allows for a completely data-orientated orchestration pattern for agents but you can also use clients in your favourite programming language (as we have implemented in Python for example). These clients become relatively thin compared to present day agentic frameworks such as Langchain/LangGraph, Pydantic.AI or Open AI's newer AgentSDK. A declarative framework avoids hard-coding graphs and agents in code, which instead, can be treated more fluidly as data. This is not always preferred - but in the limit, for complex systems with many agents and tools, a database is a great way to manage the complexity.\n\nBy doing this, agentic orchestration becomes an information retrieval problem and part of the more general agent-persistence and agent-memory challenge. Whether we are looking for agents or tools, digging up previous user conversations or searching over a knowledge base, these are all unified here as a query/routing problem.\n\nThe “iterated” in IIR refers to the fact that unlike traditional forms of information retrieval where a user initiates a query and gets a response, now an agentic system generates a query wave; the first response can provide content but also references to other content in the database in a “see also” format. This process is supported by language models that run \"offline\" as background processes to build multi-modal content indexes.\nAs the agent loop runs it is “prompted” by these references and can probe deeper before responding to the user or taking some other action on behalf of the user. The agent loop can discover not only data but other functions and agents that can be activated, possibly with a “handover” of context.\n\nAll agent frameworks execute an agent loop but Percolate builds this into the query interface between data and agent logic.\n\nIn the following sections I provide a brief historical overview of agent frameworks before describing the features of Percolate with some examples.\n\nA little history\n\nThis section is added for completeness and can be skipped by those familiar with the history of agentic frameworks and databases. It concludes with some of the main features of agent SDKs as of 2025 and emerging data-tier AI frameworks such as pgai.\n\nThe field of AI agent frameworks has seen rapid development and innovation over the past few years, with several key ideas and frameworks emerging to shape the industry. This review will explore the evolution of AI agent frameworks from 2023 to 2025, highlighting significant milestones and trends.\n\nIn the early stages, frameworks like LangChain emerged, focusing on handling large language models and text-based applications. LangChain focused on projects centered around chatbots, question-answering systems, and research tools. During this period, the concept of structured outputs gained traction, with frameworks emphasizing the importance of defining and validating agent outputs.\n\nAs the field progressed, there was a shift towards multi-agent systems and collaborative frameworks. Microsoft AutoGen, introduced in this period, represented an advancement in developing complex and specialized AI agents. AutoGen's multi-agent conversation framework and customizable agent roles allowed for more sophisticated AI-driven solutions. Simultaneously, CrewAI emerged, focusing on real-time collaboration and enabling multiple agents or humans and agents to work together effectively. CrewAI's Pythonic design and inter-agent communication features could be used for scenarios requiring coordinated teamwork.\n\nThe concept of graph-based architectures gained prominence with the introduction of frameworks like LangGraph. As an extension of LangChain, LangGraph used large language models to create stateful, multi-actor applications. This framework enabled more complex, interactive AI systems involving planning, reflection, and multi-agent coordination. LangFlow, another graph-based multi-agent framework, further solidified this trend, offering enhanced scalability and efficiency for managing multiple agents.\nOpenAI Swarm emerged as a lightweight, experimental framework for in multi-agent orchestration. Its handoff conversations feature and scalable architecture provided a flexible solution for managing multiple AI agents. Pydantic.AI introduced their own SDK building on the strengths of Pydantic, which had been so readily embraced by the community. This framework improved engineering standards compared to earlier frameworks, while also starting to think about graphs and workflow persistence.\n\nOpenAI released its Agents SDK in March 2025, offering developers a powerful toolkit for creating sophisticated AI agents. They implemented Agent Loops ie. automated process for tool calls and LLM interactions, handoffs i.e enabling seamless coordination between multiple agents, guardrails i.e. input validations to enhance security and reliability, tracing i.e built-in visualization and debugging tools for monitoring agent workflows. These illustrate what the industry perceives as critical features as of 2025.\n\nModern vector databases like Chroma and Pinecone have emerged as vector focused AI databases focusing on high-dimensional indexing for semantic search and similarity matching, real-time hybrid search combining dense/sparse vectors, cross-modal integration for tasks like text-to-image generation, dynamic personalization through continuous vector updates. The principle of Retrieval Augmented Generation (RAG) initially focused on vector embeddings for \"AI memory\". Vectors alone are limited and later applications merged graph and relational data for more interesting hybrid search.\nThe Postgres extension pg_vector allowed Postgres to support AI applications and many providers from Supabase, TimescaleDB, Lantern emphasize how this can be a better alternative to offerings such as Pinecone. With Postgres you have a mature open source ecosystem and can combine relational and semantic search.\n\nTimescaleDB's \npgai\n project is an illustration of a paradigm shift in agent frameworks by embedding AI workflows directly into PostgreSQL - just like Percolate.\n\nPercolate goes beyond these offerings by integrating not just data storage concerns but also implementing agentic orchestration in the database. Additionally, it models graph and entity-lookup query modalities.\n\nKey Principles of Percolate\n\nTo build agents in Percolate we use a similar approach to other frameworks as described in the previous section. Over the last few years the mantra “Pydantic is all you need” has become standard, leading even to Pydantic releasing their own Agent framework at the end of last year. However I would qualify that Pydantic is not all you need. Pydantic is more than you need and that is because Pydantic is (in the agentic context) simply a nice wrapper around Json Schema, which is a declarative way to express objects and structured output (for example in a database). Keep in mind that all language models today are accessed via JSON Rest APIs.\n\nSide note on motivations\n\nSince function calling was introduced in 2023, I was also leaning heavily into Pydantic, which was an obvious thing to do back then and I built an agentic framework called \nfunkyprompt\n based on these ideas. I wrote a series of articles on Medium about it, talking about building agentic systems from first principles starting with this \narticle\n.\n\nHowever, I found that increasingly the real challenges were information-retrieval related. And thats why, at the start of this year, I started working on Percolate to get to the heart of the matter, which is how to (a) perform queries over multimodal data and tools and (b) to route requests between different agents i.e. orchestrate agents.\n\nBelow is a breakdown of some of the main principles and components of Percolate.\n\nAgent and Tool storage and search\n\nPercolate stores agents and tools in the database. Agents are stored in p8.Agent table and the tools are stored in the p8.Function table. Functions can be either REST endpoints described by OpenAPI or native database functions. (We are currently working on adding MCP Servers).\n\nPlanning and routing\n\nAdding agents and tools to the database means they can be discovered by the Planner agent to dynamically load agents and functions. A help function is attached to all agents allowing for functions or agents to be recruited to a given cause. While loading dynamic functions is possible, agents are typically designed with their own functions by reference, which we will see in an example below. (If an agent has attached functions these functions can be activated at loop execution time. But if a user asks a question that is outside the scope of the prompt or functions that the agent has, it can ask for help and activate arbitrary functions.)\n\nNote: when we activate other Agents, we can execute them as black box functions. When we do this the same system prompt and message context is used in our main thread. We also have the option to do a handoff where we load the recruited agent's system prompt into the main thread.\n\nTracing sessions and AIResponses over turns\n\nWhen we run agents in Python or the database, the user questions are saved in the Session object. Additionally each request-response turn calling the language model API is logged in the AIResponse table. This object can have one or all of content, tool call data or tool call interpretation. As sessions are automatically logged this makes Percolate great for auditing agentic applications out of the box.\n\nLangauge Model Agnostic\n\nPercolate runs with any language model API via Python or the database. LLM Apis come in three major dialects all of which are implemented in Percolate.\n\nGoogle e.g. Gemini models\nAnthropic i.e. Claude models\nOpenAI (de facto standard): GPT and most other models that are not anthropic or google\n\nOne useful thing about Percolate is you can switch between these models and transform message payloads between dialects. For example you could start a conversation in one dialect e.g. Anthropic and then re-run it or resume it in another dialect e.g. OpenAI.\n\nTo learn more about how Percolate implements message structures and API calls in these dialects see this Medium \narticle\n\nMulti-modal indexing and search\n\nPercolate is not only an agent orchestration framework. It is also (and more importantly) a database and a multi-modal one at that. Percolate is built on Postgres and we have added extensions for;\n\nGraph (\nAGE Graph extension\n)\nVector (pg_vector \nextension\n)\nKey-value lookup (Using the graph)\nRelational\nHTTP (using the \nHTTP extension\n for making tool calls, requesting embeddings or calling language model APIs from the database).\n\nBy combining these modalities we can represent knowledge in different ways.\n\nThe basis of Percolate entities is structured, relational data. For example when we create agents, we store tables that represent their structure along with detailed field metadata. This field metadata, which can include field descriptions or information about embeddings, can be thought of as extended table metadata in Postgres. This metadata can be used to construct SQL queries from natural language from within the database.\n\nBeyond constructing relational predicates, we can also do semantic search since content fields on the entity that are labelled with embedding providers are automatically semantically indexed i.e. we compute embeddings for these fields. When data are inserted, a database trigger will run a worker to build an embedding table, which can later be used for vector searching that table.\n\nMore complex queries and indexes can include graph indexes. Graph indexes complement vector indexes which, alone, are insufficient for RAG applications. For an introduction to how we think about this in Percolate see this \narticle\n.\n\nIn summary, Percolate provides multiple ways to index content. Multi-modal indexes means AI/agents can use different types of queries that best suit the user's query. Indexes are constantly built in the background so the system learns over time.\n\nWalkthrough and Code Examples\nGetting started\n\nTo run Percolate locally, clone the \nrepo\n\ngit clone https://github.com/Percolation-Labs/percolate.git\n\nand from the repo, launch the docker container so you can connect with your preferred database client. This assumes you have installed Docker e.g. Docker Desktop.\n\ndocker compose up -d #The connection details are in the docker-compose file\n\nThe easiest way to get started is to run the init cli option - this will add some test data and also sync API tokens for using language models from your environment into your local database instance. This requires Poetry to be installed. Note it is assumed when you run the client by default that you have an OPEN_AI_API key for the default examples but any models can be added and used.\n\n#cd clients/python/percolate to use poetry to run the cli\n#the first time install the deps with `poetry install`\npoetry run p8 init\n\nThe above command will load standard API keys into your local environment and also add some test content for testing including indexing some Percolate code files for semantic searches and adding a test API to test function calling.\n\nYou can ask a question to make sure things are working. Using your preferred Postgres client log in to the database on port 5438 using password:password and ask a question.\n\nSelect * from percolate('How does Percolate make it easy to add AI to applications?')\nCreating and using an agent in Python\n\nWhile Percolate allows for agents to be run from the database, you can also use the Python client. Agents are just Pydantic objects. You can add the system prompt as a docstring and add fields to describe structured output. In Percolate all objects can be registered as database tables to save agent states. (Conversely, all tables are treated as agentic entities that you can talk to.)\n\nWhile you can add and use functions directly on your object, Percolate is best used with external functions because then agents can be saved declaratively and used in the database. Below is an example of referencing external functions. In this case it uses a get_pet_findByStatus, which is an endpoint from a test API registered in the initialization step above.\n\nimport percolate as p8\nfrom pydantic import BaseModel,Field\nimport typing\nimport uuid\n\nfrom percolate.models import DefaultEmbeddingField\n\nclass MyFirstAgent(BaseModel):\n    \"\"\"You are an agent that provides the information you are asked and a second random fact\"\"\"\n    #because it has no config it will save to the public database schema\n    \n    id : str | uuid.UUID\n    name: str = Field(description=\"Task name\")\n    #the default embedding field just settings json_schema_extra.embedding_provider so you can do that yourself\n    description:str = DefaultEmbeddingField(description=\"Task description\")\n    \n    @classmethod\n    def get_model_functions(cls):\n        \"\"\"i return a list of functions by key stored in the database\"\"\"\n        return {\n            'get_pet_findByStatus': \"a function i use to look up pets based on their status\",\n       }\n\nTo use this agent you can spin it up in Percolate.\n\nimport percolate as p8\nagent = p8.Agent(MyFirstAgent)\nagent('can you tell me about two pets that were sold')\nRunning agents from the database and resuming sessions\n\nPercolate's major strength is that it can run agents declaratively for example in the database.\nYou can register any agent using the repository for that object type...\n\np8.repository(MyFirstAgent).register()\n\nThis does a number of things:\n\nIt creates a table based on the given schema, which the agent can use to save structured state.\nIt adds database artifacts for managing column embeddings and for triggering embedding creation.\nIt adds other metadata for managing entities and graph relationships.\nIt adds the agent to the list of agents in the database so it can be searched and used dynamically.\n\nIn the database we can now run\n\nselect * from percolate_with_agent('list some pets that are sold', 'MyFirstAgent')\n\nThis is equivalent to using p8.Agent(MyFirstAgent).run('list some pets that are sold') except now its run entirely in the database. This means; discovering the agent and prompt (which has external function called get_pet_findByStatus to load); loading the function by name and \"activating\" it; calling the function and summarizing the data.\n\nEach iteration or turn is audited in the database in the AIResponse table and its possible to resume sessions from their saved state.\n\nNote: The examples here run synchronous agents just to illustrate. In production systems a background worker is used to manage turns without locking database resources.\n\nOther examples\n\nTo see more examples you can check out the following Medium articles;\n\nThe Basics\nIntegrating with Claude Desktop and MCP to do deep research\nWhy Percolate\nKey Features\n✅ Storage and search for agents and tools\n✅ Data tier or Python agent orchestrator with dynamic function loading and agent handoffs\n✅ Tracing and auditing for free as everything is persisted by default\n✅ Workers to automatically build vector and graph indexes\n✅ Multi-modal entity search that combines SQL predicates, semantic search and graph relationships\n✅ OpenAPI function proxy\n✅ Works with any language model using Google, Anthropic or OpenAI API (standard) dialects\n✅ Session & turn auditing\n✅ MCP client for attaching MCP servers (coming soon)\n✅ Managed cloud recipes for Kubernetes (coming soon)\nKey Use cases\n✅ Persistence proxy when calling LLMs/Services\n✅ Personalization layer to track conversation and integrate data into your agent workflows\n✅ Research assistants\n✅ Implementing multi-modal RAG\nTech stack and languages\nDocker/Kubernetes\nPostgres / Cloud Native Postgres Operator for Kubernetes\nSQL\nCypher\nPython\nFastHTML admin dashboard (coming soon)\nThe Future\n\nThis is only the beginning. Percolate is a very young project started in January of this year. Percolate is an ambitious project that aims to go beyond what today's agent frameworks can do by pioneering a convergence between agent orchestration and information retrieval. Data and memory are what power agentic systems. \"Solving\" memory is the holy grail for achieving augmented general intelligence, where humans and AI work together to create at the speed of thought.\n\nPlease take a moment to check out the repository and get back to us with issues, feature requests and general comments. If Percolate sounds interesting to you please give the \nrepo\n a ⭐\n\nStay in touch by subscribing to our conceptual overviews on \nSubstack\n and our more granular, technical deep-dives over on \nMedium\n.\n\nThanks for reading about Percolate!\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nA little history\n\nKey Principles of Percolate\n\nSide note on motivations\n\nAgent and Tool storage and search\n\nPlanning and routing\n\nTracing sessions and AIResponses over turns\n\nLangauge Model Agnostic\n\nMulti-modal indexing and search\n\nView all\nComments",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "personal-assistant-to-manage-your-google-accounts-MOfRfFxDmNj1",
    "username": "Manuel Quesada",
    "license": null,
    "title": "Personal Assistant to Manage your Google Accounts",
    "publication_description": "Back to publications\nMar 22, 2025\n●\n137 reads\n●\nCreative Commons Attribution-NonCommercial (CC BY-NC)\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nPersonal Assistant to Manage your Google Accounts\nAgent Orchestration\nCustom Agent Hierarchical Arch\nLangChain\nPersonal AI assistants\nTooling\nManuel Quesada\nLike\nBookmark\nShare\n🤖🧠🤖 Personal Assistant to Manage Your Google Accounts\n\nAgents are here to stay, and they bring new possibilities to large language models (LLMs) that, just six months ago, faced significant limitations. This Minimum Viable Product (MVP) aims to develop an agent-based system capable of efficiently managing our emails and calendars. This system, designed to handle personal and work-related Google accounts, aims to provide a comprehensive solution for managing these essential services.\n\nThe core approach of this MVP is developing a multi-agent system, leveraging foundational techniques for building agent-based architectures. The idea of decomposing an application into independent agents and then integrating them into a multi-agent system helps mitigate various challenges. These agents can range from something as simple as a prompt that invokes an LLM to something as sophisticated as a ReAct agent, a general-purpose architecture that combines three key concepts:\n\nTool Calling: The LLM can select and use various tools as needed, extending its ability to perform complex tasks.\nMemory: Enables the agent to retain and utilize information from previous steps, allowing for greater coherence and continuity in interactions.\nPlanning: Empowers the LLM to create and follow multi-step plans to achieve long-term goals, enhancing the agent’s effectiveness in handling complex tasks.\n\nThis modular and flexible approach supports greater customization and ensures that the system remains scalable and easily adaptable to future advancements in artificial intelligence and automation.\n\n👤👤 \nMulti-Agent System\n\nThere are several ways to connect agents, as outlined below:\n\nNetwork: Each agent can communicate with every other agent. Any agent may decide which other agent to call next.\n\nSupervisor: Each agent communicates with a single supervisor agent. The supervisor makes decisions about which agent should be called next.\n\nSupervisor (\ntool-calling\n): This is a special case of the supervisor architecture. Individual agents are represented as tools. In this configuration, the supervisor agent uses a tool-calling LLM to decide which agent tools to invoke and with what arguments.\n\nHierarchical: A multi-agent system can be defined with a \nsupervisor of supervisors\n. This generalization of the supervisor architecture enables more complex control flows.\n\nCustom Multi-Agent Workflow: Each agent communicates with only a subset of other agents. Some parts of the flow are deterministic, while only specific agents have the authority to decide which agents to call next.\n\nKey Benefits of Multi-Agent Systems:\n\nModularity: Isolated agents simplify agentic systems' development, testing, and maintenance.\n\nSpecialization: Agents can be designed to focus on specific domains, which improves overall system performance.\n\nControl: The system designer has explicit control over agent communication paths, as opposed to relying solely on function calling.\n\nBased on these \narchitectures\n, we propose an agent-based design that aligns well with the \ndivide and conquer\n principle. Additionally, incorporating new managers becomes a straightforward task—it only requires defining the necessary tools, building the appropriate ReAct agents, integrating them with a manager or supervisor, and registering that manager or supervisor with the orchestrator. We will discuss this architecture in detail in the following sections.\n\nMVP Details\n💬 LLM\n\nThe MVP uses \nOpenAI\n as the language model provider, with the temperature parameter set to 0. This configuration ensures the responses are fully deterministic and reproducible, avoiding random variations in the model’s output. This setup is particularly useful for evaluating the quality of the responses, as it allows consistent comparison across different prompts and iterations.\n\nFramework used ⛓️\n\nLangChain\n is a framework designed to simplify the development of applications that LLMs, enabling the composition of complex reasoning and action chains. Thanks to its flexibility and integration with tools like LangSmith and LangGraph, developers can build, debug, and scale LLM applications in a more structured and efficient way.\n\nLangGraph\n: Enables orchestration of complex workflows with multiple steps, conditional logic, branching, and loops, which is especially useful for developing AI agents that need to maintain state, execute tasks in parallel, or dynamically adapt to outcomes. Its graph-based architecture helps visualize and understand the flow of information and decision-making within the agent. Moreover, by integrating directly with LangChain, developers can build intelligent and controlled agents without implementing control logic from scratch. This accelerates development and supports rapid iteration, making it ideal for prototyping and production environments.\n\nLangSmith\n: Complements the ecosystem by focusing on monitoring, evaluating, and debugging chains and agents built with LangChain. It provides full traceability of each execution, including model messages, function calls, inputs, outputs, and errors, allowing for a precise understanding of how the application behaves at every step. Additionally, it supports automated evaluations, result comparisons across versions, and regression detection. This detailed visibility is key to ensuring quality, performance, and reliability in LLM-based applications.\n\nTogether, these tools form a powerful development stack that covers the entire lifecycle of building intelligent agents and interfaces—from modular construction (LangChain), to orchestration and management of complex logic (LangGraph), to continuous monitoring, evaluation, and improvement (LangSmith). This significantly reduces development friction and time, making the creation of LLM-powered solutions much more scalable, maintainable, and robust.\n\nTools 🛠️\n\nSince this is an MVP, we aim to avoid building the tools our agents will use from scratch. Therefore, we opted for one of the available community-driven solutions: \nComposio-dev\n.\n\nComposio is an integration platform that empowers AI agents and LLM applications to connect seamlessly with external tools, services, and APIs. With Composio, developers can rapidly build powerful AI applications without the overhead of managing complex integration workflows.\n\nMVP Functionality ⏰ 📅 📧\n\nGeneral Information Queries: Respond to general questions outside the calendar and email management scope.\n\nAccount Handling: Distinguishes between personal and work accounts, assigning requests to the appropriate account based on user context.\n\nDate Management and Interpretation: Leverages the date manager to interpret and compute dates according to user requests.\n\nInterpret dates for events or tasks mentioned by the user.\nCalculate date ranges (e.g., \"the last week of this month\").\nConvert relative dates (e.g., \"in three days\") into absolute dates based on the current date system.\n\nCalendar Management: Manages events using the date manager to validate availability and avoid conflicts.\n\nCreate events.\nUpdate existing events.\nCheck availability for specific times and calendar space.\nHandle scheduling conflicts before creating or updating events.\nDelete existing events.\n\nEmail Management: Sends, replies to, and organizes emails.\n\nSend emails to specific addresses.\nCreate email drafts before sending.\nReply to existing email threads.\nQuery emails and list threads using filters.\nRetrieve detailed information from a specific message within a thread.\nArchitecture Used 🏛️\n\nOrchestrator Input 🔗\n\nThe Orchestrator Input node uses structured output to generate an action plan, which is translated into a list of managers that need to be called before responding to the user. Given the user's request and the message history, its main goal is to return a list of managers to be invoked along with the specific query to be sent to each of them.\n\nMemory 🔗\n\nFor memory management, we leverage LangGraph’s graph-based workflow execution to store checkpoints in PostgreSQL. In LangGraph, “checkpoints” are snapshots of the graph’s state at specific moments during execution. These snapshots allow the system to pause processing at defined points, save the current state, and resume execution later as needed. This feature is particularly useful for long-running tasks, workflows requiring human intervention, or applications that benefit from resumable execution.\n\nTo avoid passing the entire memory context to the language model, we use a timer mechanism to provide only the last 5 user interactions as conversational history.\n\nOrchestrator Output 🔗\n\nThis node is responsible for generating the final response to the user by aggregating all the outputs from the managers. It also has access to the message history. Additionally, it formats the response using Markdown to improve the readability of the output.\n\nIf the Orchestrator Input determined that the request was a general question and no manager needs to be called, this node is also in charge of answering the user’s request using its general knowledge, system prompt, and the conversation history.\n\nOrchestrator Executor 🔗\n\nThis node is responsible for calling each manager in sequence while providing context from previously called managers to the next one. This allows any manager who depends on information from a previous one to access it, effectively resolving inter-manager dependencies. The Orchestrator Input node must be capable of generating an action plan that respects these dependencies, where the order of execution matters.\n\nGPT-4o instead of GPT-4o-mini: Our solution initially used GPT-4o-mini because it provides faster responses and is more cost-effective regarding token usage costs. It worked well up to a certain point — even for accessing tools and selecting the right one when the LLM acted as an Agent.\n\n \n\nHowever, during the iterative refinement of the assistant, we observed that GPT-4o-mini could not generate the action plan correctly when the reasoning complexity increased properly. Here's an example that highlights this limitation:\n\nWe used the same prompt and simply switched the model to illustrate the inconsistency:\n\n\nIn the image above, the model failed to recognize that it needed to call all three managers to fully address the user's request. In contrast, GPT-4o handles this perfectly. In its output, we can clearly see a list containing the calls to each manager along with their respective queries:\n\n\nDate Manager 🔗\n\nThis manager handles any request related to dates and time—calculating a time interval or extracting the correct date from the user's initial query.\n\nFor example, LLMs often struggle with determining future dates, such as \"next week\" or \"next month,\" even when the current date is provided. To address this limitation, we designed a system prompt using the Few-Shot Prompting technique, where we include a series of examples with their corresponding dates and expected outputs. This approach allows us to achieve high accuracy in the generated responses.\n\nCalendar Manager 🔗\n\nThis manager acts as a router, responsible for forwarding the query to the appropriate worker. To do so, it uses the following as context:\n\nThe responses from any managers that were previously called.\nThe original query that was sent by the Orchestrator Input.\n\nEach worker represents a specific account. In our case:\n\nThe personal account.\nThe work account.\n\nSome requests require checking both accounts (e.g., verifying calendar availability), while others affect only one. The personal account is assumed by default if the user does not specify an account.\n\nLet’s consider the following user request:\n\n\"Create an event tomorrow afternoon to play soccer.\"\n\nThe flow would be as follows:\n\nThe Date Manager is consulted to determine the time range corresponding to \"tomorrow afternoon.\"\nThe router manager forwards the query to the appropriate worker with this information.\nSince the user did not specify an account, the personal account worker is selected.\nThe worker processes the request and creates the event in the calendar.\n\nAs a result, the manager generates the following query to be executed on the personal account:\n\nCreate an event for tomorrow, March 25, after 12 PM.\n\nCalendar Worker 🔗\n\nThe Calendar Worker operates as a ReAct agent without memory of previous messages. This means it does not retain context from prior interactions—its true strength lies in its ability to interact with the tools at its disposal. These tools are essential to its functionality and enable it to perform calendar-related tasks.\n\nBelow are the tools the Calendar Worker has access to, along with how it uses each one to manage events:\n\nGOOGLECALENDAR_FIND_EVENT\nThis tool is used to search for existing events in the calendar. When the Calendar Worker needs to verify a specific event (e.g., before updating or deleting it), it performs a search using this tool.\n\nGOOGLECALENDAR_UPDATE_EVENT\nThe UPDATE EVENT tool is used to modify existing events in the calendar. If an event needs to be updated (such as changing the time, adding participants, or adjusting details), the Calendar Worker uses this tool. Before performing an update, the worker checks for the event’s existence and, in some cases, performs a conflict check depending on the nature of the change.\n\nGOOGLECALENDAR_CREATE_EVENT\nThis tool allows the Calendar Worker to create new events in the calendar. When a user requests to add an event (like a meeting or appointment), the worker uses this tool to create it in the appropriate calendar. Before doing so, it checks for time availability using the FIND FREE SLOTS tool.\n\nGOOGLECALENDAR_FIND_FREE_SLOTS\nThis tool is critical for the Calendar Worker, allowing it to check for available time slots in the calendar. Before creating or updating an event, the worker uses this tool to ensure there are no scheduling conflicts. If a conflict is detected (e.g., an event already scheduled in the requested time range), the worker notifies the user and does not proceed with the creation or update.\n\nGOOGLECALENDAR_DELETE_EVENT\nWhen a user requests to delete an event, the Calendar Worker uses this tool to remove existing events. This action is performed after confirming which event to delete, based on the user’s input or a prior search using FIND EVENT.\n\nSince it does not maintain the memory of past messages, the Calendar Worker focuses on efficient interaction with these tools. Each time it receives a request, it evaluates the situation based on the current context and toolset. This ensures that every action is performed independently and accurately, relying solely on the tools and data available at that moment.\n\nEmail Manager 🔗\n\nThe Email Manager functions similarly to the Calendar Manager. Like the calendar manager, it acts as a router, directing the request to the appropriate worker. In this case, each worker manages one of the user’s email accounts—either the personal or the work account.\n\nThe personal account is assumed by default if the user does not specify an account. The workflow follows the same pattern as the calendar manager: context is gathered first, and then the appropriate worker performs the requested action.\n\nEmail Worker 🔗\n\nJust like the Calendar Worker, the Email Worker operates as a ReAct agent without memory of previous messages. This means it does not retain information from past interactions, and its ability to manage email depends entirely on the tools it has access to at the time. Below are the tools available to the Email Worker, along with how each one is used to perform email-related tasks:\n\nGMAIL_REPLY_TO_THREAD\nThis tool is used to reply to an existing email conversation. The Email Worker identifies the message to respond to and uses this tool to send a reply. It ensures the response is sent to the correct recipient, either the original sender or another recipient specified by the user.\n\nGMAIL_LIST_THREADS\nUsed to list email conversations. The Email Worker utilizes this tool to retrieve the most recent threads from the inbox or labeled folders, filtered according to the user's criteria.\n\nGMAIL_SEND_EMAIL\nAllows the sending of new emails. When the Email Worker receives a request to send an email, it uses this tool. If the user does not provide a recipient, the worker does not proceed with sending and responds that a recipient must be specified. It also ensures the subject line is appropriate and that the body of the message is properly formatted.\n\nGMAIL_FETCH_EMAILS\nThis tool enables the search and retrieval of emails from the inbox or other folders. The Email Worker can search by keywords in the subject or body, sender or recipient address, and can filter by labels such as \"INBOX,\" \"SENT,\" \"DRAFT,\" and so on.\n\nGMAIL_CREATE_EMAIL_DRAFT\nThe Email Worker uses this tool to create email drafts. The worker saves the content as a draft if the user wants to compose an email without sending it immediately. A placeholder example is used to generate the draft if no recipient is specified.\n\nGMAIL_FETCH_MESSAGE_BY_THREAD_ID\nThis tool is used to retrieve all messages from a specific conversation using its thread ID. The Email Worker uses it to gather full context from the conversation, ensuring that any reply is well-informed and relevant.\n\nMVP Deploy Guide 📝\n\nBefore running the MVP, ensure you have:\n\nA Composio account\nA Telegram Bot\nAPI keys for Composio and OpenAI\nDocker installed\n\n1. Set Up Composio\n\nCreate a Composio account at \nComposio-dev\n. The free tier allows up to 2,000 API calls per month.\nGenerate an API key and add it to the .env file:\nCOMPOSIO_API_KEY=\"your_api_key_here\"\nIntegrate Composio with Gmail and Google Calendar:\nConnect both your work and personal Gmail accounts at \nGmail Integration\n.\nConnect both your work and personal Google Calendars at \nGoogle Calendar Integration\n.\nIn Composio use the following entity_id values for the integrations:\nENTITY_ID_WORK_ACCOUNT=\"work\"\nENTITY_ID_PERSONA_ACCOUNTL=\"personal\"\n\n2. Set Up OpenAI API Key\n\nAdd your OpenAI API key to the .env file:\nOPENAI_API_KEY=\"your_openai_api_key_here\"\n\n3. Set Up Telegram Bot\n\nCreate a new Telegram bot following \nthis guide\n.\nObtain your bot token and add it to the .env file:\nTELEGRAM_BOT_TOKEN=\"your_telegram_bot_token_here\"\n\n4. Run the MVP\n\nFind the code in the attached compressed file named ai-assistant-mvp.zip.\nEnsure you have Docker installed. To start the project, navigate to the root directory and run:\ndocker compose up\n\nOnce the containers are running, you can interact with your assistant via Telegram. Since we are using OpenAI services via API to run the system, large computational requirements are unnecessary. A PC with at least 8 GB of RAM would be sufficient.\n\nConcluding Remarks 🧩\n\nThroughout the development and testing of this MVP, I interacted with the assistant via Telegram and closely monitored its behavior using LangSmith. This constant feedback loop revealed that, while the assistant performs well, there are still many production-level details to refine—ranging from minor prompt tweaks in specific nodes to adding new tools to support edge cases.\n\nDespite these areas for improvement, the assistant consistently demonstrated the ability to handle complex tasks that required coordination between different services—such as retrieving availability from Google Calendar and then drafting emails in Gmail based on that context. In several instances, I was genuinely surprised by the system’s dynamic behavior. Rather than simply following a static workflow, the agent-based architecture allowed the assistant to make independent decisions and adapt to the input, showcasing its flexibility and reasoning capabilities.\n\nThis experience validated not only the technical feasibility of the multi-agent architecture but also its suitability for real-world business logic. Beyond simply automating static departmental workflows, this architecture is capable of adapting to dynamic, context-sensitive tasks that require reasoning and decision-making based on the current environment. A key strength lies in its ability to interact with the external world, not just operate based on predefined instructions or hard-coded logic. This is made possible by integrating tools within the agent system, which provides real-time access to external data and enables grounded decision-making based on up-to-date information. This approach aligns with the increasing industry trend toward agentic AI systems—capable not just of executing but of intelligently orchestrating actions across tools and services in a context-aware manner.\nFrom an industry perspective, the demand for personal AI assistants and task automation agents is growing rapidly, driven by the need to reduce repetitive workloads, improve decision-making support, and increase operational efficiency. Architectures like this one position themselves as a natural fit for enterprise use cases that require scalable, modular, and intelligent orchestration of tools and information systems.\n\nOther key takeaways:\nGPT-4o clearly outperformed GPT-4o-mini in tasks that required deeper reasoning and multi-step planning.\nThe modular nature of the architecture greatly simplified the process of isolating, debugging, and improving specific components.\nComposio enabled smooth integration with external APIs. However, ensuring clarity in API responses and robust error handling remains crucial for long-term reliability.\n\nIn conclusion, this MVP produced accurate and valuable results during testing. It not only validated the design principles and toolchain used but also highlighted the potential of this approach to evolve into a robust, production-ready assistant. The insights gained provide a strong foundation for future iterations—bringing us closer to building AI agents that can reason, adapt, and truly assist across various domains.\n\nActual Limitations 🚧\n\nWhile this MVP demonstrates the technical feasibility and adaptability of an agent-based architecture, it also presents several important limitations that must be considered for its evolution into a more robust, production-ready system:\n\nDependency on External Integrations (Composio): The current system relies on Composio as an intermediary platform to perform actions on Google services (Gmail and Calendar). While this accelerates initial development and avoids building integrations from scratch, it also introduces a critical external dependency. If Composio experiences service outages, limits functionality, or changes its usage policy, the assistant may become non-functional. Additionally, this dependency may impact overall performance and introduce constraints regarding scalability, data privacy, and long-term control.\n\nLack of Native Tools for Calendar Conflict Management: Currently, calendar conflict detection relies primarily on the LLM's reasoning ability, which is not always reliable, especially when using lighter models such as GPT-4o-mini. Ideally, a custom-built conflict management tool should be implemented to handle overlapping events, enforce custom rules or prioritization logic, and improve precision in availability checks. This would enhance the reliability and user experience of the assistant.\n\nResponse Latency Due to Hierarchical Flow: The adopted architecture—based on multiple hierarchical layers (orchestrator, managers, workers)—offers modularity and clear separation of concerns but also introduces response latency. In real-time scenarios (e.g., conversational assistants), this design can become suboptimal. Each user request must pass through several steps before a final response is generated, which can lead to delays that negatively impact user perception.\n\nOne Worker per Account: Limited Scalability: In the current implementation, each worker is tightly coupled to a specific account (personal or work), which limits the scalability of the system. This structure requires the creation of a new worker for every new account to be supported, resulting in redundancy and increased maintenance overhead. A better approach would involve dynamic, parameterized workers that receive account identifiers or credentials at runtime, enabling more flexible and reusable logic.\n\nLack of Execution Validation Mechanisms: Currently, the Orchestrator Executor executes the defined action plan without intermediate validation mechanisms to ensure that each step is completed correctly. This can result in a manager producing an incorrect or incomplete output. Yet, subsequent steps continue as if the result were valid—leading to incorrect or incoherent final responses. To mitigate this, intermediate output validation and quality checks should be introduced, along with fallback or recovery strategies in the event of partial failures, to ensure overall workflow robustness in real-world usage.\n\nFuture Work 📋\n\nWhile the current MVP lays a solid foundation, several areas have been identified for future improvement and development:\n\nPrompt Refinement & Tool Expansion: Fine-tuning specific node prompts and expanding the system’s toolset will improve reliability and adaptability in more complex scenarios.\nModel Optimization: Continue leveraging more capable models (like GPT-4o) for deep reasoning tasks while exploring hybrid strategies to balance performance and cost.\nIntegration of Additional Managers: Incorporate new functional modules, such as a contact manager, web search manager, or even enterprise data connectors, to enrich the assistant's capabilities and make it more versatile in business settings.\nImproved Error Handling: Enhance the system’s ability to handle tool/API failures gracefully, especially under high uncertainty or incomplete user input.\nFine-tuning: Perform fine-tuning to ensure more tailored responses with less margin of error.\nScalability Testing: Conduct stress tests with more users and concurrent workflows to ensure the architecture scales effectively in production environments.\n\nThis roadmap aims to transition the assistant from an MVP to a robust, production-grade system capable of supporting real-world, decision-driven workflows in personal and enterprise contexts.\n\nContact & Support 📬\n\nIf you have any questions, encounter issues, or would like to contribute to this project, feel free to reach out:\n\n📧 Email: \nmalejandroquesada@gmail.com\n🐙 GitHub: \nMAQuesada\n\nWe welcome feedback, contributions, and collaborations!\n\nSome Examples of use 🎯\n\nHere are some examples of the agent’s capabilities. We encourage you to try them out yourself. 🤖\n\n\n\n\n\n\n\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\n🤖🧠🤖 Personal Assistant to Manage Your Google Accounts\n\n👤👤 \nMulti-Agent System\n\nKey Benefits of Multi-Agent Systems:\n\nMVP Details\n\n💬 LLM\n\nFramework used ⛓️\n\nTools 🛠️\n\nMVP Functionality ⏰ 📅 📧\n\nArchitecture Used 🏛️\n\nOrchestrator Input 🔗\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nFiles\nai-assistant-mvp.zip\nYou might be interested\nGoogle Workspace Agent: Your AI Assistant for Seamless Workspace Management 🤖🚀\nU\nJul 14, 202529 reads\nAgentic AIGoogle-Workspace+2\nGoogle Workspace Agent: From Prototype to Real-World Readiness 🚀\nU\nSep 06, 2025130 reads\nAgenticAIAIProductization+3\nWhat is Agentic AI (AAIDC-Week1-Lesson-1)\nMay 15, 20255357 reads\nAAIDCAgentic AI+5\nAgentic AI Applications\nDistinguished Technical Deep-Dive\nY\nMar 24, 202583 reads\nAgentic AIArtificial intelligence+3",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "product-inspection-with-renesas-rzv2l-and-edge-impulse-juzV7l5MbLbT",
    "username": "Solomon Muhunyo Githu",
    "license": "MIT License",
    "title": "Product inspection with Renesas RZ/V2L and Edge Impulse",
    "publication_description": "Back to publications\nDec 27, 2024\n●\n66 reads\n●\nMIT License\nWinner of\nMost Innovative Project\nat the\nComputer Vision Projects Expo 2024\nProduct inspection with Renesas RZ/V2L and Edge Impulse\n3d printing\ncomputer vision\nEdge AI\nmachine learning\nquality inspection\nSolomon Muhunyo Githu\nLike\nBookmark\nShare\n\nOverview\n\nIn casting process, liquid material is poured into a mold with a cavity that corresponds to the shape of the finished product. A casting defect could develop during the liquefaction of the material. Casting defects can be in many different forms such as shrinkage, pinholes, blow holes, etc.\n\nAll industries have a quality inspection department for separating the defective products from the non-defective ones. In this case, the main issue is that the inspection accuracy normally depends on human accuracy. Computer Vision based systems are therefore the primary solution to be used in this task.\n\nSolution\n\nIn this project I implemented the approach of computer vision based cast inspection. I trained a YOLOv5 object detection model using \nEdge Impulse platform\n and deployed it to the \nRenesas RZ/V2L Evaluation Board Kit\n. A \nGoogle Coral camera\n first takes a top view image of a submersible pump impeller moving on a conveyor belt. An application running on the Renesas RZ/V2L then classifies if the impeller is good or defective. The time per inference is around 63 milliseconds which gives around 16 frames per second (fps). This low latency during inference can be related to the fact that the Renesas RZ/V2L board is designed for vision AI applications and it offers a powerful hardware acceleration through its Dynamically Reconfigurable Processor (DRP) and multiply-accumulate unit (AI-MAC).\n\nComponents and Hardware Configuration\n\nSoftware components:\n\nEdge Impulse Studio account\nEdge Impulse for Linux\n\nOptional software components for rebuilding the Web application:\n\nEdge Impulse Python SDK\nFlask\nPyInstaller\n\nHardware components:\n\nRenesas RZ/V2L Evaluation Board Kit\nUSB Type-C cable “AK-A8485011” (manufactured by Anker)\nUSB PD Charger Anker “PowerPort III 65W Pod” (manufactured by Anker)\nEthernet cable\n12V AC/DC adapter\n470 Ohm resistor\n10k Ohm multiturn trimmer potentiometer\n1M Ohm resistor\nIRF540 mosfet\nA 3cm x 3cm strip board\n4 2.5mm pin spaced male header pins\nA soldering gun and some solder wire\nAt least four male-female jumper wires\nN20 3mm DC 6V 200RPM Metal Gear motor\nMini conveyor assembly\nRenesas RZ/V2L Evaluation Board Kit support\nAt least 20 pairs of M3 bolts and nuts\nData collection process\n\nI used the \ncasting product image data for quality inspection\n dataset on Kaggle. This dataset contains top view gray-scaled and augmented images of submersible pump impellers. The dataset has folders for defective and non-defective images of submersible pump impellers.\n\nIn total, I had 1300 images: 1047 for training and 253 for testing.\n\nWith 1300 images it would be tiresome to draw bounding boxes for all the objects. Edge Impulse offers various \nAI-assisted labelling methods\n to automate this process. In my case, I chose to track object within frames and the bounding boxes as well as their labels were set automatically.\n\nTraining and building the model\n\nIn the Edge Impulse platform, an \nImpulse\n is a machine learning pipeline that indicates the type of input data, extracts features from the data and finally a neural network that trains on the features from your data.\n\nFor the YOLOv5 model, I used an image width and height of 160 pixels and the \"Resize mode\" set to \"Squash\". The processing block was set to \"Image\" and the learning block set to \"Object Detection (Images)\".\n\nUnder \"Image\" in Impulse design, the color depth of the images is set to RGB and the next step was to extract features.\n\nOn the \nFeature explorer\n, we can see that the blue and orange data don't completely separate from each other. This can be related to the fact that both defective and good impellers have similar features like their disc-shaped property. We can see that there is a trend of some sort on the data; the blue data are grouped together on one side and the orange data grouped together on another side. What separates the good and defective impellers is the bad \"features\" like cracks, holes and protrusions.\n\nIn the features tab we can also see the on-device performance for generating features during the deployment. These estimated metrics are for the Renesas RZ/V2L(with DRP-AI accelerator). The Renesas RZ/V2L Evaluation Board Kit is supported by Edge Impulse. This board is designed for vision AI applications and it offers a powerful hardware acceleration through its Dynamically Reconfigurable Processor (DRP) and multiply-accumulate unit (AI-MAC).\n\nCurrently, all Edge Impulse models can run on the RZ/V2L CPU which is a dedicated Cortex A55. However, so that I can benefit from the DRP-AI hardware acceleration, I chose a YOLOV5 model. Note that on the training page you have to select the RZ/V2L (with DRP-AI accelerator) before starting the training in order to tell the studio that you are training the model for the RZ/V2L. This can be done on the top right in the training page or by changing target device in the Dashboard page.\n\nAfter training the model on various parameters, I settled with 100 training cycles and a learning rate of 0.001. It is however advised to train a YOLOv5 model using more than 1500 photos per class and more than 10,000 instances per class to produce a robust YOLOv5 model.\n\nAfter the training process, I got a precision score of 96%. Precision is the number of True Positives divided by the number of True Positives plus the number of False Positives.\n\nModel testing\n\nAfter training a model, we need to do a test with the unseen(test) data. In my case, the model had an accuracy of 91%. This accuracy is a percent of all samples with a precision score above 98%.\n\nI chose this as an acceptable performance and proceeded to deploy the model to the Renesas RZ/V2L board.\n\nYou can find the public Edge Impulse project here: \nSubmersible pump impeller defect detection\n. To add this project to your Edge Impulse projects, click “Clone this project” at the top of the window.\n\nDeploying the model to the Renesas RZ/V2L\n\nThe Renesas RZ/V2L Evaluation Kit comes with the Renesas RZ/V2L board and a 5-megapixel Google Coral Camera. To setup the board, Edge Impulse has prepared a \nguide\n that shows how to prepare the Linux Image, install \nEdge Impulse for Linux\n and finally connecting the board to Edge Impulse Studio.\n\nAfter the Renesas RZ/V2L board has been setup we can SSH into the board by an ethernet connection between a computer and the board, or the board and a router. To SSH into the RZ/V2L we can run the following terminal/Command prompt command on a computer:\n\nssh root@smarc-rzv2l\n\n\nTo run the model locally on the RZ/V2L we can run the command edge-impulse-linux-runner which lets us log in to our Edge Impulse account and select the cloned public project. The model will be downloaded and inference will start automatically.\n\nAlternatively, we can also download an executable of the model which contains the signal processing and ML code, compiled with optimizations for the processor, plus a very simple IPC layer (over a Unix socket). This executable is called \n.eim model\n.\n\nTo do a similar method, create a directory and navigate into the directory:\n\nmkdir submersible_pump_impeller_classification_with_Edge_Impulse && \\\ncd submersible_pump_impeller_classification_with_Edge_Impulse\n\n\nNext, download the eim model with the command:\n\nedge-impulse-linux-runner --download modelfile.eim\n\n\nNow we can run the executable model locally using the command:\n\nedge-impulse-linux-runner --model-file modelfile.eim\n\n\nHere, we pass the name of the downloaded file modelfile in the command.\n\nWe can go to the provided URL(RZ/V2L IP address at port 4912) and we will see the feed being captured by the camera as well as the bounding boxes if present.\n\nResults - An industrial demo of using AI in quality inspection\n\nUsing the eim executable and \nEdge Impulse Python SDK\n, I developed a Web Application using \nFlask\n that counts the number of good and defective submersible pump impellers. The taken images and the counts are then displayed on the Webpage in real time.\n\nAfterwards I designed and 3D printed some PLA based parts that can be assembled to a mini conveyor belt. These parts can be downloaded on \nprintables.com\n. The mini conveyor mechanism assembly is made up of corner brackets, rollers, roller support, M3 bolts and even a motor housing. I assembled the conveyor on a board platform made of chip plywood. I also designed and 3D printed a crate that is used to catch the parts as they are falling from the conveyor.\n\nThe conveyor mechanism is actuated by one N20 3mm DC 6V 200RPM Metal Gear motor. The motor housing has a chamber to put a circuit board for driving the motor. A cable management channel is also provided for the wires running from the motor driver circuit to the motor.\n\nTo assemble the conveyor mechanism the following parts are needed:\n\n4 corner brackets\n4 roller supports\n2 rollers\n1 motor support\n1 motor support cover\nAt least 20 pairs of M3 bolts and nuts (if you plan to screw 2 out of 4 four holes)\n1 crate\n\nThe motor driver circuit is based on this \ntutorial\n. It has one DC input and a DC output. I used a 12V AC/DC adapter to power the conveyor mechanism. The speed of the motor can be controlled by adjusting the potentiometer.\n\nI also designed and 3D printed a support for the Renesas RZ/V2L Evaluation Board Kit components. This support is made up of two parts: a base support for the board and a part for increasing the height of the base support. The Google Coral camera can be mounted upright or facing downwards through the provided slots. This Renesas RZ/V2L EVK support is also available for downloaded on \nprintables.com\n.\n\nFinally, I printed some 3cm wide submersible pump impeller images from the testing dataset. As the conveyor belt is moving, the Google Coral camera takes a picture of the printed images and the results are shown on the WebApp.\n\nHere is a demo of the application classifying submersible pump impellers as they are moving on the conveyor belt.\n\nThe source code for the WebApp is available in this \nGitHub repository\n. It can be cloned on the Renesas RZ/V2L board and the installation steps can be found in the repository. You can run the Flask application using Python(after installing Flask) or run the binary executable built with PyInstaller for AARCH64 platform.\n\nTracking items on the conveyor belt\n\nIn the project \nObject detection with centroid-based tracking\n, I documented on how to implement object detection and tracking using a simple \nPython script\n. This object tracking algorithm can be applied to a wide range of use cases, such as counting items on a conveyor belt that transports manufactured products for inspection using computer vision.\n\nIn this project, we could enhance the conveyor belt system to perform additional actions, such as sorting good and defective submersible pump impellers. The object tracking algorithm can identify and track these impellers, enabling a robotic arm to automatically sort them based on their quality.\n\nConclusion\n\nThis project has shown that we can move closer to zero manufacturing defects on cast production by integrating Machine Learning for visual inspection. This approach does not only increase the inspection accuracy but also eliminates fatigue since machines can perform their tasks full day without fatigue.\n\nAn interesting future project could be to advance the conveyor mechanism so that the defective parts can be moved to a separate location. Renesas has a documentation on how to set the GPIO with the \nRenesas Flexible Software Package (FSP)\n for writing applications.\n\nCredits:\n\nhttps://www.homemade-circuits.com/dc-motor-speed-controller-circuits/\nhttps://www.kaggle.com/datasets/ravirajsinh45/real-life-industrial-dataset-of-casting-product\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nOverview\n\nSolution\n\nComponents and Hardware Configuration\n\nData collection process\n\nTraining and building the model\n\nModel testing\n\nDeploying the model to the Renesas RZ/V2L\n\nResults - An industrial demo of using AI in quality inspection\n\nTracking items on the conveyor belt\n\nConclusion\n\nComments\nCode\nDatasets",
    "awards": [
      "most innovative project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "realtime-monitoring-of-singapore-traffic-using-yolov11-syTbHJec6enz",
    "username": "Namit Arora",
    "license": null,
    "title": "Real-Time Monitoring of Singapore Traffic Using YOLOv11",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n112 reads\n●\nCreative Commons Attribution (CC BY)\nWinner of\nBest Technical Implementation\nat the\nComputer Vision Projects Expo 2024\nReal-Time Monitoring of Singapore Traffic Using YOLOv11\nRealtime Dashboard\nTraffic Jam Detection\nVehicle classification\nVehicle Detection\nYOLO\nNamit Arora\nLike\nBookmark\nShare\nView \nDashboard\n\nAbstract\n\nThis project focuses on the development of a \nreal-time traffic monitoring\n system for Singapore, utilizing cutting-edge machine learning techniques to detect and analyze traffic conditions. By integrating live data from government APIs, a \ndynamic dashboard\n was created to display real-time traffic information. The system employs the YOLOv11 model from RoboFlow to detect and classify various vehicle types, including cars, trucks, buses, and two-wheelers.\n\nTo enhance the model's performance, multiple RoboFlow datasets were combined to create a comprehensive dataset capable of accurately detecting and categorizing vehicles. A time-series dataset was generated by performing inference on a single camera feed, using vehicle count and occupation ratio as primary features. Anomaly detection was then applied using Z-score analysis to identify unusual traffic patterns, such as traffic jams. This approach demonstrates an effective solution for real-time traffic monitoring and congestion detection, offering valuable insights to optimize traffic flow and enhance urban mobility.\n\nIntroduction\n\nUrban traffic congestion is a growing global challenge, requiring efficient real-time solutions to manage traffic flow and enhance safety. Traditional traffic monitoring systems often fall short in scalability and real-time responsiveness. This project introduces a flexible and scalable framework for real-time vehicle monitoring, designed to be deployed in cities worldwide. The framework leverages advanced machine learning and computer vision technologies, including the YOLOv11 model for accurate vehicle detection and classification, and anomaly detection using time-series data to identify traffic disruptions like jams.\n\nAs a case study, Singapore's traffic data is utilized to demonstrate the effectiveness of this framework. By integrating publicly available government traffic APIs with real-time vehicle detection, the system provides insights into traffic patterns and enables proactive responses to congestion. The framework can be adapted to different urban environments, offering a cost-effective solution for global traffic management that enhances operational efficiency and safety.\n\nMethodology\n\nThis section describes the processes followed to obtain a vehicle detection and classification model.\n\nUnderstanding the Data\n\nTo begin with, it is essential to explore the characteristics of the data before proceeding with any further analysis or model development. For this project, we utilized web scraping to gather both images and their associated metadata from the various traffic cameras installed across Singapore, which are publicly available on government websites.\n\nAfter scraping the data, the following key observations were made:\n\nTotal number of unique expressways: This refers to the number of expressways across Singapore covered by the traffic cameras.\nTotal number of unique image sizes: Images collected from different cameras came in varying resolutions and aspect ratios.\nUnique image sizes: Specific dimensions of the images were identified, providing insight into the resolution variations across different cameras.\nTotal number of cameras installed: The dataset includes a diverse set of cameras installed across the city, capturing traffic images from multiple locations.\n\nUpon reviewing a subset of the camera images, the following observations were made:\n\nVarying Camera Angles and Lighting Conditions: Images captured from different locations show diverse perspectives due to varying camera angles. Lighting conditions also differ, especially between day and night.\n\nInterference from Light Sources: Nighttime images exhibit interference from artificial light sources such as street lights, which may cause glare and affect the clarity of vehicle detection.\n\nMotion Blur: Some images contain motion blur, likely due to fast-moving vehicles, making it difficult to clearly identify vehicles in the frame.\n\nVisibility of Vehicles: In certain images, vehicles are barely visible due to poor lighting or camera positioning, adding another layer of complexity to the analysis.\n\nObstructions in the Frame: There are instances where objects like trees or other structures obstruct the view of vehicles, further complicating the vehicle detection process.\n\nThese observations highlight the challenges involved in using traffic camera images for vehicle detection and traffic analysis, emphasizing the need for robust image processing and machine learning techniques to handle such inconsistencies and variabilities in the data.\n\nData Collection\n\nTo obtain suitable annotated data for vehicle detection, several traffic image datasets of Singapore were sourced from Roboflow Universe. The following steps were performed to prepare the data:\n\nDataset Merging: Five datasets containing annotated traffic images with detection boxes for vehicles were combined to create a comprehensive dataset.\n\nClass Remapping: Due to inconsistent labeling across the datasets, the Class Remapping feature of Roboflow was used to standardize the vehicle categories into four consistent classes: car, two-wheeler, truck, and bus.\n\nClass Distribution\n\n\nThe above bar graph shows the class distribution of dataset before data preprocessing.\n\nData Preprocessing\n\nGiven the limited number of available images, data augmentation was employed to enhance the diversity of the training dataset and improve the robustness of the model to varying image conditions. The following transformations were applied to the images:\n\nGrayscale: 12% of the images were converted to grayscale to simulate lighting variations and reduce the model’s reliance on color features.\n\nBrightness Adjustment: A random adjustment of brightness in the range of -15% to 15% was applied to introduce variability in lighting conditions across the dataset.\n\nExposure Adjustment: The exposure levels of the images were randomly modified within a range of -10 to 10% to simulate different lighting environments, such as daytime and nighttime conditions.\n\nBlur: A blur effect with a maximum radius of 1.1 pixels was applied to some images to replicate the presence of motion blur and ensure the model could handle less-than-ideal image quality.\n\nThese preprocessing steps were designed to increase the variability of the training set, thereby enhancing the model's ability to generalize and perform well under different environmental conditions and image qualities.\n\nModeling\nModel Selection\n\nYOLOv11 was selected for vehicle detection in this project due to its real-time performance, high accuracy, and flexibility. YOLOv11 excels at detecting small and overlapping objects, making it ideal for crowded traffic scenes. Its ability to be fine-tuned for specific vehicle classes (e.g., cars, trucks, buses, two-wheelers) and its scalability to handle varying image sizes further justified its use.\n\nDataset Split\n\n\nAfter data augmentation and preprocessing, most images were allocated to the training set to enhance model generalization. The test set was handpicked to ensure diversity in vehicle types, lighting, and camera angles, providing a comprehensive evaluation of the model's performance.\n\nModel Training\n\nThe YOLOv11 model was initialized with pre-trained weights from the COCO dataset and fine-tuned on our traffic dataset. Training results showed that the model achieved a mean Average Precision (mAP) of 63% on the validation set. The loss curves for box loss, class loss, and object loss were also tracked, reflecting the model’s progress in accurately predicting bounding boxes, classifying vehicles, and detecting objects.\n\nModel Evaluation\n\n\nThe performance of the YOLOv11 model was evaluated on the test set, achieving a mean Average Precision (mAP) of 63.7%, as illustrated. This indicates that the model demonstrates reasonable effectiveness in detecting and classifying vehicles in diverse traffic conditions.\n\nHyperparameter Tuning For Inference\n\nTo optimize the model's performance in real-world conditions, hyperparameters were fine-tuned based on the analysis of live traffic images from the dashboard. Specifically, the confidence threshold was set to 0.3 and the overlap value (IoU threshold) was adjusted to 0.5. A lower confidence threshold was selected to mitigate false negatives, as higher confidence levels led to missed detections in certain scenarios, such as challenging lighting conditions or obstructions. This balance ensured improved detection rates while maintaining precision in crowded and dynamic environments.\n\nTraffic Jam Detection\n\nIn this section, the methodology utilized to detect traffic jams on a selected road using traffic detection and classification is described. The process involves data collection, image preprocessing, vehicle detection, feature extraction, and anomaly detection to identify significant traffic events.\n\nData Collection\n\nFor this experiment, a specific traffic camera from the Singapore government’s publicly available api was selected. Images were collected from the camera at 5-minute intervals over a one-week period through the \ngovernment’s API\n.\n\nImage Preprocessing and Masking\n\nTo focus on the relevant section of the road, a mask was applied to each image, segmenting only one side of the road. This masked region was retained for further analysis, ensuring that only the desired road segment was considered in the traffic analysis.\n\nVehicle Detection and Classification\n\nThe count of each vehicle type was recorded for each image using model inference through model API endpoint. Additionally, the occupation ratio for each image was computed, which is defined as the ratio of the union of all detection boxes to the total segmented area (after masking the road). This ratio represents the proportion of the road occupied by vehicles.\n\nTime Series Data Construction\n\nThe detected vehicles and classification results were compiled into a time series dataset containing the following features:\n\nTotal number of vehicles\nNumber of cars\nNumber of two-wheelers\nNumber of buses\nNumber of trucks\nOccupation ratio\nTimestamp\n\nThis time series dataset formed the basis for the analysis of traffic anomalies.\n\nAnomaly Detection Using Z-Scores\n\nAnomaly detection was performed using the Z-score method, which measures how many standard deviations a data point deviates from the mean. The Z-score for a given data point  is calculated using the following formula:\n\nWhere:\n\n is the data point (either Total Vehicles or Occupation Ratio),\n is the mean of the data,\n is the standard deviation of the data.\n\nAnomalies are identified when the absolute value of the Z-score exceeds a predefined threshold , which corresponds to the significance level. For example, for a significance level of 0.1% (0.001 p-value), the threshold is set at . Thus, an anomaly is flagged if:\n\nThe anomaly detection process identified significant deviations in both Total Vehicles and Occupation Ratio across several time window of 15 minutes. These deviations were flagged as high traffic anomalies, indicative of potential traffic jams or congestion events.\nThis approach utilizes both the Total Vehicles and Occupation Ratio to identify anomalies, as both features are critical for detecting traffic congestion. The Total Vehicles feature provides information on the volume of traffic, while the Occupation Ratio reflects the extent of road occupancy. By considering both features together, the method ensures that anomalies are detected only when both the volume of traffic and the level of road occupancy exceed normal conditions. This combined approach helps to better capture scenarios where both high traffic volume and high occupancy indicate potential traffic jams, thus providing a more accurate and reliable anomaly detection.\n\n\nA scatter plot of Total Vehicles versus Occupation Ratio was generated to visually represent the detected anomalies. Data points representing normal traffic conditions were depicted in blue, while the high traffic anomalies were highlighted in red.\n\nThe image above depicts a potential traffic jam observed during the time window from 18:45 to 19:00 on December 11, 2024. During this period, the mean number of vehicles detected was 40, with an average occupation ratio of 0.17.\n\nConclusion\n\nThis work presents a scalable traffic monitoring framework that automatically detects, classifies, and identifies traffic congestion using computer vision. By processing traffic camera feeds, the system classifies vehicles and assesses road occupancy, enabling traffic jam detection without the need for manual labeling. A real-time dashboard displays live images and traffic data from Singapore’s cameras, offering an intuitive interface for monitoring traffic conditions.\n\nWhile demonstrated with Singapore data, this framework is adaptable to other urban settings, providing valuable insights for traffic management and urban planning.\n\nCode and Resources\n\nThe code for the project can be referenced using \nthis colab notebook\n.\nThe github repo for the traffic jam detection : \nhttps://github.com/BlazeStorm001/singvms-anomaly\n\nThe github repo for the website dashboard : \nhttps://github.com/BlazeStorm001/singvms\n\nReferences\nRedmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You Only Look Once: Unified, Real-Time Object Detection. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nChandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly Detection: A Survey. ACM Computing Surveys (CSUR).\nRoboflow. (2023). Roboflow Docs. \nRoboflow Documentation\n.\nUltralytics. (2024). YOLOv11. \nYOLOv11 Documentation\n. Accessed: 2024-12-30.\nAcknowledgements\n\nThanks to the Roboflow users \"fyp car dataset\", \"Singapore\", \"is3107\", \"Traffic Dataset\" for making the datasets for Singapore Traffic available on the Roboflow Universe.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nView \nDashboard\n\nAbstract\n\nIntroduction\n\nMethodology\n\nUnderstanding the Data\n\nData Collection\n\nClass Distribution\n\nData Preprocessing\n\nModeling\n\nModel Selection\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nYou might be interested\nTraffixVision\nV\nDec 31, 202415 reads\nRoad Traffic Recognition Using Convolutional Neural Networks (CNN) | OpenCV Python\nS\nDec 30, 202435 reads\nAI Traffic ManagementConvolutional Neural Networks (C+14\nRevolutionizing Traffic Control: Harnessing the Power of YOLOv8 Object Detection System\nK\nDec 28, 202449 reads\nComputer VisionCV+3\nTrafficEYE\nNov 21, 202440 reads\nComputer VisionFYP+4",
    "awards": [
      "best technical implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "resumate-resume-optimizer-VAwSIObapKkf",
    "username": "Namit Arora",
    "license": null,
    "title": "ResuMate: Resume Optimizer ",
    "publication_description": "Back to publications\nMar 31, 2025\n●\n53 reads\n●\nCreative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nResuMate: Resume Optimizer\nAgentic AI\nLLM\nWorkflow\nNamit Arora\nLike\nBookmark\nShare\nAbstract\n\nHave you spent countless hours trying to tailor your resume for a specific job? Needless to say, it is one of the most tedious and boring tasks that you can ever do. Even if you have the right skills for the job, if your resume does not catch the eye of the Hiring Manager, it is rendered worthless. In the era of artificial intelligence, your resume will go through not only a human but also an applicant tracking system (ATS) that may not be able to parse your resume properly and may even reject you if it does not find the right set of keywords. Keeping all these things in mind, there was a need to develop a system that can generate resumes that are tailored to the job description, are easily understood by the ATS, and have a clean and professional outlook with minimal language errors. ResuMate is designed to produce such resumes within minutes and make job hunting easier for people in technology.\n\nIntroduction\n\nMost resumes get rejected by an automated Applicant Tracking well before they are seen by an Hiring Manager. These applicant tracking systems, may not be able to parse resumes that have a complex layout or contain too many unnecessary items other than text. Also, most HR deal with many resumes daily, and give at max 30 seconds of attention to any resume. Thus, it becomes inevitable to design a resume that not only looks professional but is also well understood by the Applicant Tracking Systems. ResuMate is an agentic workflow that uses LLMs to transform your existing resume to a professional-looking, minimalistic, and easily parseable PDF. The workflow can be accessed using the \nWeb App\n interface. It has the following salient features:\n\nProvides suggestions to improve your existing resume based on the parseability, language usage, job description. and actionable insights.\n\nGeneration of a clean single-column LaTeX-based PDF resume with all the suggestions incorporated.\n\nMethodology\n\nThis section explains the architecture of the application and workings of the agentic workflow.\n\nWorkflow Architecture\n\nThe figure above shows the architecture of the agentic workflow. It was created keeping three things in mind:\n\nAccurate and reliable parsing of resumes with minimal errors\nMinimizing the number of API calls to LLMs to reduce cost.\nTransforming data into appropriate formats to ensure consistency and easier display on the frontend.\nUser Input\n\nThe user inputs the resume in pdf format along with an optional job description. The resume is first converted into jpg images (one image for each page) using \npdf2image\n and then converted to base64 strings that can be used to make API calls with the Multimodal Large Language Model (MLLM). Here we are using the Gemini Flash 2.0 model (see Experiments on why this model was selected) as the MLLM, which offers fast inference times and low hallucination rates. Below snippet shows the code for conversion:\n\nfrom pdf2image import convert_from_path\n\npdf_path = '/content/resume.pdf'\nimages = convert_from_path(pdf_path)\nfor i, image in enumerate(images):\n    image.save(f'page_{i + 1}.jpg', 'JPEG')\nParsing PDF\n\nWe use the \npdfminer.six\n library to extract information such as the most commonly used font and its size. The font information can be used to alert the user if an unprofessional-looking font or a small font size is being used. Also, the \nPyPDF2\n library has been used to extract text from the PDF. This acts as a fallback mechanism in case the MLLM model fails to service the API call. Moreover, PyPDF extracts hyperlinks such as Github Account URL, LinkedIn Page URL, email address, and other important Links from the PDF. A sample snippet has been shown which extracts information using PyPDF:\n\ndef extract_text_num_pages_and_links(pdf_stream):\n    reader = PdfReader(pdf_stream)\n    text = \"\"\n    links = []\n    num_pages = len(reader.pages)\n    # Extract text from all pages\n    for page in reader.pages:\n        text += page.extract_text() or \"\"\n\n        # extract links from annotations\n        if '/Annots' in page:\n                for annot in page['/Annots']:\n                    obj = annot.get_object()\n                    \n                    # Check if it's a link annotation\n                    if obj['/Subtype'] == '/Link':\n                        if '/A' in obj and '/URI' in obj['/A']:\n                            uri = obj['/A']['/URI']\n                            links.append(uri)\n\n    return {\n        \"resume_parsed_text\": text,\n        \"resume_num_pages\": num_pages,\n        \"resume_links\": links,\n    }\nPrompt for Resume Section Parsing\n\nResumes can be highly unstructured documents and may have completely different sections based on the candidate. Upon investigating many tech resumes, the most common sections present in tech resumes were found. For segregating the resume into the different sections the following prompt was used:\n\n\n**Instruction:**  \nExtract the following sections from the resume **if present** and output them **strictly in Markdown format**. Each section should have its own subheading (`##`). **Do not modify any text** from the resume—preserve all original formatting and wording. **Output only the extracted content in Markdown format, without any additional text.**\n\n----------\n\n**Resume Extraction Format:**\n\n```markdown\n## Candidate Name\n[Extracted Name]\n\n## Location of Candidate\n[Extracted Location]\n\n## Experience/Work History\n[Extracted Work History, including Freelance Experience]\n\n## Education\n[Extracted Education Details]\n\n## Position of Responsibilities\n[Extracted Responsibilities]\n\n## Skills\n[Extracted Skills]\n\n## Profile Summary / Objective\n[Extracted Profile Summary or Objective]\n\n## Projects\n[Extracted Projects]\n\n## Publications\n[Extracted Publications]\n\n## Interests / Hobbies / Extracurricular Activities\n[Extracted Interests, Hobbies, or Extracurricular Activities]\n\n## Awards / Achievements\n[Extracted Awards or Achievements]\n\n## Certifications\n[Extracted Certifications]\n\n## Languages Known\n[Extracted Languages]\n\n## Relevant Coursework\n[Extracted Relevant Coursework]\n\n\n\n**Rules for Extraction:**\n\n-   If a section is missing in the resume, **do not generate it** in the output.\n    \n-   **Do not add, modify, or infer** any information—strictly extract only what is present.\n    \n-   Maintain the **original text, order, and formatting** from the resume.\n    \n-   Ensure **only Markdown-formatted output** without any additional explanations or notes.\n\nHaving each section in Markdown format helps with easy display on the frontend side so that user can edit their resumes before improving them.\n\nPrompts for Markdown to JSON conversion\nTake the given **Markdown-formatted resume** and extract its contents into a structured **JSON format** as shown below. **Ensure the response strictly follows JSON syntax**, and the output should be a valid Python dictionary.  \n### **Instructions:**  \n- Extract information under the following sections:  \n  1. **Education**  \n  2. **Projects**  \n  3. **Experience** (includes Work History and Freelance)  \n  4. **Position of Responsibilities**  \n- **If a section is missing**, include the key in the JSON but leave it as an **empty list (`[]`)**.  \n- **Do not modify the original text**—preserve all content exactly as it appears in the resume.  \n- **All dates must be formatted as \"Abbreviated Month-Year\" (e.g., \"Jan-2023\").**  \n- **Strictly output only the JSON**—no additional text, explanations, or formatting.  \n---\n### **Output Format:**  \n\n\n{\n  \"Education\": [\n    {\n      \"Degree\": \"<Degree Name>\",\n      \"Institution\": \"<Institution Name>\",\n      \"Location\": \"<City/Remote>\",\n      \"Period\": {\n        \"Start\": \"<Start Date>\",\n        \"End\": \"<End Date>\"\n      }\n    }\n  ],\n  \"Experience\": [\n    {\n      \"Role\": \"<Role Title>\",\n      \"Company\": \"<Company Name>\",\n      \"Location\": \"<City/Remote>\",\n      \"Period\": {\n        \"Start\": \"<Start Date>\",\n        \"End\": \"<End Date>\"\n      },\n      \"Description\": \"<Brief Description of Responsibilities>\"\n    }\n  ],\n  \"Position of Responsibilities\": [\n    {\n      \"Title\": \"<Position Title>\",\n      \"Organization\": \"<Organization Name>\",\n      \"Location\": \"<City/Remote>\",\n      \"Period\": {\n        \"Start\": \"<Start Date>\",\n        \"End\": \"<End Date>\"\n      },\n      \"Description\": \"<Brief Description of Responsibilities>\"\n    }\n  ],\n  \"Projects\": [\n    {\n      \"Name\": \"<Project Name>\",\n      \"Description\": \"<Project Description>\",\n      \"Technologies\": \"<Technologies Used>\",\n      \"Period\": {\n        \"Start\": \"<Start Date>\",\n        \"End\": \"<End Date>\"\n      }\n    }\n  ]\n}\n\n-\n### **Strict Rules:**  \n**If a section is missing**, return an **empty list (`[]`)** instead of omitting it.  \n**No explanations, additional text, or formatting**—**only valid JSON output**.  \n**Preserve original text** exactly as found in the resume.  \n**Ensure JSON is well-formed and can be parsed directly in Python.**  \n\n---\n\n\nThe above prompt converts a group of sections from Markdown to json. The sections are divided into multiple groups, and separate identical prompts are made for each group. This is done to avoid any conflicts between sections and make it easier for the LLM to generate accurate json responses. Although one could directly use the MLLM to generate JSON data but it would lead to a higher number of API calls to a much costlier MLLM.\nEach LLM response is then checked using \nPydantic AI\n to ensure that only valid Json responses are received. Also, we use the Deepseek V3 model, which has a good balance of inference time and accuracy, performing well on general-purpose tasks with an \nMMLU(General) score of 88.5%\n.\n\nPrompts for Enhancing Sections\n\nNow that we have structured sections in front of us, we can prepare specific prompts to enhance each section using certain guidelines. For example, take a look at the prompt which can be used to improve the 'Projects' section in a resume.\n\n**Prompt:**  \n\nYou will receive **two inputs**:  \n1. A **Job Description (JD)** in text format.  \n2. A **JSON object** containing **projects** extracted from a resume in the following format:  \n\n\n{\n  \"Projects\": [\n    {\n      \"Name\": \"<Project Name>\",\n      \"Description\": \"<Project Description>\",\n      \"Technologies\": \"<Technologies Used>\",\n      \"Period\": {\n        \"Start\": \"<Start Date>\",\n        \"End\": \"<End Date>\"\n      }\n    }\n  ]\n}\n\n\n### **Task:**  \n- **Analyze the job description** and **align each project's description** according to it.  \n- **Prioritize** projects that match the **technologies, skills, and keywords** found in the job description. The most relevant projects should appear **first** in the JSON.  \n- **Modify the project descriptions** to subtly align them with the job description while keeping them **concise, clear, and well-structured**.  \n- **Format the project descriptions in Markdown**, ensuring:  \n  - Use of **bullet points** for clarity.  \n  - **Bold formatting** (`**skill**`) for skills/technologies mentioned in both the **job description** and the **project**.  \n- **Preserve all other data as is** (name, technologies, period, etc.), making no other modifications.  \n\n---\n\n### **Output Format:**  \n\nThe output must be **strictly JSON**, preserving the original structure but with:  \n1. **Projects reordered** based on relevance to the job description.  \n2. **Descriptions formatted in Markdown** with relevant skills in **bold**.  \n\nExample Output:  \n\n\n{\n  \"Projects\": [\n    {\n      \"Name\": \"AI Chatbot\",\n      \"Description\": \"- Developed an AI-driven chatbot using **Python** and **TensorFlow**.\\n- Integrated with **FastAPI** for real-time communication.\\n- Implemented **NLP models** for enhanced user interactions.\",\n      \"Technologies\": \"Python, TensorFlow, FastAPI, NLP\",\n      \"Period\": {\n        \"Start\": \"Jan-2023\",\n        \"End\": \"Dec-2023\"\n      }\n    },\n    {\n      \"Name\": \"E-commerce Website\",\n      \"Description\": \"- Built a full-stack e-commerce platform with **React** and **Next.js**.\\n- Implemented a **Node.js** backend with a **MongoDB** database.\\n- Optimized performance and user experience with **server-side rendering (SSR)**.\",\n      \"Technologies\": \"React, Next.js, Node.js, MongoDB, SSR\",\n      \"Period\": {\n        \"Start\": \"Mar-2022\",\n        \"End\": \"Oct-2022\"\n      }\n    }\n  ]\n}\n\n\n---\n\n### **Strict Rules:**  \n**Only output valid JSON**—no additional text, explanations, or formatting.  \n**Preserve all original fields** (except reordering projects and modifying descriptions as instructed).  \n**Descriptions must be formatted in Markdown**, using bullet points and **bolding** relevant skills.  \n**No other modifications** beyond those explicitly stated.  \n\n**Final instruction:** **Ensure the response is strictly JSON-compliant, ready to be parsed as a Python dictionary.**\n\nSimilarly, prompts are created for each section that is present in the resume. Also, the LLM responses are validated by Pydantic AI to ensure accuracy.\n\nPopulating the LaTeX template\n\nWith the enhanced resume sections ready, we populate the empty \nLaTeX template\n with the extracted information and obtain a nice and minimalistic resume.\n\nConversion to PDF\n\nAfter we have the LaTeX (.tex) file ready, we can compile it to PDF directly using \nTinyTeX\n, which has a much smaller footprint ideal for resource-constrained servers.\n\npdflatex document.tex \nQuery Optimization\nasync def get_llm_response(self, api_key, messages,  temperature=1, top_p=1, max_retries=10, initial_backoff=1):\n        \"\"\"\n        Fetches a response from the API with retry logic to handle rate limits.\n\n        Args:\n            api_key (str): API key for authentication.\n            messages (list): Messages to send to the model.\n            temperature (float): Sampling temperature for the response.\n            top p (float): top p value for the response.\n            max_retries (int): Maximum number of retries in case of rate limit errors.\n            initial_backoff (int): Initial backoff time in seconds.\n\n        Returns:\n            str: The content of the model response.\n        \"\"\"\n        attempt = 0\n\n        # Define the API URL\n        url = \"https://openrouter.ai/api/v1/chat/completions\"\n\n        # Define the headers for authentication\n        headers = {\n            \"Authorization\": f\"Bearer {api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        all_models = [\"deepseek/deepseek-chat-v3-0324:free\", \"meta-llama/llama-3.1-70b-instruct:free\",\"google/gemma-2-9b-it:free\",\"meta-llama/llama-3.2-3b-instruct:free\", \"meta-llama/llama-3.1-405b-instruct:free\", \"mistralai/mistral-7b-instruct:free\"]\n\n        # Define the request payload\n        payload = {\n            \"model\": None,\n            \"messages\": messages,\n            \"temperature\": temperature,\n            \"top_p\": 0.6,\n        }\n\n\n        async with aiohttp.ClientSession() as session:\n            while attempt < max_retries:\n                try:\n                    payload[\"model\"] = all_models[attempt % len(all_models)]\n                    print(f\"Attempt {attempt + 1} with API key: {api_key}\")\n                    # proxy_list = await FreeProxy().get(count=1)\n                    async with session.post(url, headers=headers,  data=json.dumps(payload), timeout=30, proxy=None) as response:\n                        if response.status == 200:\n                            data = await response.json()\n                            return data[\"choices\"][0][\"message\"][\"content\"]\n                        else:\n                            print(f\"Error: {response.status}, {await response.text()}\")\n\n                except aiohttp.ClientError as e:\n                    print(f\"HTTP error: {e}\")\n                except Exception as e:\n                    print(f\"Unexpected error: {e}\")\n\n                attempt += 1\n                wait_time = initial_backoff * (2 ** (attempt - 1))  # Exponential backoff\n                print(f\"Retrying in {wait_time} seconds (attempt {attempt}/{max_retries})...\")\n                await asyncio.sleep(wait_time)  # Wait before retrying\n\n        print(f\"Max retries reached. Could not fetch response.\")\n        return None  # Return None if all retries fail\n\nThe above snippet shows the mechanism for sending queries to LLM, it ensures that other fallback models are used in case of failure. Also, we use asynchronous programming to send multiple queries in an efficient way without blocking other queries.\n\nTech Stack\nBackend - FastAPI\nFrontend - Next.js with React\nOther - Docker\n\nThe web application was built using FastAPI for the backend and \nNext.js with React\n for the frontend. \nFastAPI\nis a modern, high-performance Python framework that leverages ASGI for faster execution, making it a popular choice for AI applications. Next.js was used for the frontend, providing a robust framework for building production-ready web applications.\nThe whole prompt framework was designed from scratch without using any readily available agentic frameworks to ensure optimum performance and pinpoint customization.\n\nExperiments\n\nThis section describes the various experiments that were performed to finalize the various tools in the project.\n\nPDF Parsing Tools\n\nAccurately parsing resumes is a critical task in this project. PDF, while commonly used for resumes, presents significant challenges due to its complex layout and the fact that each character is positioned at a specific location in the document. Several tools are available for parsing PDFs in Python, which can be broadly categorized into non-AI-based and AI-based solutions. Non-AI-based tools, such as PyPDF2 and pdfminer.six, offer fast parsing capabilities but struggle with accuracy, especially when handling complex or unstructured layouts. On the other hand, AI-based solutions like MLLMs (Multimodal Large-Language Models) and marker-pdf, an open-source tool that converts PDFs into structured Markdown format, provide more accurate parsing but at the cost of speed.\n\nBelow is a comparison of these tools based on speed, accuracy, and the availability of free APIs:\n\nTool\tSpeed\tAccuracy\tFree API Available\nNon-AI-based Tools\tHigh\tLow\tNot needed\nMLLM\tLow\tHigh\tYes\nmarker-pdf\tModerate\tModerate\tYes (Limited)\n\nWhile non-AI-based tools are fast, they fall short in terms of accuracy, which is crucial for parsing resumes accurately. Additionally, these tools do not directly output data in a structured format like Markdown. During experimentation, it was found that MLLM provided the best balance of high accuracy and usability, especially since it offers a free API with generous limits through \nOpenRouter\n. Therefore, MLLM was chosen for this project due to its superior overall performance compared to marker-pdf.\n\nSelection of MLLM\n\nThe selection of MLLM was based mainly on two criteria:\n\nHallucination Rates\nInference Time\n\nInference time of a Large Language Model (LLM) refers to the time it takes for the model to process input data (such as text, images, or both) and generate an output (e.g., a response, prediction, or transformation). Alongside inference time, the \nhallucination rate\n —which measures how frequently an LLM generates inaccurate or non-factual information—is another critical metric, especially when tasks like document summarization are involved. Both of these metrics are essential: reducing hallucinations ensures the accuracy and reliability of generated content, while faster inference times improve API response times, contributing to a more efficient user experience. Considering both factors, the Google Gemini Flash 2.0 model was selected for its strong performance in minimizing hallucinations while delivering \nfast inference times\n as compared to other MLLMs, making it an ideal choice for this project.\n\nOptimizing Hyperparameters\n\nThe two key hyperparameters considered were:\n\nTop P: This parameter controls the cumulative probability distribution of the next token generated by the model. It restricts the token sampling to a subset of candidates that together make up the top P probability mass. A lower value (e.g., 0.4) makes the model more focused, while a higher value (e.g., 0.8) allows for more diverse token choices.\nTemperature: Temperature controls the randomness of predictions. A lower temperature (e.g., 0.2) makes the model more deterministic and focused on high-probability tokens, while a higher temperature (e.g., 0.8) introduces more randomness and creativity in token selection.\n\nBoth parameters were tested with values ranging from 0 to 1, in increments of 0.2.\n\nFor the MLLM, a top P value of 0.4 and temperature of 0.2 were chosen to prioritize factual accuracy in responses.\n\nFor the LLM, which required some degree of creativity to enhance the resume, the temperature was set to 0.4 and top P to 0.6 to allow for more diverse and creative outputs.\n\nFuture Work\nAddition of different varieties of templates for users.\nSafeguarding privacy by concealing sensitive information contained in resumes.\nCode and Resources\nPreview Website\nBackend Code Repo\n [To be released soon]\nFrontend Code Repo\n [To be released soon]\nReferences\nOpenRouter Docs\nHallucination Leaderboard\nVellum LLM Leaderboard\nDeepSeek V3 model info\nGemini 2.0 Flash Model Info\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nMethodology\n\nWorkflow Architecture\n\nUser Input\n\nParsing PDF\n\nPrompt for Resume Section Parsing\n\nCandidate Name\n\nLocation of Candidate\n\nExperience/Work History\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nYou might be interested\nLLM Cover Letter Generation Script\nW\nMar 31, 20259 reads\nAutomationDeep Learning+2\nAI-Powered Recruitment Assistant: Smart Query Handling, CV Analysis, and Context-Aware Conversations\nL\nMar 09, 202517 reads\nATSChatbot+5\nResume Insights\nL\nMar 31, 202510 reads\nagentic-systemsgenai+4\nResume Application Tracking System\nOct 31, 202418 reads",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "retrieval-augmented-generation-rag-case-study-a-resume-analysis-tool-g1E903d62F6L",
    "username": "aAmina Javaid",
    "license": null,
    "title": "Retrieval Augmented Generation (RAG) Case Study - A Resume Analysis Tool",
    "publication_description": "Back to publications\nOct 25, 2024\n●\n132 reads\nWinner of\nDistinguished Technical Deep-Dive\nat the\nNLP Projects Expo 2024\nRetrieval Augmented Generation (RAG) Case Study - A Resume Analysis Tool\nChatbot\nGenerative AI\nLangChain\nLLM\nNLP\nPrompt Engineering\nResume Analysis\nRetrieval Augmented Generation\nA\nAmina Javaid\nLike\nBookmark\nShare\nIntroduction\n\nThe world of artificial intelligence (AI) is rapidly evolving and one of the most used techniques in AI, specifically natural language processing (NLP), nowadays is Retrieval Augmented Generation (RAG). The word retrieve means to get some information from a data source, augment means to expand or increase something, and generate means to create something new. Retrieval Augmented Generation (RAG) is a technique through which a user communicates with a specialized knowledge base utilizing the power of LLMs and gets accurate and contextually relevant information. RAG augments LLM data with private or domain-specific data, enabling the generation of more accurate and relevant responses. There are three main components of a RAG system - data ingestion, retrieval, and synthesis. In this article, we'll discuss the importance and components of a RAG system in detail and use that knowledge to build an end to end RAG application for a real-world use case - resume analysis - utilizing the powerful capabilities of large language models (LLMs) and LangChain — a Python framework to build robust RAG applications.\n\nImportance of RAG\n\nIn the age of information, thousands of resources are available on any given topic. Large language models have made our lives much easier in terms of information retrieval as they have been pre-trained on billions of parameters. We can ask LLMs any question, and they provide relevant information structured in the required format.\n\nOne major issue with LLMs is hallucination, where they fabricate answers to questions they don’t know, often presenting these fabrications as factual without indicating their uncertainty. This becomes a big question mark on the authenticity of the information being retrieved. Secondly, LLMs are pretrained on very large datasets and this training takes a lot of time, so they don’t have access to latest or real-time information. While some LLMs provide real-time information, the retrieved content can be inaccurate and filled with irrelevant details. Moreover, the responses aren’t verified as they lack access to domain-specific knowledge or personal data. LLMs are powerful in understanding the context of user queries, but they require new data to be attached to them that they don’t already have access to.\n\nOne way to solve these problems is to fine tune a particular LLM on our specific data but this process has a huge training cost associated with it. The other way is to use retrieval augmented generation (RAG) in which relevant information from our knowledge base is retrieved based on our query and then sent to an LLM to analyze and generate a response. Using RAG utilizes the capabilities of LLMs to craft a context augmented response for us. We attach external data to an LLM which then gives answers to our queries from that data. This approach gives us more control, allowing us to restrict the LLM to generate answers solely from our own data rather than from its pre-trained information.\n\nHigh Level Components of RAG\n\nRAG is a combination of retrieval-based and generation-based methods. It first retrieves relevant documents from a knowledge base and then uses that information to generate a response. Following are the high level components of RAG:\n\nPrompt\nData Source | Knowledge Base\nRetriever\nGenerator | LLM\nResponse\n\nBasic RAG Workflow\n\nHere's an overview of a basic RAG workflow:\n\nData is ingested into the system.\nThe user sends a prompt to the system.\nThe retriever identifies and retrieves relevant documents according to the prompt.\nLLM generates accurate and relevant response using the prompt and retrieved information.\n\nAdvantages of RAG\nCustomized Information — RAG enables access to the latest and domain specific information in a well-structured format.\nAccurate and Relevant Information — RAG ensures that the retrieved information is accurate, relevant, and useful.\nContext-Augmented Information — RAG generates meaningful responses based on the prompt and retrieved information.\nApplications of RAG\n\nEvery individual has a unique knowledge source from which they may need to extract relevant information at different times. For a student, it’s the syllabus and curriculum. For a doctor, it’s patient medical histories. For a lawyer, it’s case records and related information. For a bank manager, it could be the bank’s policies. The strength of RAG lies in its ability to take a user’s natural language query and return precise, relevant information from a knowledge source — presented in a way that is easily understood by even non-technical users. Additionally, RAG systems support multilingual environments, offering customized responses in the user’s language. They can also be multimodal, integrating images, speech, and videos alongside text to provide comprehensive answers.\n\nThere are numerous applications of RAG already being used in the industry. Chatbots and AI assistants are few of them. Here is a list of some real-world applications which can be built using RAG:\n\nCustomer support chatbots\nConversational agents | Virtual assistants\nDocument summarizers\nPersonalized recommendation systems\nEducational tools | AI Tutors\nLegal Advisors\nMedical Diagnosis Assistants\nImplementation of RAG using LangChain\n\nLangChain is a widely used Python framework that helps build state-of-the-art LLM applications. It facilitates the implementation of robust RAG systems by providing reusable components. This makes it easy to integrate and customize different RAG components according to application needs. LangChain documentation provides an extensive explanation of each RAG component in detail. It has built-in tools for document transformation, connection to various data sources, and chaining multiple operations.\n\nRAG Pipeline\nIngestion — Process and attach an external data source to the system.\nRetrieval — Correctly retrieve the information related to the prompt entered by a user.\nSynthesis — Process the retrieved response, refine it, and present it in a readable format.\n\nData Ingestion\n\nData ingestion is the process of loading external documents and storing them in a vector database for retrieval. These documents can be in various formats, such as PDF, DOC, TXT, HTML, and more. The ingestion process includes the following steps:\n\nLoad — Loading the data into documents\nSplit — Splitting the data into smaller, manageable chunks\nEmbed — Creating document embeddings\nStore — Storing the embeddings in a vector database\n\nRAG systems are extremely beneficial because they enhance the capabilities of LLMs by providing real-time and factual information to the user in natural language utilizing the powerful capabilities of pre-trained LLMs. A reliable external data source is vital for the success of a RAG based system, making data ingestion the first step in building these systems. Data is ingested through an external data source into a RAG system to make it available for smart and efficient retrieval and synthesis. A robust RAG system depends on a well-constructed ingestion pipeline, as it retrieves responses based on the context of the external data. The higher the quality of the ingestion process, the more accurate the results.\n\nLet's look at each of the data ingestion steps in detail.\n\n1. Data Loading\n\nData loading involves extracting data from a source and converting it into a suitable format for use in a RAG application.\n\nLoaders\n\nLangChain offers a comprehensive set of loaders that facilitate data ingestion and preprocessing from various sources, converting it into documents for further processing. Data is loaded into a Document object, which consists of the text content and its associated metadata.\n\nLoaders can handle multiple sources, including text files, PDFs, word documents, web pages, and more.\nThey also perform essential preprocessing tasks such as tokenization, normalization, and format conversion, ensuring the data is ready for LLMs.\nLoaders are customizable to manage specific data formats and seamlessly integrate with other LangChain components.\nExamples: Data Loading using LangChain\nLoad a TEXT document (.txt)\nfrom langchain_community.document_loaders import TextLoader\n\nfile_path = \"data/my_document.txt\"  # Path of the document to be loaded\nloader = TextLoader(file_path)      # Initialize the text loader\ndocuments = loader.load()           # Load the text document\nLoad a PDF document (.pdf)\nfrom langchain_community.document_loaders import PyPDFLoader\n\nfile_path = \"data/my_document.pdf\"  # Path of the document to be loaded\nloader = PyPDFLoader(file_path)     # Initialize the pdf loader\ndocuments = loader.load()           # Load the pdf document\nLoad a WORD document (.docx)\nfrom langchain_community.document_loaders import Docx2txtLoader\n\nfile_path = \"data/my_document.docx\"  # Path of the document to be loaded\nloader = Docx2txtLoader(file_path)   # Initialize the word document loader\ndocuments = loader.load()            # Load the word document \nLoad a WEB page (.html)\nfrom langchain_community.document_loaders import WebBaseLoader\n\nweb_path = \"https://www.example.com\"   # Path of the web page to be loaded\nloader = WebBaseLoader(web_path)       # Initialize the web page loader\ndocuments = loader.load()              # Load the web page\n\nThe choice of a loader depends on the nature of the data, as each loader has unique characteristics, and returns information differently. It’s essential to explore various loaders to understand their offerings and determine how to best utilize them when building AI products with RAG.\n\nMetadata — Importance and Usage\n\nMetadata essentially refers to data about data. When loaders extract content, they also provide metadata such as the data source or page numbers, which can be highly useful. For instance, if you’re building an application where users need to retrieve information along with its source or page location, metadata offers all the necessary details, simplifying information verification. These features can significantly enhance your application, and the metadata can even be edited to introduce additional value-added features.\n\n2. Data Splitting/Chunking\n\nWhen performing preprocessing for RAG, it’s crucial to consider the nature of the data, the type of RAG system being built, and its limitations. After loading documents, we’ll often need to transform them to fit our application’s requirements. For example, if we have a book with more than 1000 pages, we cannot pass the entire content to an LLM due to its limited context window. This is where chunking or splitting the data into smaller, manageable parts becomes essential.\n\nThere are three key reasons for splitting data:\n\nManageability: Breaking the data into smaller parts ensures it fits within the LLM’s context window, making it easier to handle.\nEmbedding model compatibility: Embedding models have limits on the amount of data they can process at a time, so chunking ensures that the data aligns with the model’s capacity.\nEfficient retrieval: When a user sends a query, they only need specific information, which is often found in smaller chunks. By retrieving only relevant chunks, we avoid processing the entire corpus for a single query.\n\nChunking can be done at different levels — sentences, paragraphs, or chapters — depending on the problem and type of data. Think of an LLM like a young child learning to read. When teaching children how to read, we first break words into characters and then combine them to form words. Similarly, we break the text into smaller chunks to enhance the LLM’s understanding and response generation.\n\nSplitters\n\nLangChain provides a variety of built-in document transformers that simplify tasks like splitting, combining, filtering, and manipulating documents. Splitters are tools used to divide large text documents into smaller, semantically meaningful chunks. These chunks can then be processed individually for various NLP tasks, ensuring they stay within the size constraints of the model.\n\nWith an unlimited number of users, each sending prompts differently, the system must semantically relate the information in the text to provide accurate responses. Text splitters can be customized based on how the text is divided and the chunk size, helping ensure relevant answers while minimizing computation cost. This is important because the more data we send to an LLM, the higher will be the computation cost.\n\nSplitters maintain the context and integrity of the text, making it easier to manage and analyze. During splitting, the chunk size must be carefully decided. For instance, if a page contains three interrelated paragraphs, dividing them into separate chunks would break the context. To address this, we use overlapping, where each chunk includes information from both the previous and next chunks, preserving context. However, if each paragraph introduces distinct topics, minimal or no overlapping may be needed, and the system will still generate a high-quality response.\n\nDetermining the maximum chunk size depends on the limitations of the model to which the chunks will be passed, particularly the embedding model in use. Each embedding model, available on platforms like Hugging Face, has specific input and output token limits. It is crucial to consider these limits when deciding on chunk size to ensure that no valuable information is lost.\n\nTypes of Splitters\n\nHere are some of the text splitters used for different purposes:\n\nRecursive Splitter: This widely-used splitter adopts a hierarchical approach, initially splitting text at larger, natural breakpoints (e.g., paragraphs). It recursively processes chunks to fit size constraints while preserving context as much as possible.\nHTML Splitter: Designed for HTML content, this splitter divides text based on HTML tags and structure. It is ideal for processing web pages and HTML documents, maintaining the integrity of HTML elements.\nMarkdown Splitter: This splitter handles markdown-formatted text by splitting based on markdown syntax, such as headers and lists. It is useful for processing markdown documents and notes while preserving the structure of markdown elements.\nCode Splitter: Tailored for source code, this splitter divides text based on code structures like functions and classes. It maintains logical integrity, making it suitable for processing and analyzing code snippets in various programming languages.\nToken Splitter: This splitter segments text based on token count, ensuring each chunk contains a specified number of tokens. It is useful for models with token-based input limits and can handle various tokenization schemes, including subword, character, and word tokenization.\nCharacter Splitter: Splitting text based on character count, this splitter offers a straightforward approach when character-based limits are crucial. It can be combined with other splitters for finer control.\nSemantic Chunker: An experimental feature of LangChain, the semantic chunker creates chunks based on semantic similarity. It generates document embeddings, compares them, and combines similar embeddings into chunks. It uses thresholds like percentile and standard deviation to determine similarity and does not require specifying chunk size. This splitter first divides text into sentences and then combines those with similar semantic meaning, preserving the context and meaning of each chunk.\nExamples: Data Splitting using LangChain\nCharacterTextSplitter\nfrom langchain_text_splitters import CharacterTextSplitter\n\n# Initialize the character text splitter\ntext_splitter = CharacterTextSplitter(              \n    separator=\"\",\n    chunk_size=100,\n    chunk_overlap=20\n)   \n\n# Split the documents into chunks\nchunks = []\nfor doc in documents:\n    texts = text_splitter.split_text(doc.page_content)\n    chunks.extend(texts)\nRecursiveCharacterTextSplitter\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Initialize the recursive character text splitter\ntext_splitter = RecursiveCharacterTextSplitter(              \n    separators=\"\",\n    chunk_size=100,\n    chunk_overlap=20\n)   \n\n# Split the documents into chunks\nchunks = []\nfor doc in documents:\n    texts = text_splitter.split_text(doc.page_content)\n    chunks.extend(texts)\nSemantic Chunker\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\n# Use a smaller, faster HuggingFace model for embeddings\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Initialize the semantic chunker\ntext_splitter = SemanticChunker(embeddings, breakpoint_threshold_type='percentile')\n\n# Split the documents into chunks\nchunks = []\nfor doc in documents:\n    texts = text_splitter.split_text(doc.page_content)\n    chunks.extend(texts)\n3. Embeddings\n\nAll artificial intelligence is fundamentally mathematics. Machines don’t understand text; they process numbers. To bridge this gap, we must convert textual data into numerical form, a process known as vector embeddings. These embeddings are stored in a vector store or vector database for efficient retrieval.\n\nEmbeddings are dense vector representations that capture the semantic meaning of words, phrases, or sentences. Words with similar meanings have closer numerical representations, while words with different meanings have more distinct vectors. Importantly, embeddings aren’t single numbers but multi-dimensional vectors, representing the nuanced relationships between words. LLMs learn these representations, making words with smaller mathematical differences more semantically related.\n\nEmbeddings are calculated using embedding models, which are typically built from the encoder part of a transformer model. While large language models (LLMs) are developed from the decoder part, the encoder is responsible for learning meaningful representations of data, and the decoder generates new content based on these learned representations.\n\nIn machine learning, embeddings are widely used for tasks like classification, where words with similar embeddings can be grouped into one class. They are also applied in anomaly detection, identifying outliers with vastly different embeddings, and in clustering, where related items are grouped based on their vector similarities.\n\nWe can choose between open-source embedding models from Hugging Face or paid models from OpenAI. While OpenAI’s models come with usage fees, infrastructure cost of using open-source models is our own responsibility. Some models may require GPUs, so selecting the right model depends on our system’s requirements and budget.\n\nExamples: Embeddings using LangChain\nHuggingFace Embeddings\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\n\n# Initialize the Hugging Face embedding model\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n\n# Create embeddings of text chunks\nfor i, chunk in enumerate(chunks):\n    print(\"Text chunk \", i)\n    print(\"--------------\")\n    print(chunk, \"\\n\")\n    query_result = embeddings.embed_query(chunk)\n    print(\"Embeddings\", \"\\n\", query_result, \"\\n\\n\")\n4. Data Storage — Vector Stores\n\nA vector is a mathematical representation used to store data in a computer, a concept known as representation learning. To store these representations efficiently, vector stores are employed. They are highly optimized databases designed for storing and querying high-dimensional vector data, making them essential in AI, especially in applications like recommendation systems and retrieval-augmented generation (RAG).\n\nVector embeddings are stored in specialized databases known as vector stores, which differ significantly from traditional or relational databases. In traditional databases, data is organized in rows and columns, similar to an Excel sheet. However, storing unstructured data like images, speech, or text in this format is inefficient, making vector stores a better choice.\n\nLet’s explore the difference with an example. Exact search (e.g., looking for a specific product by its exact ID or name in an online store) is handled by traditional relational databases (RDBMS). Suggestions or recommendations (e.g., finding products with similar features, style, or price range when you don’t know the exact item) rely on vector stores, which compare items based on their vector embeddings to provide relevant recommendations. Therefore, conventional databases retrieve exact matches, while vector stores focus on similarity-based retrieval.\n\nVector stores are primarily used in machine learning and data retrieval applications where the goal is to find semantically similar information. Following are some of their use cases:\n\nRetrieval in RAG-based systems.\nRecommendation systems through similarity search.\nAnomaly detection by identifying unusual or unknown queries.\nVector search for NLP applications like document retrieval.\nImage and video retrieval.\nStructure of a Vector Store\n\nA vector store consists of the following essential elements:\n\nVectors/Embeddings: These are mathematical representations of images, audio, or text and form the core component of a vector store.\nMetadata: Additional information about the vector such as: time, date, and location for images, or author, publication year, and topic for books.\nOriginal data: While not always necessary, in RAG applications it can be useful to store the original text or data in the vector store.\nUnique ID: Each row in the vector store is identified by a unique ID generated by the system.\nPopular Vector Stores\n\nFollowing are some of the commonly used vector stores:\n\nPinecone\nFAISS\nQdrant\nChroma\nKey Features of Vector Stores\n\nThe strength of a vector store lies in its\n\nHigh-speed similarity search, and\nScalability and integration with machine learning pipelines.\nHow Do Vector Stores Work?\n\nWhen querying a vector store, two main operations are commonly used:\n\nCosine Similarity: Used for recommendations, such as finding similar products. The input (e.g., a product description) is converted into a vector, and the vector store applies cosine similarity to identify the top k similar vectors. The metadata then helps retrieve relevant information, like product details. E-commerce websites and social media platforms use similar recommendation systems.\nDot Product: Another common method for comparing vectors.\nTypes of Vector Stores\nCloud-based, Fully Managed Services — These are hosted on the cloud, requiring only an API key for access. You don’t have to manage the infrastructure, making them easy to set up and scale. However, they come with associated cloud service costs. Examples include Pinecone and Qdrant.\nLocally Managed Vector Databases — If you prefer to keep sensitive data off the cloud or have specific client requirements for local RAG systems, this is a suitable option. You will need to manage storage, scalability, and resource allocation yourself. Examples include FAISS and Chroma.\nKey Benefits of Vector Stores\nEfficient Data Storage: Vector stores are optimized for high-dimensional vector data.\nFast Retrieval: They are designed for quick similarity searches and responses.\nImproved Semantic Understanding: Embedding models store vectors based on their semantic relationships, ensuring meaningful representation of data.\n\nChoosing between vector stores depends on your specific needs, resources, and project requirements. When pushing data into a vector store, you have the flexibility to structure and customize its metadata based on the specific needs of your application. You can even add additional fields to enrich the metadata, making your system more efficient and tailored to your use case.\n\nIt’s essential to consider optimization techniques to ensure your system performs well. Factors like token cost, the number of chunks created, the vectors stored in the vector store, the top n documents retrieved from the store, and the chunks sent to the LLM in a RAG system all play a critical role.\n\nAdditionally, always ensure that keys, such as API keys and credentials, are securely stored in secret files to protect your system from potential security risks. Failing to do so could compromise the entire system setup.\n\nExamples: Data Storage using LangChain\n\nLangChain integrates seamlessly with vector stores, allowing us to build powerful language-based applications that use vector embeddings for enhanced retrieval and processing.\n\nStore Data using FAISS\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\n\n# Initialize the Hugging Face embedding model\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n\n# Store embeddings into the vector store\nvector_store = FAISS.from_documents(\n    documents=chunks,\n    embedding=embeddings\n)\nData Retrieval\n\nData retrieval is the process of finding and extracting relevant information or documents from a vector store based on user prompt. A user asks questions from the specialized knowledge source to get the most relevant answers, so the vector store needs to be used programmatically for the retrieval process.\n\nTypes of Retrievers\n\nThere are many different types of retrievers in LangChain including:\n\nVector store-backed retriever — It uses a vector store as a retriever to retrieve documents using search methods like similarity search, MMR, etc.\nMultiQuery retriever — It addresses the shortcomings of distance based retrieval by generating multiple queries from a prompt and retrieving relevant chunks by taking union across all queries. It captures different perspectives of a complex prompt and produces better results.\nContextual compression retriever — It reduces the cost of LLM calls by compressing or shortening the documents having relevant information or dropping the irrelevant documents altogether to generate a good quality response.\nCustom retriever — We can design our own retriever by extending the BaseRetriever in LangChain to retrieve information from an external knowledge source.\nEnsemble retriever — It ensembles the results of multiple retrievers for better performance and more robust results.\n\nSome other retrievers are:\n\nSVM retriever —It matches the most relevant documents by distinguishing them from irrelevant ones.\nTF/IDF retriever — It focuses on the words which are most repeated or most diverse in our system for document retrieval.\n\nThese retrievers are important when you are expected to get keywords from a user instead of a query.\n\nThe Process of Retrieval\n\nWhen a user asks a question, it is converted into a vector. This vector then goes to the vector store through a retriever interface. The vector store finds n number of vectors similar to the query vector. To match the user’s question to similar vectors, there are various methods, some of which are:\n\nDot product\nCosine similarity\nMMR — Maximum marginal relevance\n\nWhy do we want to match these vectors for similarity? Basically we want to find out which vectors in the stored content match with the question asked by the user as only those similar vectors would be useful for us to generate a response through LLM. So we actually fetch the data relevant to the question from the vector store.\n\nVector Store as a Retriever\n\nThe simplest form of information retrieval is to use a vector store as a retriever. A vector store-backed retriever uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses similarity search to retrieve information in the form of relevant chunks based on distance between vectors. So we retrieve only the required information from a source in order to give response to a user instead of retrieving all the information.\n\nA vector store contains:\n\nVector,\nText associated with the vector, and\nMetadata\n\nThe vectors are stored in the vector store in a meaningful representation through an embedding model. Our ultimate goal is to connect our data to an LLM. To interact with the LLM, we need to retrieve data from the vector store and then send it to the LLM along with our query or prompt to get the desired response in a structured format. That’s why we retrieve semantically similar chunks based on the query. The query is converted into vectors and meaningful comparisons are made to retrieve the similar vectors. Using vector store retrievers is ideal when handling large amounts of embedded data because they allow efficient access to relevant chunks.\n\nSemantic Similarity\n\nWhen data is retrieved, we only get its associated text chunk along with metadata and not the original vector because we only need those text chunks to be used in our application for further processing. Vectors are used for mathematical operations which cannot be performed on text. This concept is referred to as semantic search. Semantic search goes beyond keyword search (which relies on the occurrence of specific words in the search input) to find contextually relevant data based on the conceptual similarity of the input string. The vector store retriever uses similarity search by default.\n\nMaximum Marginal Relevance (MMR)\n\nMaximum marginal relevance (MMR) is a method used to avoid redundancy while retrieving relevant items to a query.\n\nTo process repeated information is a waste of cost and time. We want to filter out repeated information from relevant information. MMR finds closest chunks and removes or discards the chunks with repeated information. Instead of merely retrieving the most similar items, MMR ensures a balance between relevancy and diversity in the items retrieved. It reduces redundancy by selecting the most diverse set of relevant information. This is particularly important when balancing coverage and efficiency in information retrieval.\n\nWhether to use MMR or not depends on the application you are building. For example, if a lawyer wants to retrieve all relevant cases from different regions for a particular scenario, MMR won’t be useful because he requires to have all the information. If limited information is required, you may use MMR. So this is case specific. Whenever you need diversity in response, you’ll need to use MMR.\n\nTo choose the appropriate retriever, you need to know:\n\nWhat type of data do you have?\nWho is the end user of your application?\nExample: Data Retrieval using LangChain\nVector Store as Retriever\n\nSimilarity Search Retrieval\n\n# Retrieve relevant information using similarity search\nretriever = vector_store.as_retriever() # uses similarity search by default\ndocs = retriever.invoke(\"Ask any question from the document here\")\nMaximum Marginal Relevance (MMR) Retrieval\n# Retrieve relevant information using maximum marginal relevance retrieval\nretriever = vector_store.as_retriever(search_type=\"mmr\") # uses mmr as search type\ndocs = retriever.invoke(\"Ask any question from the document here\")\nSimilarity Score Threshold Retrieval\n# Retrieve relevant information using similarity score threshold\nretriever = vector_store.as_retriever(\n    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n)\ndocs = retriever.invoke(\"Ask any question from the document here\")\nSpecifying Top k Documents\n# Retrieve top k relevant documents\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\ndocs = retriever.invoke(\"Ask any question from the document here\")\nUse of Large Language Models (LLMs) in RAG\n\nLarge language models (LLMs) are highly complex neural networks trained on vast amounts of text data and typically consist of billions of parameters, enabling them to understand human language at an advanced level. They are capable of generating high quality text and inference. There are a lot of variants of LLMs which can be used in a RAG based application. Some of them include:\n\nGPT (Generative Pre-trained Transformer) by OpenAI\nClaude by Anthropic\nLlama by Meta\nGemini by Google, and many more.\n\nEach LLM has a predefined context window, which refers to the number of tokens it can process simultaneously. Using these LLMs has a usage cost associated with each of them. Additionally, multimodal LLMs are gaining popularity due to their capability to process and generate text, speech, images, and video content.\n\nEach large language model varies in its reasoning capabilities, particularly in mathematical reasoning and generating nuanced responses. While some tasks require the advanced reasoning of larger LLMs, mini models are often sufficient for specific tasks, especially where we don’t rely on the internal knowledge of the LLM — such as in RAG-based systems. In these cases, mini models can be more cost-effective and focused on refining responses rather than using a model’s full knowledge base.\n\nUnderstanding the capacity, reasoning power, and cost implications of each LLM ensures that the right model is chosen for the right use case, balancing power with efficiency. Therefore, specialized LLMs optimize cost and performance in practical applications.\n\nThe quality of response generated through a RAG system highly depends on the correctness of the retrieved information as this retrieved information along with the user prompt is passed to the LLM for response generation. Ultimately, the success of a RAG system lies in its ability to retrieve the most relevant information effectively. Careful selection of retrieval methods and LLM models tailored to the task ensures optimal performance and cost efficiency.\n\nSynthesis\n\nSynthesis is the final component of a RAG pipeline where a response is generated for the user through an LLM. The user prompt and relevant chunks retrieved from a vector database during data retrieval are passed to the LLM which then generates a context augmented response that answers the question asked by the user. The response can be structured in any required format.\n\nTo make our RAG applications controllable and customizable, we use the concept of chains which enable us to connect different components of a RAG-based system.\n\nWhat are Chains?\n\nChains refer to a sequence of calls to combine multiple components of a RAG-based application. Let’s say we want to develop an AI-powered chatbot. We’ll need to develop a conversation chain for this purpose.\n\nLangChain — a Python framework to build LLM-powered applications — provides us the facility to use chains. There are many different types of chains available in LangChain. Let’s explore the components and elements of different chains, how they work, and how we can use them in our applications. The most common way to use chains is through LCEL (LangChain Expression Language).\n\nWhat is LCEL?\n\nLCEL (LangChain Expression Language) is a standard way in LangChain that allows for the creation and execution of custom chains. It helps in the integration of different components and provides streaming, asynchronous support, and parallel execution. The chains can be easily modified or extended. Additionally, LangChain implements a Runnable protocol for interaction between components.\n\nRunnable\n\nEvery LCEL object implements the Runnable interface. Runnables are used to build custom chains.\n\nThere are three common methods to call the Runnable interface:\n\nstream — Streams chunks of a response.\ninvoke — Calls the chain on an input.\nbatch — Calls the chain for multiple inputs.\n\nIf you have a single query, you can simply invoke the chain and you’ll get the response. If you have a list of inputs, you’ll use batch call and it will give you the responses in a list. The stream method is used to display the response in chunks so that you can view the response while it’s being generated.\n\nRunnable Lambda\n\nRunnable Lambda converts a Python callable into a Runnable.\n\nfrom langchain_core.runnables import RunnableLambda\n\n# Define a RunnableLambda that takes a string as input,\n# reverses it, and then repeats the reversed string twice.\nrunnable = RunnableLambda(lambda s: (s[::-1]) * 2)\n\n# Invoke the runnable with the string \"hello\"\nresult = runnable.invoke(\"hello\")\n\nprint(result)  # Output: \"olleholleh\"\nRunnable Sequence\n\nA Runnable sequence is a composition of multiple Runnables invoked sequentially, where the output of one serves as the input to the next. It can be instantiated directly or by using the | operator. Whenever you need to invoke a chain, you make its Runnable interface and the Runnable sequence executes all the steps of that chain in a sequence.\n\nImplementation using the | operator:\n\nfrom langchain_core.runnables import RunnableLambda\n\n# Define a sequence of RunnableLambda instances that process a string\n# Step 1: Remove leading and trailing whitespace\n# Step 2: Convert the string to uppercase\n# Step 3: Replace spaces with underscores\n# Step 4: Add \"Processed:\" prefix to the string\nsequence = RunnableLambda(lambda s: s.strip()) | \\\n           RunnableLambda(lambda s: s.upper()) | \\\n           RunnableLambda(lambda s: s.replace(\" \", \"_\")) | \\\n           RunnableLambda(lambda s: f\"Processed: {s}\")    \n\n# Invoke the sequence with the input string \"  hello world  \"\nresult = sequence.invoke(\"  hello world  \")\n\nprint(result)  # Output: \"Processed: HELLO_WORLD\"\n\nAlternative implementation by directly calling the RunnableSequence:\n\nfrom langchain_core.runnables import RunnableLambda, RunnableSequence\n\n# Define individual RunnableLambda steps for processing a string\nstrip = RunnableLambda(lambda s: s.strip())             # Step 1: Remove leading and trailing whitespaces\nuppercase = RunnableLambda(lambda s: s.upper())         # Step 2: Convert the string to uppercase\nreplace = RunnableLambda(lambda s: s.replace(\" \", \"_\")) # Step 3: Replace spaces with underscores\nprefix = RunnableLambda(lambda s: f\"Processed: {s}\")    # Step 4: Add \"Processed:\" prefix to the string\n\n# Create a RunnableSequence that combines all the individual steps\nsequence = RunnableSequence(strip, uppercase, replace, prefix)\n\n# Invoke the sequence with the input string \"  hello world  \"\nresult = sequence.invoke(\"  hello world  \")\n\nprint(result)  # Output: \"Processed: HELLO_WORLD\"\nRunnable Parallel\n\nA Runnable parallel is a composition that invokes multiple Runnables concurrently, providing the same input to each.\n\nfrom langchain_core.runnables import RunnableLambda, RunnableParallel\n\n# Define individual RunnableLambda functions for different string operations\nreverse = RunnableLambda(lambda s: s[::-1])      # Function to reverse the input string\nuppercase = RunnableLambda(lambda s: s.upper())  # Function to convert the string to uppercase\nlength = RunnableLambda(lambda s: len(s))        # Function to calculate the length of the string\n\n# Create a RunnableParallel with a mapping (dictionary) of operations\nparallel = RunnableParallel({\n    \"reverse\": reverse,\n    \"uppercase\": uppercase,\n    \"length\": length\n})\n\n# Invoke the parallel sequence with the input string \"hello\"\nresult = parallel.invoke(\"hello\")\n\nprint(result)  # Output: {'reverse': 'olleh', 'uppercase': 'HELLO', 'length': 5}\n\nRunnable parallel can only be used when input of the next chain is not dependent on the output of the previous one, therefore, you can use this only if the inputs are independent of each other.\n\nRunnable Passthrough\n\nA Runnable passthrough allows to pass through the inputs unchanged.\n\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Define a RunnablePassthrough that passes the input unchanged\npassthrough = RunnablePassthrough()\n\n# Invoke the passthrough\nresult = passthrough.invoke(\"hello world\")\n\nprint(result)  # Output: \"hello world\"\nPrompt Templates\n\nPrompt templates enable us to dynamically incorporate user prompts into our RAG system. We have to send the retrieved context along with the prompt to an LLM in order to retrieve information through a RAG system, therefore we use prompt templates for this purpose. Let’s look at a few of them.\n\nBasic Prompt Template\n\nA simple template that allows for dynamic input replacement within a predefined string.\n\nfrom langchain_core.prompts import PromptTemplate\n\n# Define a template for the prompt that includes a placeholder for the country\ntemplate = \"What is the capital of {country}?\"\n\n# Create a PromptTemplate instance with the defined template\nprompt_template = PromptTemplate(template=template)\n\n# Format the template by replacing the placeholder with the specified country (\"Pakistan\")\nprompt = prompt_template.format(country=\"Pakistan\")\n\nprint(prompt)  # Output: What is the capital of Pakistan?\nFew-Shot Prompt Template\n\nA template designed to provide a few examples to guide the model’s responses, enhancing its understanding of the desired output.\n\nfrom langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n\n# Define a list of example queries and their corresponding answers\nexamples = [\n    {\"query\": \"How can I improve my time management skills?\", \n     \"answer\": \"Start by prioritizing your tasks using the Eisenhower Matrix to distinguish between what's urgent and important.\"},\n     \n    {\"query\": \"What are some effective strategies for setting goals?\", \n     \"answer\": \"Use the SMART criteria: make your goals Specific, Measurable, Achievable, Relevant, and Time-bound.\"}\n]\n\n# Create a PromptTemplate for formatting the examples\nexample_template = PromptTemplate(\n    input_variables=[\"query\", \"answer\"],  # Define the input variables for the template\n    template=\"User: {query}\\nAI: {answer}\"  # Template structure for each example\n)\n\n# Create a FewShotPromptTemplate using the examples and example_template\nfew_shot_prompt = FewShotPromptTemplate(\n    examples=examples,                                        \n    example_prompt=example_template,                           \n    prefix=\"Here are some examples of questions and answers:\",  \n    suffix=\"User: {query}\\nAI: {answer}\\n\", \n    input_variables=[\"query\"],              \n    example_separator=\"\\n\\n\"\n)\n\n# Format the few-shot prompt with a new user query and corresponding answer\nprompt = few_shot_prompt.format(query=\"How do I stay motivated during long projects?\",\n                                answer=\"Break down the project into smaller milestones and celebrate each achievement.\")\n\nprint(prompt)\nChat Prompt Template\n\nA specialized template for chat models that formats a sequence of messages allowing for distinct roles (e.g., human, AI).\n\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Define a ChatPromptTemplate from a list of message tuples. \n# The first element in each tuple represents the speaker (\"human\" or \"ai\"), \n# and the second element is the message template.\nchat_template = ChatPromptTemplate.from_messages([\n    (\"human\", \"Can you tell me about {author}?\"),   # Human asks about the author\n    (\"ai\", \"{author} is a renowned author known for {notable_work}.\")  # AI responds with author details\n])\n\n# Format the chat prompt with specific values for the placeholders\n# Replaces {author} with \"Jane Austen\" and {notable_work} with \"Pride and Prejudice\"\nmessages = chat_template.format_messages(author=\"Jane Austen\", notable_work=\"Pride and Prejudice\")\n\nprint(messages)\nOutput Parsers\n\nAn output parser is a component that processes outputs from AI models, transforming them into a desired format or structure. Different output parsers can be used to get the response in a required format such as a string, a dictionary, or JSON format.\n\nStrOutputParser\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Initialize the parser\nparser = StrOutputParser()\n\n# Define a string output from an AI explaining a concept\nai_response = \"\"\"\nMachine learning is a subset of artificial intelligence that focuses on enabling systems to learn from data patterns and make decisions without being explicitly programmed. It is widely used in applications like recommendation systems, image recognition, and natural language processing.\n\"\"\"\n\n# Use the parser to process the AI response\nparsed_output = parser.invoke(ai_response)\n\nprint(parsed_output)\nJsonOutputParser\nfrom langchain_core.output_parsers import JsonOutputParser\n\n# Initialize the JSON parser\nparser = JsonOutputParser()\n\n# Simulated AI response in JSON format (string)\nai_response = \"\"\"\n{\n    \"name\": \"John Doe\",\n    \"age\": 30,\n    \"occupation\": \"Software Engineer\",\n    \"skills\": [\"Python\", \"Machine Learning\", \"Data Analysis\"]\n}\n\"\"\"\n\n# Parse the JSON response\nparsed_output = parser.invoke(ai_response)\n\n# Print the parsed output as a dictionary\nprint(parsed_output)\n\n# Access specific fields from the parsed JSON output\nprint(f\"Name: {parsed_output['name']}\")\nprint(f\"Skills: {', '.join(parsed_output['skills'])}\")\nTypes of Chains\n\nThere are four common types of chains:\n\nQuestion/Answering (Q/A) Chain\nQuestion/Answering (Q/A) Retrieval Chain\nConversational Chain\nConversational Retrieval Chain\n\nThe selection of a chain depends on the requirements of the RAG system being built. Let’s explore each of these chains in detail.\n\n1. Question/Answering (Q/A) Chain\n\nA Q/A chain is used to do direct question answering with an LLM. The model answers directly using its internal knowledge.\n\n# Q/A Chain\n\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_groq import ChatGroq\n\n# Define a prompt template for formatting the question\nprompt_template = \"Q: {question}\\n\"\nprompt = PromptTemplate(template=prompt_template, input_variables=[\"question\"])\n\n# Instantiate the ChatGroq LLM with the specified model\nllm = ChatGroq(model=\"mixtral-8x7b-32768\")\n\n# Combine the prompt and LLM into a chain where the prompt output is passed to the LLM\nqa_chain = prompt | llm  # This creates a chain that takes the formatted question and passes it to the model\n\n# Invoke the chain with a specific question\nresponse = qa_chain.invoke(\"What is the capital of Japan?\")\n\n# Print the content of the response\nprint(response.content)\n2. Question/Answering (Q/A) Retrieval Chain\n\nA Q/A retrieval chain is used to add context to an LLM response. It retrieves relevant documents from a knowledge base (e.g. personal documents, research papers) and generates an answer.\n\n# Q/A Retrieval Chain\n\nfrom langchain_groq import ChatGroq\nfrom langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Initialize the ChatGroq LLM with the specific model\nllm = ChatGroq(model=\"mixtral-8x7b-32768\")\n\n# Define the system prompt that instructs the LLM how to answer questions based on retrieved context\nsystem_prompt = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer \"\n    \"the question. If you don't know the answer, say that you \"\n    \"don't know. Use three sentences maximum and keep the \"\n    \"answer concise.\"\n    \"\\n\\n\"\n    \"{context}\"  # Placeholder for the retrieved context\n)\n\n# Create a chat prompt template with a system message and human message\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),  # System message contains instructions and context\n        (\"human\", \"{input}\"),  # Human message includes the user's input question\n    ]\n)\n\n# Create a document chain that combines the LLM with the prompt\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\n\n# Combine the retrieval chain with the question-answering chain\n# The retrieval chain retrieves relevant documents and feeds them into the question-answering chain\nretrieval_chain = create_retrieval_chain(retriever, question_answer_chain)\n\n# Invoke the chain with a question, and the retriever will provide context for the LLM to generate an answer\nresponse = retrieval_chain.invoke({\"input\": \"Ask the question from the user here\"})\n\nprint(response['answer'])\n3. Conversational Chain\n\nA conversational chain is used to add chat history to simple question answering. It maintains context from previous interactions without document retrieval.\n\n# Conversational Chain\n\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_groq import ChatGroq\n\n# Create an external store (a dictionary) for maintaining chat history across multiple sessions\nstore = {}\n\n# Function to get or create chat history for a specific session based on session_id\ndef get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n    # If the session_id doesn't exist in the store, create a new InMemoryChatMessageHistory\n    if session_id not in store:\n        store[session_id] = InMemoryChatMessageHistory()\n    # Return the chat history associated with the given session_id\n    return store[session_id]\n\n# Initialize the ChatGroq model\nllm = ChatGroq(model=\"mixtral-8x7b-32768\")\n\n# Create a chain that can handle message history, linking it with the LLM and session history retrieval\nchain = RunnableWithMessageHistory(llm, get_session_history)\n\n# First conversation with AI in session \"1\"\nquestion_1 = \"Hello, there?\"\nresponse_1 = chain.invoke(\n    question_1,\n    config={\"configurable\": {\"session_id\": \"1\"}},  # Specify the session_id for tracking message history\n)\n\n# Second conversation in the same session \"1\"\nquestion_2 = \"What can you help me with today?\"\nresponse_2 = chain.invoke(\n    question_2,\n    config={\"configurable\": {\"session_id\": \"1\"}},  # Continue in the same session \"1\"\n)\n\n# Third conversation in the same session \"1\"\nquestion_3 = \"Can you explain the concept of AI in one sentence?\"\nresponse_3 = chain.invoke(\n    question_3,\n    config={\"configurable\": {\"session_id\": \"1\"}},  # Continue in session \"1\"\n)\n\n# Print the responses to see the conversation flow\nprint(\"Human:\", question_1)\nprint(\"AI:   \", response_1.content, \"\\n\")\n\nprint(\"Human:\", question_2)\nprint(\"AI:   \", response_2.content, \"\\n\")\n\nprint(\"Human:\", question_3)\nprint(\"AI:   \", response_3.content, \"\\n\")\n4. Conversational Retrieval Chain\n\nA conversational retrieval chain is used to add chat history to a simple retrieval chain. It retrieves documents and maintains conversational context across multiple questions.\n\n# Conversational Retrieval Chain\n\nfrom langchain.chains import create_history_aware_retriever, create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_community.chat_message_histories import ChatMessageHistory\nfrom langchain_core.chat_history import BaseChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_groq import ChatGroq\n\n# Initialize the large language model\nllm = ChatGroq(model=\"mixtral-8x7b-32768\")\n\n### Contextualize question ###\n# Define a system prompt to contextualize the user question by making it standalone\ncontextualize_q_system_prompt = (\n    \"Given a chat history and the latest user question \"\n    \"which might reference context in the chat history, \"\n    \"formulate a standalone question which can be understood \"\n    \"without the chat history. Do NOT answer the question, \"\n    \"just reformulate it if needed and otherwise return it as is.\"\n)\n\n# Create a ChatPromptTemplate for contextualizing questions with chat history\ncontextualize_q_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", contextualize_q_system_prompt),\n        MessagesPlaceholder(\"chat_history\"),      \n        (\"human\", \"{input}\"),                      \n    ]\n)\n\n# Create a history-aware retriever to handle chat history\nhistory_aware_retriever = create_history_aware_retriever(\n    llm, retriever, contextualize_q_prompt\n)\n\n### Answer question ###\n# Define a system prompt to generate concise answers using retrieved context\nsystem_prompt = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer \"\n    \"the question. If you don't know the answer, say that you \"\n    \"don't know. Use three sentences maximum and keep the \"\n    \"answer concise.\"\n    \"\\n\\n\"\n    \"{context}\"  # Placeholder for retrieved context\n)\n\n# Create a ChatPromptTemplate for the question-answering chain\nqa_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),            \n        MessagesPlaceholder(\"chat_history\"),   \n        (\"human\", \"{input}\"),                \n    ]\n)\n\n# Create a document chain for handling question answering using the LLM and the QA prompt\nquestion_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n\n# Combine the history-aware retriever and the QA chain to create the retrieval chain\nretrieval_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n\n### Statefully manage chat history ###\n# Store for maintaining chat history across multiple sessions\nstore = {}\n\n# Function to get or create chat history for a given session\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\n    # If no session history exists for the given session_id, create a new history\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n\n# Create a conversational retrieval chain that manages chat history statefully\nconversational_retrieval_chain = RunnableWithMessageHistory(\n    retrieval_chain,                 \n    get_session_history,           \n    input_messages_key=\"input\",     \n    history_messages_key=\"chat_history\",\n    output_messages_key=\"answer\",\n)\n\n# First conversation: question about the document\nquestion_1 = {\"input\": \"What is the document about?\"}\nresponse_1 = conversational_retrieval_chain.invoke(\n    question_1,\n    config={\n        \"configurable\": {\"session_id\": \"abc123\"}\n    },\n)[\"answer\"]\n\n# Second conversation in the same session: asking to summarize the main points\nquestion_2 = {\"input\": \"Summarize its main points?\"}\nresponse_2 = conversational_retrieval_chain.invoke(\n    question_2,\n    config={\"configurable\": {\"session_id\": \"abc123\"}},\n)[\"answer\"]\n\n# Printing the responses to see the conversational retrieval flow\nprint(\"Human:\", question_1[\"input\"])\nprint(\"AI:   \", response_1, \"\\n\")\n\nprint(\"Human:\", question_2[\"input\"])\nprint(\"AI:   \", response_2, \"\\n\")\nResume Analyzer using RAG\n\nLet’s say you are a recruiter or an HR Manager and you have a pool of CVs or resumes to analyze and see if they have the right skillset according to your job description. Now looking at each and every resume is a time consuming and tedious task. How about uploading a resume and your job description with the required skills and getting a detailed analysis about how much the resume matches a particular job description. Sounds interesting, Right!\n\nWell, this can be easily automated through a RAG-based system where your external knowledge base would be the resume or CV you want to analyze. You can chat with it and ask different questions to see if it meets the requirements or not. This tool can help you quickly segregate matching resumes from the unmatched ones letting you focus on just the useful ones having the required skillset.\n\nLet’s build this application step by step. We’ll be using Streamlit — an open source Python framework for interactive data apps — to develop the frontend and LangChain — a Python framework for LLM-powered applications — to develop the backend. Additionally, we’ll use Groq — a Generative AI solutions platform for fast inference — to connect with LLMs. Here’s a glimpse of what the application would look like.\n\nInstalling necessary libraries\n\nInstall the following python libraries before building the application from the requirements.txt file.\n\nstreamlit\nlangchain_community\nlangchain-huggingface\nlangchain-groq\npypdf\nfaiss-cpu\npython-dotenv\nSetting up the Environment Variables\n\nSet up the Groq API key in a .env file.\nGROQ_API_KEY=\"Your_API_Key\"\n\nBuilding the Frontend\nmain_app.py\n\nThis is the main interface to upload the resume and job description and is displayed at the left side of the application. As soon as both the resume and job description have been uploaded, it will send the PDF file to the backend — Data Ingestion component of the RAG system — to load it, split it into text chunks, generate embeddings of text chunks, and store them into a vector store. After data ingestion, it will show a button to analyze the resume. When the user clicks on that button, the resume will be analyzed through the backend and the response will be displayed in the form of a detailed analysis.\n\nimport streamlit as st\nfrom backend.pdf_ingestion import load_split_pdf\nfrom backend.vector_store import create_vector_store\nfrom backend.analysis import analyze_resume\nimport os\nimport shutil\n\n# Main application including \"Upload Resume\" and \"Resume Analysis\" sections\ndef render_main_app():\n    \n    # Apply custom CSS to adjust the sidebar width\n    st.markdown(\n        \"\"\"\n        <style>\n        [data-testid=\"stSidebar\"] {\n            min-width: 25%;\n            max-width: 25%;\n        }\n        </style>\n        \"\"\",\n        unsafe_allow_html=True\n    )\n\n    # Moving the upload section to the sidebar\n    with st.sidebar:\n        st.header(\"Upload Resume\")  # Header for the upload section\n        \n        # File uploader for PDF resumes\n        resume_file = st.file_uploader(\"Upload Resume (PDF)\", type=\"pdf\")\n\n        # Text area for job description input\n        job_description = st.text_area(\"Enter Job Description\", height=300)\n\n        if resume_file and job_description:  # Check if both inputs are provided\n            # Create a temporary directory if it doesn't exist\n            temp_dir = \"temp\"\n            os.makedirs(temp_dir, exist_ok=True)\n\n            # Save the uploaded file to the temporary directory\n            with open(os.path.join(temp_dir, resume_file.name), \"wb\") as f:\n                f.write(resume_file.getbuffer())\n            \n            # Load and split the PDF file into documents and chunks\n            resume_file_path = os.path.join(\"temp\", resume_file.name)\n            resume_docs, resume_chunks = load_split_pdf(resume_file_path)\n\n            # Create a vector store from the resume chunks\n            vector_store = create_vector_store(resume_chunks)\n            st.session_state.vector_store = vector_store  # Store vector store in session state\n                \n            # Remove the temporary directory and its contents\n            shutil.rmtree(temp_dir)\n\n            # Button to begin resume analysis\n            if st.button(\"Analyze Resume\", help=\"Click to analyze the resume\"):\n                # Combine all document contents into one text string for analysis\n                full_resume = \" \".join([doc.page_content for doc in resume_docs])\n                # Analyze the resume\n                analysis = analyze_resume(full_resume, job_description)\n                # Store analysis in session state\n                st.session_state.analysis = analysis    \n        else:\n            st.info(\"Please upload a resume and enter a job description to begin.\")\n\n    # Display the analysis result if it exists in session state \n    if \"analysis\" in st.session_state:\n        st.header(\"Resume-Job Compatibility Analysis\")\n        st.write(st.session_state.analysis)\n    else:\n        st.header(\"Welcome to the Ultimate Resume Analysis Tool!\")\n        st.subheader(\"Your one-stop solution for resume screening and analysis.\")\n        st.info(\"Do you want to find out the compatibility between a resume and a job description? So what are you waiting for?\")\n\n        todo = [\"Upload a Resume\", \"Enter a Job Description\", \"Click on Analyze Resume\"]\n        st.markdown(\"\\n\".join([f\"##### {i+1}. {item}\" for i, item in enumerate(todo)]))\n\nGitHub - \nmain_app.py\n\nchat_interface.py\n\nThis is the interface to chat with the uploaded resume and is displayed at the right side of the application. It uses the Data Retrieval and Synthesis components of a RAG system to have an interactive conversation with the resume. The user can ask different questions related to the resume and get context augmented response through the RAG system. The conversation between the user and the application is maintained through a conversational retrieval chain.\n\nGitHub - \nchat_interface.py\n\nBuilding the Backend\npdf_ingestion.py\n\nThis is the function to load and split a PDF file using PyPDFLoader to load the file and RecursiveCharacterTextSplitter to split it into chunks. It returns both the documents and text chunks.\n\nGitHub - \npdf_ingestion.py\n\nvector_store.py\n\nThis function uses Hugging Face embeddings to store the text embeddings into a vector store. We are using FAISS (Facebook AI Similarity Search) as the vector store here.\n\nGitHub - \nvector_store.py\n\nanalysis.py\n\nThis function analyzes the resume using the LLM mixtral-8x7b-32768 from Groq API and full resume text as the context. The response is generated through a simple Q/A chain. The prompt template used here is extremely important because it defines the structure of the response in much detail. Here we can use the magic of prompt engineering to generate the response of our choice.\n\nGitHub - \nanalysis.py\n\nMain Application\napp.py\n\nThis is the main entry point to the application. It renders the user interface to interact with the Resume Analyzer.\n\nGitHub - \napp.py\n\nDeployment\n\nThe application we have built is a basic version of a resume analysis tool just to get an understanding of how a RAG application works. It can be deployed on any cloud platform such as Hugging Face or Streamlit where you can easily interact with it. You can experiment with its different features — adding new functionalities or trying to modify the existing ones.\n\nResume Analyzer - Hugging Face\n\nConclusion\n\nRetrieval augmented generation (RAG) is the most widely used technique in NLP applications. It augments latest and domain specific information to LLMs and utilizes their powerful capabilities to generate relevant, coherent, and useful responses. LangChain provides robust tools for building RAG applications. Many industry applications, especially customized chatbots, are built using RAG. Retrieval augmented generation makes it possible to interact with any external knowledge source in your own natural language. Its main components include data ingestion, retrieval, and synthesis. There can be hundreds of applications of RAG that can make our lives easier. A resume analysis tool is one such application that can be really helpful when there are hundreds of CVs to go through and you have very little time. A recruiter can get benefit from this amazing technology making his day to day tasks simple and effective. Even a job applicant can use this tool to see if his resume is the right fit for a particular job description.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nIntroduction\n\nImportance of RAG\n\nHigh Level Components of RAG\n\nBasic RAG Workflow\n\nAdvantages of RAG\n\nApplications of RAG\n\nImplementation of RAG using LangChain\n\nRAG Pipeline\n\nData Ingestion\n\nData Loading\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nYou might be interested\nRAG101\nT\nMar 10, 202521 reads\nchatLangChain+1\nStreamlining Employee Q&A Using Retrieval-Augmented Generation (RAG).\nBest Overall Project\nT\nK\nJ\nOct 29, 2024275 reads\nLangchainLlama+5\nEnterprise level self reflecting Agentic RAG Chatbot\nJ\nOct 31, 202441 reads\nFineTunningGenAI+3\nRAG Turns General AI into Domain Experts by Connecting It to Specific Knowledge\nS\nJ\nA\nJun 16, 202529 reads\nAAIDC2025AI chatbot+9",
    "awards": [
      "distinguished technical deep-dive"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "safeschool-realtime-weapon-detection-using-yolov8-with-langchainbased-threat-analysis-agent-XLE3KDaNRIPq",
    "username": "n@narasimhakarthik2",
    "license": "MIT License",
    "title": "SafeSchool: Real-time Weapon Detection using YOLOv8 with LangChain-based Threat Analysis Agent",
    "publication_description": "Back to publications\nDec 29, 2024\n●\n127 reads\n●\nMIT License\nWinner of\nDistinguished Social Impact Innovation\nat the\nComputer Vision Projects Expo 2024\nSafeSchool: Real-time Weapon Detection using YOLOv8 with LangChain-based Threat Analysis Agent\nAgentic system\nautonomus\nComputer Vision\nGPT-4 Vision\nlangchain\nLLM\nObject detection\nschool\nYolo\nN\n@narasimhakarthik2\nLike\nBookmark\nShare\nAbstract\n\nGun violence in U.S. schools continues to be a critical safety concern, exposing the limitations of traditional surveillance systems that rely on human monitoring. This research presents SafeSchool, an AI-powered security system that combines YOLO-based object detection with a Langchain-based monitoring agent, replacing human surveillance with continuous automated threat assessment. The system processes security camera feeds in real-time, detecting weapons with high confidence and triggering an AI agent for comprehensive scene analysis. This approach achieves initial weapon detection in under 50ms and delivers detailed threat analysis within 6 seconds, including weapon identification, suspect description, and location-specific security protocols. The system was developed and validated using a custom dataset of 5,149 annotated frames, collected across three strategic camera positions in a controlled university environment. Results demonstrate substantial improvements in threat detection speed and response time compared to traditional surveillance methods, while maintaining a 99% detection rate and minimizing false positives.\n\nIntroduction\nThe Challenge of School Security\n\nGun violence in U.S. schools represents an ongoing critical safety concern that demands innovative technological solutions. Current security infrastructure, while extensive, is limited by its dependence on human monitoring of surveillance systems. Security personnel tasked with monitoring multiple video feeds face inevitable attention fatigue, leading to potential oversights in threat detection and delayed response times during critical situations.\n\nLimitations of Current Systems\n\nTraditional school surveillance systems face several fundamental challenges:\n\nDependency on human attention span for continuous monitoring\nDelayed threat verification due to manual assessment processes\nInconsistent threat evaluation based on individual interpretation\nExtended response times due to multi-step alert protocols\nLimited scalability constrained by available security personnel\nThe SafeSchool Solution\n\nThis research introduces SafeSchool, implementing an AI-driven dual-detection approach. The system's first layer employs YOLO (You Only Look Once) for continuous weapon detection across surveillance feeds. Upon detection, a Langchain-based AI agent activates to perform comprehensive scene analysis, effectively replacing human monitoring with consistent, automated threat assessment.\nThis automated approach provides several key advantages:\n\nContinuous, fatigue-free monitoring of security feeds\nStandardized threat assessment protocols\nRapid multi-frame analysis for threat confirmation\nLocation-aware security response recommendations\nAutomated alert generation and distribution\n\nThe integration of an AI monitoring agent significantly reduces the critical time between initial detection and response initiation while maintaining high accuracy standards. Most importantly, the system demonstrates effectiveness in early threat detection and assessment, potentially preventing escalation of security incidents in school environments.\n\nhttps://github.com/narasimhakarthik2/SafeSchool\n\nRelated work\n\nReal-time weapon detection in surveillance systems remains a significant challenge in security applications. Salazar González et al. (2020) conducted foundational research on gun detection in CCTV systems, establishing the complexity of achieving reliable real-time detection under varying conditions. Their work highlighted key limitations in processing speed and accuracy that continue to influence current research.\n\nRecent developments in deep learning, particularly YOLO architectures, have improved detection capabilities but still face challenges in threat assessment and response time. Traditional security systems rely heavily on human monitoring for threat verification, leading to potential delays and oversights due to operator fatigue.\n\nThe integration of Large Language Models (LLMs) with vision systems represents a novel approach to automated surveillance. While existing systems focus primarily on object detection, this research extends the state-of-the-art by combining YOLO-based weapon detection with LLM-powered scene analysis, addressing key limitations identified in previous studies.\n\nMethodology\n\nThis section details the system architecture and implementation of SafeSchool. The system consists of three main components: weapon detection using YOLO, scene analysis using a Langchain-based AI agent, and a location-aware alert system.\n\nSystem Architecture\nWeapon Detection Module\nImplementation of YOLOv8 for real-time weapon detection\nConfidence threshold set at 75% to minimize false positives\nDetection categories: Handgun, Short rifle, Knife\nProcessing speed: <50ms per frame\nAI Monitoring Agent\nLangchain-based implementation using GPT-4-Vision\nThree-stage analysis protocol:\n\nInitial threat assessment upon weapon detection\nConfirmatory analysis at 2-second intervals\nFinal threat evaluation with location-specific recommendations\nScene analysis completion within 6 seconds\nLocation-Aware Alert System\nIntegration with camera mapping system\nAutomated security response generation based on threat location\nExperiments\nDataset\n\nThis study utilizes the publicly available mock attack dataset by Salazar González et al. (2020), which simulates realistic threat scenarios in a university environment. The dataset comprises 5,149 annotated frames from three surveillance cameras covering distinct environmental conditions:\n\nCamera 1 (Corridor): 607 frames of uniform lighting with common obstacles such as doors and bins, recorded at 2 FPS over 5 minutes.\nCamera 7 (Corridor): 3,511 frames with additional environmental challenges including wall-mounted objects and fire extinguishers, captured at 2 FPS across 29 minutes.\nCamera 5 (Entrance): 1,031 frames featuring irregular lighting conditions and challenging surface materials like black carpeting, recorded at 2 FPS over 8 minutes.\nTraining Configuration\n\nThe model was trained using YOLOv8n architecture with the following specifications:\nThe network was configured to detect three classes: Handgun, Knife, and Short_rifle. Training parameters included a batch size of 8, input resolution of 640x640 pixels, and Adam optimizer with an initial learning rate of 0.001 and weight decay of 0.0005. Real-time data augmentation was employed during training.\n\nTraining Results\n\nPerformance metrics tracked through Weights & Biases demonstrate strong detection capabilities across all weapon classes. The F1-confidence curves show optimal performance at a confidence threshold of 0.75, with handguns achieving the highest F1 score of 0.8. The precision-recall curves indicate robust model performance, particularly for handgun detection, maintaining precision above 0.9 across a wide range of recall values.\n\nThe recall-confidence analysis reveals effective detection sensitivity, with handguns and short rifles maintaining recall rates above 0.7 at the operational confidence threshold. Knife detection showed comparatively lower recall rates, likely due to their smaller visual signature in surveillance footage.\n\nPerformance Analysis\n\nAt the operational confidence threshold of 0.75:\n\nHandgun detection achieved precision of 0.95 and recall of 0.85\nShort rifle detection maintained precision of 0.92 and recall of 0.82\nKnife detection showed precision of 0.88 with recall of 0.65\n\nThese results demonstrate the model's effectiveness for real-world weapon detection applications, particularly for firearm identification in surveillance scenarios.\n\n\t\n\n\t\n\n\n\t\nResults\nWeapon Detection Performance\n\nYOLOv8n model demonstrated robust detection capabilities:\n\nHandgun: 0.95 precision, 0.85 recall at 0.75 confidence threshold\nShort_rifle: 0.92 precision, 0.82 recall\nKnife: 0.88 precision, 0.65 recall\n\nAverage inference time: <50ms per frame\n\nLLM-based Threat Monitoring\n\nThe Langchain-powered AI agent provides continuous threat assessment through a three-stage analysis:\n\nInitial Assessment\nTriggered immediately upon weapon detection\nStructured analysis of weapon type, suspect description, and risk level\nResponse generated in <2 seconds\n\nThreat Confirmation\nMultiple frame analysis at 2-second intervals\nConversationBufferMemory maintains analysis history for context\nContinuous threat level assessment based on scene changes\n\nFinal Assessment\nGenerated after analyzing multiple frames\nRisk level determined from historical analyses stored in memory\nLocation-aware security recommendations based on camera mapping\n\nThe integrated system demonstrated significant advantages:\nContinuous, automated monitoring without fatigue\nStructured threat assessment with context retention\nAverage response time of 6.2 seconds from detection to final assessment\nLocation-specific security protocols based on camera positioning\nConclusion\n\nThis research presents SafeSchool, an automated security system that successfully integrates YOLO-based weapon detection with LLM-powered threat analysis. The system addresses the critical limitations of traditional surveillance by replacing human monitoring with an AI agent capable of continuous, fatigue-free threat assessment.\n\nThe YOLOv8n model achieved high detection accuracy across weapon classes, with precision rates exceeding 0.90 for firearms at a 0.75 confidence threshold.\n\nThe integration of a Langchain-based AI agent enables comprehensive threat analysis through a three-stage assessment process, storing contextual information in memory for improved decision-making.\n\nKey system achievements include:\n\nSub-50ms weapon detection speed\n6.2-second response time from detection to final assessment\nLocation-aware threat analysis and response protocols\nContinuous monitoring capability without human fatigue\nReferences\n\nSalazar González, J. L., Zaccaro, C., Álvarez-García, J. A., Soria-Morillo, L. M., & Sancho Caparrini, F. (2020). Real-time gun detection in CCTV: An open problem. Neural Networks, 132, 297-308. \nhttps://doi.org/10.1016/j.neunet.2020.09.013\n\nLim, J., et al. (2021). Deep multi-level feature pyramids: Application for non-canonical firearm detection in video surveillance. Engineering applications of artificial intelligence 97, 104094.\n\nJocher, G., et al. (2023). ultralytics/ultralytics: v8.0.0 First Release (v8.0.0). Zenodo. \nhttps://doi.org/10.5281/zenodo.7747343\n\nChase, H., et al. (2023). LangChain: Building applications with LLMs through composability. GitHub repository. \nhttps://github.com/hwchase17/langchain\n\nOpenAI (2023). GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nThe Challenge of School Security\n\nLimitations of Current Systems\n\nThe SafeSchool Solution\n\nRelated work\n\nMethodology\n\nSystem Architecture\n\nWeapon Detection Module\n\nAI Monitoring Agent\n\nView all\nCode\nDatasets",
    "awards": [
      "distinguished social impact innovation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "streamlining-employee-qa-using-retrievalaugmented-generation-rag-m2VlZTBnZVgT",
    "username": "t@tanveersingh182764",
    "license": "MIT License",
    "title": "Streamlining Employee Q&A Using Retrieval-Augmented Generation (RAG).",
    "publication_description": "Back to publications\nOct 29, 2024\n●\n275 reads\n●\nMIT License\nWinner of\nBest Overall Project\nat the\nNLP Projects Expo 2024\nStreamlining Employee Q&A Using Retrieval-Augmented Generation (RAG).\nLangchain\nLlama\nLLM\nNLP\nnomic\nOpen source\nRAG\nT\n@tanveersingh182764\nK\n@kritdoshi\nJ\n@jagrut90\nLike\nBookmark\nShare\nProject\n\nWe have developed a Q&A bot for our new employees by leveraging various Retrieval-Augmented Generation (RAG) techniques, using the Llama 3.1 70B model. This project showcases the different results obtained after training the RAG system on 150+ company manuals and onboarding documents. Implementing an interactive chatbot for new joiners streamlined HDFC Bank's onboarding process by providing 24*7 instant support, reducing HR dependency, and ensuring consistent responses to repetitive queries. This automation enables HR to focus on strategic tasks, enhancing overall efficiency for both central HR teams and new joiners. This publication aims to present our findings and demonstrate the effectiveness of RAG in enhancing the employee onboarding experience.\n\nIntroduction\n\nFor a large-scale bank of HDFC's stature, the new employees crave for immediate support for smoother integration in their formative months with the Bank. Hence, having a readily available assistance would foster confidence, speed up acclimation, and enhance the overall experience during the initial 6 months, laying a strong foundation for long-term engagement.\n\nBackground\n\nHDFC Bank, one of India's largest and most prominent private sector banks, was incorporated in August 1994 as a part of the Housing Development Finance Corporation (HDFC) Group. The bank was established with a vision to offer a wide range of banking and financial services to individuals and businesses, and over time, it has grown into a leader in the Indian banking sector.\n\nAs of 31st Mar'24, the Bank employs 2.2 lakh individuals across 9k+ locations. Roughly, 5,000 new employees join the Bank every month - naturally have multiple doubts related to the Bank's policies/ processes / onboarding process. 5 central onboarding teams typically handle these queries. New employees often reach out to their respective HR Business Partners (HRBPs) for assistance, and the HRBPs, receive around 1k queries/day across the Bank, for which their TAT hovers anywhere between an hour to 2 working days depending on the backlog created. This entire process introduces significant human dependency, leading to delays and inefficiencies.\n\nMoreover, the repetitive nature of the questions often creates a bottleneck in the onboarding process, with HRBPs handling multiple similar queries instead of focusing on higher-value tasks. The manual nature of addressing these concerns not only slows down communication but also increases the potential for human error. The volume of queries typically overwhelm the centralized teams, further extending response times and leading to dissatisfaction among new joiners.\n\nTo streamline this process, there was a dire need for an automation solution, such as an AI-powered chatbot or self-service knowledge platform, which would provide immediate and consistent responses. Implementing such systems would significantly enhance the onboarding experience, improve response times, and ensure that new employees can access the information they need quickly and accurately.\n\nRef:- \nHDFC Bank\nApproach\n\nThe 10 step high level process for building the requisite solution could be summarized as below -\n\n 1. Data Aggregation: Collection of diverse data across formats and topics, including 150+ process manuals, documents, HTML files, pdfs and spreadsheets, to ensure a broad information base\n 2. Domain-Specific Evaluation: Utilizing domain experts to manually evaluate and enrich data, enhancing relevance and accuracy for end-users\n 3. Development of the RAG Lifecycle: Development of a comprehensive framework for RAG to guide the chatbot’s response generation process\n 4. Initial RAG Implementation: Deploying a basic RAG model to serve as a foundation for information retrieval and response accuracy\n 5. Adaptive RAG Development: Refining the RAG model to adjust dynamically based on query complexity and context\n 6. Raptor Integration: Enhancing the solution with Raptor for increased speed and efficiency in query handling\n 7. Self-RAG Advancement: Implementing a self-learning RAG model to improve response quality through adaptive feedback\n 8. Graph-RAG Application: Integrating Graph-RAG to leverage graph structures for more contextualized information retrieval\n 9. Human Evaluation: Conducting thorough assessments with central HR teams to validate chatbot responses and ensure quality standards\n 10. Feedback Loop: Looping back insights/feedback into the system for continuous improvement and relevancy.\nArchitecture\n\nA high level process flow is depicted in the image below\n\nThe code snippets and flowcharts for RAG implementations provided in this publication are illustrative examples sourced from publicly available references and repositories. They are intended for educational purposes to aid understanding of the concepts discussed. The actual code developed for this project is proprietary to HDFC Bank Ltd and is not disclosed publicly.\n\nRAG Techniques with Code Implementations\nSimple RAG\n\nWe started with a basic implementation of a RAG system tailored for processing and querying PDF and word documents. The system encoded the content of the documents into a vector store, enabling efficient retrieval of relevant information based on user queries.\n\nKey Features\n\nModular Design: The encoding process was wrapped in a single function for easy reuse and integration into larger systems\nConfigurable Chunking: Allowed adjustment of chunk size and overlap to fine-tune the balance between retrieval accuracy and performance\nEfficient Retrieval: Employed Chroma\nEvaluation Capability: Included an evaluate_rag function to assess the system's performance, enabling iterative improvements.\nUsage Example\nThe code provides a test query—\"How can I apply for a privilege leave?\"—to demonstrate how the retriever fetched relevant context from the processed documents. This example showcases the system's ability to deliver precise information based on the encoded content.\n\nImg Credit: BentoML\n\ndef encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Encodes a PDF book into a vector store using nomic embeddings.\n\n    Args:\n        path: The path to the PDF file.\n        chunk_size: The desired size of each text chunk.\n        chunk_overlap: The amount of overlap between consecutive chunks.\n\n    Returns:\n        A Chroma vector store containing the encoded book content.\n    \"\"\"\n\n    # Load PDF documents\n    loader = PyPDFLoader(path)\n    documents = loader.load()\n\n    # Split documents into chunks\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n    )\n    texts = text_splitter.split_documents(documents)\n    cleaned_texts = replace_t_with_space(texts)\n\n    # Create embeddings and vector store\n    embeddings = nomic\n    vectorstore = chroma\n    return vectorstore\n\n\n\nchunks_vector_store = encode_pdf(path, chunk_size=1000, chunk_overlap=200)\nchunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 2})\n\ntest_query = \"How can I apply for a privilege leave?\"\ncontext = retrieve_context_per_question(test_query, chunks_query_retriever)\nshow_context(context)\n\nCode Example: NirDiamont/Rag_techniques\n\nImpact:\n\nThis simple RAG system provided a solid foundation for building more complex information retrieval and question-answering systems. By encoding document content into a searchable vector store, we tried to enable efficient retrieval of relevant information in response to queries. This approach is particularly useful for applications requiring quick access to specific information within large documents or document collections. However, given its limitation in terms of context understanding, self-learning and accuracy in high-diversity data, this standalone architecture was inadequate to deploy.\n\nAdaptive Retrieval-Augmented Generation (RAG) System\n\nNext we explored an advanced RAG approach that adapted its retrieval strategy based on the type of query. By leveraging Language Models (LLMs) at various stages, it provided more accurate, relevant, and context-aware responses to user queries.\n\nKey Components\n\nQuery Classifier: Determines the type of query (Factual, Analytical, Opinion, or Contextual).\nAdaptive Retrieval Strategies: Four distinct strategies tailored to different query types were tried:\n\nFactual Strategy:- Beneficial for Direct Information Queries\nExample:- New joiners often have straightforward questions requiring specific answers, such as\n\"What is the company's dress code?\" or \"How do I access my employee portal?\"\n\nAnalytical Strategy:-Beneficial for Complex Processes and Deep Understanding\nExample:- New employees may need to understand intricate processes or systems, like \"How does the annual performance review process work?\" or \"Explain the steps involved in project initiation.\"\n\nOpinion Strategy-Beneficial for Cultural Insights and Diverse Perspectives\nExample: New joiners might be curious about company culture or employee sentiments, asking questions like \"What do employees think about the remote work policy?\" or \"How is the work-life balance here?\"\n\nContextual Strategy-Beneficial for Personalized Guidance\nExample: Queries that depend on the individual's role or previous interactions, such as \"What training should I prioritize as a marketing associate?\" or \"Who should I contact for IT support?\"\n\nLLM Integration for Enhanced Retrieval and Ranking\n\nBy integrating Language Models (LLMs) throughout the retrieval process, the chatbot for new joiners was able to better understand the nuances and intent behind their questions. This enhanced the way information was fetched and prioritized, allowing the chatbot to refine search queries and rank the most relevant documents higher. As a result, new employees can receive more accurate and helpful information that is tailored to their specific needs during the onboarding process.\n\nLlama Model for Contextualized Response Generation\n\nWe further used the Llama Model to generate responses from retrieved documents, for more accurate, detailed answers enriched with Bank's information.\n\nImage Credit: NirDiamont/Rag_techniques\n\n\nclass AdaptiveRAG:\n    def __init__(self, texts: List[str]):\n        adaptive_retriever = AdaptiveRetriever(texts)\n        self.retriever = PydanticAdaptiveRetriever(adaptive_retriever=adaptive_retriever)\n        self.llm = Llama\n        \n        # Create a custom prompt\n        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n        {context}\n\n        Question: {question}\n        Answer:\"\"\"\n        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n        \n        # Create the LLM chain\n        self.llm_chain = prompt | self.llm\n\n\n    def answer(self, query: str) -> str:\n        docs = self.retriever.get_relevant_documents(query)\n        input_data = {\"context\": \"\\n\".join([doc.page_content for doc in docs]), \"question\": query}\n        return self.llm_chain.invoke(input_data)\n\nCode Example: NirDiamont/Rag_techniques\n\nImpact :\n\nThe adaptive RAG system was superior to the simple RAG system because it provided a more nuanced, accurate, and user-specific experience. It adapted to the nature of each query, ensuring that responses were relevant and comprehensive, and enhanced overall user satisfaction. For a chatbot assisting new joiners, these improvements translated into a smoother onboarding process, better understanding of company practices, and a more engaging interaction, all of which were less effectively achieved with a simple RAG approach.\n\nRAPTOR: Recursive Abstractive Processing and Thematic Organization for Retrieval\n\nAs speed and precision were critical for real-time responses we integrated RAPTOR, an advanced system to enhance information retrieval and question-answering capabilities. Here's how each component of RAPTOR enhanced the chatbot's effectiveness:\n\nKey Components\n\nHierarchical Summarization (Tree Building)-Beneficial for Navigating Extensive Onboarding Materials\nUse Case: New joiners often need to understand both high-level overviews and specific details about the Bank, such as organizational structure, policies, procedures, and culture.\n\nEmbedding and Clustering-Beneficial for Organizing and Retrieving Relevant Information\nUse Case: When a new joiner asks a question, the chatbot needs to search through vast amounts of Bank documents and resources to find the most relevant information.\n\nVector store - Beneficial for Fast and Scalable Information Access\nUse Case: New employees expect prompt and accurate responses from the chatbot, even when dealing with a large repository of documents.\n\nContextual Retriever-Beneficial for Tailored and Precise Information Delivery\nUse Case: The chatbot must provide answers that are not only accurate but also directly relevant to the specific context of the new joiner's query.\n\nAnswer Generation Using Language Models-Beneficial for Creating Clear and Coherent Explanations\nUse Case: New joiners may have questions that require synthesizing information from multiple sources to provide a comprehensive answer.\n\nImage Credit: langchain.ca\n\ndef build_raptor_tree(texts: List[str], max_levels: int = 3) -> Dict[int, pd.DataFrame]:\n    \"\"\"Build the RAPTOR tree structure with level metadata and parent-child relationships.\"\"\"\n    results = {}\n    current_texts = [extract_text(text) for text in texts]\n    current_metadata = [{\"level\": 0, \"origin\": \"original\", \"parent_id\": None} for _ in texts]\n    \n    for level in range(1, max_levels + 1):\n        logging.info(f\"Processing level {level}\")\n        \n        embeddings = embed_texts(current_texts)\n        n_clusters = min(10, len(current_texts) // 2)\n        cluster_labels = perform_clustering(np.array(embeddings), n_clusters)\n        \n        df = pd.DataFrame({\n            'text': current_texts,\n            'embedding': embeddings,\n            'cluster': cluster_labels,\n            'metadata': current_metadata\n        })\n        \n        results[level-1] = df\n        \n        summaries = []\n        new_metadata = []\n        for cluster in df['cluster'].unique():\n            cluster_docs = df[df['cluster'] == cluster]\n            cluster_texts = cluster_docs['text'].tolist()\n            cluster_metadata = cluster_docs['metadata'].tolist()\n            summary = summarize_texts(cluster_texts)\n            summaries.append(summary)\n            new_metadata.append({\n                \"level\": level,\n                \"origin\": f\"summary_of_cluster_{cluster}_level_{level-1}\",\n                \"child_ids\": [meta.get('id') for meta in cluster_metadata],\n                \"id\": f\"summary_{level}_{cluster}\"\n            })\n        \n        current_texts = summaries\n        current_metadata = new_metadata\n        \n        if len(current_texts) <= 1:\n            results[level] = pd.DataFrame({\n                'text': current_texts,\n                'embedding': embed_texts(current_texts),\n                'cluster': [0],\n                'metadata': current_metadata\n            })\n            logging.info(f\"Stopping at level {level} as we have only one summary\")\n            break\n    \n    return results\ndef build_vectorstore(tree_results: Dict[int, pd.DataFrame]) -> Chroma:\n    \"\"\"Build a chroma vectorstore from all texts in the RAPTOR tree.\"\"\"\n    all_texts = []\n    all_embeddings = []\n    all_metadatas = []\n    \n    for level, df in tree_results.items():\n        all_texts.extend([str(text) for text in df['text'].tolist()])\n        all_embeddings.extend([embedding.tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in df['embedding'].tolist()])\n        all_metadatas.extend(df['metadata'].tolist())\n    \n    logging.info(f\"Building vectorstore with {len(all_texts)} texts\")\n    \n    # Create Document objects manually to ensure correct types\n    documents = [Document(page_content=str(text), metadata=metadata) \n                 for text, metadata in zip(all_texts, all_metadatas)]\n    \n    return chroma.from_documents(documents, embeddings)\ndef tree_traversal_retrieval(query: str, vectorstore: chroma, k: int = 3) -> List[Document]:\n    \"\"\"Perform tree traversal retrieval.\"\"\"\n    query_embedding = embeddings.embed_query(query)\n    \n    def retrieve_level(level: int, parent_ids: List[str] = None) -> List[Document]:\n        if parent_ids:\n            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n                query_embedding,\n                k=k,\n                filter=lambda meta: meta['level'] == level and meta['id'] in parent_ids\n            )\n        else:\n            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n                query_embedding,\n                k=k,\n                filter=lambda meta: meta['level'] == level\n            )\n        \n        if not docs or level == 0:\n            return docs\n        \n        child_ids = [doc.metadata.get('child_ids', []) for doc, _ in docs]\n        child_ids = [item for sublist in child_ids for item in sublist]  # Flatten the list\n        \n        child_docs = retrieve_level(level - 1, child_ids)\n        return docs + child_docs\n    \n    max_level = max(doc.metadata['level'] for doc in vectorstore.docstore.values())\n    return retrieve_level(max_level)\ndef create_retriever(vectorstore: Chroma) -> ContextualCompressionRetriever:\n    \"\"\"Create a retriever with contextual compression.\"\"\"\n    logging.info(\"Creating contextual compression retriever\")\n    base_retriever = vectorstore.as_retriever()\n    \n    prompt = ChatPromptTemplate.from_template(\n        \"Given the following context and question, extract only the relevant information for answering the question:\\n\\n\"\n        \"Context: {context}\\n\"\n        \"Question: {question}\\n\\n\"\n        \"Relevant Information:\"\n    )\n    \n    extractor = LLMChainExtractor.from_llm(llm, prompt=prompt)\n    \n    return ContextualCompressionRetriever(\n        base_compressor=extractor,\n        base_retriever=base_retriever\n    )\ndef hierarchical_retrieval(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> List[Document]:\n    \"\"\"Perform hierarchical retrieval starting from the highest level, handling potential None values.\"\"\"\n    all_retrieved_docs = []\n    \n    for level in range(max_level, -1, -1):\n        # Retrieve documents from the current level\n        level_docs = retriever.get_relevant_documents(\n            query,\n            filter=lambda meta: meta['level'] == level\n        )\n        all_retrieved_docs.extend(level_docs)\n        \n        # If we've found documents, retrieve their children from the next level down\n        if level_docs and level > 0:\n            child_ids = [doc.metadata.get('child_ids', []) for doc in level_docs]\n            child_ids = [item for sublist in child_ids for item in sublist if item is not None]  # Flatten and filter None\n            \n            if child_ids:  # Only modify query if there are valid child IDs\n                child_query = f\" AND id:({' OR '.join(str(id) for id in child_ids)})\"\n                query += child_query\n    \n    return all_retrieved_docs\n\n\ndef raptor_query(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> Dict[str, Any]:\n    \"\"\"Process a query using the RAPTOR system with hierarchical retrieval.\"\"\"\n    logging.info(f\"Processing query: {query}\")\n    \n    relevant_docs = hierarchical_retrieval(query, retriever, max_level)\n    \n    doc_details = []\n    for i, doc in enumerate(relevant_docs, 1):\n        doc_details.append({\n            \"index\": i,\n            \"content\": doc.page_content,\n            \"metadata\": doc.metadata,\n            \"level\": doc.metadata.get('level', 'Unknown'),\n            \"similarity_score\": doc.metadata.get('score', 'N/A')\n        })\n    \n    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n    \n    prompt = ChatPromptTemplate.from_template(\n        \"Given the following context, please answer the question:\\n\\n\"\n        \"Context: {context}\\n\\n\"\n        \"Question: {question}\\n\\n\"\n        \"Answer:\"\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    answer = chain.run(context=context, question=query)\n    \n    logging.info(\"Query processing completed\")\n    \n    result = {\n        \"query\": query,\n        \"retrieved_documents\": doc_details,\n        \"num_docs_retrieved\": len(relevant_docs),\n        \"context_used\": context,\n        \"answer\": answer,\n        \"model_used\": llm.model_name,\n    }\n    \n    return result\n\nCode Example: NirDiamont/Rag_techniques\n\nImpact\n\nRAPTOR surpassed adaptive RAG and simple RAG by introducing hierarchical summarization to efficiently manage large document collections through a multi-level tree of summaries. This structure enabled the system to provide both high-level overviews and detailed information. By combining this approach with embedding-based retrieval and contextual answer generation, RAPTOR delivered more accurate and relevant responses.\n\nSelf-RAG: A Dynamic Approach to Retrieval-Augmented Generation\n\nTo further integrate self-learning, Self-RAG was deployed to combine the power of retrieval-based and generation-based approaches in natural language processing. It dynamically decides whether to use retrieved information and how to best utilize it in generating responses, aiming to produce more accurate, relevant, and useful outputs.\n\nKey Components\n\nRetrieval Decision: To determine if retrieval was necessary for a given query.\nDocument Retrieval: To fetch potentially relevant documents from a vector store.\nRelevance Evaluation: To assess the relevance of retrieved documents to the query.\nResponse Generation: To generate responses based on relevant contexts.\nSupport Assessment: To evaluate how well the generated response is supported by the context.\nUtility Evaluation: To rate the usefulness of the generated response.\n\nImage Credit: blog.langchain.dev\n\ndef self_rag(query, vectorstore, top_k=3):\n    print(f\"\\nProcessing query: {query}\")\n    \n    # Step 1: Determine if retrieval is necessary\n    print(\"Step 1: Determining if retrieval is necessary...\")\n    input_data = {\"query\": query}\n    retrieval_decision = retrieval_chain.invoke(input_data).response.strip().lower()\n    print(f\"Retrieval decision: {retrieval_decision}\")\n    \n    if retrieval_decision == 'yes':\n        # Step 2: Retrieve relevant documents\n        print(\"Step 2: Retrieving relevant documents...\")\n        docs = vectorstore.similarity_search(query, k=top_k)\n        contexts = [doc.page_content for doc in docs]\n        print(f\"Retrieved {len(contexts)} documents\")\n        \n        # Step 3: Evaluate relevance of retrieved documents\n        print(\"Step 3: Evaluating relevance of retrieved documents...\")\n        relevant_contexts = []\n        for i, context in enumerate(contexts):\n            input_data = {\"query\": query, \"context\": context}\n            relevance = relevance_chain.invoke(input_data).response.strip().lower()\n            print(f\"Document {i+1} relevance: {relevance}\")\n            if relevance == 'relevant':\n                relevant_contexts.append(context)\n        \n        print(f\"Number of relevant contexts: {len(relevant_contexts)}\")\n        \n        # If no relevant contexts found, generate without retrieval\n        if not relevant_contexts:\n            print(\"No relevant contexts found. Generating without retrieval...\")\n            input_data = {\"query\": query, \"context\": \"No relevant context found.\"}\n            return generation_chain.invoke(input_data).response\n        \n        # Step 4: Generate response using relevant contexts\n        print(\"Step 4: Generating responses using relevant contexts...\")\n        responses = []\n        for i, context in enumerate(relevant_contexts):\n            print(f\"Generating response for context {i+1}...\")\n            input_data = {\"query\": query, \"context\": context}\n            response = generation_chain.invoke(input_data).response\n            \n            # Step 5: Assess support\n            print(f\"Step 5: Assessing support for response {i+1}...\")\n            input_data = {\"response\": response, \"context\": context}\n            support = support_chain.invoke(input_data).response.strip().lower()\n            print(f\"Support assessment: {support}\")\n            \n            # Step 6: Evaluate utility\n            print(f\"Step 6: Evaluating utility for response {i+1}...\")\n            input_data = {\"query\": query, \"response\": response}\n            utility = int(utility_chain.invoke(input_data).response)\n            print(f\"Utility score: {utility}\")\n            \n            responses.append((response, support, utility))\n        \n        # Select the best response based on support and utility\n        print(\"Selecting the best response...\")\n        best_response = max(responses, key=lambda x: (x[1] == 'fully supported', x[2]))\n        print(f\"Best response support: {best_response[1]}, utility: {best_response[2]}\")\n        return best_response[0]\n    else:\n        # Generate without retrieval\n        print(\"Generating without retrieval...\")\n        input_data = {\"query\": query, \"context\": \"No retrieval necessary.\"}\n        return generation_chain.invoke(input_data).response\n\nCode Example: NirDiamont/Rag_techniques\n\nImpact\n\nSelf-RAG was a more sophisticated approach to question-answering and information retrieval tasks. By incorporating multiple evaluation steps and dynamically deciding on the use of retrieved information, it produced responses that were not only relevant and accurate but also useful to the end-user. This method showcases the potential of combining retrieval and generation techniques in a thoughtful, evaluated manner to enhance the quality of AI-generated responses.\n\nGraphRAG: Graph-Enhanced Retrieval-Augmented Generation\n\nTo further improve performance, we tested out GraphRAG, an advanced question-answering system that combines the power of graph-based knowledge representation with retrieval-augmented generation. It processes input documents to create a rich knowledge graph, which is then used to enhance the retrieval and generation of answers to user queries. The system leverages natural language processing, machine learning, and graph theory to provide more accurate and contextually relevant responses.\n\nKey Components\n\nDocumentProcessor: Handles the initial processing of input documents, creating text chunks and embeddings.\n\nKnowledgeGraph: Constructs a graph representation of the processed documents, where nodes represent text chunks and edges represent relationships between them.\n\nQueryEngine: Manages the process of answering user queries by leveraging the knowledge graph and vector store.\n\nImage Credit: NirDiamont/Rag_techniques\n\nclass GraphRAG:\n    def __init__(self):\n        \"\"\"\n        Initializes the GraphRAG system with components for document processing, knowledge graph construction,\n        querying, and visualization.\n        \n        Attributes:\n        - llm: An instance of a large language model (LLM) for generating responses.\n        - embedding_model: An instance of an embedding model for document embeddings.\n        - document_processor: An instance of the DocumentProcessor class for processing documents.\n        - knowledge_graph: An instance of the KnowledgeGraph class for building and managing the knowledge graph.\n        - query_engine: An instance of the QueryEngine class for handling queries (initialized as None).\n        - visualizer: An instance of the Visualizer class for visualizing the knowledge graph traversal.\n        \"\"\"\n        self.llm = LLAMA\n        self.embedding_model = NOMIC\n        self.document_processor = DocumentProcessor()\n        self.knowledge_graph = KnowledgeGraph()\n        self.query_engine = None\n        self.visualizer = Visualizer()\n\n    def process_documents(self, documents):\n        \"\"\"\n        Processes a list of documents by splitting them into chunks, embedding them, and building a knowledge graph.\n        \n        Args:\n        - documents (list of str): A list of documents to be processed.\n        \n        Returns:\n        - None\n        \"\"\"\n        splits, vector_store = self.document_processor.process_documents(documents)\n        self.knowledge_graph.build_graph(splits, self.llm, self.embedding_model)\n        self.query_engine = QueryEngine(vector_store, self.knowledge_graph, self.llm)\n\n    def query(self, query: str):\n        \"\"\"\n        Handles a query by retrieving relevant information from the knowledge graph and visualizing the traversal path.\n        \n        Args:\n        - query (str): The query to be answered.\n        \n        Returns:\n        - str: The response to the query.\n        \"\"\"\n        response, traversal_path, filtered_content = self.query_engine.query(query)\n        \n        if traversal_path:\n            self.visualizer.visualize_traversal(self.knowledge_graph.graph, traversal_path)\n        else:\n            print(\"No traversal path to visualize.\")\n        \n        return response\n\nCode Example: NirDiamont/Rag_techniques\n\nImpact\n\nBy incorporating a graph-based knowledge representation and intelligent traversal mechanisms, GraphRAG offered improved context awareness, decent accurate retrieval, and enhanced explainability. The system's ability to visualize its decision-making process provided valuable insights into its operation, making it a powerful tool for both end-users and developers. However, Graph RAG was much more resource intensive than Self-RAG and difficult to scale, also Self-RAG's continuous self-learning capability proved superior in terms of evaluation to GraphRAG over a definite period of training.\n\nResults\n\nHuman evaluation was considered the primary evaluation criteria for all the RAG architectures.\nOn a sample of 200+ questions, each of the above RAG techniques were evaluated with the results being the following:-\n\nTechnique\tHuman Evaluations %\nSIMPLE RAG\t28\nADAPTIVE RAG\t37\nRAPTOR\t43\nSELF RAG\t59\nGRAPH RAG\t64\nConclusion\n\nBased on the highest score achieved by the Graph RAG system on the human evaluation metric, we have decided to move forward with this approach - we are able to address each query in less than 5 seconds, moving down the TAT drastically and work proportional to ~100 daily working hours has been considerably reduced to ~2 daily working hours. With the central HR team also kept in the loop for each query answered, the human element has not been totally removed, wherever the chatbot response has not been adequate enough. Overall, the entire onboarding experience has become much more seamless and efficient.\nIn addition, we are experimenting with different retrieval, chunking, and compression techniques to enhance the overall performance of the RAG system. The goal is to develop a more robust mechanism that can handle a wider range of user needs and further improve the system’s effectiveness.\n\nReferences\nhttps://github.com/NirDiamant/RAG_Techniques\nhttps://blog.langchain.dev/agentic-rag-with-langgraph/\nhttps://www.langchain.ca/blog/efficient-information-retrieval-from-complex-pdfs-using-raptor-rag/\nhttps://www.langchain.com/\nhttps://huggingface.co/nomic-ai/nomic-embed-text-v1\nhttps://huggingface.co/meta-llama/Llama-3.1-70B\nhttps://www.hdfcbank.com/personal/about-us/overview/who-we-are\nhttps://www.bentoml.com/blog/building-rag-with-open-source-and-custom-ai-models\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nProject\n\nIntroduction\n\nBackground\n\nApproach\n\nArchitecture\n\nRAG Techniques with Code Implementations\n\nSimple RAG\n\nKey Features\n\nImpact:\n\nAdaptive Retrieval-Augmented Generation (RAG) System\n\nView all\nComments\n(1)\nYou might be interested\nEnterprise level self reflecting Agentic RAG Chatbot\nJ\nOct 31, 202441 reads\nFineTunningGenAI+3\nRAG Assistant - AAIDC\nK\nJun 13, 202513 reads\nAAIDC2025\nRetrieval Augmented Generation (RAG) Case Study - A Resume Analysis Tool\nDistinguished Technical Deep-Dive\nA\nOct 25, 2024132 reads\nChatbotGenerative AI+6\nRAG based chatbot\nM\nSep 09, 20259 reads",
    "awards": [
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "towards-queryable-earth-via-relational-reasoningbased-remote-sensing-visual-question-answering-3omC6S0l8pEn",
    "username": "l@ltesm",
    "license": null,
    "title": "Towards Queryable Earth via Relational Reasoning-Based Remote Sensing Visual Question Answering",
    "publication_description": "Back to publications\nDec 20, 2024\n●\n57 reads\n●\nCreative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\nWinner of\nMost Innovative Project\nat the\nComputer Vision Projects Expo 2024\nTowards Queryable Earth via Relational Reasoning-Based Remote Sensing Visual Question Answering\nRemote sensing\nVision-langauge model\nVisual question answering\nL\n@ltesm\nZ\n@zihang\nLike\nBookmark\nShare\nAbstract\n\nEarth vision research typically focuses on extracting geospatial object locations and categories but neglects the exploration of relations between objects and comprehensive reasoning. Based on city planning needs, we develop a multi-modal multi-task VQA dataset (EarthVQA) to advance relational reasoning-based judging, counting, and comprehensive analysis. The EarthVQA dataset contains 6000 images, corresponding semantic masks, and 208,593 QA pairs with urban and rural governance requirements embedded. As objects are the basis for complex relational reasoning, we propose a Semantic OBject Awareness framework (SOBA) to advance VQA in an object-centric way. To preserve refined spatial locations and semantics, SOBA leverages a segmentation network for object semantics generation. The object-guided attention aggregates object interior features via pseudo masks, and bidirectional cross attention further models object external relations hierarchically. To optimize object counting, we propose a numerical difference loss that dynamically adds difference penalties, unifying the classification and regression tasks. Experimental results show that SOBA outperforms both advanced general and remote sensing methods. We believe this dataset and framework provide a strong benchmark for Earth vision's complex analysis.\n\nIntroduction\n\nHigh-spatial resolution (HSR) remote sensing images can assist us in quickly obtaining essential information (Zvonkov et al. 2023; Xiao et al. 2023). Most research focuses on the perception of object categories and locations, deriving related tasks such as semantic segmentation (Liu et al. 2023), species detection (Zhao et al. 2022), and urban understanding (Shi et al. 2023). However, the existing methods and datasets ignore the relations between the geospatial objects, thus limiting their ability to knowledge reasoning in complex scenarios. Especially in city planning (Bai, Shi, and Liu 2014), the relations between the transportation hubs and schools, water situations around the farmland, and greenery distributions in residential areas are also significant and urgent to be analyzed. Hence, it is necessary to go beyond object perception and explore object relations, bridging the gap between information and comprehensive knowledge (Li and Krishna 2022).\n\nVisual question answering (VQA) aims to answer customized questions by searching for visual clues in the provided image. Since linguistic questions determine the task properties, the algorithms are flexible and can be developed for reasoning required answers. Recently, preliminary VQA datasets and methods have emerged in the remote sensing field (Lobry et al. 2020; Zheng et al. 2021; Rahnemoonfar et al. 2021). However, most of these researches have the following drawbacks: 1) As for most datasets, QA pairs are\nautomatically labeled based on existing data, such as Open Street Map (OSM) and classification datasets. Most tasks are simple counting and judging questions with no relational reasoning required. The automatic QA pairs do not match actual needs, limiting their practicalities. 2) The development of the remote sensing VQA model lags, and most research directly fuses the global visual and language features to predict the final answers. They ignore the local semantics and relations, which are unsuitable for the complex reasoning of multiple geospatial objects. To this end, we propose a multi-modal multi-task VQA dataset and a semantic object awareness framework to advance complex remote sensing VQA tasks. The main contributions are as follows:\n\nWe propose the EarthVQA dataset with triplet samples (image-mask-QA pairs). The 208,593 QA pairs encompass six main categories. EarthVQA features diverse tasks from easy basic judging to complex relation reasoning and even more challenging comprehensive analysis. Specifically, the residential environments, traffic situations, and renovation needs of waters and unsurfaced roads are explicitly embedded in various questions.\nTo achieve relational reasoning-based VQA, we propose a semantic object awareness framework (SOBA). SOBA utilizes segmentation visual prompts and pseudo masks to generate pixel-level features with accurate locations. The object awareness-based hybrid attention models the relations for object-guided semantics and bidirectionally aggregates multi-modal features for answering.\nTo add distance sensitivity for regression questions, we propose a numerical difference (ND) loss. The dynamic ND penalty is seamlessly integrated into cross-entropy loss for the regression task. ND loss introduces the sensitivity of numerical differences into the model training.\nRelated Work\nGeneral Visual Question Answering\n\nThe vanilla VQA model (Antol et al., 2015) includes three parts: a convolutional neural network (CNN), a long-short term memory (LSTM), and a fusion classifier. Specifically, CNN extracts visual features for input images, and LSTM embeds the language features for the questions. Global features are interacted in the fusion classifier and finally generate the answer. Based on this architecture, more powerful encoders and fusion modules were proposed.\n\nTo obtain local visual features, the bottom-up top-down attention (BUTD) mechanism (Anderson et al., 2018) introduced objectness features generated by Faster-RCNN (Ren et al., 2015) pretrained on Visual Genome (Krishna et al., 2017) data. For computational efficiency, a recurrent memory, attention, and composition (MAC) cell (Hudson & Manning, 2018) was designed to explicitly model the relations between image and language features. Similarly, the stacked attention network (SAN) (Yang et al., 2016) located the relevant visual clues guided by questions layer-by-layer. By combining objectness features with attention, the modular co-attention network (MCAN) (Yu et al., 2019) adopted a transformer to model intra- and inter-modality interactions.\n\nTo alleviate language biases, D-VQA ](Wen et al., 2021) applied a unimodal bias detection module to explicitly remove negative biases. BLIP-2 (Li et al., 2023) and Instruct-BLIP (Dai et al., 2023) bridge large pre-trained vision and language models using the Q-Former, addressing VQA as a generative task. Additionally, many advanced VQA methods (Marino et al., 2021) eliminate statistical bias by accessing external databases.\n\nRemote Sensing Visual Question Answering\n\nThe remote sensing community has some early explorations, including both datasets and methods. The QA pairs of the RSVQA dataset (Luca et al., 2020) are queried from OSM, and images are obtained from Sentinel-2 and other sensors. RSIVQA dataset (Zheng et al., 2021) is automatically generated from existing classification and object detection datasets, i.e., AID (Xia et al., 2017), HRRSD (Zhang et al., 2019), etc. The FloodNet dataset (Rahnemoonfar et al., 2021) was designed for disaster assessment, mainly concerned with the inundation of roads and buildings.\n\nCompared with these datasets, the EarthVQA dataset has two advantages:\n\nMulti-level annotations. The annotations include pixel-level semantic labels, object-level analysis questions, and scene-level land use types. Supervision from different perspectives advances a comprehensive understanding of complex scenes.\nComplex and practical questions. The existing datasets focus on counting and judging questions, which only involve simple relational reasoning about one or two types of objects. In addition to counting and judging, EarthVQA also contains various object analysis and comprehensive analysis questions. These promote complex relational reasoning by introducing spatial or semantic analysis of more than three types of objects. Only basic judging and counting answers are auto-generated from the LoveDA masks. Other reasoning answers (Figure 1) are manually annotated (reasoning distances, layouts, topologies, sub-properties, etc.) for city planning needs.\n\nRemote sensing algorithms are mainly modified from general methods. For example, RSVQA is based on vanilla VQA (Antol et al., 2015). RSIVQA (Zheng et al., 2021) designed a mutual attention component to improve interactions for multi-modal features. CDVQA (Yuan et al., 2022) introduced VQA into the change detection task. We novelly introduce pixel-level features for the guidance of VQA tasks, making it suitable for scenes with compact objects.\n\nEarthVQA Dataset\n\nThe EarthVQA dataset was extended from the LoveDA dataset (Wang et al., 2021), which encompasses 18 urban and rural regions from Nanjing, Changzhou, and Wuhan. LoveDA dataset provides 5,987 HSR images and semantic masks with seven common land-cover types. There are three significant revisions:\n\nQuantity expansion. Eight urban and five rural samples are added to expand capacity to 6,000 images (WorldView-3 0.3m).\nLabel refinement. A playground class was added as an important artificial facility, and some errors in semantic labels were revised.\nAddition of QA pairs. We added 208,593 QA pairs to introduce VQA tasks for city planning. Each urban image has 42 QAs, and each rural image has 29 QAs.\n\nFollowing the balanced division (Wang et al., 2021), the dataset is split as follows: Train set: 2,522 images with 88,166 QAs , Validation set: 1,669 images with 57,202 QAs ,Test set: 1,809 images with 63,225 QAs .\n\n\nFigure 1. Urban and rural samples (image-mask-QA pairs) from the EarthVQA dataset. The QA pairs are designed to based on city planning needs, including judging, counting, object situation analysis, and comprehensive analysis types.\nThis multi-modal and multi-task dataset poses new challenges, requiring object-relational reasoning and knowledge summarization.\n\n\n(a) Annotation procedure of relational-based QA.\t\n\n(b) Statistics of questions.\t\n\n(c) Distributions of 15 most frequent answers.\n\nFigure 2. Details of questions and answers in the EarthVQA dataset. Each urban image has a set of 42 questions, and each rural image has a set of 29 questions, ensuring a relatively balanced distribution for each question. However, the imbalanced distribution of answers presents challenges when applied to real-world Earth environments.\n\nAnnotation Procedure\n\nEarthVQA currently does not involve ambiguous questions such as geographical orientations. For example, the question Are there any intersections near the school? in Figure 2(a) is answered by judging topology. The recognized Road#1 and Road#2 first form Intersection#5. Similarly, Ground#4 and Building#3 jointly form the scene of School#6. Using the ArcGIS toolbox, the polygon-to-polygon distance between School#6 and Intersection#5 is calculated as 94.8m < 100m, resulting in the answer Yes. Each step in the annotation process has fixed thresholds and conditions.\n\nStatistics for Questions\n\nAs shown in Figure 2(b), urban and rural scenes have both common and unique questions based on city planning demands. The number of questions for urban and rural scenes is balanced, eliminating geographical statistical bias.\n\nBasic questions involve statistics and inferences about a certain type of object, e.g., What is the area of the forest?.\nRelational-based questions require semantic or spatial relational reasoning between different objects.\nComprehensive analysis focuses on more than three types of objects, including a summarization of traffic facilities, water sources around agriculture, and land-use analysis, etc.\nStatistics for Answers\n\nAs shown in Figure 2(c), we selected the top 15 most frequent answers from 166 unique answers in the dataset. Similar to the common VQA datasets, the imbalanced distributions of answers bring more challenges when faced with\nthe actual Earth environment.\n\nSemantic Object Awareness Framework\n\nTo achieve efficient relational reasoning, we design the SOBA framework for complex city scenes. SOBA includes a two-stage training process:\n\nSemantic segmentation network training for generating visual features and pseudo masks.\nHybrid attention training for reasoning and answering.\n\n\nFigure 3. The architecture of SOBA includes (a) deep semantic segmentation for visual features, (b) object-awareness-based hybrid attention, and (c) object-counting enhanced optimization.\n\nSemantic Segmentation for Visual Features\n\nFaced with HSR scenes containing multiple objects, we novelly adopt a segmentation network for refined guidance. For an input image , we utilize the encoder outputs  as the visual features.  denotes feature dimension and  according to common settings. Pseudo semantic output  is also adopted for object awareness.\n\nCompared with the existing Faster-RCNN-based algorithms [Yu et al., 2019; Anderson et al., 2018], which average box features in one vector, the pixel-level visual features preserve the locations and semantic details inside objects. This contributes to the modeling of various compact objects in HSR scenes.\n\nObject Awareness-Based Hybrid Attention\n\nGuided by questions and object masks, object awareness-based hybrid attention reasons visual cues for final answers. As shown in Figure 1, there are three components:\n\nObject-guided attention (OGA)\nVisual self-attention (VSA)\nBidirectional cross-attention (BCA)\nOGA for Object Aggregation.\n\nBecause the segmentation output  contains object details (including categories and boundaries), it is adopted to explicitly enhance visual features. OGA is proposed to dynamically weight  and  from the channel dimension. Using the nearest interpolation,  is resized to the same size as . One-hot encoding followed by a pre-convolutional embedding (3×3 convolution, batch normalization, and ReLU) serializes the object semantics. They are concatenated to obtain object-guided features  as inputs for OGA. The reduction and reverse projections further refine the features dimensionally. After activation, we use the refined features to calibrate subspaces of  from the channel dimension.\n\nVSA for Feature Enhancement.\n\nTo capture long-distance relations between geospatial objects, VSA [Dosovitskiy et al., 2020] hierarchically transforms the refined features. VSA includes  transformer blocks, and each block includes a multi-head self-attention (MSA) and a feed-forward network (FFN). The refined features are reduced by a  convolution and reshaped to generate patches .  denotes token size and  is hidden size.\n\nAt each block , features are transformed into a triplet:\n\nwhere , ,  denote the weights of three linear projections and  is the reduction dimension of each head. The self-attention calculates the similarities between each patch and weights their values:\n\nMSA repeats the attention operation  times in parallel and concatenates outputs. Finally, outputs are fused by a linear projection:\n\nwhere  and  denotes projection weights. MSA models long-distance dependency by calculating the similarities between each geospatial object. FFN consists of two linear transformation layers and a GELU to improve visual representations. The formulation is shown as:\n\nwhere  and  represent the learnable projection parameters, and  denotes the hidden size of FFN.\n\nBCA for Multi-Modal Interaction.\n\nBCA advances the interaction with visual and language features via a bidirectional fusion mechanism. BCA consists of two series of  transformer blocks. The first stage aggregates useful language features to enhance visual features , and the second stage implicitly models object external relations according to keywords, boosting language features . The implementation can be formulated as follows:\n\nFinally, the fused  and  are used for the final analysis. Compared with previous research (Cascante et al., 2022), which only uses one-way cross-attention, the bidirectional attention mechanism hierarchically aggregates multi-modal features by simulating the human process of finding visual cues (Savage et al., 2019). Additionally, we have conducted comparative experiments with alternative cross-attention variants (Table 3 and Table 6).\n\nObject Counting Enhanced Optimization.\n\nVQA tasks include both classification and regression (object counting) questions. However, existing methods regard them as a multi-classification task, which is processed with cross-entropy (CE) loss. CE loss is defined as:\n\nwhere  specifies the one-hot encoded ground truth, and  denotes the predicted probabilities.\n\nTo introduce a difference penalty for the regression task, we add a modulating factor:\n\nHere,  and  represent the predicted and ground truth numbers, respectively.  and  are tunable distance awareness factors.  represents the distance penalty . Finally, we design the numerical difference (ND) loss as follows:\n\nND loss unifies classification and regression objectives into one optimization framework. The parameter  controls the overall penalty for regression tasks compared to classification tasks, while  determines the sensitivity of the regression penalty to numerical differences. As  increases, the overall penalty increases, meaning that optimization focuses more on regression tasks. With , the ND loss degenerates into the original CE loss with a constant penalty ( for ). The sensitivity of the regression penalty increases as  increases, and when , the penalty curve changes from concave to convex.\n\nExperiments\n\nEvaluation Metrics. Following common settings (Yu et al., 2019), we adopt the classification accuracy and root-mean-square error (RMSE) as evaluation metrics. Especially, RMSE is used to evaluate counting tasks. We use mean Union over Intersection (mIoU) to report semantic segmentation performance. All experiments were performed under the PyTorch framework using one RTX 3090 GPU.\n\nExperimental Settings. For comparison, we selected eight general (SAN (Yang et al., 2016), MAC (Hudson et al., 2018), BUTD (Anderson et al., 2018), BAN (Kim et al., 2018), MCAN (Yu et al., 2019), D-VQA (Wen et al., 2021), BLIP-2 (Li et al., 2023), Instruct-BLIP (Dai et al., 2023)) and two remote sensing (RSVQA (Luca et al., 2020), RSIVQA (Zheng et al., 2021)) VQA methods.\nBecause MCAN, BUTD, BAN, and D-VQA need object guidance, we adopt visual features from Semantic-FPN (Kirillov et al., 2019) fairly. All VQA models were trained for 40k steps with a batch size of 16. We set the two-layer LSTM with the hidden size of 384 and ResNet50 as default. As for large vision-language models, BLIP-2 and Instruct-BLIP trained Q-Former following their original settings. The vision encoder adopts ViT-g/14, and the language decoder is FlanT5XL. Following (Wang et al., 2021), Semantic-FPN was trained for 15k steps using the same batch size, generating visual features and semantic masks. Segmentation augmentations include random flipping, rotation, scale jittering, and cropping for  patches. We used the Adam solver with  and . The initial learning rate was set to , and a `poly' schedule with a power of 0.9 was applied. The hidden size of the language and image features was . The number of heads  was set to 8, and the numbers of layers in self- and cross-attention modules were . We set  and  for ND loss.\n\nComparative Experiments\nMain Comparative Results\n\nThanks to the diverse questions, EarthVQA can measure multiple perspectives of VQA models. Table 1 shows that all methods achieve high accuracies on basic judging questions. The models with pixel-level visual features obtain higher accuracies, especially for the counting tasks. This is because the semantic locations provide more spatial details, which benefit the object statistics. Compared with advanced methods, SOBA achieves the best overall performance with similar or lower complexity.\n\nTable 1. Compared results with other VQA methods on EarthVQA test set.\n\nMethod\tPromp.\tBas Ju\tRel Ju\tBas Co\tRel Co\tObj An\tCom An\t↑OA(%)\t↓RMSE (Bas Co)\t↓RMSE (Rel Co)\t↓OR\tParam.\nSAN\t✗\t87.59\t81.79\t76.26\t59.23\t55.00\t43.25\t75.66\t1.136\t1.318\t1.160\t32.30\nMAC\t✗\t82.89\t79.46\t72.53\t55.86\t46.32\t40.50\t71.98\t1.407\t1.337\t1.398\t38.64\nBUTD\t✓\t90.01\t82.02\t77.16\t60.95\t56.29\t42.29\t76.49\t0.890\t1.292\t0.950\t34.95\nBAN\t✓\t89.81\t81.87\t77.58\t63.71\t55.67\t45.06\t76.74\t0.819\t1.241\t0.883\t58.73\nMCAN\t✓\t89.65\t81.65\t79.83\t63.16\t57.28\t43.71\t77.07\t0.816\t1.230\t0.879\t55.17\nD-VQA\t✓\t89.73\t82.12\t77.38\t63.99\t55.14\t43.20\t76.59\t0.916\t1.238\t0.962\t37.79\nBLIP-2\t✗\t88.13\t81.92\t70.26\t58.58\t42.72\t28.34\t71.07\t1.879\t1.320\t1.818\t≈4B\nInstruct-BLIP\t✗\t89.67\t79.69\t76.96\t63.34\t59.72\t45.68\t75.25\t0.799\t1.217\t0.862\t≈4B\nRSVQA\t✗\t82.43\t79.34\t70.68\t55.53\t42.45\t35.46\t70.70\t1.733\t1.359\t1.691\t30.21\nRSIVQA\t✗\t85.32\t80.44\t75.01\t56.63\t51.55\t39.25\t73.71\t1.718\t1.346\t1.676\t41.41\nSOBA (ours)\t✓\t89.63\t82.64\t80.17\t67.86\t61.40\t49.30\t78.14\t0.785\t1.145\t0.839\t40.46\nObject Guided Attention\n\nOGA introduces object semantics into visual features, and we compare it with related variants. Table 2 shows comparative results for spatial, channel, and combined attentions (e.g., SA (Woo et al., 2018), SCSE (Roy et al., 2018), CBAM (Woo et al., 2018), SE (Hu et al., 2018) GC (Cao et al., 2019)). Channel attentions bring more stable improvements than spatial attentions. Because pseudo masks and visual features are concatenated dimensionally, spatial attentions are hard to calibrate the subspaces of visual features and object masks. Channel attentions enhance key object semantics and weaken uninterested background features. Hence, our OGA abandoned spatial attention and achieved the best accuracies.\n\nTable 2. Compared results with other attention mechanisms. C and S denote channel and spatial attention.\n\nObject Guidance\tAtt. Type\t↑OA(%)\t↓OR\nOnly Concat\t-\t77.61\t0.856\n+SA\tS\t77.72\t0.861\n+SCSE\tC&S\t77.89\t0.854\n+CBAM\tC&S\t77.95\t0.857\n+SE\tC\t78.02\t0.853\n+GC\tC\t78.03\t0.847\n+OGA (ours)\tC\t78.14\t0.839\nOne-Way vs. Bidirectional Cross-Attention.\n\nExisting transformer-based methods (Yu et al., 2019; Cascante et al., 2022) utilize one-way (vanilla) attention to perform interactions, where visual features are only treated as queries. In contrast, we further gather enhanced visual features via the keywords (language features as queries), simulating the human process of finding visual cues. As cross-attention consists of six transformer blocks, we compare the different combinations. Table 3 shows that in one-way attention, querying visual features outperforms querying the language features. This is because visual features are more informative, and their enhancement brings more improvements. Bidirectional attention outperforms the one-way structure due to more comprehensive interactions.\n\nTable 3. Compared results between one-way (vanilla) and bidirectional cross-attention. V and L denote visual and language features, respectively.\n\nCross-Attention\tQuery\t↑OA(%)\t↓OR\nOne-way (vanilla)\tLLLLLL\t77.11\t0.977\n\tVVVVVV\t77.53\t0.880\nBidirectional\tLLL-VVV\t77.57\t0.867\n\tVVV-LLL\t78.14\t0.839\nModule Analysis\nArchitecture of SOBA\n\nSOBA was disassembled into five sub-modules: 1. VSA 2. BCA 3. Semantic features 4. OGA 5. ND loss.\nTable 4 shows that each module enhances the overall performance in distinct ways. BCA produces a more significant improvement than VSA, and they complement each other (jointly obtaining OA = 74.91%). OGA further improves the OA by explicitly adding the objectness semantics. ND loss significantly boosts the counting performance from the aspect of optimization. All modules are compatible with each other within the SOBA framework.\n\nTable 4. Architecture ablation study.\n\nVSA\tBCA\tPromp.\tOGA\tND\t↑OA (%)\t↓OR\n✓\t\t\t\t\t72.55\t1.509\n\t✓\t\t\t\t73.78\t1.520\n✓\t✓\t\t\t\t74.91\t1.128\n✓\t✓\t✓\t\t\t77.30\t0.866\n✓\t✓\t✓\t✓\t\t77.54\t0.859\n✓\t✓\t✓\t✓\t✓\t78.14\t0.839\nEncoder Variants\n\nTable 5 shows the effects brought by segmentation networks with advanced CNN and Transformer encoders, such as HRNet (Wang et al., 2020), Swin Transformer (Liu et al., 2021), Mix Transformer (Xie et al., 2021), and ConvNeXt (Liu et al., 2022). SOBA is compatible with the mainstream encoders, and VQA performance is stable at a high level (OA > 77.22%). Although MiT-B3 achieves lower segmentation accuracies than HR-W40, their features provide similar VQA performances. For similar segmentation architectures, larger encoders (Swin-S and ConvX-S) outperform smaller encoders (Swin-T and ConvX-T) in both segmentation and VQA tasks. With Wikipedia's external knowledge, pretrained BERT-Base [Kenton et al., 2019] brings stable improvements. With abundant computing power and time, larger encoders are recommended.\n\nTable 5. Encoder variants analysis.\n\nImg Enc\tLan Enc\t↑mIoU(%)\t↑OA(%)\nHR-W40\tLSTM\t57.31\t77.92\nMiT-B3\tLSTM\t56.44\t77.43\nSwin-T\tLSTM\t56.89\t77.22\nSwin-S\tLSTM\t57.44\t78.01\nConvX-T\tLSTM\t57.17\t78.24\nConvX-S\tLSTM\t57.34\t78.43\nSwin-T\tBERT-Base\t56.89\t77.63\nSwin-S\tBERT-Base\t57.44\t78.23\nConvX-S\tBERT-Base\t57.34\t78.65\nBidirectional Cross-Attention Variants\n\nWe explored BCA variants with different orders of query, where V (visual features) and L (language features) were processed alternately, in cascade, and in parallel. Table 6 shows that the cascade structure VVV-LLL achieves the best accuracies. VVV hierarchically aggregates language features to enhance visual features, and LLL compresses the visual features to supplement language features. Compared with LLL, first considering VVV retains the most information. Hence, VVV-LLL represents the integration process from details to the whole, which conforms to human perception (Savage et al., 2019). The parallel structure obtains a sub-optimal accuracy, and frequent alternation of cross-attentions may lead to feature confusion.\n\nTable 6. BCA Variants\n\nQuery\t↑OA(%)\nLV-LV-LV\t77.51\nVL-VL-VL\t77.58\nLLL-VVV\t77.57\nVVV-LLL\t78.14\nParallel\t77.98\n\tTable 7. Optimization Analysis\n\nOptim.\t↑OA(%)\nCE\t77.54\nDIW\t77.38\nFocal\t77.57\nOHEM\t77.85\nSOM\t77.58\nND\t78.14\nOptimization Analysis\n\nWe compare ND loss with similar optimization algorithms designed to address the sample imbalance problem, including: 1. Dynamic inverse weighting (DIW) (Rajpurkar et al., 2017), 2. Focal loss (Lin et al., 2017), 3. Online hard example mining (OHEM) (Shrivastava et al., 2016), 4. Small object mining (SOM) (FactSeg, 2020). In Table 7, Focal loss obtains better performance by adaptively balancing weights of easy and hard examples. DIW failed to exceed CE due to its extreme weighting strategies. OHEM dynamically focuses on hard samples during the training, slightly improving OA (+0.31%).\n\n\nFigure 4. Experimental results with varied  and  for ND loss. The optimal values range from 0.125 to 1.25 with wide ranges of hyperparameters selection. The mean values and standard deviations are reported after five runs.\n\nThese optimization algorithms only address sample imbalances but are not sensitive to numerical distances, inherently limiting their contribution to regression tasks. In contrast, ND loss demonstrates excellent performance on both classification and regression tasks.\n\nHyperparameter Analysis for ND Loss\n\nAs ND loss introduces two hyperparameters: : Controls the overall penalty, : Determines sensitivity to numerical differences. To evaluate their effects, we vary  and  individually from 0 to 2, and the results are shown in Figure 4 . Compared with CE loss, the additional difference penalty in ND loss provides stable gains:\n\nThe suitable range for  is 0.125 to 1.25, with the highest OA achieved at . For , performance drops due to instability during training.\nWith  fixed at 1, the optional range for  is also 0.125 to 1.25, where OA fluctuates between 77.99% and 78.14%.\nWhen , the influence curve changes from concave to convex, increasing the difference penalty significantly.\n\nThe model performance is not highly sensitive to these hyperparameters, reflecting high fault tolerance and robustness. Overall, ND loss outperforms the CE baseline with a wide range of hyperparameter selection.\n\n\n(a) Classification loss (γ=0.5)\t\n\n(b) Regression loss (γ=0.5)\n\n\n(c) Classification loss (α=1.0)\t\n\n(d) Regression loss (α=1.0)\n\nFigure 5. The training losses of classification and regression tasks with different  and . The changes in  and  primarily affect the regression task optimization.\n\nND loss comprises two components: 1. The original classification loss. 2. An enhanced regression loss. Figure 5 illustrates the effects of varying  and  on these two types of loss: Changes have little impact on classification optimization since the difference penalty is only applied to the regression loss. As the values of  and  increase, the regression losses become larger and more unstable. However, as training progresses: The regression losses gradually stabilize and eventually converge. These two parameters control the numerical difference penalty in different ways. This decomposition analysis of training loss can provide valuable references for tuning  and  to balance stability and optimization effectiveness.\n\nResults\n\n(a) How many `intersections' are in this scene?\n\n\n(b) What are the needs for the renovation of `residents'?\n\n\n(c) What are the `water' types in this scene?\n\nFigure 6. Visualization of attention maps in BCA with language features as queries. From left to right are the , , and  layers. Three examples are queried by different keywords: intersections', residents', and `water'.\n\nTo analyze the mechanism of multi-modal feature interaction, we visualize the attention maps in each layer of BCA according to different queries.\n\nExample 1: Intersection Query\nThe question in Figure 3(a) is \"How many intersections are in this scene?\", where intersections is selected as the query word.\n\nEarly layers: The first attention map shows some incorrect activations on scattered roads and playground tracks.\nDeeper layers: As the layer deepens, BCA successfully reasons the correct spatial relation for the key roads, and the attention map focuses on the intersection in the upper-left corner.\n\nExample 2: Residential Area Query\nFigure 3(b) shows another example with the query word residential.\n\nThe visualization displays the process of gradually attending to the residential area as layers progress.\n\nExample 3: Water Query in Rural Scene\nThe third example in Figure 3(c) shows a rural scene with the query word water.\n\nEarly layers: The attention map initially focuses on trees and waters due to their similar spectral values.\nLater layers: The correct water regions are enhanced, while uninterested trees are filtered out.\nConclusion\n\nTo go beyond information extraction, we introduce VQA into remote sensing scene understanding, enabling relational reasoning. Based on city planning needs, we designed a multi-modal and multi-task VQA dataset named EarthVQA. In addition, we proposed a two-stage semantic object awareness framework (SOBA) to enhance VQA tasks effectively. Extensive experiments demonstrated the superiority of SOBA. Future work will focus on exploring the interactions between segmentation and VQA tasks.\n\nReferences\n\n[1] Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 6077–6086.\n\n[2] Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.; Zitnick, C. L.; and Parikh, D. 2015. VQA: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision, 2425–2433.\n\n[3] Bai, X.; Shi, P.; and Liu, Y. 2014. Society: Realizing China’s urban dream. Nature, 509(7499): 158–160.\n\n[4] Krishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.; Kravitz, J.; Chen, S.; Kalantidis, Y.; Li, L.-J.; Shamma, D. A.; et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1): 32–73.\n\n[5] Li, F.-F.; and Krishna, R. 2022. Searching for computer vision north stars. Daedalus, 151(2): 85–99.\n\n[6] Li, J.; Li, D.; Savarese, S.; and Hoi, S. C. H. 2023. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In Proceedings of the International Conference on Machine Learning (ICML 2023), 19730–19742.\n\n[7] Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; and Dollár, P. 2017. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, 2980–2988.\n\n[8] Liu, Y.; Zhong, Y.; Ma, A.; Zhao, J.; and Zhang, L. 2023. Cross-resolution national-scale land-cover mapping based on noisy label learning: A case study of China. International Journal of Applied Earth Observation and Geoinformation, 118: 103265.\n\n[9] Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 10012–10022.\n\n[10] Liu, Z.; Mao, H.; Wu, C.-Y.; Feichtenhofer, C.; Darrell, T.; and Xie, S. 2022. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 11976–11986.\n\n[11] Lobry, S.; Marcos, D.; Murray, J.; and Tuia, D. 2020. RSVQA: Visual Question Answering for Remote Sensing Data. IEEE Transactions on Geoscience and Remote Sensing, 58(12): 8555–8566.\n\n[12] Ma, A.; Wang, J.; Zhong, Y.; and Zheng, Z. 2022. FactSeg: Foreground Activation-Driven Small Object Semantic Segmentation in Large-Scale Remote Sensing Imagery. IEEE Transactions on Geoscience and Remote Sensing, 60: 1–16.\n\n[13] Marino, K.; Chen, X.; Parikh, D.; Gupta, A.; and Rohrbach, M. 2021. Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based VQA. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14111–14121.\n\n[14] Rahnemoonfar, M.; Chowdhury, T.; Sarkar, A.; Varshney, D.; Yari, M.; and Murphy, R. R. 2021. FloodNet: A high-resolution aerial imagery dataset for post flood scene understanding. IEEE Access, 9: 89644–89654.\n\n[15] Rajpurkar, P.; Irvin, J.; Zhu, K.; Yang, B.; Mehta, H.; Duan, T.; Ding, D.; Bagul, A.; Langlotz, C.; Shpanskaya, K.; et al. 2017. ChexNet: Radiologist-level pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225.\n\n[16] Cao, Y.; Xu, J.; Lin, S.; Wei, F.; and Hu, H. 2019. GCNet: Non-local networks meet squeeze-excitation networks and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 0–0.\n\n[17] Cascante-Bonilla, P.; Wu, H.; Wang, L.; Feris, R. S.; and Ordonez, V. 2022. SimVQA: Exploring simulated environments for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5056–5066.\n\n[18] Dai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.; Li, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. arXiv:2305.06500.\n\n[19] Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations.\n\n[20] Hu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7132–7141.\n\n[21] Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster RCNN: Towards real-time object detection with region proposal networks. Advances in Neural Information Processing Systems, 28.\n\n[22] Roy, A. G.; Navab, N.; and Wachinger, C. 2018. Recalibrating fully convolutional networks with spatial and channel “squeeze and excitation” blocks. IEEE Transactions on Medical Imaging, 38(2): 540–549.\n\n[23] Savage, N. 2019. How AI and neuroscience drive each other forwards. Nature, 571(7766): S15–S15.\n\n[24] Shi, S.; Zhong, Y.; Liu, Y.; Wang, J.; Wan, Y.; Zhao, J.; Lv, P.; Zhang, L.; and Li, D. 2023. Multi-temporal urban semantic understanding based on GF-2 remote sensing imagery: From tri-temporal datasets to multi-task mapping. International Journal of Digital Earth, 16(1): 3321–3347.\n\n[25] Shrivastava, A.; Gupta, A.; and Girshick, R. 2016. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 761–769.\n\n[26] Wang, J.; Sun, K.; Cheng, T.; Jiang, B.; Deng, C.; Zhao, Y.; Liu, D.; Mu, Y.; Tan, M.; Wang, X.; et al. 2020. Deep high-resolution representation learning for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n[27] Wang, J.; Zheng, Z.; Ma, A.; Lu, X.; and Zhong, Y. 2021. LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1.\n\n[28] Wen, Z.; Xu, G.; Tan, M.; Wu, Q.; and Wu, Q. 2021. Debiased visual question answering from feature and sample perspectives. Advances in Neural Information Processing Systems, 34: 3784–3796.\n\n[29] Woo, S.; Park, J.; Lee, J.-Y.; and Kweon, I. S. 2018. CBAM: Convolutional block attention module. In Proceedings of the European Conference on Computer Vision (ECCV), 3–19.\n\n[30] Xia, G.-S.; Hu, J.; Hu, F.; Shi, B.; Bai, X.; Zhong, Y.; Zhang, L.; and Lu, X. 2017. AID: A benchmark data set for performance evaluation of aerial scene classification. IEEE Transactions on Geoscience and Remote Sensing, 55(7): 3965–3981.\n\n[31] Xiao, Y.; Yuan, Q.; Jiang, K.; He, J.; Wang, Y.; and Zhang, L. 2023. From degrade to upgrade: Learning a self-supervised degradation guided adaptive network for blind remote sensing image super-resolution. Information Fusion, 96: 297–311.\n\n[32] Xie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.; and Luo, P. 2021. SegFormer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34: 12077–12090.\n\n[33] Yang, Z.; He, X.; Gao, J.; Deng, L.; and Smola, A. 2016. Stacked attention networks for image question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 21–29.\n\n[34] Yu, Z.; Yu, J.; Cui, Y.; Tao, D.; and Tian, Q. 2019. Deep modular co-attention networks for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6281–6290.\n\n[35] Yuan, Z.; Mou, L.; Xiong, Z.; and Zhu, X. X. 2022. Change detection meets visual question answering. IEEE Transactions on Geoscience and Remote Sensing, 60: 1–13.\n\n[36] Zhang, Y.; Yuan, Y.; Feng, Y.; and Lu, X. 2019. Hierarchical and Robust Convolutional Neural Network for Very High-Resolution Remote Sensing Object Detection. IEEE Transactions on Geoscience and Remote Sensing, 57(8): 5535–5548.\n\n[37] Zhao, H.; Zhong, Y.; Wang, X.; Hu, X.; Luo, C.; Boitt, M.; Piiroinen, R.; Zhang, L.; Heiskanen, J.; and Pellikka, P. 2022. Mapping the distribution of invasive tree species using deep one-class classification in the tropical montane landscape of Kenya. ISPRS Journal of Photogrammetry and Remote Sensing, 187: 328–344.\n\n[38] Zheng, X.; Wang, B.; Du, X.; and Lu, X. 2021. Mutual attention inception network for remote sensing visual question answering. IEEE Transactions on Geoscience and Remote Sensing, 60: 1–14.\n\n[39] Zvonkov, I.; Tseng, G.; Nakalembe, C.; and Kerner, H. 2023. OpenMapFlow: A Library for Rapid Map Creation with Machine Learning and Remote Sensing Data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 14655–14663.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nRelated Work\n\nGeneral Visual Question Answering\n\nRemote Sensing Visual Question Answering\n\nEarthVQA Dataset\n\nAnnotation Procedure\n\nStatistics for Questions\n\nStatistics for Answers\n\nSemantic Object Awareness Framework\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nCode\nDatasets\nFiles\nEarthVQA-AAAI2024.pdf\nalpha_gamma.pdf\nYou might be interested\nWild-scene-image-segmentation\nF\nDec 23, 202418 reads\nHigh-Resolution Remote Sensing Image Captioning Based on Structured Attention and SAM Network\nI\nS\nDec 19, 202428 reads\nImage captioningimage segmentation+2\nBuilding CLIP from Scratch: A Tutorial on Multi-Modal Learning\nOct 22, 202465 reads\nCLIPComputer Vision+6\nAdvanced Deep Learning Project: Urban Scene Segmentation with Fast-SCNN\nMay 23, 20254 reads\nacademic-projectcamvid-dataset+5",
    "awards": [
      "most innovative project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "uniquestai-your-ultimate-ai-companion-to-discover-university-admissions-worldwide-P2Xnw8MhQZPV",
    "username": "aAmina Javaid",
    "license": "MIT License",
    "title": "UniQuestAI - Your Ultimate AI Companion to Discover University Admissions Worldwide",
    "publication_description": "Back to publications\nMar 29, 2025\n●\n46 reads\n●\nMIT License\nWinner of\nOutstanding Solution Implementation\nat the\nAgentic AI Innovation Challenge 2025\nUniQuestAI - Your Ultimate AI Companion to Discover University Admissions Worldwide\nAgenticAI\nAgents\nAISolution\nArtificialIntelligence\nHigherEducation\nLLMs\nTools\nA\nAmina Javaid\nLike\nBookmark\nShare\n\nAbstract\n\nStudying abroad is a dream of many individuals all around the world. It provides exposure, enhances one's personality, and fosters cultural interaction, opening doors to new experiences and academic excellence. It is a great opportunity to learn from people with diverse backgrounds. But where to start from? Finding and selecting the right university is not an easy task. Many students struggle with university admissions research, specifically while searching for universities abroad. They face multiple challenges, from navigating different university ranking websites to manually extracting admission details from official university pages while exploring their programs of interest. UniQuestAI utilizes the power of Agentic AI to automate and simplify the university search process tailored to each student's preferences. Not just finding universities, it brings and compiles all the essential information required for admissions and applications to those universities. It addresses a real-world problem with Agentic AI, transforming the process of university admissions research, making it faster, more efficient, and accessible for students worldwide.\n\nIntroduction\n\nThe world has become a global village, thanks to the widespread availability and connectivity of the internet. Access to information has made it possible for anyone, anywhere, to explore educational opportunities across the globe. With hundreds of thousands of universities offering diverse programs, students have endless possibilities to pursue their academic goals. While university websites provide detailed information about their programs, navigating through them to find relevant details is a daunting task.\nUniversity ranking websites, such as US News Rankings, QS Rankings, and Times Higher Education Rankings, help narrow down choices based on university reputation and subject rankings. However, even after shortlisting universities, students must manually visit each university's official website to gather crucial admission-related details - ranging from program requirements and application deadlines to standardized test scores and tuition fees. This process can take weeks or even months, making university admissions research overwhelming and time-consuming.\nThis is where UniQuestAI comes in. It is an AI-powered multi-agent system designed to automate and streamline the entire university search and admissions research process. By intelligently fetching and compiling accurate, up-to-date admission details, UniQuestAI saves students countless hours of manual work - allowing them to focus on what truly matters - preparing strong applications and securing their dream university.\n\nTechnologies Used\nStreamlit - Frontend: Streamlit is an open-source Python library to build minimalist user interfaces for web applications. It helps build interactive data apps quickly.\nFastAPI - Backend: FastAPI is a modern, high performance, web framework for building APIs with Python. \nAgno - Agentic AI Framework: Agno is an open-source framework to build high performance multimodal agentic systems with memory, knowledge, and tools.\nUser Inputs\nDiscipline (Field of Study)\nEducation Level (Bachelors | Masters | PhD)\nLocation (Country, Default: Worldwide)\nMaximum Number of Universities\nOutput\n\nUniversities information organized in an Excel format, including:\n\nGeneral Information\nUniversity Name\nLocation (City, Country)\nStatus (Public | Private)\nFounded (Year)\nRankings\nUS News Ranking\nQS Ranking\nTimes Higher Education Ranking\nProgram Details\nProgram (Bachelors, Masters, PhD)\nProgram Begins (Month, Year)\nGRE Requirement\nMinimum TOEFL Score\nMinimum IELTS score\nDuolingo Accepted? (Yes/No)\nEnglish Proficiency Waiver Availability\nApplication & Fees\nApplication Fee ($)\nApplication Fee Waiver Availability\nTuition | Stipend Details\nImportant Dates\nApplication Opens\nApplication Deadline\nAdditional Information\nOffice Contact and Email (Graduate/Undergraduate Admissions)\nAcceptance Rate\nRequired Documents\nNumber of Letters of Recommendation (LoRs)\nAdditional Notes\nProgram Link\n\nAll the above information is essential for the applicants to effectively prepare and submit their applications for different universities.\n\nSystem Design\n\nUniQuestAI is a multi-agent collaborative system where four specialized agents work together in a structured, agentic workflow. Each agent is designed with a specific goal, ensuring it operates in a focused and efficient manner without unnecessary distractions. The workflow is sequential, meaning the output of one agent serves as the input for the next, enabling a smooth and systematic execution of tasks.\nHere is a high level architectural diagram of this multi-agent collaborative system:\n\nAgents\n1. University Search Agent\n\nThis agent searches a list of top universities based on their rankings from the world university ranking websites (US News Ranking, QS Ranking, and Times Higher Education Ranking) based on the following four user preferences:\n\nDiscipline\nEducation level\nLocation\nMaximum universities\nLLM\n\nIt uses Llama 3.3 70B Instruct Turbo Free from Together API as a large language model.\n\nTools\n\nIt uses GoogleSearchTools in the Agno Toolkit to search for the top universities from ranking websites.\n\n2. Program Search Agent\n\nThis agent searches for the links to admission requirements and program information pages on official university websites for the universities shortlisted by the University Search Agent. It makes sure that the links are authentic, latest, and are according to the user preferences.\n\nLLM\n\nIt uses Llama 3.3 70B Instruct Turbo Free from Together API as a large language model.\n\nTools\n\nIt uses GoogleSearchTools in the Agno Toolkit to search for the required links from official university websites.\n\n3. Information Extraction Agent\n\nThis agent searches and extracts real time information from the admissions and program specific page links found for each university by the Program Search Agent.\n\nLLM\n\nIt uses Llama 3.3 70B Instruct Turbo Free from Together API as a large language model.\n\nTools\n\nIt uses WebsiteTools from the Agno Toolkit to crawl and extract information from the admission requirements and program information pages on an official university website.\n\n4. Information Processing Agent\n\nThis agent organizes and processes the information extracted by the Information Extraction Agent, and converts it into JSON format for displaying at the frontend and saving into an Excel file.\n\nLLM\n\nIt uses Llama 3.3 70B Instruct Turbo Free from Together API as a large language model.\n\nImplementation\n\nWe'll use the Agno Framework to build this multi-agent collaborative system. The project structure is as follows:\n\nProject Structure\nuniquest-ai/                    # Main project folder\n│── frontend/                    # Frontend directory\n│   ├── app.py                   # Streamlit application\n│\n│── backend/                     # Backend directory\n│   ├── main.py                   # FastAPI backend\n│   ├── utils.py                  # Utility functions\n│\n│   ├── agents/                   # Agents folder\n│   │   ├── university_search_agent.py\n│   │   ├── program_search_agent.py\n│   │   ├── information_extraction_agent.py\n│   │   ├── information_processing_agent.py\n│   │\n│   ├── workflows/                # Workflows folder\n│   │   ├── uniquest_workflow.py\n│\n│── requirements.txt              # Dependencies\n│── .env                          # Environment variables\n│── LICENSE                       # License file\n│── README.md\nSetting up the environment\n\nCreate a new virtual environment and install the dependencies using requirements.txt file.\n\nrequirements.txt\nfastapi\nuvicorn\nstreamlit\npandas\nopenpyxl\nagno\nrequests\nbeautifulsoup4\ngooglesearch-python\npycountry\nsqlalchemy\n.env\n\nCreate a .env file and add your API key.\n\nTOGETHER_API_KEY=\"your_api_key_here\"\nFRONTEND\n\nThe frontend of UniQuestAI looks like this:\n\napp.py\nThis is the main interface of the application built using Streamlit. It provides the user input fields in the sidebar at the left. The list of top universities would be displayed at the right in a tabular format along with their detailed information as shown below.\n\nimport streamlit as st\nimport requests\nimport json\nimport pandas as pd\nimport os\n\nAPI_URL = \"http://127.0.0.1:8000\"\n\nst.set_page_config(page_title=\"UniQuestAI\", layout=\"wide\")\n\n# Inject custom CSS to set the width of the sidebar\nst.markdown(\n    \"\"\"\n    <style>\n        section[data-testid=\"stSidebar\"] {\n            width: 500px !important;\n        }\n    </style>\n    \"\"\",\n    unsafe_allow_html=True,\n)\n\n# Initialize session state for results\nif \"result_json\" not in st.session_state:\n    st.session_state.result_json = None\n\n# Initialize session state for research topic\nif \"discipline\" not in st.session_state:\n    st.session_state.discipline = \"\"\n\n# Initialize session state for max papers\nif \"education_level\" not in st.session_state:\n    st.session_state.education_level = \"\"\n\n# Initialize session state for selected paper\nif \"location\" not in st.session_state:\n    st.session_state.location = \"Worldwide\"\n\n# Initialize session state for chat history\nif \"max_universities\" not in st.session_state:\n    st.session_state.max_universities = 10\n\n# Create a two-column layout\ncol1, col2 =st.columns([1, 11])\nwith col1:\n    st.image(\"../images/uniquest_logo.png\", width=150)\nwith col2:\n    st.title(\"UniQuestAI\")\n\n# Display the main title and search form in the sidebar  \nwith st.sidebar:\n    st.subheader(\"Search the best unviversities\")\n    st.info(\"Enter your preferences to get started.\")\n    \n    # User input fields\n    discipline = st.text_input(\"Enter Discipline:\")\n    education_level = st.selectbox(\"Select Education Level:\", [\"Bachelors\", \"Masters\", \"PhD\"])\n    location = st.text_input(\"Enter Location:\", \"Worldwide\")\n    max_universities = st.number_input(\"Number of Universities:\", 5, 50, 10)\n\n    col_btn1, col_btn2 = st.columns([3, 1])\n\n    with col_btn1:\n        if st.button(\"Search\"):\n            if discipline:\n                st.session_state.discipline = discipline\n                st.session_state.education_level = education_level\n                st.session_state.location = location\n                st.session_state.max_universities = max_universities\n\n                try:\n                    with st.spinner(\"Searching for top universities...\"):\n                        st.warning(\"It may take a few minutes for the search to complete. Please be patient!\")\n                        response = requests.post(API_URL+\"/search/\", json={\"discipline\": discipline, \"education_level\": education_level, \"location\": location, \"max_universities\": max_universities})\n\n                    if response.status_code != 200:\n                        st.error(\"Error: Invalid response from server.\")\n                        st.stop()\n\n                    result_decoded = response.content.decode(\"utf-8\")\n                    st.session_state.result_json = json.loads(result_decoded)  # Store in session state\n\n                except json.JSONDecodeError:\n                    st.error(\"Error: Received an unreadable response from the server.\")\n                    st.stop()\n                except requests.exceptions.RequestException:\n                    st.error(\"Error: Could not connect to the server.\")\n                    st.stop()\n\n    with col_btn2:\n        if st.button(\"🔄 Refresh\"):\n            st.session_state.result_json = None  # Clear stored results\n            st.session_state.discipline = \"\"  # Clear stored discipline\n            st.session_state.education_level = \"\"  # Clear stored education level\n            st.session_state.location = \"Worldwide\"  # Clear stored location\n            st.session_state.max_universities = 10  # Reset max universities\n            st.rerun()  # Force a full page refresh\n            \n# Display results if available\nif st.session_state.result_json:\n    st.title(f\"Top Universities for {discipline} ({education_level}) in {location if location else 'the World'}\")\n    # Display the results\n    # st.write(st.session_state.result_json.get(\"response\", \"No results to display.\"))\n\n    # If an excel path is provided in the response, display download option\n    if \"excel_path\" in st.session_state.result_json:\n        excel_path = st.session_state.result_json[\"excel_path\"]\n        excel_path = os.path.join(\"../backend\", excel_path)\n\n        filename = f\"Top_Universities_{education_level}_{discipline}_{location}.xlsx\"\n\n        # Create a row layout for success message + download button\n        col_success, col_download = st.columns([11, 1])\n\n        with col_success:\n            st.success(\"Top Universities Found!\")\n\n        with col_download:\n            st.download_button(label=\"📥 Download\", \n                            data=open(excel_path, \"rb\").read(), \n                            file_name=filename,\n                            mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n\n    # Display universities information\n    response = st.session_state.result_json.get(\"response\")\n\n    processed_info = response[\"processed_info\"]\n\n    json_info = json.loads(processed_info)\n\n    universities = json_info.get(\"universities\", [])  \n\n    if universities:\n        # Convert the list of university dictionaries into a Pandas DataFrame\n        df = pd.DataFrame(universities)\n        df.rename(columns={\"name\":\"University Name\", \"location\":\"Location\", \"status\":\"Status\", \"founded\":\"Founded\", \n                           \"us_news_ranking\":\"US News Ranking\", \"qs_ranking\":\"QS Ranking\", \"times_ranking\":\"THE Ranking\", \n                           \"program\":\"Program\", \"program_begins\":\"Program Begins\", \"gre_requirement\":\"GRE Requirement\", \n                           \"toefl_score\":\"TOEFL Score\", \"ielts_score\":\"IELTS Score\", \"duolingo_accepted\":\"Duolingo Accepted?\",\n                           \"english_proficiency_waiver\":\"English Proficiency Waiver\", \"application_fee\":\"Application Fee\", \n                           \"application_deadline\":\"Application Deadline\", \"office_contact_and_email\":\"Office Contact & Email\",\n                           \"acceptance_rate\":\"Acceptance Rate\", \"required_documents\":\"Required Documents\",\n                           \"number_of_lors\":\"Number of LORs\", \"additional_notes\":\"Additional Notes\", \"link\":\"Program Link\"}, inplace=True,)\n        st.dataframe(df)\n\n        for i, university in enumerate(universities):\n            st.subheader(f\"🎓 {i+1}. {university['name']}\")\n            st.markdown(f\"**📍 Location:** {university['location']}\")\n            st.markdown(f\"**📚 Status:** {university['status']}\")\n            st.markdown(f\"**📆 Founded:** {university['founded']}\")\n            st.markdown(f\"**📈 US News Ranking:** {university['us_news_ranking']}\")\n            st.markdown(f\"**📈 QS Ranking:** {university['qs_ranking']}\")\n            st.markdown(f\"**📈 Times Higher Education Ranking:** {university['times_ranking']}\")\n            st.markdown(f\"**📚 Program:** {university['program']}\")\n            st.markdown(f\"**📅 Program Begins:** {university['program_begins']}\")\n            st.markdown(f\"**📝 GRE Requirement:** {university['gre_requirement']}\")\n            st.markdown(f\"**📝 TOEFL Score:** {university['toefl_score']}\")\n            st.markdown(f\"**📝 IELTS Score:** {university['ielts_score']}\")\n            st.markdown(f\"**📝 Duolingo Accepted?:** {university['duolingo_accepted']}\")\n            st.markdown(f\"**📝 English Proficiency Waiver:** {university['english_proficiency_waiver']}\")\n            st.markdown(f\"**💸 Application Fee:** {university['application_fee']}\")\n            st.markdown(f\"**💸 Application Fee Waiver:** {university['application_fee_waiver']}\")\n            st.markdown(f\"**💸 Tuition|Stipend:** {university['tuition_stipend']}\")\n            st.markdown(f\"**📅 Application Opens:** {university['application_opens']}\")\n            st.markdown(f\"**📅 Application Deadline:** {university['application_deadline']}\")\n            st.markdown(f\"**📞 Office Contact and Email:** {university['office_contact_and_email']}\")\n            st.markdown(f\"**📝 Acceptance Rate:** {university['acceptance_rate']}\")\n            st.markdown(f\"**📝 Required Documents:** {university['required_documents']}\")\n            st.markdown(f\"**📝 Number of LoRs:** {university['number_of_lors']}\")\n            st.markdown(f\"**📝 Additional Notes:** {university['additional_notes']}\")\n            st.markdown(f\"**📝 Link to the Program:** {university['link']}\")\n            st.divider()             \n    else:\n        st.error(\"No universities found!\")\nelse:\n    st.subheader(\"Your ultimate AI companion for finding the best universities around the world!\")\n    st.success(\"**Welcome to UniQuestAI**, your trusted AI companion for finding the best universities around the world!\")\n    st.markdown(\"- Here, you can search for universities based on your **education level**, **discipline**, and **preferred location**.\")\n    st.markdown(\"- Our AI-powered platform will help you find the top universities that match your preferences and requirements.\")\n    st.markdown(\"- Not only that, we'll compile a list of universities for you, including their names, locations, admission requirements, and other relevant information.\")\n    st.info(\"**So, what are you waiting for? Let's get started and find your dream university today!**\")\n\n    todo = [\"📚 Enter a Discipline\", \"🎓 Select an Education Level\", \"📌 Enter a Location\", \"#️⃣ Enter Number of Universities\", \"🔍 Click Search\"]\n    st.markdown(\"\\n\".join([f\"##### {i+1}. {item}\" for i, item in enumerate(todo)]))\nBACKEND\n\nmain.py\n\nsearch- This endpoint searches for the top universities through Uniquest workflow and saves the response in an Excel file.\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nfrom agno.agent import RunResponse\nfrom workflows.uniquest_workflow import uniquest_workflow\nfrom utils import save_info_to_excel\n\napp = FastAPI()\n\nclass SearchRequest(BaseModel):\n    discipline: str\n    education_level: str\n    location: str\n    max_universities: int = 5\n\n@app.post(\"/search/\")\nasync def search_universities(request: SearchRequest):\n    # Execute the workflow\n    # Returns a RunResponse object containing the generated content\n    response: RunResponse = uniquest_workflow.run(discipline=request.discipline, education_level=request.education_level, location=request.location, max_universities=request.max_universities)\n\n    if response:\n        print(\"Response:\", response.content)\n         \n        # Save the universities info to an Excel file\n        excel_path = save_info_to_excel(response.content[\"processed_info\"], request.discipline, request.education_level, request.location)\n        return {\"message\": \"Top universities found!\", \"response\": response.content, \"excel_path\": excel_path}\n    else:\n        return {\"error\": \"No universities found\"}\nutils.py\n\nThis file contains a helper function to save universities information into an Excel file.\n\nimport json\nimport pandas as pd\nimport os\n\ndef save_info_to_excel(json_data, discipline, education_level, location):\n    \"\"\"\n    Save information about universities to an Excel file.\"\n\n    Args:\n        json_data (dict): A dictionary containing information about universities.\n        discipline (str): The discipline searched for.\n        education_level (str): The education level searched for.\n        location (str): The location searched for.\n\n    Returns:\n        str: The name of the saved Excel file.\n    \"\"\"\n\n    save_dir = \"excel_files\"\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    file_path = os.path.join(save_dir, f\"Top_Universities_{education_level}_{discipline}_{location}.xlsx\")\n\n    # Ensure json_data is a dictionary\n    if isinstance(json_data, str):\n        data = json.loads(json_data)\n    else:\n        data = json_data\n\n    # Convert to a DataFrame\n    df = pd.DataFrame(data[\"universities\"])\n    \n    # Save to an Excel file\n    df.to_excel(file_path, index=False)\n    \n    print(f\"Information saved successfully to {file_path}\")\n    return file_path\nuniversity_search_agent.py\n\nThis is the first agent in the workflow. It searches for the top universities based on user preferences from university ranking websites using GoogleSearchTools.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.tools.googlesearch import GoogleSearchTools\nfrom agno.utils.pprint import pprint_run_response\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the university search agent\nuniversity_search_agent = Agent(\n    name=\"university-search-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\")), \n    tools=[GoogleSearchTools()],\n    description=\n    '''\n        You are a university search agent that will search for the top universities based on a given discipline from top university ranking websites.\n        You will generate a list of universities with their names, rankings, and official website links based on the given preferences.\n    ''',\n    instructions=[\n        \"Search the following university ranking websites one by one based on the given preferences:\",\n        \"Preferences include discipline, education level, location, and maximum number of universities.\",\n            \"US News Rankings: https://www.usnews.com\",\n            \"QS Rankings: https://www.topuniversities.com\",\n            \"Times Higher Education Rankings: https://www.timeshighereducation.com\",\n        \"If the location is not specified, consider all locations.\",\n        \"Search for the best universities for the given discipline and education level from each ranking website.\",\n        \"The rankings MUST be according to the latest information available on the ranking websites\",\n        \"Select the top ranked universities from the extracted information from each ranking website.\",\n        \"The response MUST be in the following JSON format:\",\n            \"{\",\n                '\"universities\": [',\n                    '\"https://university1.com\",',\n                    '\"https://university2.com\",',\n                    '\"https://university3.com\",',\n                    '\"https://university4.com\",',\n                    '\"https://university5.com\",',\n                \"]\",\n                '\"discipline\": [DISCIPLINE],',\n                '\"education_level\": [EDUCATION_LEVEL],',\n            \"}\",\n        \"The response MUST NOT include ranking websites as universities.\",\n        \"The response MUST only include top universities found from the ranking websites.\",\n        \"The response MUST be in proper JSON format with keys and values in double quotes.\",\n        \"The final response MUST not include any other text or anything else other than the JSON response.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\nprogram_search_agent.py\n\nThis is the second agent in the workflow which searches for the admission and program page links for each university found by the university search agent using GoogleSearchTools.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.tools.googlesearch import GoogleSearchTools\nfrom agno.utils.pprint import pprint_run_response\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the link browsing agent\nprogram_search_agent = Agent(\n    name=\"program-search-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\")), \n    tools=[GoogleSearchTools()],\n    description=\n    '''\n        You are a program search agent that will search for the admission and program page links for a given list of universities based on the given preferences.\n        You will generate a list of universities with their admission and program page links in JSON format.\n    ''',\n    instructions=[\n        \"Search the given university websites one by one based on the given preferences\",\n        \"For each university, search for the admission page link based on the given education level.\",\n        \"For each university, search for the program page link based on the given discipline and education level.\",\n        \"Do NOT generate links. Instead, search them directly from the official university website.\",\n        \"The links MUST depict the latest admissions and program information available on the university website.\",\n        \"Do NOT retrieve links having old or outdated admissions and program information.\",\n        \"If you don't find a specific program in a university, search for the most similar or closest related program page link.\",\n        \"Validate the links by ensuring they do NOT return 404 errors or redirects to unrelated pages.\",\n        \"Double-check that the links are correct and functional.\",\n        \"The response MUST be in the following JSON format:\",\n        \"{\"\n        \"    'links': [\",\n        \"       {\",\n        \"           'university': '[Official website link]',\",\n        \"           'admission': '[Admission page link]',\",\n        \"           'program': '[Program page link]'\",\n        \"       }\",\n        \"       {...}\",\n        \"    ]\",\n        \"}\",\n        \"The response MUST be in proper JSON format with keys and values in double quotes.\",\n        \"The final response MUST not include any other text or anything else other than the JSON response.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\ninformation_extraction_agent.py\n\nThis is the third agent in the Uniquest workflow which works in a loop. The workflow calls this agent iteratively to extract information for each link found through the Program Search Agent. This agent uses WebsiteTools to extract the content from each link.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.tools.website import WebsiteTools\nfrom agno.utils.pprint import pprint_run_response\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the information extraction agent\ninfo_extraction_agent = Agent(\n    name=\"info-extraction-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\"), max_tokens=500), \n    tools=[WebsiteTools()],\n    description=\n    '''\n        You are an information extraction agent that will search and extract information from a given link.\n    ''',\n    instructions=[\n        \"Search the given link for admissions and program related information:\",\n        \"Find whichever of the below information is available in the given link.\",\n            \"University Name: [Name of the University]\",\n            \"Location: [City, Country]\",\n            \"Public/Private: [Public/Private]\",\n            \"Founded (Year): [Year of Foundation]\",\n            \"US News Ranking: [Ranking by US News]\",\n            \"QS Ranking: [Ranking by QS]\",\n            \"Times Higher Education Ranking: [Ranking by Times Higher Education]\",\n            \"Program: [Program Name]\",\n            \"Program Begins: [Month and Year]\",\n            \"GRE Requirement: [GRE Required/Not Required]\",\n            \"Min TOEFL Score: [Min TOEFL Score]\",\n            \"Min IELTS Score: [Min IELTS Score]\",\n            \"Duolingo Accepted?: [Yes/No]\",\n            \"English Proficiency Waiver: [Waiver Available/Not Available]\",\n            \"Application Fee: [Application Fee]\",\n            \"Application Fee Waiver: [Waiver Available/Not Available]\",\n            \"Tuition|Stipend: [Tuition|Stipend Available/Not Available]\",\n            \"Application Opens: [Month and Year]\",\n            \"Application Deadline: [Deadline]\",\n            \"Office Contact and Email (Graduate | Undergraduate): [Contact and Email]\",\n            \"Acceptance Rate: [Acceptance Rate]\",\n            \"Required Documents: [List of Required Documents]\",\n            \"Number of LoRs: [Number of LoRs]\",\n            \"Additional Notes (if any): [Notes of any additional important information]\",\n            \"Link to the Program: [Link to the Program - MUST be a correct link to the program page]\",\n        \"Do NOT generate information. Only extract information from the given link.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\ninformation_processing_agent.py\n\nThis is the fourth and the last agent in the workflow which processes the information extracted by the Information Extraction Agent into JSON format to be displayed at frontend and saved as an Excel file for downloading.\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.together import Together\nfrom agno.utils.pprint import pprint_run_response\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# Define the information processing agent\ninfo_processing_agent = Agent(\n    name=\"info-search-agent\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", api_key=os.getenv(\"TOGETHER_API_KEY\")),\n    description=\n    '''\n        You are an information processing agent that structures the information about universities' admissions and programs in JSON format. \n    ''',\n    instructions=[\n        \"Structure the information received about universities' admissions and programs in JSON format.\",\n        \"Important: Your final response MUST be in JSON format with the following structure:\",\n        \"{\"\n            \"universities: [\"\n                \"{\"\n                \"   'name': 'Name of the University',\"\n                \"   'location': 'Location[City, Country] of the University',\"\n                \"   'status': 'Public or Private',\"\n                \"   'founded': 'Year of Foundation',\"\n                \"   'us_news_ranking': 'Ranking by US News',\"\n                \"   'qs_ranking': 'Ranking by QS',\"\n                \"   'times_ranking': 'Ranking by Times Higher Education',\"\n                \"   'program': 'Program Name',\"\n                \"   'program_begins': 'Month and Year',\"\n                \"   'gre_requirement': 'GRE Required/Not Required',\"\n                \"   'toefl_score': 'Min TOEFL Score',\"\n                \"   'ielts_score': 'Min IELTS Score',\"\n                \"   'duolingo_accepted': 'Yes/No',\"\n                \"   'english_proficiency_waiver': 'Waiver Available/Not Available',\"\n                \"   'application_fee': 'Application Fee',\"\n                \"   'application_fee_waiver': 'Waiver Available/Not Available',\"\n                \"   'tuition_stipend': 'Tuition/Stipend Available/Not Available',\"\n                \"   'application_opens': 'Month and Year',\"\n                \"   'application_deadline': 'Deadline',\"\n                \"   'office_contact_and_email': 'Contact and Email',\"\n                \"   'acceptance_rate': 'Acceptance Rate',\"\n                \"   'required_documents': 'List of Required Documents',\"\n                \"   'number_of_lors': 'Number of LoRs',\"\n                \"   'additional_notes': 'Notes of any additional important information',\"\n                \"   'link': 'Link to the Program'\"\n                \"},\"\n                \"{...}\"\n            \"]\"\n        \"}\",\n         \"The response MUST be in proper JSON format with keys and values in double quotes.\",\n        \"The final response MUST not include anything else other than the JSON response.\"\n    ],\n    markdown=True,\n    show_tool_calls=True,\n    debug_mode=True\n)\nuniquest_workflow.py\n\nThis is the main workflow of the agentic AI application. It calls the agents one by one feeding the output of one into the input of the other agent to achieve its end goal.\n\nfrom agno.agent import Agent\nfrom agno.workflow import Workflow, RunResponse, RunEvent\nfrom agno.storage.workflow.sqlite import SqliteWorkflowStorage\nfrom agno.utils.pprint import pprint_run_response\nfrom agno.utils.log import logger\nimport json\n\nfrom agents.university_search_agent import university_search_agent\nfrom agents.program_search_agent import program_search_agent\nfrom agents.information_extraction_agent import info_extraction_agent\nfrom agents.information_processing_agent import info_processing_agent\n\nclass UniquestWorkflow(Workflow):\n    university_search_agent: Agent = university_search_agent\n    program_search_agent: Agent = program_search_agent\n    info_extraction_agent: Agent = info_extraction_agent\n    info_processing_agent: Agent = info_processing_agent\n\n    def run(self, discipline: str, education_level: str, location: str, max_universities: int = 5) -> RunResponse:\n        logger.info(f\"Generating a list of top {max_universities} universities for a {education_level} in {discipline}\")\n\n        # Step 1: Generate a list of top universities for the given education level, discipline and location\n        prompt = f\"Search for {max_universities} top universities for a {education_level} in {discipline} based in {location}.\"\n        top_universities: RunResponse = self.university_search_agent.run(prompt)\n        # If no universities are found, end the workflow\n        if top_universities is None:\n            return RunResponse(\n                event=RunEvent.workflow_completed,\n                content=f\"Sorry, could not find any universities for {education_level} in {discipline}\",\n            )\n       \n        print(\"Top universities:\", top_universities.content)\n\n        # Step 2: Find the admission and program links for each university\n        prompt = f\"Search for the admission and program page links for a {education_level} in {discipline} for the following universities: {top_universities.content}\"\n        links: RunResponse = self.program_search_agent.run(prompt)\n        if links is None:\n            return RunResponse(\n                event=RunEvent.workflow_completed,\n                content=\"Sorry, could not find the admission and program links for the universities.\",\n            )\n        \n        print(\"Links:\", links.content)\n       \n        links_content = json.loads(links.content)\n        links = links_content[\"links\"]\n\n        # Step 3: Extract the information from each admission and program link for each university\n        information = []\n        for link in links:\n            logger.info(f\"Extracting information for {link['university']}\")\n\n            # Initialize with default values\n            admission_info_content = \"No information found\"\n            program_info_content = \"No information found\"\n\n            try:\n                prompt = f\"Search for the admission and program related information of {link[\"university\"]} from the following link: {link['admission']}.\"\n                admission_info: RunResponse = self.info_extraction_agent.run(prompt)\n                admission_info_content = admission_info.content\n            except Exception as e:\n                logger.error(f\"Error extracting admission information for {link['university']}: {e}\")\n            \n            try:\n                prompt = f\"Search for the admission and program related information of {link['university']} from the following link: {link['program']}.\"\n                program_info: RunResponse = self.info_extraction_agent.run(prompt)\n                program_info_content = program_info.content\n            except Exception as e:\n                logger.error(f\"Error extracting program information for {link['university']}: {e}\")\n\n            information.append({\"university\":link[\"university\"], \"admission_info\": admission_info_content, \"program_info\": program_info_content})\n\n        print(\"Information:\", information)\n\n        # Step 4: Process the information and structure it in JSON format.\n        prompt = f\"Process the following information: {information}\"\n        processed_info: RunResponse = info_processing_agent.run(prompt)\n        if processed_info is None:\n            return RunResponse(\n                event=RunEvent.workflow_completed,\n                content=\"Sorry, could not process the information.\",\n            )\n\n        print(\"Processed information:\", processed_info.content)\n\n        return RunResponse(\n            event=RunEvent.workflow_completed,\n            content={\"universities\": top_universities.content, \"links\": links, \"information\": information, \"processed_info\": processed_info.content},\n        )\n    \n# Initialize the Uniquest workflow\nuniquest_workflow = UniquestWorkflow(\n        session_id=\"find-top-universities\",\n        storage=SqliteWorkflowStorage(\n            table_name=\"generate_uniquest_workflows\",\n            db_file=\"workflows/db/workflows.db\",\n        ),\n    )\nDEMO - UniQuestAI\n\nHere is a demonstration of using UniQuestAI:\nNote: The whole agentic workflow takes a few minutes to complete, therefore, the demo only shows the output generated after the search has been completed.\n\nRunning UniQuestAI\nClone the repository from GitHub. The link has been provided in the References section.\nNavigate to the backend folder and start the FastAPI server:\ncd backend\nuvicorn main:app --reload\nNavigate to the frontend folder and start the Streamlit application:\ncd frontend\nstreamlit run app.py\n\nAccess the application through the following link.\n\nhttp://localhost:8501\nChallenges Faced\n1. Selecting Top Universities\nChallenge - University ranking websites have strict protocols due to which we cannot directly scrape information from them. Many of these sites actively block automated requests. It is also ethically not correct to scrape information without the consent of these websites. This posed a challenge to extract the top universities ranked in specific disciplines.\nSoluion - We used GoogleSearchTools available in the Agno toolkit. This allowed us to retrieve ranking data from top university ranking websites without violating ethical guidelines.\n2. Finding Correct Programs Links\nChallenge - Scanning entire university websites for admissions and program details is impractical and inefficient. We needed a way to find relevant pages without extracting unnecessary data.\nSolution - Instead of scraping the entire websites, we focused on finding and extracting URLs specifically related to admissions and program details. GoogleSearchTools from the Agno Toolkit helped us efficiently retrieved these targeted links, significantly improving the accuracy and efficiency of data extraction.\n3. Verification of Links\nChallenge - Large language models (LLMs) tend to hallucinate and generate incorrect links.\nSolution -To prevent this, we explicitly instructed the LLMs not to generate links themselves. Additionally, we instructed them to perform link validation checks to ensure that all retrieved URLs are correct and functional.\n4. Token Usage Limit \nChallenge - The API we use has a strict token limit, making it impossible to extract all information simultaneously.\nSolution: To address this, we optimized our system by iteratively processing links - extracting information one link at a time and then aggregating the results before feeding them into the information processing agent. This approach allowed us to stay within the token limits while maintaining efficiency.\n5. Model and Tool Selection\nChallenge - Selection of the right models and tools was critical for this application. We couldn't use paid models because of cost constraints. Free models often come with token limitations. Additionally, extracting structured content from websites required specialized tools.\nSolution - After trying and testing various APIs available in the Agno framework, we selected Together API for LLM-based tasks, as it provided the best balance between performance and token usage. For content extraction, we utilized WebsiteTools from the Agno framework, which effectively handled data retrieval from university websites.\n6. Ensuring Correct Information\nChallenge - LLMs are not always reliable in retrieving factual, real-time data. We needed to minimize errors and prevent hallucinations while extracting university admission details.\nSolution - We implemented Reflection and ReAct patterns to guide the LLMs in verifying and cross-checking extracted data before presenting it. These techniques helped ensure that the information retrieved was accurate, up-to-date, and directly sourced from university websites.\nImportant Considerations\nIn a multi-agent system, each agent should remain focused on a single task to maximize efficiency and avoid unnecessary complexity.\nHuman-in-the-loop is essential - while UniQuestAI streamlines the search process, applicants are strongly advised to visit the official university websites and verify the information from the provided links before making any decisions.\nEffective prompt engineering is crucial for optimizing agent performance. Clearly defining an agent's role, description, and instructions ensures accurate and relevant outputs.\nFuture Work\nCurrently, UniQuestAI retrieves top-ranked universities based on user preferences. Future iterations can introduce more options to further narrow down the searches based on specific needs.\nSupport for mid-tier and lower-ranked universities can be added, allowing a broader range of institutions to be explored.\nFiltering universities by status (Public vs. Private) can be incorporated, giving users more control over their selections.\nConclusion\n\nUniQuestAI is an agentic AI application designed to help students worldwide find top universities based on their preferences. It serves as an excellent starting point for those exploring global academic opportunities. The system provides tailored recommendations for both undergraduate and graduate applicants. It leverages the agentic patterns and workflows to automate the whole process of information extraction and compilation - a task that is otherwise time consuming and tedious. With this system, students can focus on preparing strong university applications and gathering essential documentation instead of spending their precious time searching for requirements.\n\nReferences\nAgentic AI - A Step towards Artificial General Intelligence\nDocumentation: agno\nUniQuestAI - GitHub\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nIntroduction\n\nTechnologies Used\n\nUser Inputs\n\nOutput\n\nGeneral Information\n\nRankings\n\nProgram Details\n\nApplication & Fees\n\nImportant Dates\n\nView all\nComments\nCode\nYou might be interested\nStudy Abroad Assistance AI\nK\nAug 27, 202513 reads\nResuMate: Resume Optimizer\nOutstanding Solution Implementation\nMar 31, 202553 reads\nAgentic AILLM+1\nTechFreshBot - University Enquiry Chatbot for Freshmen\nDec 29, 202410 reads\nFastAPIGenAI+7\nSmart Search for Free Courses on Analytics Vidhya\nN\nFeb 17, 20259 reads\nAICosineSimilarity+9",
    "awards": [
      "outstanding solution implementation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "visionbased-perception-systems-for-autonomous-vehicles-YyyC85nU4bdu",
    "username": "fFatima Saud",
    "license": "MIT License",
    "title": "Vision-Based Perception Systems for Autonomous Vehicles",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n72 reads\n●\nMIT License\nWinner of\nMost Promising Innovation\nat the\nComputer Vision Projects Expo 2024\nVision-Based Perception Systems for Autonomous Vehicles\nautonomous\nAutonomous vehicles\nCARLA\ncomputer vision\nDepth estimation\nGPS\nImage classification\nlane detection\nLiDAR integration\nObject detection\nperception\npytorch\nRobosense\nROS\nsimulator\ntensorflow\nVisual SLAM\nyolov5\nF\nFatima Saud\nA\nAleema Saleem\nA\nAnsharah Mobeen\nI\nInzamam Ul Haq\nF\n@fatima.k02shah\nLike\nBookmark\nShare\nAbstract\n\nVision-based perception systems are the foundation of self-driving vehicle technology, allowing real-time analysis and interpretation of the surrounding environment. Our proposed work investigates a hybrid strategy for integrating lane detection and object identification, with a focus on warning generation for impending collisions. Our algorithm detects objects using deep learning models such as YOLOv5, as well as a custom lane detection technique that includes curvature estimation. We have also incorporated traffic sign and signal recognition along with depth estimation to help you make better decisions. The built system has extensive detection and classification capabilities, paving the way for safer and more reliable autonomous navigation.\n\nObjective\n\nOur research proposes a comprehensive framework that combines lane detection with object recognition and collision warnings, utilizing cutting-edge deep learning models and real-time processing methods. Our aim is to add to the ongoing initiatives aimed at enhancing the safety and capabilities of autonomous vehicles, making them ready for real-world implementation.\n\nIntroduction\n\nRecent developments in sensor technology, computer vision, and artificial intelligence have sparked a lot of interest in autonomous vehicles. These vehicles utilize a variety of sensory systems to comprehend their surroundings and navigate in a safe and efficient manner. In an effort to provide a safe, effective, and dependable transportation system, cameras and computer vision techniques—in particular, vision-based perception—have gained popularity as a means of distinguishing between objects, identifying road restrictions, and perceiving the full picture.\n\nCamera model and calibration\n\nCamera model and callibration are an important aspect of dealing with vision-based perception. With the advancement in cameras and their features, they now serve as primary sensors for the purpose of achieving accurate modelling and calibration, as these elements are crucial for dependable object detection, localization, and scene comprehension.\n\nObject detection and tracking\n\nPerception systems in autonomous vehicles depend on object identification and tracking to enable real-time observation and detection of things such as cars, pedestrians, and barriers. Tracking techniques like SORT, Deep SORT, and Kalman filters allow continuous surveillance across frames, maintaining object IDs and providing temporal context, while models like YOLO, Faster R-CNN, and SSD effectively classify items. This combination enables precise velocity estimation, distance measurement, and forecasting abilities, which are vital for secure navigation. Furthermore, techniques for sensor fusion that integrate cameras, LiDAR, and radar improve the detection and tracking effectiveness, ensuring dependability in dynamic or challenging conditions, including low-light situations or adverse weather.\n\nLane detection\n\nIn autonomous driving, lane detection is essential because it provides vital data for path planning and vehicle positioning. Techniques range from sophisticated deep learning methods like semantic segmentation utilizing convolutional neural networks (CNNs) to more conventional edge detection and Hough transform methods. Even in difficult situations, including occlusions, faded lines, or changing lighting, these models are able to recognize lane markers with accuracy. In order to improve accuracy and enable adaptive lane following and lane departure warning features in actual driving situations, robust lane-detecting systems frequently integrate optical data with sensor inputs such as LiDAR.\n\nDepth estimation\n\nFurthermore, depth estimation aids in assessing the distance to surrounding objects, improving the vehicle’s ability to respond suitably to changing circumstances, and understanding the spatial layout of the environment, enabling accurate distance measurement to surrounding objects. Techniques like stereovision, monocular depth estimation using deep learning, and LiDAR integration are commonly employed to enhance depth perception. This information allows autonomous vehicles to make informed decisions, such as maintaining safe distances and executing obstacle avoidance maneuvers effectively.\n\nThe fusion of these technologies allows for a thorough understanding of the environment, supporting real-time decision-making.\n\nDespite substantial advancements in these fields, challenges persist in merging multiple perception tasks into a single, efficient system that can function reliably in diverse environmental conditions.\n\nMethodology\n\nOur approach integrates vision-based perception tasks for autonomous vehicles, focusing on lane detection, object recognition, and collision warnings.\n\nData Collection\n\nWe collected essential data such as GPS GNSS FIXED corrections, ZED2i stereo vision camera inputs, and IMU pose information, all while driving the electric vehicle.\n\nObject Detection\n\nUsing deep learning models (YOLOv5 and Traffic Sign Detection), we perform real-time object detection for identifying vehicles, pedestrians, and traffic signs, critical for obstacle avoidance and compliance with traffic rules.\n\nCustom model\nA custom YOLOv5 model trained on traffic sign data and a traffic light classification model were loaded for detecting relevant objects and traffic signals and the signal colors (red, yellow, and green). We fine-tuned the parameters, confidence thresholds and IoU thresholds for optimal detection accuracy.\nTraffic Signal Color Detection\nA lightweight convolutional neural network (CNN) is used to classify traffic light colors. The design consists of a convolutional layer followed by ReLU activation, max pooling, and a fully linked layer.\nThe pre-trained weights are loaded, and the model is switched to evaluation mode for real-time inference.\nLane Detection\n\nWe applied lane detection algorithms that not only identify lanes but also estimate their curvature to ensure safe navigation in real-world driving scenarios.\n\nWe defined a region of interest (ROI) as a trapezoidal mask to focus on the lane area.\nPreprocessed frames undergo Canny edge detection and Hough line transform to extract lane boundaries, and then the detected lines are fitted with polynomial equations to calculate lane curvature and estimate vehicle position relative to the lane center. The area in between the detected lane lines was filled using the OpenCV fillPoly( ) function.\n\nDepth Estimation for Distance Measurement\n\nThe depth of detected objects is calculated using median depth values extracted from a depth map. Invalid or missing depth values are handled gracefully to ensure reliability. The bounding box coordinates for detected objects guide depth analysis.\n\nCollision Warning System\n\nThe system calculates the distance to detected objects using depth information, providing warnings when objects come within a predefined safety range, thereby enhancing vehicle safety.\n\nA side panel displays dynamic warnings and vehicle information, including lane curvature, vehicle position, and potential hazards.\n\nWe also subscribed to camera topic on ROS and published proximity warning. The topic published true upon detection of object within the set distance threshold and false if far.\n\nWe combine these perception tasks into a real-time video processing pipeline, overlaying detected information on the video feed and providing visual warnings when necessary.\n\nAdditionally, we also developed pedestrian behavior-based brake and throttle control using computer vision, ROS, a ZED camera, and BLDC controllers. The system applies brakes when objects of specific classes are detected within a specified distance and re-engages throttle when the detected objects move beyond the threshold distance.\n\nSimulation in CARLA\n\nAlong with designing and testing our methods in real time environment, we also worked on CARLA Simulator environment. Below mentioned are the approaches we used in the simulator environment:\n\nObject Detection\n\nTo validate object detection capabilities, we incorporated the object detection algorithm into the CARLA environment, a high-fidelity platform for simulating urban driving environment.\n\nPath Control Using MPC and CARLA Visualization\n\nWe also integrated the CARLA autonomous driving simulator with ROS to enable real-time control of physical vehicle actuators. A Model Predictive Controller (MPC) was implemented in CARLA for precise throttle and brake control, ensuring smooth data communication between CARLA and a microcontroller via ROS. The control outputs, including steering, throttle, and brake, were successfully applied to a physical vehicle’s tires equipped with BLDC motor hall sensors during on-ground testing.The results are visualized in CARLA, demonstrating precise vehicle control and navigation.\n\nVehicle Simulation with Real-Life Prototype Integration\n\nThe simulated vehicle in CARLA is synchronized with a real-life prototype using Arduino and ROS. This setup allows the prototype’s wheels to mimic the steering movements of the virtual car, with throttle and brake actions reflected in real time. The integration ensures seamless interaction between the virtual simulation and the physical prototype, providing a comprehensive validation framework.\n\nReal-time camera feed was visualized via Pygame along with semantic segmentation for detailed environmental data collection.\n\nResults\n\nDespite complex driving settings, our system performs consistently in real-time lane detection, object recognition, and distance calculation. Lane curvature is reliably identified, allowing for smooth navigation, while object detection models (YOLOv5 and Traffic Sign Detection) effectively identify vehicles, pedestrians, and traffic signs. The collision warning system responds quickly to objects that approach too closely, delivering timely alerts. The integrated system successfully overlays pertinent information into the video feed, making it acceptable for use in real-world settings.\n\nThe Robosense LiDAR sensor—serving as the \"eyes\" of our self-driving car—helped generate a detailed 3D map of the environment. Combined with precise GPS localization, we’ve made significant progress in sensing, localization, mapping, and perception.\n\nConclusion\n\nThis work showcases the potential of leveraging computer vision techniques for improved navigation and decision-making in autonomous vehicles, contributing to the development of safer self-driving systems.\n\nFuture Work\n\nFuture developments might concentrate on improving the system's accuracy in dynamic settings, such tracking and identifying moving cars in various weather conditions. Furthermore, incorporating more sophisticated object detection models, such as multi-camera or multi-sensor fusion, may improve performance in difficult situations. Additionally, we intend to investigate environmental-condition-based adaptive warning methods, such as modifying proximity criteria for various object categories. Lastly, using predictive analytics with machine learning-based models may enhance long-term navigation and decision-making techniques.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nObjective\n\nIntroduction\n\nCamera model and calibration\n\nObject detection and tracking\n\nLane detection\n\nDepth estimation\n\nMethodology\n\nData Collection\n\nObject Detection\n\nView all\nCode",
    "awards": [
      "most promising innovation"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "vrdocr-multihead-long-text-recognition-in-low-quality-images-s3TebOv2I3iT",
    "username": "k@khadija.irfan",
    "license": null,
    "title": "vRD-OCR: Multi-Head Long Text Recognition in Low Quality Images",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n64 reads\nWinner of\nMost Promising Innovation\nat the\nComputer Vision Projects Expo 2024\nvRD-OCR: Multi-Head Long Text Recognition in Low Quality Images\nK\n@khadija.irfan\nO\n@osama.amjad\nA\n@amur.saqib\nLike\nBookmark\nShare\nvRD-OCR Multi-Head Long Text Recognition in Low Quality Images\n\n\nAbstract\n\nAlthough state-of-the-art Optical Character Recognition (OCR) systems are highly accurate on most usecases, their performance depends on certain key factors; image quality, clarity of the text, and most importantly length of the text and/or image width. Most sophisticated models are unreliable when tested on highly compressed images with faded text containing more than 50 characters, and are unsuitable for accuracy-sensitive tasks such as text-based forgery detection. In this work, we present vrdOCR, a text recognition model which specializes on long sentences and achieves a character-level recognition accuracy of  on a custom dataset containing synthesized and real images of A4-width i.e. over 2000 pixels wide and more than 100 characters per image. Our model outperforms PP-LCNetV4 by  after finetuning on the same dataset and addresses edge cases much better than the current state-of-the-art.\n\nIntroduction\n\nOCR’s detection capabilities go beyond basic text recognition. Modern OCR systems can analyze the positioning, formatting, and spacing of characters to uncover irregularities that would be challenging for a human inspector to spot. For example, an altered invoice might replace a period with a comma, subtly changing an amount due, or adjust character spacing to insert unauthorized text. OCR systems, especially those enhanced with AI-driven anomaly detection, are trained to identify such inconsistencies. By flagging even minute variations in text and punctuation, OCR allows for a level of scrutiny that ensures minor alterations, which could have major implications, are not missed. This precision is crucial in high-stakes fields like finance, law, and government, where document integrity is paramount.\n\nIn addition to accuracy, OCR’s processing power and scalability set it apart from human inspection. While a person might carefully examine a few documents, an AI-enhanced OCR engine like vRD-OCR can process thousands of pages in a fraction of the time, maintaining a consistent level of precision and thoroughness.\n\nMethodology\n\nThis section outlines the methodology implemented in developing vRD-OCR, a robust framework for optical character recognition (OCR). The pipeline addresses challenges such as noisy backgrounds, diverse fonts, and varying document qualities by leveraging advanced feature extraction and a dual-head decoding architecture. The components of the pipeline include preprocessing, training with multi-loss mechanisms, and postprocessing.\n\nPreprocessing\n\nPreprocessing ensures that the input images are normalized and resized for consistent feature extraction. Specifically, the following steps are performed:\n\nNormalization: The input images are normalized with a mean and standard deviation of (0.5, 0.5, 0.5) for each channel.\n\nResizing: Images are resized to a fixed height (H=64) and width (W=640) to maintain a consistent aspect ratio suitable for OCR.\n\nThese steps prepare the data for efficient feature extraction and training, ensuring that the model remains robust to variations in input.\n\nTraining\n\nvrdOCR is trained on a single NVIDIA GeForce RTX 3060 Ti system running \\textbf{Ubuntu 20.04.6} on an 11th Gen Intel i5-11400F (12) @ 4.400GHz processor. We use Python 3.8.19 and PyTorch 2.3.1 for our experiments. The training process employs a multi-loss strategy to optimize the performance of the dual decoding heads: Connectionist Temporal Classification (CTC) and NRTR. This design ensures that the strengths of both mechanisms are utilized, providing a balance between sequence alignment and context modeling.\n\nThe following table summarizes the key hyperparameters used during the training process:\n\nHyperparameter 1\tValue\nLearning Rate\t1 x \nBatch Size\t4\nMaximum Characters\t150\nLoss Functions\n\nThe vRD-OCR framework implements a multi-loss approach: CTC loss and NRTR loss. Each loss is weighted to optimize different aspects of the OCR task. The total loss in the multi-loss framework is computed as:\n\n and  are the weighting factors for the CTC and NRTR losses, respectively. The multi-loss module dynamically combines the outputs of these loss functions, enabling the system to balance alignment-centric and context-aware learning. This integration ensures robust performance across a wide range of text styles, formats, and noise levels.\n\nCTC Loss\n\nConnectionist Temporal Classification (CTC) loss is utilized for unsegmented sequence labeling, which is essential in OCR tasks where explicit alignment between input and output sequences is unavailable. The CTC loss function aims to find the most probable alignment between the predicted sequence and the ground truth sequence .\n\nThe CTC loss is defined as:\n\nwhere  is the probability of the alignment  of the predicted sequence  to the target sequence , and  is the set of all possible alignments. The log-softmax operation ensures the predictions are normalized, and the sum over all possible alignments accounts for the uncertainty in character positioning.\n\nAdditionally, an optional focal loss mechanism is integrated into the CTC loss to prioritize harder-to-classify samples, especially when the model encounters ambiguous or difficult text. The weighted focal loss is given by:\n\nwhere  is the predicted probability of the correct class, and  is the focusing parameter that controls the emphasis on harder examples.\n\nNRTR Loss\n\nThe NRTR (Non-Recurrent Transformer) loss is designed for the transformer branch of the network. It employs a cross-entropy loss function to calculate the difference between predicted logits (\\mathbf{p}) and the ground truth labels (\\mathbf{t}). The NRTR loss with label smoothing is defined as:\n\nwhere  represents the predicted probability for class , and  is the smoothed target label. Label smoothing modifies the target distribution to reduce overfitting and improve generalization:\n\nwhere  is the smoothing factor and  is the number of classes.\n\nAdditionally, the NRTR loss function considers padding tokens by masking out the loss for zero entries in the target sequence using the following mask :\n\nBackbone: High-Resolution Network (HRNet32)\n\nThe backbone of the vRD-OCR framework is HRNet [2], a network that preserves high-resolution representations throughout its structure. Unlike conventional CNNs, HRNet retains high-resolution feature maps across all stages. Moreover it fuses multi-resolution representations to capture fine-grained details and global context. This design ensures that spatial information is preserved, which is essential for recognizing complex text layouts and long sentences.\n\nDecoding Heads\nCTC Head\n\nThe CTC [1] decoding head is optimized for variable-length sequence alignment, making it particularly effective for text recognition tasks. By predicting the most probable alignment between input features and the target text, the CTC head accommodates sequences of varying lengths, ensuring robust performance across diverse document layouts.\n\nNRTR Head\n\nThe NRTR [4] decoding head leverages a self-attention mechanism to capture long-range dependencies in the text. Its architecture includes a modality-transform block to convert 2D visual features into 1D sequences. It also employs a stacked self-attention layers for parallel processing of text sequences. This design enables the NRTR head to process complex textual structures efficiently, achieving faster training and competitive recognition performance.\n\nPostprocessing\n\nPostprocessing involves decoding the outputs of the CTC head to generate the final recognized text. The decoding process maps the predicted sequences to readable text using dynamic programming.\n\nBy combining preprocessing, advanced training techniques, and efficient postprocessing, the vRD-OCR framework delivers state-of-the-art performance in optical character recognition, setting a strong foundation for future advancements in the field.\n\nDataset\n\nFirst, a generalized recognition network is trained on synthetic data, created using the open-source tool TextRecognitionDataGenerator [5]. We generate a total of 500,000 images containing random strings from Wikipedia, 400,000 of which are used for training and 100,000 are used for validation. The dataset also consists of images that are randomly skewed and distorted, occurring with probabilities of 0.3 and 0.4, respectively.\n\nThe final network is finetuned on  synthesized and  real images, and evaluated on  synthesized and  real images. These images are of varying quality and present their own unique and production-specific set of challenges in recognition.\n\nResults\n\nWe conducted extensive experimentation using multiple backbone architectures to evaluate their effectiveness in the vRD-OCR model. Each backbone was trained and validated on the same dataset to ensure a fair comparison. The results, presented in Table below, highlight the significant impact of backbone selection on model accuracy. While PP-LCNetv4 and vRD-OCR with Resnet50 backbone provided reasonable performance, HRNet32 backbone emerged as the most effective backbone, achieving an impressive accuracy of . This demonstrates the importance of high-resolution representations in extracting fine-grained details for text recognition tasks.\n\nModel\tBackbone\tAccuracy\nPP-LCNetV4\tResNet50\t63.20%\nvRD-OCR\tResNet50\t85.09%\nvRD-OCR\tHRNet\t97.36%\nDiscussion\n\nvRD-OCR surpasses several pre-existing OCR technologies by achieving higher accuracy in recognizing long sentences, even in complex layouts or noisy environments. This capability is a result of robust feature extraction architectures, and flexible decoding mechanisms. The combined use of CTC and NRTR losses, provides a scalable solution for OCR. By employing this multihead comprehensive loss strategy, the vRD-OCR model achieves superior performance in recognizing long sentences and complex text layouts, addressing challenges posed by noisy data and document variability. The Connectionist Temporal Classification (CTC) layer allows for precise sequence decoding without explicit character alignment. This feature is particularly advantageous for processing documents with irregular character spacing or variable-length sentences.\n\nThe model's accuracy and robustness rely heavily on the availability of large, diverse training datasets. Insufficient or biased data can lead to reduced performance when encountering unseen text styles or formats. To address these limitations and build upon the system's strengths, one avenue that can be explored is Integration of Text Detectors. Incorporating a robust text detector component would allow for end-to-end processing, seamlessly identifying and recognizing text regions in a unified architecture.\n\nConclusion\n\nIn this work, we introduced vRD-OCR, a robust and efficient OCR framework designed to tackle the challenges of recognizing long and complex text in low-quality images. By leveraging state-of-the-art feature extraction backbones such as HRNet32 and employing a dual-head decoding mechanism with CTC and NRTR, the system demonstrated significant improvements in accuracy and adaptability. Extensive experimentation with various backbone architectures validated the effectiveness of our approach, with HRNet32 achieving an impressive accuracy of 97.36%.\n\nReferences\n\nGraves, Alex, Santiago Fern{'a}ndez, Faustino Gomez, and J{\"u}rgen Schmidhuber.\n\"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.\"\nProceedings of the 23rd international conference on Machine learning, 2006, pp. 369–376.\n\nSun, Ke, Bin Xiao, Dong Liu, and Jingdong Wang.\n\"Deep High-Resolution Representation Learning for Human Pose Estimation.\"\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.\"\nProceedings of the IEEE International Conference on Computer Vision (ICCV), December 2015.\n\nSheng, Fenfen, Zhineng Chen, and Bo Xu.\n\"NRTR: A No-Recurrence Sequence-to-Sequence Model for Scene Text Recognition.\"\n2019 International Conference on Document Analysis and Recognition (ICDAR), 2019, pp. 781–786. DOI: \n10.1109/ICDAR.2019.00130\n.\n\nBelval, Claude. \"TextRecognitionDataGenerator.\" 2018. Available at \nhttps://github.com/Belval/TextRecognitionDataGenerator\n. Accessed: 2024-12-30.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nvRD-OCR Multi-Head Long Text Recognition in Low Quality Images\n\nAbstract\n\nIntroduction\n\nMethodology\n\nPreprocessing\n\nTraining\n\nLoss Functions\n\nCTC Loss\n\nNRTR Loss\n\nBackbone: High-Resolution Network (HRNet32)\n\nView all\nComments\n\nNo comments yet!\n\nSign in or create an account to comment.\n\nDatasets\nYou might be interested\nOCR system for Vietnamese text recognition\nJul 15, 202513 reads\nDeep LearningNLP+2\nMulti-lingual OCR for Devanagari and English Language using CRNN architectures\nBest Overall Project\nDec 22, 2024196 reads\n#IAM-handwritingCNN+9\nFrom Pixels To JSON: A Vision-Language Approach to Document Understanding\nS\nJan 02, 202530 reads\nAI-Powered Framework for Automated, Objective Evaluation of Handwritten Responses Using OCR & NLP\nN\nFeb 10, 202536 reads\nAIAIFramework+11",
    "awards": [
      "most promising innovation",
      "best overall project"
    ],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "we-ask-ai-to-generate-images-from-a-podcasts-audio-KXvlYJgFryRJ",
    "username": "Sergio Solorzano",
    "license": "MIT License",
    "title": "We Ask AI To Generate Images From A Podcasts' Audio",
    "publication_description": "Back to publications\nDec 30, 2024\n●\n81 reads\n●\nMIT License\nWinner of\nMost Engaging Presentation\nat the\nComputer Vision Projects Expo 2024\nWe Ask AI To Generate Images From A Podcasts' Audio\nchatgpt\nonnxruntime\nstable_diffusion\nunity\nwhisper\nSergio Solorzano\nMaurizio Raffone\nLike\nBookmark\nShare\nAbstract\nWe Ask AI To Generate Images From Podcasts' Audio\nPrototype App Creates Contextual Images For Audio Podcasts\n\n\nMotivation For This Post\n\nWatching a sound wave or unrelated images for a podcast that is published in a platform that supports video and images can be dull.\n\nWe propose asking a trio of AI models, bundled in an app we call Talkomic, to generate images that that are closely tied to the audio content discussed in the podcast.\n\nThe team at \nRendergon Ltd\n responsible for the development of the \nTapgaze apps\n has worked on this prototype. We are new to this field, keen to tinker and learn ! The prototype has been tested on the Unity Editor and Windows 11 build. It also provides a good start as a proof of concept to test the ability of AI models to help audio media augment its reach.\n\nAudio-To-Text-To-Image\n\nThe app’s AI Models transcribe an audio file to text and generate contexual images closely tied to the transcribed text.\n\nVisit the \nGithub repo\n where full implementation steps into Unity are further described.\n\n\n\nMethodology\nHow The Talkomic App Works\n\n\nChain AI-Model WorkFlow\n\nWe chain the results of three AI models to generate the images for an audio podcast.\n\nWe initially got the project to run with Unity’s neural network \nSentis\n, currently on closed beta, and added some Onnxruntime to make it work. The project in this blog is exclusively based on Onnxruntime. Unity’s team is making great strides with Sentis and I look forward to future updates !\n\nIn our Unity project, we run two of the AI models locally and access a third AI model remotely via an API.\n\nWe bundle these models inside \nUnity3D\n.\n\n\nIn a Unity scene we loop the AI Models over each podcast audio section to generate the contextual images.\n\n\nThe AI Models\n\nWe use \nOnnx AI\n models format.\n\n1. Whisper AI Model\n\nThis model transcribes audio into text. Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. This is the smallest of the 5 Whisper models available with 39M parameters, a great candidate to run on a mobile device.\n\nWe convert the \ntransformer whisper-tiny AI model\n into \nOnnx format using Microsoft Olive\n. The Olive github repo has a \nuseful example\n with the configuration required to optimize this model using ONNXRuntime tools:\n\nWhisper requires pre-and-post processing steps for audio inputs / model outputs. It would have been challenging to do this using python libraries in Unity. Fortunately Onnx optimization runs \nWhisperProcessor\n inside the AI Model, removing the need to code these outside! You can learn more about this feature and more in \nMicrosoft’s Build Onnxruntime demo\n.\n\nWhisper configuration with Olive\n\nI can suggest these steps for a quick whisper configuration with Olive:\n\ncreate and activate python 3.10 environment\ngit clone\ncd \nOlive\npython -m pip install .\ncd ./examples/whisper\npython -m pip install -r requirements.txt\n\nWhisper workflow configuration (watch out with copy/paste formatting char errors):\n\ncd examples/whisper\nprepare the workflow config json: python prepare_whisper_configs.py –model_name openai/whisper-tiny.en\nRun config to optimize the model: python -m olive.workflows.run –config whisper_cpu_int8.json –setup\nOptimize the model: python -m olive.workflows.run –config whisper_cpu_int8.json\nTest a transcription: python test_transcription.py –config whisper_cpu_int8.json\nN.B. – To re-make the model you’d need to clean the cache and existing generated directories within the whisper directory\nTo export the onnx model, the best candidate for the hardware config requested is in the zip models/ whisper_cpu_int8.zip directory CandidateModels\\device-device\\BestCandidateModel_1\\model.onnx. Config settings at configurations.json and inference_config.json in this same directory.\n\nChunked Audio\n\nThe Whisper model is designed to work on audio samples of up to 30s in duration. Hence we chunk the podcast for each section in chunks of max 30 seconds but load these as a loop in Whisper-tiny for each podcast section.\n\n2. ChatGPT Model\n\nAs a large language model, and unlike Unet, Chatgpt can help us generate text that describes an image that is closely tied to the discussion in the podcast.\n\nRequests: Using \nAzure OpenAI’s REST API\n, we request with \nUnityWebRequest\n this description from ChatGPT 3.5-turbo AI model.\nOur prompt to ChatGPT which attempts to limit faces drawn for podcast participants: “Provide a concise answer of no more than {maxChatgptRequestedResponseWords} words: Describe an abstract image for this conversation. Do not use names of people or persons. Do not say in the description it is an image of a person. The center or protagonist of the image you describe is not a person. Do not explain the image you describe, only describe the image. This is the conversation:” <section_transcribed_audio_chunks>\n3. Stable Diffusion AI Model\n\nThese models are trained on 256-A 100 GPUs, ~150k hours, ~120M image-text pairs.\n\nGenerally speaking, these are models trained to denoise random Gaussian noise step by step to get to an image: the neural network is trained to predict the noise at step t on a corrupted image, and reverse the probability density function to t0 or denoised image. A drawback is that they can be slow.\n\n\nImage courtesy of this suggested blog \nvisually intuitive blog\n and I can also suggest this \nblog\n to learn more.\n\nImage courtesy of this blog.\n\nStable Diffusion Implementation\n\nWe clone \nthe stable diffusion v1.4 model from branch onnx\n in Huggingface:\nThis is a type of diffusion model called \nLatent Diffusion\n: The model is trained to generate latent (compressed) representations of the images, which is faster than using the actual (larger) pixel space\n\nGet the onnx models and unet weights .pb file:\n\nthe \nclone v1.4\n above\ngit \nclone v1.4 onnx branch\nor \nalso here\n\nThese are the main components in Latent Diffusion pipeline:\n\nText Tokenizer (get cliptokenizer.onnx \nhere\n): Your text prompt (of what to draw) is tokenized (into a list of tokens). The tokenizer used must match the one used by the Text Encoder model, in this case the CLIP model.\nText Encoder\n: An AI model trained independently of the stable diffusion model used. It learns to associate text with images.\nWe use it to encode text to embeddings at UNet inference: In this step we transform your tokenized text prompt into into a vector that is used to guide the image generation\nThis stable diffusion model uses the \nCLIP\n model (Contrastive Language–Image Pre-training)\nOpenAI claims CLIP efficiently learns visual concepts from natural language supervision.\nScheduler: The scheduling algorithm used to progressively add noise to the image during training. It helps compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\nU-Net Model: Stable diffusion is implemented on a U-Net architecture. The main concepts involved at Inference:\nA latent noisy image is created as a starting point. The U-Net model steps through; at each step the output, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. At step 0 we get the clear denoised image.\nWe create with the Text Encoder a set of (conditioned) embeddings for our text prompt (tokenized with the Text Tokenizer) and another unconditioned, meaning empty (random). We apply a guidance scale to lean closer towards what our conditioned embedding suggests rather than pure random unconditioned embedding.\nAutoencoder (VAE): Separately trained to the U-Net model. At training it converts an image into a low dimensional latent (compressed) representation (AI model input). At inference it converts denoised latents (AI model output) generated by the reverse diffusion process into images.\n\nThe images for this podcast were generated with the following text prompt input to the stable diffusion model: “Create an image cyberpunk style. ”\n\n4. Real-ESRGAN AI Model\n\nWe enhance the details of the 512×512 images generated by the stable diffusion AI model to crisper 2048×2048 resolutions.\n\nMy suggested steps for quick git installation (steps up to date in my forked \nrepo\n):\n\nClone the repo\nGet the models and move to project Real-ESRGAN/weights\nAs it’s customary, a python environment is helpful and worked for me on a conda python 3.10 env\ncd into project Real-ESRGAN/\npython -m pip install -r requirements.txt\npython setup.py develop\nSet the target resolution in main.py, i.e. load_weights RealESRGAN_x8.pth and model scale (model = RealESRGAN(device, scale=4)) where scale=2/4/8\nExecute: python main.py\nFast on linux running on cuda, slow on VBox running on cpu\nResults\n\nWatch the Trailer of the Talkomized Podcast\n\n\n\n\n\nWatch the Final Talkomized Podcast in Full !\n\n\n\n\n\nView and Download The Podcast AI Images\nHere\nSee AI Images in \nAugmented Reality with the Tapgaze App Bobile App Here\n\n\n\n\n\nESRGAN Enhanced example image\n\nAll enhanced images available to download \nhere\n.\n\nPre-processed Image:\n\n\nESRGAN Post-Processed Image:\n\nThank you and Acknowledgements\n\nSpecial thanks to Jason Gauci co-host at \nProgramming Throwdown podcast\n whose idea shared on this podcast served as inspiration for this prototype.\n\nI am thrilled and truly grateful to \nMaurizio Raffone at Tech Shift F9 Podcast\n for trusting me to run a proof of concept of the Talkomic app prototype with the audio file of a fantastic episode in this podcast.\n\nWe thank \n@sd-akashic\n \n@Haoming02\n \n@Microsoft\n for helping to better understand onnxruntime implementation in Unity.\n\nWe thank \nai-forever\n for posting the models and \ngit repo\n.\n\nand \nyasirkula's Simple File Browser\n.\n\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAbstract\n\nWe Ask AI To Generate Images From Podcasts' Audio\n\nPrototype App Creates Contextual Images For Audio Podcasts\n\n\n\nMotivation For This Post\n\nAudio-To-Text-To-Image\n\n\nMethodology\n\nHow The Talkomic App Works\n\n\n\nChain AI-Model WorkFlow\n\n\nThe AI Models\n\nWhisper AI Model\nView all\nCode\nDatasets",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  },
  {
    "id": "wordpress-n8n-pipeline-ai-pulizia-llm-traduzione-e-tts-YWzdKERfhdvx",
    "username": "p@paolo.ronco2000",
    "license": "MIT License",
    "title": "WordPress + n8n, pipeline AI: pulizia LLM, traduzione e TTS",
    "publication_description": "Back to publications\nAug 22, 2025\n●\n424 reads\n●\nMIT License\nWordPress + n8n, pipeline AI: pulizia LLM, traduzione e TTS\nautomations\ngcp\ngoogle cloud platform\nn8n\ntext-to-speech\ntts\nwordpress\nworkflows\nP\n@paolo.ronco2000\nLike\nBookmark\nShare\nAI Pipeline for WordPress on n8n: Text Normalization, Translation, and TTS\n\nKeywords: WordPress, n8n, LLM text normalization, machine translation, speech synthesis, Google Cloud Run, reproducibility\n\nAbstract\n\nI present a reproducible, self‑hosted pipeline that converts WordPress articles into multilingual voiceovers (Italian/English). The system orchestrates LLM‑based text cleaning, machine translation, and neural text‑to‑speech (TTS) via n8n. A lightweight microservice on Google Cloud Run performs long‑form TTS and writes audio artifacts (.wav) to Google Cloud Storage (GCS). Metadata and processing state are tracked through Google Sheets, enabling deterministic batch runs and idempotent processing. We describe the architecture, interfaces, and evaluation protocol (latency, throughput, and audio integrity checks) to facilitate reuse and extension.\n\n1. Problem Statement\n\nBlogs often publish long‑form content without an accessible audio counterpart. Manual conversion to voice is time‑consuming and error‑prone (HTML artifacts, punctuation issues, malformed text). The goal is to automate the post → clean text → translation → TTS → publish loop with traceability and repeatability, while keeping infrastructure self‑hosted for control and cost predictability.\n\n2. System Overview\n\nThe solution is composed of three subsystems:\n\nOrchestration (n8n)\nScheduled ingestion of WordPress posts.\nText normalization with an LLM (“Clean” nodes per language).\nOptional translation to EN.\nHTTP requests to the TTS microservice.\nUpdate of two WordPress pages (IT/EN) with an <audio> player block.\nStatus tracking in Google Sheets (work queue).\nTTS Microservice (Cloud Run)\nFastAPI endpoint /long-tts receiving {text, language, voice, filename}.\nUses Google Cloud Text‑to‑Speech to synthesize .wav files.\nStreams output to a GCS bucket and returns artifact metadata.\nArtifacts & State\nAudio: gs://<bucket>/<ID>.wav and gs://<bucket>/<ID>EN.wav.\nIndex: Google Sheets with columns ID, Title, Date, Content, Link, Status.\nPresentation: WordPress pages IT/EN updated with an audio player snippet.\n\nWorkflow JSONs are provided in /n8n-workflows. Replace every INSERT_YOUR_ID_HERE with your instance‑specific IDs before running.\n\n3. Data Flow\n\nIngest (Part 1) → fetch posts from WordPress → strip HTML → append/update rows in Google Sheets.Process (Part 2) → pick next row with empty Status → clean text (LLM) → translate to EN → call /long-tts twice (IT/EN) → update WordPress pages → set Status=done.\n\n\n\n4. Methods\n4.1 Text Normalization (LLM)\nObjective: remove markup and special tokens that degrade TTS (e.g., HTML entities, escaped quotes, code fragments).\nApproach: two dedicated “Clean” steps (IT/EN) to avoid cross‑language artifacts; simple post‑processing (newline collapse).\n4.2 Machine Translation\nObjective: generate a high‑fidelity English version for bilingual synthesis.\nApproach: Google Translate node in n8n; downstream clean step to stabilize punctuation/spacing.\n4.3 Neural TTS\nObjective: synthesize natural speech from normalized text.\nApproach: Google Cloud TTS (e.g., it-IT-Neural2-F and en-US-Neural2-D), invoked via Cloud Run microservice to support long texts, centralized logging, and uniform storage in GCS.\nOutput: uncompressed or PCM‑linear .wav files named by post ID (plus EN suffix for English).\n4.4 Publication\nObjective: consistent exposure of artifacts to end users.\nApproach: WordPress REST updates a dedicated IT page and an EN page; an <audio> block references GCS URLs. Pages act as an index/catalog of voiceovers distinct from the original posts.\n5. Implementation Details\n5.1 Orchestration (n8n)\nTriggers: nightly schedule.\nQueueing: Google Sheets serves as a work queue with Status acting as a completion flag.\nIdempotency: the pipeline selects a single pending row (oldest by Date), ensuring one‑by‑one processing without duplicates.\nError Handling: if a step fails, the row remains pending; you can re‑run Part 2 safely.\n5.2 Cloud Run Service\n\n/GCP Code contains:\n\nmain.py: FastAPI app exposing /long-tts.\nrequirements.txt: Python dependencies.\nDockerfile.txt: image build.\n\nDeploy:\n\ngcloud auth login\ngcloud config set project YOUR_PROJECT_ID\ngcloud builds submit --tag gcr.io/YOUR_PROJECT_ID/long-tts\ngcloud run deploy long-tts \\\n  --image gcr.io/YOUR_PROJECT_ID/long-tts \\\n  --region europe-west1 \\\n  --platform managed \\\n  --allow-unauthenticated\n\n\nExample request:\n\ncurl -X POST \"https://<service>.run.app/long-tts\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"text\": \"Hello World\",\n        \"language\": \"en-US\",\n        \"voice\": \"en-US-Neural2-D\",\n        \"filename\": \"sample.wav\"\n      }'\n\n5.3 WordPress Integration\nTwo page IDs (IT/EN) are updated by the workflow.\nThe audio player uses <audio><source src=\"https://storage.googleapis.com/<bucket>/<ID>.wav\" type=\"audio/mpeg\"></audio>; the EN version appends EN.wav.\n6. Evaluation Protocol\n\nWe propose a lightweight, reproducible protocol. Record results per batch in a CSV or Sheet.\n\nLatency (ms/char): measure end‑to‑end synthesis time divided by input character count.\nThroughput (posts/hour): number of posts fully processed in a 60‑minute window.\nAudio Integrity: detect silent output or truncated files by checking duration vs. expected duration (given TTS speaking rate).\nText Cleanliness Score: pre/post token ratio (e.g., removed HTML entities, code spans) as a proxy for normalization quality.\nPublication Success Rate: ratio of successful WordPress page updates vs. attempts.\n\nNo human MOS is reported here; if you need perceptual quality, add a small listening test protocol with 5‑point MOS and inter‑rater agreement.\n\n7. Reproducibility\nDeterministic scheduling: nightly cron with a single‑row pick policy.\nPinned artifacts: TTS voices (it-IT-Neural2-F, en-US-Neural2-D) fixed for runs.\nVersioning: keep a changelog of node versions and main.py image digest (from Cloud Run).\nConfig surface: all instance‑specific IDs are explicit in the JSON as INSERT_YOUR_ID_HERE—replace them but do not change field names.\n8. Limitations\nReliance on external services (Translate/TTS) may introduce latency spikes or regional availability constraints.\nTTS prosody depends on input punctuation; text normalization reduces but cannot fully remove artifacts.\nThe catalog approach (IT/EN pages) centralizes voiceovers, not embedding per‑post players (by design).\n9. Ethical & Operational Considerations\nCopyright & consent: ensure you are authorized to transform and redistribute post content as audio.\nAttribution: link back to the original article in the audio block.\nPrivacy: avoid synthesizing PII; clean steps should strip emails/API keys/code fragments from the TTS input.\nAccessibility: provide transcripts when feasible for hearing‑impaired users.\n10. How to Use (Quick Start)\nDeploy the Cloud Run service in /GCP Code.\nCreate the Google Sheet (ID, Title, Date, Content, Link, Status).\nImport /n8n-workflows/WordPress_Automations__Part1.json and Part2.json into n8n.\nReplace all INSERT_YOUR_ID_HERE with your WordPress page IDs, Google Sheet IDs, and credential IDs.\nMap credentials (WordPress, Google, OpenAI if you keep LLM cleaning).\nRun Part 1 to populate the Sheet; Run Part 2 to synthesize and publish.\nEvaluate using the protocol in §6 and iterate.\n11. Future Work\nAdd exponential backoff/retry on TTS HTTP calls.\nSupport additional languages/voices; auto‑select voice based on locale.\nOptional per‑post embedding (update the article body instead of a catalog page).\nIntroduce signed URLs for private buckets.\nArtifact Locations\nWorkflows: /n8n-workflows/WordPress_Automations__Part1.json, /n8n-workflows/WordPress_Automations__Part2.json\nService code: /GCP Code\nDiagrams: /images/WordPress_Automations__Part1.png, /images/WordPress_Automations__Part2.png\nYour publication could be next!\n\nJoin us today and publish for free\n\nSign Up for free!\nTable of contents\n\nAI Pipeline for WordPress on n8n: Text Normalization, Translation, and TTS\n\nAbstract\n\nProblem Statement\nSystem Overview\nData Flow\nMethods\n\n4.1 Text Normalization (LLM)\n\n4.2 Machine Translation\n\n4.3 Neural TTS\n\n4.4 Publication\n\nView all\nComments\n(5)\nCode\nYou might be interested\nGCP TTS on WordPress | WordPress to Voice\nP\nJun 18, 202525 reads\nAI VoiceAPI+11\nAudio To Answer Generator\nY\nSep 01, 202516 reads\nagenticai+3\nAudio Transcriber AI\nM\nMar 31, 202518 reads\nSpeechFlow: Modular real-time audio transcription and conversational AI.\nBest AI Tool Innovation\nM\nFeb 26, 202550 reads\nAIAudio Transcription+8",
    "awards": [],
    "elements": {
      "div.awards": [],
      "span.badge": [],
      "div[class*='award']": [],
      "li[class*='award']": []
    }
  }
]